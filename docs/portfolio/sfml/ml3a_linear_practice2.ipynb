{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Линейная регрессия Практика 2\n\n### <b><span style='color:#686dec'>Сравнение подходов</span></b>\n\n- Сравним матричные операции с градиентном спуске\n- Посмотрим как используя линейные модели моделировать нелинейную взаимосвязь\n\n### Импортируем модули"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error, f1_score, accuracy_score, roc_curve, roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\nfrom matplotlib import pyplot as plt",
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Когда использовать матричные операции \n\nвместо градиентного впуска "
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "def print_regression_metrics(y_true, y_pred):\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    print(f'MSE = {mse:.2f}, RMSE = {rmse:.2f}')\n    \ndef prepare_boston_data():\n    data = load_boston()\n    X, y = data['data'], data['target']\n    # Нормализовать даннные с помощью стандартной нормализации\n    X = (X - X.mean(axis=0)) / X.std(axis=0)\n    # Добавить фиктивный столбец единиц (bias линейной модели)\n    X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])\n    \n    return X, y",
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Прежде чем начать, обернем написанную нами \n- линейную регрессию **методом матричны операций** в класс:"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "class LinRegAlgebra():\n    def __init__(self):\n        self.theta = None\n    \n    def fit(self, X, y):\n        self.theta = np.linalg.inv(X.T.dot(X)).dot(X.transpose()).dot(y)\n    \n    def predict(self, X):\n        return X.dot(self.theta)",
   "execution_count": 26,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Проведем замеры скорости работы алгоритмов на матричных операциях и на градиентном спуске. \n- Предварительно найдем параметры для метода, основанного на градиентном спуске, так, чтобы значения метрик максимально совпадало со значениями в случае первого алгоритма."
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "X, y = prepare_boston_data()",
   "execution_count": 27,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "linreg_alg = LinRegAlgebra()\nlinreg_alg.fit(X, y)\ny_pred = linreg_alg.predict(X)\n\n# Посчитать значение ошибок MSE и RMSE для тренировочных данных\nprint_regression_metrics(y, y_pred)",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "MSE = 21.89, RMSE = 4.68\n"
    }
   ]
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "class RegOptimizer():\n    def __init__(self, alpha, n_iters):\n        self.theta = None\n        self._alpha = alpha\n        self._n_iters = n_iters\n    \n    def gradient_step(self, theta, theta_grad):\n        return theta - self._alpha * theta_grad\n    \n    def grad_func(self, X, y, theta):\n        raise NotImplementedError()\n\n    def optimize(self, X, y, start_theta, n_iters):\n        theta = start_theta.copy()\n\n        for i in range(n_iters):\n            theta_grad = self.grad_func(X, y, theta)\n            theta = self.gradient_step(theta, theta_grad)\n\n        return theta\n    \n    def fit(self, X, y):\n        m = X.shape[1]\n        start_theta = np.ones(m)\n        self.theta = self.optimize(X, y, start_theta, self._n_iters)\n        \n    def predict(self, X):\n        raise NotImplementedError()",
   "execution_count": 29,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "class LinReg(RegOptimizer):\n\n    def grad_func(self, X, y, theta):\n        n = X.shape[0]\n        grad = 1. / n * X.T.dot(X.dot(theta) - y)\n        return grad\n    \n    def predict(self, X):\n        if self.theta is None:\n            raise Exception('You should train the model first')\n        \n        y_pred = X.dot(self.theta)\n        \n        return y_pred",
   "execution_count": 30,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "linreg_crit = LinReg(0.2,1000)\nlinreg_crit.fit(X, y)\ny_pred = linreg_crit.predict(X)\n\n# Посчитать значение ошибок MSE и RMSE для тренировочных данных\nprint_regression_metrics(y, y_pred)",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "MSE = 21.89, RMSE = 4.68\n"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Теперь измерим скорость выполнения"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "%timeit linreg_alg.fit(X, y)",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "165 µs ± 1.29 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
    }
   ]
  },
  {
   "metadata": {
    "scrolled": true,
    "trusted": false
   },
   "cell_type": "code",
   "source": "%timeit linreg_crit.fit(X, y)",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "15.4 ms ± 79.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
    }
   ]
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "linreg_crit.fit(X, y)",
   "execution_count": 34,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Когда какой метод использовать\n\nРеализация на матричных операциях опережает реализацию на градиентном спуске в сотни раз\n- Но всегда ли это так и какие подводные камни могут быть?\n- Ниже приведен набор случаев, при которых версия с градентным спуском предпочтительнее:\n\n1. Градиентный спуск работает быстрее в случае матриц с большим количеством признаков. Основная по сложности операция — нахождение обратной матрицы $(X^T X)^{-1}$.\n1. Нахождение обратной матрицы может также потребовать больше оперативной памяти\n1. Матричные операции могут также проигрывать и в случае небольших объемов данных, но при плохой параллельной реализации или недостаточных ресурсах.\n1. Градиентный спуск может быть усовершенствован до так называемого **стохастического градиентного спуска**, (данные для оптимизации подгружаются небольшими наборами), что уменьшает требования по памяти.\n1. В некоторых случаях (например, в случае линейно-зависимых строк) алгебраический способ решения не будет работать совсем в виду невозможности найти обратную матрицу."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### <b><span style='color:#686dec'>Превращение линейной модели в нелинейную</span></b>\n\n- Нелинейные зависимости в данных встречаются намного чаще линейных\n- На самом деле простейшая линейная регрессия способна обнаруживать нелинейные зависимости\n- Для этого необходимо рассмотреть дополнительные признаки, полученные из исходных применением различных нелинейных функций \n- Возьмем уже знакомый датасет с ценами на квартиры в Бостоне и последовательно станем применять различные функции к исходным признакам:"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# Boston Data. Attribute Information (in order):\n#     - CRIM     per capita crime rate by town\n#     - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n#     - INDUS    proportion of non-retail business acres per town\n#     - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n#     - NOX      nitric oxides concentration (parts per 10 million)\n#     - RM       average number of rooms per dwelling\n#     - AGE      proportion of owner-occupied units built prior to 1940\n#     - DIS      weighted distances to five Boston employment centres\n#     - RAD      index of accessibility to radial highways\n#     - TAX      full-value property-tax rate per `$10000`\n#     - PTRATIO  pupil-teacher ratio by town\n#     - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n#     - LSTAT    % lower status of the population\n#     - MEDV     Median value of owner-occupied homes in $1000's",
   "execution_count": 35,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "def prepare_boston_data_new():\n    data = load_boston()\n    X, y = data['data'], data['target']\n    \n    X = np.hstack([X, np.sqrt(X[:, 5:6]), X[:, 6:7] ** 3])\n    \n    # Нормализовать даннные с помощью стандартной нормализации\n    X = (X - X.mean(axis=0)) / X.std(axis=0)\n    # Добавить фиктивный столбец единиц (bias линейной модели)\n    X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])\n    \n    return X, y",
   "execution_count": 36,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Создадим несколько нелинейных признаков\n\nМы добавили два новых признака:\n1. Взяли корень из признака RM (среднее число комнат на сожителя)\n1. Возвели в куб значения признака AGE\n\nЭто только два примера. Всевозможных комбинаций признаков и примененных к ним функций неограниченное количество."
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "def train_validate(X, y):\n    # Разбить данные на train/valid\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=1)\n\n    # Создать и обучить линейную регрессию\n    linreg_alg = LinRegAlgebra()\n    linreg_alg.fit(X_train, y_train)\n\n    # Сделать предсказания по валидционной выборке\n    y_pred = linreg_alg.predict(X_valid)\n\n    # Посчитать значение ошибок MSE и RMSE для валидационных данных\n    print_regression_metrics(y_valid, y_pred)",
   "execution_count": 37,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# Подготовить данные без модификации признаков\nX, y = prepare_boston_data()\n# Провести эксперимент\ntrain_validate(X, y)",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "MSE = 23.38, RMSE = 4.84\n"
    }
   ]
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# Подготовить данные без модификации признаков\nX, y = prepare_boston_data_new()\n# Провести эксперимент\ntrain_validate(X, y)",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "MSE = 14.28, RMSE = 3.78\n"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Как видно из результатов, мы добились улучшения точности предсказаний на 40%, всего лишь добавив пару нелинейных признаков в имеющимся. Можете поиграть с признаками и еще улучшить результат."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### <b><span style='color:#686dec'>Задание</span></b>\n\n#### <b>Задание <span style='color:#686dec'>3.7.1</span></b> \n\n- Сделайте для градиентного спуска остановку алгоритма, сли максимальное из абсолютных значений компонент градиента становится меньше 0.01. \n- Сравните скорость обучения градиентным спуском и матричными операциями\n- Для градиентного спуска установите alpha = 0.2. \n- На какой итерации останавливается градиентный спуск?\n"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# Функция для гдариентного спуска\nclass RegOptimizer():\n    \n    def __init__(self, alpha, n_iters, limiter=None):\n        self.theta = None\n        self._alpha = alpha\n        self._n_iters = n_iters\n        self._limiter = limiter\n    \n    def gradient_step(self, theta, theta_grad):\n        return theta - self._alpha * theta_grad\n    \n    def grad_func(self, X, y, theta):\n        raise NotImplementedError()\n\n    def optimize(self, X, y, start_theta, n_iters):\n\n        theta = start_theta.copy()\n        for i in range(n_iters):\n            theta_grad = self.grad_func(X, y, theta)\n            theta = self.gradient_step(theta, theta_grad)\n            if(self._limiter != None):\n                if(max(theta_grad)<self._limiter):\n                    print(f'max theta_grad reached: {i}')\n                    break\n        \n        return theta\n    \n    def fit(self, X, y):\n        m = X.shape[1]\n        start_theta = np.ones(m)\n        self.theta = self.optimize(X, y, start_theta, self._n_iters)\n        \n    def predict(self, X):\n        raise NotImplementedError()\n\n# Линейная регрессия используя градиентный спуск\nclass LinReg(RegOptimizer):\n    \n    def grad_func(self, X, y, theta):\n        n = X.shape[0]\n        grad = 1. / n * X.transpose().dot(X.dot(theta) - y)\n        return grad\n    \n    def predict(self, X):\n        if self.theta is None:\n            raise Exception('You should train the model first')\n        \n        y_pred = X.dot(self.theta)\n        \n        return y_pred",
   "execution_count": 40,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# Для задания 3.7.1 \ndef train_validate_limiter(X, y, limiter=0.01):\n    \n    # Разбить данные на train/valid\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                          test_size=0.2, \n                                                          shuffle=True, \n                                                          random_state=1)\n\n    # Создать и обучить линейную регрессию\n    linreg_alg = LinReg(0.2,1000,limiter)\n    linreg_alg.fit(X_train, y_train)\n\n    # Сделать предсказания по валидционной выборке\n    y_pred = linreg_alg.predict(X_valid)\n\n    # Посчитать значение ошибок MSE и RMSE для валидационных данных\n    print_regression_metrics(y_valid, y_pred)\n\nX1,y1 = prepare_boston_data()\ntrain_validate_limiter(X1,y1,0.01)",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "max theta_grad reached: 217\nMSE = 23.40, RMSE = 4.84\n"
    }
   ]
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "data = load_boston()\ndata.feature_names\ndf = pd.DataFrame(data.data,columns=data.feature_names)\ndf.head()",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0.0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1.0</td>\n      <td>296.0</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n\n   PTRATIO       B  LSTAT  \n0     15.3  396.90   4.98  \n1     17.8  396.90   9.14  \n2     17.8  392.83   4.03  \n3     18.7  394.63   2.94  \n4     18.7  396.90   5.33  "
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "list(df.columns).index('DIS')",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "7"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### <b>Задание <span style='color:#686dec'>3.7.2</span></b> \n\n- Добавьте к признакам нелинейной модели квадрат признака <code>DIS</code> и переобучите модель\n- Какой получился RMSE? \n- Подсказка: используйте написанную нами линейную регрессию методом матричных операций.\n"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# Предобработка данных \n\ndef prepare_boston_data_new():\n    \n    data = load_boston()\n    X, y = data['data'], data['target']\n    \n    # Добавяем нелинейные признаки\n    X = np.hstack([X, \n                   np.sqrt(X[:, 5:6]), \n                   X[:, 6:7] ** 3])\n    \n    # Нормализовать даннные с помощью \n    # стандартной нормализации\n    model = StandardScaler()\n    X = model.fit_transform(X)\n    \n    # Добавить фиктивный столбец единиц \n    # (bias линейной модели)\n    X = np.hstack([np.ones(X.shape[0])[:, None], X])\n    \n    return X, y\n\n# Разбтваем выборку на train,test\n# Тренируем линейную регрессию (на основе матричных умнажении)\n\ndef train_validate_algebra(X, y):\n    \n    # Разбить данные на train/valid\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                          test_size=0.2, \n                                                          shuffle=True, \n                                                          random_state=1)\n\n    # Создать и обучить линейную регрессию\n    linreg_alg = LinRegAlgebra()\n    linreg_alg.fit(X_train, y_train)\n\n    # Сделать предсказания по валидционной выборке\n    y_pred = linreg_alg.predict(X_valid)\n\n    # Посчитать значение ошибок MSE и RMSE для валидационных данных\n    print_regression_metrics(y_valid, y_pred)\n    \n# Разбтваем выборку на train,test\n# Тренируем линейную регрессию (градиентный спуск)\n    \ndef train_validate(X, y):\n    \n    # Разбить данные на train/valid\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2,shuffle=True,random_state=1)\n\n    # Создать и обучить линейную регрессию\n    linreg_alg = LinReg(0.2,1000)\n    linreg_alg.fit(X_train, y_train)\n\n    # Сделать предсказания по валидционной выборке\n    y_pred = linreg_alg.predict(X_valid)\n\n# Посчитать значение ошибок MSE и RMSE для валидационных данных\n    print_regression_metrics(y_valid, y_pred)",
   "execution_count": 44,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "from sklearn.preprocessing import StandardScaler\n\n# Предобработка данных \ndef prepare_boston_data_new_mod():\n    \n    data = load_boston()\n    X, y = data['data'], data['target']\n    \n    # Добавяем нелинейные признаки\n    X = np.hstack([X, \n                   np.sqrt(X[:, 5:6]), \n                   X[:, 6:7] ** 3,\n                   X[:, 7:8] ** 2])\n                   \n# Нормализовать даннные с помощью \n    # стандартной нормализации\n    model = StandardScaler()\n    X = model.fit_transform(X)\n    \n    # Добавить фиктивный столбец единиц \n    # (bias линейной модели)\n    X = np.hstack([np.ones(X.shape[0])[:, None], X])\n    \n    return X, y\n\nX,y = prepare_boston_data_new_mod()\ntrain_validate_algebra(X,y)\n\n# 3.69",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "MSE = 13.59, RMSE = 3.69\n"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### <b>Задание <span style='color:#686dec'>3.7.3</span></b> \n\nУберите нормализацию и оставьте добавленные признаки на основе <code>RM</code> и <code>AGE</code>, Какой получился RMSE?\n"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# Предобработка данных \ndef prepare_boston_data_new_mod():\n    \n    data = load_boston()\n    X, y = data['data'], data['target']\n    \n    # Добавяем нелинейные признаки\n    X = np.hstack([X, \n                   np.sqrt(X[:, 5:6]), \n                   X[:, 6:7] ** 3])\n                   \n    # Нормализовать даннные с помощью \n    # стандартной нормализации\n#     model = StandardScaler()\n#     X = model.fit_transform(X)\n    \n    # Добавить фиктивный столбец единиц \n    # (bias линейной модели)\n    X = np.hstack([np.ones(X.shape[0])[:, None], X])\n    \n    return X, y\n\nX,y = prepare_boston_data_new_mod()\ntrain_validate_algebra(X,y)\n",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "MSE = 14.28, RMSE = 3.78\n"
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}