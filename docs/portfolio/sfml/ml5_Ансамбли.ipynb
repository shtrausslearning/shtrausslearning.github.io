{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Ансамбли Практика\n\nBagging — это параллельный способ построения ансамбля. Коротко о способе построения:\n\n- Обучающая выборка сэмплируется  раз с помощью bootstrap (выбор с возвращением).\n- На каждом сэмпле обучается отдельная базовая модель.\n- Ответы моделей усредняются (возможно, с весами)\n\n### <b><span style='color:#686dec'>Задача Ухода Работника</span></b>\n\n- Разберёмся с `ансамблями` алгоритмов и со `случайным лесом`\n- Рассмотрим данные о сотрудниках компании, где указывается, ушёл сотрудник или нет, задача бинарной классификации"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### <b><span style='color:#686dec'>Концепции</span></b>\n\n\nМетод Бутстрэп (или `Bootstrap`) — это статистический метод, который используется для оценки распределения выборочных статистик, таких как среднее, медиана, стандартное отклонение и другие, путем повторного выборки из имеющихся данных с возвращением. В контексте машинного обучения этот метод часто применяется для оценки надежности моделей и уменьшения переобучения.\n\nВот основные шаги метода Бутстрэп:\n\n1. Создание бутстрэп-выборок: Из исходного набора данных (размером N) многократно (обычно несколько сотен или тысяч раз) создаются новые выборки того же размера N путем случайного выбора объектов с возвращением. Это значит, что один и тот же объект может быть выбран несколько раз в одной выборке.\n\n2. Обучение модели: Для каждой бутстрэп-выборки обучается модель машинного обучения.\n\n3. Оценка производительности: После обучения модели на каждой бутстрэп-выборке производится оценка ее производительности на оставшихся данных (тех, которые не были выбраны в данной выборке). Это позволяет получить множество оценок производительности модели.\n\n4. Агрегация результатов: Полученные оценки производительности можно усреднить или использовать для построения доверительных интервалов, что позволяет лучше понять стабильность и надежность модели.\n\nМетод Бутстрэп особенно полезен, когда размер исходной выборки мал, так как он позволяет создать множество вариантов данных для обучения и тестирования модели. Он также используется в ансамблевых методах, таких как Random Forest, где каждое дерево обучается на различных бутстрэп-выборках."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Бэггинг (от англ. \"`Bootstrap Aggregating`\") — это метод ансамблевого обучения, который используется для повышения точности и стабильности моделей машинного обучения. Он особенно эффективен для уменьшения вариативности и предотвращения переобучения.\n\nОсновные шаги бэггинга:\n\n1. Создание подвыборок: Из исходного набора данных создаются несколько подвыборок с использованием метода бутстрэп — то есть случайным образом выбираются образцы с возвращением. Это означает, что один и тот же объект может быть выбран несколько раз, а некоторые объекты могут быть не выбраны вовсе.\n\n2. Обучение моделей: Для каждой из созданных подвыборок обучается отдельная модель. Это могут быть как однотипные модели (например, несколько деревьев решений), так и разные модели.\n\n3. Агрегация результатов: После того как все модели обучены, их предсказания объединяются. Для регрессии обычно используется среднее значение предсказаний, а для классификации — голосование (то есть класс, который получил наибольшее количество голосов, считается итоговым).\n\nБэггинг помогает улучшить обобщающую способность модели, так как снижает вероятность переобучения за счет усреднения результатов нескольких моделей. Один из наиболее известных алгоритмов, использующих бэггинг, — это Random Forest (Случайный лес), который строит множество деревьев решений и объединяет их предсказания."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`Random Subspaces` (RSS)\n\nДля построения набора различных моделей используется также метод выбора случайных подвыборок признаков Random Subspaces. Метод обеспечивает устойчивость алгоритма к набору доступных признаков."
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets\n\ndf = pd.read_csv('HR-dataset.csv')\n\nnp.random.seed(42)\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\ntarget = 'left'\nfeatures = df.columns.drop(target)\nfeatures = features.drop('empid')  # Удалим идентификатор пользователя как нерепрезентативный признак\nprint(features)\n\nX, y = df[features].copy(), df[target]",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Index(['satisfaction_level', 'last_evaluation', 'number_project',\n       'average_montly_hours', 'time_spend_company', 'Work_accident',\n       'promotion_last_5years', 'dept', 'salary'],\n      dtype='object')\n"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Предобработка\n\n`Частотное кодирование`\n- Заменим идентификатор отдела `dept`, к которому относился сотрудник, на количество людей в отделе\n\n`Ординальноге кодирование`\n\n- Зарплату `salary` — на ординальную категорию, используя `salary_ordinals`\n\n\n- **Масштабируем** признаки для последующего сравнения результатов с помошью `StandardScaler`"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# частотное кодирование\ndept_val_counts = X['dept'].value_counts()\nX['dept'] = X['dept'].map(dept_val_counts)\n\n# ординальное кодирование\nsalary_ordinals = {'low': 1, 'medium': 2, 'high': 3}\nX['salary'] = X['salary'].map(salary_ordinals)\n# X = X.copy()\n\n# ьаштабируем данные\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX = pd.DataFrame(data=scaler.fit_transform(X), \n                 columns=X.columns)",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Оценка Модели\n\nКак будем оценивать качество модели?\n\nВ дальнейшем будем оценивать качество модели\n- на **кросс-валидации** (cross validation) `cross_val_score`\n- на пяти фолдах при помощи **f1 score** (`f1 меры`)."
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "from sklearn.model_selection import cross_val_score\n\ndef estimate_accuracy(clf, X, y, cv=5,metric='f1'):\n    cv_mean = cross_val_score(clf, X, y, cv=5, scoring=metric).mean()\n    return round(cv_mean,3)",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n#### Бэггинг (Bagging)\n\n- Mетод построения `композиции алгоритмов`, в котором каждый алгоритм строится независимо от других `на подвыборках` обучающей выборки\n- `Итоговый алгоритм` принимает решения посредством `голосования` среди всех алгоритмов (возвращается самый частый ответ)\n- Обертование исходного класса (`BaggingClassifier(clf)`)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Посмотрим на точность одного дерева решении с максимальной глубиной 30 (`max_depth=30`)\n- Проведём `бэггинг` : для этого достаточно обернуть исходный классификатор в `BaggingClassifier`"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# Посмотрим на точность одного дерева.\ntree = DecisionTreeClassifier(max_depth=30)\nprint(\"Decision tree:\", estimate_accuracy(tree, X, y))\n\n# Проведём бэггинг: для этого достаточно обернуть исходный классификатор в BaggingClassifier.\nbagging_trees = BaggingClassifier(tree)\nprint(\"Decision tree bagging:\", estimate_accuracy(bagging_trees, X, y))\n\n#  Это явно улучшает результат не только беггинга но модель одного дерева",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Decision tree: 0.945\nDecision tree bagging: 0.975\n"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`Композиция` отдельных деревьев показывает себя лучше, чем одно дерево"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Приемущество Бэггинга\n\nСтруктура дерева серьёзно зависит от **обучающей выборки**\n- Это значит, что если немного изменить обучающую выборку, то **дерево сильно изменится**\n- Kомпозиция алгоритмов **при помощи голосования** работает наилучшим образом, когда модели различны\n- Увеличить **различность построенных деревьев** можно, указав параметры `max_features` и `max_depth`\n"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "mfeats = int(np.sqrt(len(features)))\nprint(f'Number of features: {mfeats}')\n\nrandom_tree = DecisionTreeClassifier(max_features=mfeats,\n                                     max_depth=30)\nprint(\"Random tree:\", estimate_accuracy(random_tree, X, y))\n\nbagging_random_trees = BaggingClassifier(random_tree)\nprint(\"Random tree bagging:\", estimate_accuracy(bagging_random_trees, X, y))",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Number of features: 3\nRandom tree: 0.954\nRandom tree bagging: 0.979\n"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Случайный Лес\n\nИменно так внутри и работает так называемый `случайный лес`\n\nOн обучает набор деревьев `n_esimators`, каждое из которых:\n \n - обучается на подмножестве признаков (`метод случайных подпространств`)\n - и на подмножестве объектов (`бэггинг`)\n - `случайный лес` получается случайным по двум этим параметрам\n - `ответы аггрегируются` при помощи `голосования`"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# Стандартная эвристика: \n# - в задаче классификации брать **квадратный корень числа признаков**, задаче регрессии — **треть числа признаков**\n\nmfeats = int(np.sqrt(len(features)))\nrandom_forest = RandomForestClassifier(n_estimators=100,\n                                       n_jobs=-1,\n                                       max_features=mfeats,\n                                       max_depth=30)\nprint(\"Random Forest:\", estimate_accuracy(random_forest, X, y))",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Random Forest: 0.983\n"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Метрика OOB\n\nЕщё одно преимущество использования \nбеггинга для аггрегации моделей\n\nПолучение оценки работы классификатора без дополнительного проведения `кросс-валидации`при помощи `out-of-bag` метрики\n\nЭто метод вычисления произвольной оценки качества во время обучения беггинга\nДля подсчёта требуется указать параметр **`oob_score = True`**"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "mfeats = int(np.sqrt(len(features)))\nrandom_forest = RandomForestClassifier(n_estimators=100,\n                                       max_features=mfeats,\n                                       max_depth=30,\n                                       oob_score=True,\n                                       n_jobs=-1)\nrandom_forest.fit(X, y)\n\n# тектируем модель на данных который алгоритм не использовал\nround(random_forest.oob_score_,3)",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "0.993"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Бэггинг Логистической Регресии\n\nМетод `бэггинга` можно применять к произвольным алгоритмам, например, к `логистической регрессии`"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "from sklearn.linear_model import LogisticRegression\nimport warnings; warnings.filterwarnings('ignore')\n\n# базовый алгоритм\nlr = LogisticRegression(solver='saga', \n                        max_iter=200)\nlr.fit(X, y)\nprint(\"LR:\", estimate_accuracy(lr, X, y))\n\nfrom sklearn.ensemble import BaggingClassifier\n\n# бэггинг классификатор\nrandom_logreg = BaggingClassifier(lr,\n                                  n_estimators=10,\n                                  n_jobs=-1,\n                                  random_state=42)\nprint(\"Bagging for LR:\", estimate_accuracy(random_logreg, X, y))",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "LR: 0.442\nBagging for LR: 0.437\n"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Почему так?\n\n- В её случае он не так сильно повышает качество, поскольку `линейные` модели не так сильно зависят от состава обучающей выборки\n- Попробуем убрать часть признаков с помощью `max_features`"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "random_logreg = BaggingClassifier(\n    lr,\n    n_estimators=10,\n    n_jobs=-1,\n    max_features=0.5,  # выбираем только часть фич\n    random_state=42\n)\nprint(\"Bagging for LR:\", estimate_accuracy(random_logreg, X, y))",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Bagging for LR: 0.22\n"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "В случае линейной регрессии:\n- Повышение `разнообразности` моделей не дает такого прироста, как с деревьями, поскольку модели сильно теряют в качестве. \n- Случайный лес на примере нашей задачи справляется лучше."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### <b><span style='color:#686dec'>Задача Классификации изображений</span></b>\n \n#### датасет\n        \n- Загрузите датасет digits с помощью функции `load_digits` из sklearn.datasets\n- Нам предстоит решать задачу `классификации изображений` с цифрами по численным признакам\n\n#### оценка качества\n\n- Для **оценки качества** мы будем использовать `cross_val_score` из sklearn.model_selection с параметром . Эта функция реализует k-fold cross validation c  равным значению параметра . \n- Предлагается использовать k=10, чтобы полученные оценки качества имели небольшой разброс, и было проще проверить полученные ответы. На практике же часто хватает и k=5. Функция cross_val_score будет возвращать numpy.ndarray, в котором будет  чисел — качество в каждом из  экспериментов k-fold cross validation.\n- Для получения среднего значения (которое и будет оценкой качества работы) вызовите метод .mean() у массива, который возвращает `cross_val_score`\n        \n#### <b>Задание <span style='color:#686dec'>5.7.1</span></b> \n\nСоздайте `DecisionTreeClassifier` с настройками по умолчанию и измерьте качество его работы с помощью cross_val_score"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "import pandas as pd\nfrom sklearn.datasets import load_digits\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\ndigits = load_digits()\nX = digits.data\ny = digits.target\n\nscaler = StandardScaler()\nX_view = pd.DataFrame(data=scaler.fit_transform(X), \n                       columns=digits.feature_names)\nfeatures = digits.feature_names\nX_view.head()",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pixel_0_0</th>\n      <th>pixel_0_1</th>\n      <th>pixel_0_2</th>\n      <th>pixel_0_3</th>\n      <th>pixel_0_4</th>\n      <th>pixel_0_5</th>\n      <th>pixel_0_6</th>\n      <th>pixel_0_7</th>\n      <th>pixel_1_0</th>\n      <th>pixel_1_1</th>\n      <th>...</th>\n      <th>pixel_6_6</th>\n      <th>pixel_6_7</th>\n      <th>pixel_7_0</th>\n      <th>pixel_7_1</th>\n      <th>pixel_7_2</th>\n      <th>pixel_7_3</th>\n      <th>pixel_7_4</th>\n      <th>pixel_7_5</th>\n      <th>pixel_7_6</th>\n      <th>pixel_7_7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-0.335016</td>\n      <td>-0.043081</td>\n      <td>0.274072</td>\n      <td>-0.664478</td>\n      <td>-0.844129</td>\n      <td>-0.409724</td>\n      <td>-0.125023</td>\n      <td>-0.059078</td>\n      <td>-0.624009</td>\n      <td>...</td>\n      <td>-0.757436</td>\n      <td>-0.209785</td>\n      <td>-0.023596</td>\n      <td>-0.299081</td>\n      <td>0.086719</td>\n      <td>0.208293</td>\n      <td>-0.366771</td>\n      <td>-1.146647</td>\n      <td>-0.505670</td>\n      <td>-0.196008</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>-0.335016</td>\n      <td>-1.094937</td>\n      <td>0.038648</td>\n      <td>0.268751</td>\n      <td>-0.138020</td>\n      <td>-0.409724</td>\n      <td>-0.125023</td>\n      <td>-0.059078</td>\n      <td>-0.624009</td>\n      <td>...</td>\n      <td>-0.757436</td>\n      <td>-0.209785</td>\n      <td>-0.023596</td>\n      <td>-0.299081</td>\n      <td>-1.089383</td>\n      <td>-0.249010</td>\n      <td>0.849632</td>\n      <td>0.548561</td>\n      <td>-0.505670</td>\n      <td>-0.196008</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>-0.335016</td>\n      <td>-1.094937</td>\n      <td>-1.844742</td>\n      <td>0.735366</td>\n      <td>1.097673</td>\n      <td>-0.409724</td>\n      <td>-0.125023</td>\n      <td>-0.059078</td>\n      <td>-0.624009</td>\n      <td>...</td>\n      <td>0.259230</td>\n      <td>-0.209785</td>\n      <td>-0.023596</td>\n      <td>-0.299081</td>\n      <td>-1.089383</td>\n      <td>-2.078218</td>\n      <td>-0.164037</td>\n      <td>1.565686</td>\n      <td>1.695137</td>\n      <td>-0.196008</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>-0.335016</td>\n      <td>0.377661</td>\n      <td>0.744919</td>\n      <td>0.268751</td>\n      <td>-0.844129</td>\n      <td>-0.409724</td>\n      <td>-0.125023</td>\n      <td>-0.059078</td>\n      <td>1.879691</td>\n      <td>...</td>\n      <td>1.072563</td>\n      <td>-0.209785</td>\n      <td>-0.023596</td>\n      <td>-0.299081</td>\n      <td>0.282736</td>\n      <td>0.208293</td>\n      <td>0.241430</td>\n      <td>0.379040</td>\n      <td>-0.505670</td>\n      <td>-0.196008</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>-0.335016</td>\n      <td>-1.094937</td>\n      <td>-2.551014</td>\n      <td>-0.197863</td>\n      <td>-1.020657</td>\n      <td>-0.409724</td>\n      <td>-0.125023</td>\n      <td>-0.059078</td>\n      <td>-0.624009</td>\n      <td>...</td>\n      <td>-0.757436</td>\n      <td>-0.209785</td>\n      <td>-0.023596</td>\n      <td>-0.299081</td>\n      <td>-1.089383</td>\n      <td>-2.306869</td>\n      <td>0.849632</td>\n      <td>-0.468564</td>\n      <td>-0.505670</td>\n      <td>-0.196008</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 64 columns</p>\n</div>",
      "text/plain": "   pixel_0_0  pixel_0_1  pixel_0_2  pixel_0_3  pixel_0_4  pixel_0_5  \\\n0        0.0  -0.335016  -0.043081   0.274072  -0.664478  -0.844129   \n1        0.0  -0.335016  -1.094937   0.038648   0.268751  -0.138020   \n2        0.0  -0.335016  -1.094937  -1.844742   0.735366   1.097673   \n3        0.0  -0.335016   0.377661   0.744919   0.268751  -0.844129   \n4        0.0  -0.335016  -1.094937  -2.551014  -0.197863  -1.020657   \n\n   pixel_0_6  pixel_0_7  pixel_1_0  pixel_1_1  ...  pixel_6_6  pixel_6_7  \\\n0  -0.409724  -0.125023  -0.059078  -0.624009  ...  -0.757436  -0.209785   \n1  -0.409724  -0.125023  -0.059078  -0.624009  ...  -0.757436  -0.209785   \n2  -0.409724  -0.125023  -0.059078  -0.624009  ...   0.259230  -0.209785   \n3  -0.409724  -0.125023  -0.059078   1.879691  ...   1.072563  -0.209785   \n4  -0.409724  -0.125023  -0.059078  -0.624009  ...  -0.757436  -0.209785   \n\n   pixel_7_0  pixel_7_1  pixel_7_2  pixel_7_3  pixel_7_4  pixel_7_5  \\\n0  -0.023596  -0.299081   0.086719   0.208293  -0.366771  -1.146647   \n1  -0.023596  -0.299081  -1.089383  -0.249010   0.849632   0.548561   \n2  -0.023596  -0.299081  -1.089383  -2.078218  -0.164037   1.565686   \n3  -0.023596  -0.299081   0.282736   0.208293   0.241430   0.379040   \n4  -0.023596  -0.299081  -1.089383  -2.306869   0.849632  -0.468564   \n\n   pixel_7_6  pixel_7_7  \n0  -0.505670  -0.196008  \n1  -0.505670  -0.196008  \n2   1.695137  -0.196008  \n3  -0.505670  -0.196008  \n4  -0.505670  -0.196008  \n\n[5 rows x 64 columns]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\nrandom_tree = DecisionTreeClassifier()\nprint(\"Random tree:\", estimate_accuracy(random_tree, X, y,metric='accuracy'))",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Random tree: 0.782\n"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### <b>Задание <span style='color:#686dec'>5.7.2</span></b> \n\n- Теперь давайте обучим `BaggingClassifier` на основе `DecisionTreeClassifier`\n- Из sklearn.ensemble импортируйте `BaggingClassifier`, все параметры задайте по умолчанию\n- Нужно изменить только количество **базовых моделей**, задав его равным **100**\n- Подумайте, какие выводы можно сделать из соотношения качества **одиночного дерева** и **беггинга деревьев**?"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# Количество базовых моделей 100\nbagging_random_trees = BaggingClassifier(random_tree,\n                                         n_estimators=100)\nprint(\"Random Tree Bag:\", estimate_accuracy(bagging_random_trees,\n                                                X, y,metric='accuracy'))",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Random Tree Bag: 0.91\n"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### <b>Задание <span style='color:#686dec'>5.7.3</span></b> \n\nТеперь изучите параметры `BaggingClassifier` и выберите их такими\n- Чтобы каждый **базовый алгоритм** обучался не на всех **d** признаках, а на **sqrt(d)** случайных признаках\n\nКорень из числа признаков - часто используемая эвристика в задачах классификации\n- В задачах `регрессии` же часто берут число признаков, деленное на три, **log(d)** тоже имеет место быть\n\n\nНо в общем случае ничто не мешает вам выбирать любое другое число случайных признаков, добиваясь лучшего качества на **кросс-валидации**"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "import warnings; warnings.filterwarnings('ignore')\nimport numpy as np\n\n# базовый коассификатор\ndecision_tree = DecisionTreeClassifier()\nprint(\"Decision Tree:\", estimate_accuracy(decision_tree,X,y,metric='accuracy'))\n\n# Используем только часть признаков\nmfeats = int(np.sqrt(len(features)))\nprint('bagging',mfeats,'features')\n      \n# Бэггинг классификатор\nbagging_random_trees = BaggingClassifier(decision_tree,\n                                         n_estimators=100,\n                                         max_features=mfeats,\n                                         random_state=42)\n\nprint(\"Random Tree Bag:\", estimate_accuracy(bagging_random_trees,X,y,metric='accuracy'))\nprint('')",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Decision Tree: 0.786\nbagging 8 features\nRandom Tree Bag: 0.919\n\n"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### <b>Задание <span style='color:#686dec'>5.7.4</span></b> \n\n- В предыдущем пункте мы _выбирали подмножество один раз для каждого очередного дерева_\n- Следующим нашим шагом будет построение беггинга на основе деревьев, **которые выбирают случайное подмножество признаков** для каждой вершины дерева\n\n\n- Для этого нам потребуется перенести, отвечающий за это параметр из `BaggingClassifier` в `DecisionTreeClassifier` \n- Для этого вам из документации нужно выяснить, какой параметр `DecisionTreeClassifier` за это отвечает\n- По-прежнему сэмплируем `sqrt(d)` признаков"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "'''\nDecision Tree Arguments\ncriterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0\n'''\n\n'''\nBaggingClassifier Arguments\nbase_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0\n'''",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nBaggingClassifier Arguments\\nbase_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0\\n'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "# mfeats = int(np.log(len(features)))\nmfeats = int(np.sqrt(len(features)))\nprint('bagging',mfeats,'features')    \n\ndecision_tree = DecisionTreeClassifier()\n# decision_tree = DecisionTreeClassifier(max_features=mfeats)\nprint(\"Decision Tree:\", estimate_accuracy(decision_tree,X,y,metric='accuracy'))\n    \nbagging_random_trees = BaggingClassifier(base_estimator=decision_tree,\n                                         n_estimators=100,\n                                         n_jobs=-1,\n                                         random_state=42)\n    \nprint(\"Random Tree Bag:\", estimate_accuracy(bagging_random_trees,X,y,metric='accuracy'))",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "bagging 8 features\nDecision Tree: 0.782\nRandom Tree Bag: 0.914\n"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### <b>Задание <span style='color:#686dec'>5.7.5</span></b> \n\n- Полученный в задании 4 классификатор - бэггинг на рандомизированных деревьях (в которых при построении каждой вершины выбирается случайное подмножество признаков и разбиение ищется только по ним).\n- Это в точности соответствует алгоритму Random Forest, поэтому почему бы не сравнить качество работы классификатора с RandomForestClassifier из sklearn.ensemble.\n-Сделайте это, а затем изучите, как качество классификации на данном датасете зависит от количества деревьев, количества признаков, выбираемых при построении каждой вершины дерева, а также ограничений на глубину дерева.\n- Для наглядности лучше построить графики зависимости качества от значений параметров, но для сдачи задания это делать не обязательно."
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = RandomForestClassifier()\ngs = GridSearchCV(model,{'n_estimators':[5,10,15,20,25,30,35,40,45,50]})\ngs.fit(X,y)\nresults = gs.cv_results_\nresults.keys()",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_n_estimators', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score'])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "print('валидационная ')\ndict(zip([i for i in range(5,55,5)],results['mean_test_score']))",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "валидационная \n"
    },
    {
     "data": {
      "text/plain": "{5: 0.8698050139275765,\n 10: 0.9048808418446301,\n 15: 0.9237790157845869,\n 20: 0.9243376663571649,\n 25: 0.9304642525533892,\n 30: 0.9332389353141443,\n 35: 0.9326864747756114,\n 40: 0.9310182606004334,\n 45: 0.9338037759207676,\n 50: 0.9349117920148562}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- При очень маленьком числе деревьев (5, 10, 15) случайный лес работает хуже, чем при большем числе деревьев\n- С ростом количества деревьев в случайном лесе, в какой-то момент деревьев становится достаточно для высокого качества классификации, а затем качество существенно не меняется.\n- При большом количестве признаков (для данного датасета - 40-50) качество классификации становится хуже, чем при малом количестве признаков (10-15). Это связано с тем, что чем меньше признаков выбирается в каждом узле, тем более различными получаются деревья (ведь деревья сильно неустойчивы к изменениям в обучающей выборке), и тем лучше работает их композиция."
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "model = RandomForestClassifier()\ngs = GridSearchCV(model,{'max_depth':[5,10,15,20,25,30,35,40,None]})\ngs.fit(X,y)\nresults = gs.cv_results_\n\nprint('валидационная ')\nparam_vals = [i for i in range(5,45,5)]\nparam_vals.append(None)\ndict(zip(param_vals,results['mean_test_score'])) ",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "валидационная \n"
    },
    {
     "data": {
      "text/plain": "{5: 0.9009594552770039,\n 10: 0.9321324667285671,\n 15: 0.9343562364593005,\n 20: 0.937137109254101,\n 25: 0.9421417517796348,\n 30: 0.937137109254101,\n 35: 0.9382497678737233,\n 40: 0.9421417517796348,\n None: 0.9382513153822346}"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- При небольшой максимальной глубине деревьев (5-6) качество работы случайного леса заметно хуже, чем без ограничений, т.к. деревья получаются недообученными. \n- С ростом глубины качество сначала улучшается, а затем не меняется существенно, т.к. из-за усреднения прогнозов и различий деревьев их переобученность в бэггинге не сказывается на итоговом качестве (все деревья преобучены по-разному, и при усреднении они компенсируют переобученность друг друга).\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}