{
 "cells": [
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T19:35:09.373351Z",
     "iopub.execute_input": "2025-02-10T19:35:09.373686Z",
     "iopub.status.idle": "2025-02-10T19:38:32.131023Z",
     "shell.execute_reply.started": "2025-02-10T19:35:09.373654Z",
     "shell.execute_reply": "2025-02-10T19:38:32.130217Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "!pip install -q replay-rec rs_datasets --quiet",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.8/196.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.6/390.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.5/290.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.7/413.7 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for fixed-install-nmslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T19:38:32.131798Z",
     "iopub.execute_input": "2025-02-10T19:38:32.132086Z",
     "iopub.status.idle": "2025-02-10T19:38:37.305921Z",
     "shell.execute_reply.started": "2025-02-10T19:38:32.132053Z",
     "shell.execute_reply": "2025-02-10T19:38:37.305022Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "from datetime import datetime as dt\nimport numpy as np\nimport pandas as pd\nimport random\nimport torch\nimport torch.nn as nn\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm.auto import tqdm\n\nfrom rs_datasets import MovieLens\nfrom replay.splitters.time_splitter import TimeSplitter\nfrom replay.preprocessing.filters import MinCountFilter, LowRatingFilter\nfrom replay.metrics import NDCG, HitRate, Coverage, Experiment\nfrom dataclasses import dataclass",
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2025-02-10T19:38:37.306797Z",
     "iopub.execute_input": "2025-02-10T19:38:37.307401Z",
     "iopub.status.idle": "2025-02-10T19:38:37.315859Z",
     "shell.execute_reply.started": "2025-02-10T19:38:37.307363Z",
     "shell.execute_reply": "2025-02-10T19:38:37.315135Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "from dataclasses import dataclass\n\n@dataclass\nclass config:\n\n    USER_COL : str = 'user_id'\n    ITEM_COL : str = 'item_id'\n    RATING_COL : str = 'rating'\n    TIMESTAMP : str = 'timestamp'\n    NUM_EPOCHS : int = 30\n\n    K = 10\n    SEED = 123\n\nconfig = config()\nrandom.seed(config.SEED)\ntorch.manual_seed(config.SEED)\nnp.random.seed(config.SEED)",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Background**\n\n***\n\nIn this notebook we will look at how to use a neural network approach to recommendations\n- Implicit feedback will be used\n- Scalar product of both the **`user_id`** and **`item_id`** embeddings will be our relevancy scores\n- User film interactions will be **`positive`** feedback & negative samples which will be created randomly are our **`negative`** samples\n- The dataset is split into two, **`train`** will be used to train a model on historical user data, **`test`** will be used to provide user recommendations\n- What we will be telling the model is to learn and differentiate between\n\n***\n\n## **1 | Load Dataset**\n\nWe will be using a simplified dataset **`MovieLens`** with 100,000 interactions"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T19:38:37.317789Z",
     "iopub.execute_input": "2025-02-10T19:38:37.317995Z",
     "iopub.status.idle": "2025-02-10T19:38:38.620907Z",
     "shell.execute_reply.started": "2025-02-10T19:38:37.317978Z",
     "shell.execute_reply": "2025-02-10T19:38:38.620262Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "class MovieLensPrepare:\n\n    def __init__(self):\n        rs = MovieLens('1m')\n        self.data = rs.ratings\n        self.u_features = rs.users\n        self.i_features = rs.items\n\n\n    def preprocess(self):\n\n        data = self.data\n        u_features = self.u_features\n        i_features = self.i_features\n        \n        data = MinCountFilter(num_entries=20).transform(data)\n\n        # interactions and user & item features must be synchronised\n        data = data[data[config.USER_COL].isin(u_features[config.USER_COL].unique())]\n        data = data[data[config.ITEM_COL].isin(i_features[config.ITEM_COL].unique())]\n\n        print(f\"Number of unique users {data['user_id'].nunique()}\")\n        print(f\"Number of unique items {data['item_id'].nunique()}\")\n\n        # interactions and user & item features must be synchronised\n        data = data[data[config.USER_COL].isin(u_features[config.USER_COL].unique())]\n        data = data[data[config.ITEM_COL].isin(i_features[config.ITEM_COL].unique())]\n\n        data[config.TIMESTAMP] = pd.to_datetime(data['timestamp'],unit='s')\n\n        self.data = data\n\n    def split_data(self):\n\n        data = self.data\n        u_features = self.u_features\n        i_features = self.i_features\n\n        splitter = TimeSplitter(time_threshold=0.2,  # 20% into test subset\n                            drop_cold_users=True,\n                            drop_cold_items=True,\n                            query_column=config.USER_COL)\n        \n        train,test = splitter.split(data)\n        print('train size',train.shape[0])\n        print('test size', test.shape[0])\n\n        # user features and item features must be present in interactions dataset and only\n        u_features = u_features[u_features[config.USER_COL].isin(train[config.USER_COL].unique())]\n        i_features = i_features[i_features[config.ITEM_COL].isin(train[config.ITEM_COL].unique())]\n\n        # encoders for users\n        encoder_user = LabelEncoder()\n        encoder_user.fit(train[config.USER_COL])\n        \n        # encoders for items\n        encoder_item = LabelEncoder()\n        encoder_item.fit(train[config.ITEM_COL])\n\n        train[config.USER_COL] = encoder_user.transform(train[config.USER_COL])\n        train[config.ITEM_COL] = encoder_item.transform(train[config.ITEM_COL])\n        \n        test[config.USER_COL] = encoder_user.transform(test[config.USER_COL])\n        test[config.ITEM_COL] = encoder_item.transform(test[config.ITEM_COL])\n        \n        u_features[config.USER_COL] = encoder_user.transform(u_features[config.USER_COL])\n        i_features[config.ITEM_COL] = encoder_item.transform(i_features[config.ITEM_COL])\n\n        self.train = train \n        self.test = test\n\n        self.u_features = u_features\n        self.i_features = i_features\n        \n    def filter_test(self):\n        filter_rating = LowRatingFilter(value=4)        \n        self.test = filter_rating.transform(self.test)\n        \nstudy = MovieLensPrepare()",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "text": "5.93MB [00:00, 10.8MB/s]                            \n",
     "output_type": "stream"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Extract the relevant subsets of data; interactions, user features & item features. We should also note that the **`timestamp`** feature is in unix seconds time, which we'll need to convert to **`datetime`** later"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **2 | Preprocessing** \n\n- **`Replay`** contains a handy & quick way for preprocessing **interactions**\n- **`MinCountFilter`** can be used for filtering our interactions that have less than **num_entries**\n- Lets use this method for removing user interactions with less than 20 items "
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T19:38:38.622112Z",
     "iopub.execute_input": "2025-02-10T19:38:38.622449Z",
     "iopub.status.idle": "2025-02-10T19:38:39.389139Z",
     "shell.execute_reply.started": "2025-02-10T19:38:38.622411Z",
     "shell.execute_reply": "2025-02-10T19:38:39.388270Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "study.preprocess()",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "Number of unique users 6040\nNumber of unique items 3706\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Lets check the **`user_id`** and **`item_id`** statistics after our filtration and make sure our **interactions** are synchronised with both the **user** and **item** feature subsets"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's convert the time feature to datetime, so we can more easily interpret how to split the dataset in time. \n\nIn our problem, we assume that the test interactions have not been made yet"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **3 | Splitting Dataset in time**\n\n- The next step after preprocessing the dataset to our liking is to split it into subsets, so we can train the model on one subset and use another for model validation (20%)\n- **replay** has a function named **`TimeSplitter`**, which we will to create our subsets\n\n> class TimeSplitter(replay.splitters.base_splitter.Splitter)\n |  TimeSplitter(time_threshold: Union[datetime.datetime, str, float], query_column: str = 'query_id', drop_cold_users: bool = False, drop_cold_items: bool = False, item_column: str = 'item_id', timestamp_column: str = 'timestamp', session_id_column: Optional[str] = None, session_id_processing_strategy: str = 'test', time_column_format: str = '%Y-%m-%d %H:%M:%S')\n"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T19:38:39.390020Z",
     "iopub.execute_input": "2025-02-10T19:38:39.390314Z",
     "iopub.status.idle": "2025-02-10T19:38:39.703747Z",
     "shell.execute_reply.started": "2025-02-10T19:38:39.390292Z",
     "shell.execute_reply": "2025-02-10T19:38:39.702918Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "study.split_data()",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": "train size 800164\ntest size 104452\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "<ipython-input-4-142900e37db3>:66: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  u_features[config.USER_COL] = encoder_user.transform(u_features[config.USER_COL])\n<ipython-input-4-142900e37db3>:67: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  i_features[config.ITEM_COL] = encoder_item.transform(i_features[config.ITEM_COL])\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **4 | Rating Filter**\n\nWe want to recommend only items that have been rated highly, so for the **`test`** subset, we will be using **`LowRatingFilter`** to remove iteractions with low ratings"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T19:38:39.704523Z",
     "iopub.execute_input": "2025-02-10T19:38:39.704790Z",
     "iopub.status.idle": "2025-02-10T19:38:39.710774Z",
     "shell.execute_reply.started": "2025-02-10T19:38:39.704753Z",
     "shell.execute_reply": "2025-02-10T19:38:39.710099Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "study.filter_test()",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **5 | Create Torch Dataset**\n\nWe need to create a torch dataset from our matrix of interactions **`data`**\n\n- The dataset **`TowerTrain`** **`get_item`** for each index inputs the **`user_id`** and **`item_id`** (which will be our positive feedback) from the interaction dataset : (positive_item_id)\n- Additionally for this user **`user_id`**, we generate an additional number of random **`item_id`** which will be the negative samples, which the user hasn't watched \n- Both of these are concatenated into a single array vector (items)\n- Lastly we also return the labels, corresponding to either the positive (1) or negative (0) sample id "
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T19:38:39.711569Z",
     "iopub.execute_input": "2025-02-10T19:38:39.711865Z",
     "iopub.status.idle": "2025-02-10T19:38:39.724157Z",
     "shell.execute_reply.started": "2025-02-10T19:38:39.711834Z",
     "shell.execute_reply": "2025-02-10T19:38:39.723479Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "from torch.utils.data import Dataset, DataLoader\n\nclass TowerTrain(Dataset):\n    \n    def __init__(self, \n                 data, \n                 num_negatives=10, \n                 i_features=None, \n                 u_features=None):\n\n        # user, item\n        self.data = data[[config.USER_COL,config.ITEM_COL]].to_numpy()\n        self.num_negatives = num_negatives\n        self.num_items = len(np.unique(self.data[:, 1]))\n        self.i_features = i_features\n        self.u_features = u_features\n\n    def __len__(self):\n        return len(self.data)\n\n    # get item of row in data\n    def __getitem__(self, idx):\n\n        # index to -> user_id, item_id\n        user_id, positive_item_id = self.data[idx, 0], self.data[idx, 1]\n\n        # create positive, negative samples\n        # torch tensor for each item_id (pos sample) create 10 neg samples\n        items = torch.tensor(np.hstack([positive_item_id,\n                                       np.random.randint(\n                                           low=0,\n                                           high=self.num_items,\n                                           size=self.num_negatives)]),\n                             dtype=torch.int32)\n\n        # set all labels to 0\n        labels = torch.zeros(self.num_negatives + 1, dtype=torch.float32)\n        labels[0] = 1. # positive label\n\n        return {'user_ids': torch.tensor([user_id], dtype=torch.int32),\n                'item_ids': items,\n                'labels': labels}",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We create the dataset and dataloaders containing a batch size of 1024 rows, we'll define the training dataloader and show a batch sample output **`batch`**, which is the result of the `__forward__` method pass for the number of items in the batch"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T19:38:39.724979Z",
     "iopub.execute_input": "2025-02-10T19:38:39.725258Z",
     "iopub.status.idle": "2025-02-10T19:38:39.908108Z",
     "shell.execute_reply.started": "2025-02-10T19:38:39.725231Z",
     "shell.execute_reply": "2025-02-10T19:38:39.907268Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "# create dataset\nds_train = TowerTrain(study.train)\n\n# create data loader\ndl_train = DataLoader(ds_train,\n                          batch_size=2,\n                          shuffle=True,\n                          num_workers=0)\n\nbatch = next(iter(dl_train))\nbatch",
   "execution_count": 9,
   "outputs": [
    {
     "execution_count": 9,
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'user_ids': tensor([[ 163],\n         [3579]], dtype=torch.int32),\n 'item_ids': tensor([[1036, 3582, 3437, 3454, 1346, 1122, 1766, 3089, 2154, 1147, 1593],\n         [1806, 3286, 1761,   96, 2161, 2686,   47,   73, 1568,  942, 2272]],\n        dtype=torch.int32),\n 'labels': tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])}"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **7 | Model Definition**\n\nWe will be creating a subclass **`SimpleTower`**, which only includes the embeddings of both **`user_id`** and **`item_id`**\n\n- The **`forward`** method, when called simply returns the user/item row of the corresponding embedding matrix\n- Calculates the dot product between the **`user_id`** & **`item_id`** matrices"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T19:38:39.909004Z",
     "iopub.execute_input": "2025-02-10T19:38:39.909274Z",
     "iopub.status.idle": "2025-02-10T19:38:39.915734Z",
     "shell.execute_reply.started": "2025-02-10T19:38:39.909253Z",
     "shell.execute_reply": "2025-02-10T19:38:39.914832Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "# subclass contains only embedding layer but we can \n# expand on this by importing user, item features\nclass SimpleTower(nn.Module):\n    def __init__(self, num_embeddings, emb_dim):\n        super().__init__()\n        self.emb = nn.Embedding(num_embeddings, emb_dim)\n\n    def forward(self, ids, features=None):\n        return self.emb(ids)\n\n\nclass BaseTwoHead(nn.Module):\n    \n    def __init__(self, \n                 emb_dim, \n                 user_config=None,\n                 item_config=None):\n        \n        super().__init__()\n        self.emb_dim = emb_dim\n        self.user_tower = SimpleTower(emb_dim=emb_dim, **user_config) # (emb_dim,n_users)\n        self.item_tower = SimpleTower(emb_dim=emb_dim, **item_config) # (emb_dim,n_items)\n\n    # forward method defines two 'towers'\n    # and the scalar product of the two\n    # which will gives us the scores\n    def forward(self, batch):\n        item_emb = self.item_tower(batch[\"item_ids\"]) # (batch,1,16) \n        user_emb = self.user_tower(batch[\"user_ids\"]) # (batch,11,16)\n        dot_product = (user_emb * item_emb).sum(dim=-1) # (batch,11)\n        return dot_product\n\n    # methods for extracting embeddings\n    def infer_users(self, batch):\n        return self.user_tower(batch[\"user_ids\"])\n\n    def infer_items(self, batch):\n        return self.item_tower(batch[\"item_ids\"])",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's check the contents of the model, if we utilise an embedding size of 16"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T19:38:39.916674Z",
     "iopub.execute_input": "2025-02-10T19:38:39.916966Z",
     "iopub.status.idle": "2025-02-10T19:38:39.936756Z",
     "shell.execute_reply.started": "2025-02-10T19:38:39.916936Z",
     "shell.execute_reply": "2025-02-10T19:38:39.936047Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "# model parameters\nembed_config = {'emb_dim' : 16}  # embedding dimension\nuser_config = {'num_embeddings' : study.train[config.USER_COL].max() + 1,} # number of users\nitem_config = {'num_embeddings' : study.train[config.ITEM_COL].max() + 1,} # number of items\n\n# import the embedding dimension \nmodel = BaseTwoHead(**embed_config, \n                    user_config=user_config, \n                    item_config=item_config)\nmodel",
   "execution_count": 11,
   "outputs": [
    {
     "execution_count": 11,
     "output_type": "execute_result",
     "data": {
      "text/plain": "BaseTwoHead(\n  (user_tower): SimpleTower(\n    (emb): Embedding(5400, 16)\n  )\n  (item_tower): SimpleTower(\n    (emb): Embedding(3662, 16)\n  )\n)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Model `forward` pass**\n\n- The output of the model will give us the logits for each of the 11 items, for each user row"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T19:38:39.937613Z",
     "iopub.execute_input": "2025-02-10T19:38:39.937900Z",
     "iopub.status.idle": "2025-02-10T19:38:39.948335Z",
     "shell.execute_reply.started": "2025-02-10T19:38:39.937878Z",
     "shell.execute_reply": "2025-02-10T19:38:39.947700Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "# output for a single batch\noutput = model(batch)\noutput",
   "execution_count": 12,
   "outputs": [
    {
     "execution_count": 12,
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[  1.6632,   5.8888,   0.0997,   7.6885,   8.2156,   4.0495,   3.0272,\n           1.9775,  -1.8750,   4.3952,   0.2714],\n        [  5.3873, -10.4797,  -4.2230,  -0.4488,   0.9215,  -5.0823,  -0.5018,\n           4.9579,   0.8251,  -6.3608,  -4.5723]], grad_fn=<SumBackward1>)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Output size**"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T19:38:39.951010Z",
     "iopub.execute_input": "2025-02-10T19:38:39.951263Z",
     "iopub.status.idle": "2025-02-10T19:38:39.957480Z",
     "shell.execute_reply.started": "2025-02-10T19:38:39.951239Z",
     "shell.execute_reply": "2025-02-10T19:38:39.956785Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "output.size()",
   "execution_count": 13,
   "outputs": [
    {
     "execution_count": 13,
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([2, 11])"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **8 | Extracting Embeddings**\n\nWe can extract the embeddings for both the **user** and **items** using the following method and use it for **`inference`**. In this type of simple model, it won't be such a big problem to extract it anyway. Below is an example for a batch size of two"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T19:38:39.958533Z",
     "iopub.execute_input": "2025-02-10T19:38:39.958778Z",
     "iopub.status.idle": "2025-02-10T19:38:39.971943Z",
     "shell.execute_reply.started": "2025-02-10T19:38:39.958759Z",
     "shell.execute_reply": "2025-02-10T19:38:39.971261Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "# extract embeddings from model (here items)\ni_embeddings = model.infer_items(batch)\ni_embeddings.size()",
   "execution_count": 14,
   "outputs": [
    {
     "execution_count": 14,
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([2, 11, 16])"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T19:38:39.972560Z",
     "iopub.execute_input": "2025-02-10T19:38:39.972751Z",
     "iopub.status.idle": "2025-02-10T19:38:39.986440Z",
     "shell.execute_reply.started": "2025-02-10T19:38:39.972735Z",
     "shell.execute_reply": "2025-02-10T19:38:39.985734Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "u_embeddings = model.infer_users(batch)\nu_embeddings.size()",
   "execution_count": 15,
   "outputs": [
    {
     "execution_count": 15,
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([2, 1, 16])"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **9 | Training the network**\n\nWe need to define an **`optimiser`**, **`loss function`** and the datasets in the form of a **`data loader`**. As we are setting up the problem as a binary classification problem, we'll be using **BCEWithLogitsLoss**"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T19:38:39.987305Z",
     "iopub.execute_input": "2025-02-10T19:38:39.987550Z",
     "iopub.status.idle": "2025-02-10T19:38:41.757456Z",
     "shell.execute_reply.started": "2025-02-10T19:38:39.987523Z",
     "shell.execute_reply": "2025-02-10T19:38:41.756763Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "optimizer = torch.optim.Adam(model.parameters(),\n                             lr=0.001)\nloss_fn = nn.BCEWithLogitsLoss()\n\n# create train dataset\nds_train = TowerTrain(study.train)\n\n# create train data loader\ndl_train = DataLoader(ds_train,\n                      batch_size=1024,\n                      shuffle=True,\n                      num_workers=0)\n\n# create test dataset\nds_test = TowerTrain(study.test)\n\n# create test data loader\ndl_test = DataLoader(ds_test,\n                      batch_size=1024,\n                      shuffle=True,\n                      num_workers=0)",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T19:38:41.758276Z",
     "iopub.execute_input": "2025-02-10T19:38:41.758710Z",
     "iopub.status.idle": "2025-02-10T20:02:42.647317Z",
     "shell.execute_reply.started": "2025-02-10T19:38:41.758686Z",
     "shell.execute_reply": "2025-02-10T20:02:42.646332Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "train_loss_per_epoch = []\ntest_loss_per_epoch = []\n\n# loop through all epochs\nfor epoch in tqdm(range(config.NUM_EPOCHS)):\n\n    # training loop for all batches\n    model.train()\n    train_loss = 0.0\n    for iteration, batch in enumerate(dl_train):\n        optimizer.zero_grad()\n        preds = model(batch)\n        loss = loss_fn(preds, batch['labels'])\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    train_loss /= len(dl_train)\n\n    # evaluation loop for all batches\n    model.eval()\n    test_loss = 0\n    for iteration, batch in enumerate(dl_test):\n        preds = model(batch)\n        loss = loss_fn(preds, batch['labels'])\n        test_loss += loss.item()\n\n    # evaluation of loss\n    test_loss /= len(dl_test)\n    test_loss_per_epoch.append(test_loss)\n    train_loss_per_epoch.append(train_loss)",
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/30 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71f7550d8bca49e5bc5df5cf25f0ed67"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save our trained model for later use!"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T20:02:42.648137Z",
     "iopub.execute_input": "2025-02-10T20:02:42.648467Z",
     "iopub.status.idle": "2025-02-10T20:02:42.656286Z",
     "shell.execute_reply.started": "2025-02-10T20:02:42.648435Z",
     "shell.execute_reply": "2025-02-10T20:02:42.655626Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "# save our model state\ntorch.save(model.state_dict(), f\"/content/model_{config.NUM_EPOCHS}\")",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **8 | Generating user recommendations**\n\nTime has come to use our trained model!\n\n- We will be making recommendations by using the model that we trained on the **train** dataset and using the **test** users to make predictions\n- To make predictions, we will extract the **embedding** matrix weights for user and items, calculate the scores, get the top k results for each user based on the largest score values"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **8.1. Load Weights**\n\nFirst things first, we need to load the model weights"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T20:02:42.657217Z",
     "iopub.execute_input": "2025-02-10T20:02:42.657510Z",
     "iopub.status.idle": "2025-02-10T20:02:42.674966Z",
     "shell.execute_reply.started": "2025-02-10T20:02:42.657480Z",
     "shell.execute_reply": "2025-02-10T20:02:42.674264Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "import warnings; warnings.filterwarnings('ignore')\n\nmodel = BaseTwoHead(**embed_config, \n                    user_config=user_config, \n                    item_config=item_config)\nmodel.load_state_dict(torch.load(f\"/content/model_{config.NUM_EPOCHS}\"))\nmodel.eval()",
   "execution_count": 19,
   "outputs": [
    {
     "execution_count": 19,
     "output_type": "execute_result",
     "data": {
      "text/plain": "BaseTwoHead(\n  (user_tower): SimpleTower(\n    (emb): Embedding(5400, 16)\n  )\n  (item_tower): SimpleTower(\n    (emb): Embedding(3662, 16)\n  )\n)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **8.2. Get test users**\n\nGet the user identifiers that are in the test test, the test set was saved in **`study.test`**"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T20:02:42.675707Z",
     "iopub.execute_input": "2025-02-10T20:02:42.675996Z",
     "iopub.status.idle": "2025-02-10T20:02:42.697093Z",
     "shell.execute_reply.started": "2025-02-10T20:02:42.675969Z",
     "shell.execute_reply": "2025-02-10T20:02:42.696241Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "test_users = study.test[[config.USER_COL]].drop_duplicates().reset_index(drop=True)\ntest_users.head()",
   "execution_count": 20,
   "outputs": [
    {
     "execution_count": 20,
     "output_type": "execute_result",
     "data": {
      "text/plain": "   user_id\n0     1238\n1        0\n2     2146\n3     1380\n4     3180",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1238</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2146</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1380</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3180</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **8.3. Extract Weights**\n\nExtract the embedding weights for all users and items which is located in the model"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T20:02:42.697987Z",
     "iopub.execute_input": "2025-02-10T20:02:42.698285Z",
     "iopub.status.idle": "2025-02-10T20:02:42.703773Z",
     "shell.execute_reply.started": "2025-02-10T20:02:42.698257Z",
     "shell.execute_reply": "2025-02-10T20:02:42.702894Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "# extract the user / item embedding weights\nuser_embed = model.user_tower.emb.weight.detach().cpu().numpy()\nitem_embed = model.item_tower.emb.weight.detach().cpu().numpy()\nuser_embed.shape, item_embed.shape",
   "execution_count": 21,
   "outputs": [
    {
     "execution_count": 21,
     "output_type": "execute_result",
     "data": {
      "text/plain": "((5400, 16), (3662, 16))"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **8.4. Scalar product**\n\nCalculate the scores for each user & item combination by calculating the scalar product of them"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T20:02:42.704499Z",
     "iopub.execute_input": "2025-02-10T20:02:42.704777Z",
     "iopub.status.idle": "2025-02-10T20:02:42.731719Z",
     "shell.execute_reply.started": "2025-02-10T20:02:42.704749Z",
     "shell.execute_reply": "2025-02-10T20:02:42.731062Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "# calcualate the scores (751,1616)\nscores = user_embed[test_users[config.USER_COL].values] @ item_embed.T\nprint(scores)",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "text": "[[-8.98283534e-03 -2.23088098e+00 -1.21248198e+00 ... -5.93518734e+00\n  -6.74669886e+00 -3.57267737e+00]\n [ 1.21680886e-01 -1.92409587e+00 -2.67904949e+00 ... -5.99316978e+00\n  -3.68449354e+00 -1.20745528e+00]\n [-3.68786573e-01 -3.50460958e+00 -3.39616084e+00 ... -4.76325846e+00\n  -2.48652792e+00 -9.43809271e-01]\n ...\n [ 5.35706937e-01 -3.02921438e+00 -2.88318610e+00 ... -3.14549780e+00\n  -3.66571522e+00 -1.52589762e+00]\n [-6.87072992e-01 -1.82784998e+00 -2.26169515e+00 ... -6.18041801e+00\n  -5.24198675e+00 -3.18532085e+00]\n [-7.03839183e-01 -1.79352736e+00 -2.59766245e+00 ... -9.07467270e+00\n  -7.98052073e+00 -4.78053665e+00]]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **8.5. Get highest scores**\n\nGet the highest value indicies (idx) & their corresponding values (scores). The scores correspond to the index of the item in the **encoder** **`encoder_item`**, which we stored in class instance **`study`**"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T20:02:42.732507Z",
     "iopub.execute_input": "2025-02-10T20:02:42.732794Z",
     "iopub.status.idle": "2025-02-10T20:02:42.772002Z",
     "shell.execute_reply.started": "2025-02-10T20:02:42.732765Z",
     "shell.execute_reply": "2025-02-10T20:02:42.771124Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "# get top 10 idx by value & get its value\nids = np.argpartition(scores, -config.K)[:, -config.K:]\nscores = np.take_along_axis(scores, ids, axis=1)\nscores[:5]",
   "execution_count": 23,
   "outputs": [
    {
     "execution_count": 23,
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0.37186688,  0.3992171 ,  0.4394618 ,  0.4884671 ,  0.5174091 ,\n         0.87457657,  0.6083931 ,  0.58975196,  0.8642359 ,  0.78722566],\n       [ 0.4257152 ,  0.4830652 ,  0.50204533,  0.50319844,  0.6602011 ,\n         0.8257402 ,  0.8278695 ,  0.69933504,  0.88355327,  0.8429366 ],\n       [ 0.9651828 ,  0.9905893 ,  1.058987  ,  1.0769    ,  1.2267288 ,\n         1.4366896 ,  1.284672  ,  1.255043  ,  1.2931157 ,  1.2413594 ],\n       [-0.11466098, -0.10546018, -0.1026658 , -0.09919392, -0.0560105 ,\n        -0.03253222,  0.02805819, -0.07913139,  0.00199754, -0.0307106 ],\n       [ 0.5338577 ,  0.558586  ,  0.6305356 ,  0.6141302 ,  0.632113  ,\n         0.7719943 ,  0.6591141 ,  0.8380035 ,  0.65727115,  0.7047775 ]],\n      dtype=float32)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **8.6. Recommendations Matrix**\n\nPrepare the usual format, **`user_id`**, **`item_id`** and rating **`rating`**, which will enable us to quickly evaluate the metrics using **`experiment`** function from **replay**. We need to add both lists to each user & expand them together\n"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T20:02:42.772888Z",
     "iopub.execute_input": "2025-02-10T20:02:42.773157Z",
     "iopub.status.idle": "2025-02-10T20:02:42.798413Z",
     "shell.execute_reply.started": "2025-02-10T20:02:42.773125Z",
     "shell.execute_reply": "2025-02-10T20:02:42.797610Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "# prepare recommendations matrix\ndef prepare_recs(test_users, \n                 rec_item_ids, \n                 rec_relevances):\n    \n    predict = test_users.copy()\n    predict[config.ITEM_COL] = rec_item_ids.tolist()  # add list of indicies for each user\n    predict['rating'] = rec_relevances.tolist() # add rating list of scores for each user\n    predict = predict.explode(column=[config.ITEM_COL, 'rating']).reset_index(drop=True) # expand both lists\n    predict[config.ITEM_COL] = predict[config.ITEM_COL].astype(int)\n    predict['rating'] = predict['rating'].astype(\"double\")\n    return predict\n\n\nmodel_recommendations = prepare_recs(test_users,      # user columns \n                                     rec_item_ids=ids,  # indicies of top 10 in scores\n                                     rec_relevances=scores) # scores of top 10\nmodel_recommendations",
   "execution_count": 24,
   "outputs": [
    {
     "execution_count": 24,
     "output_type": "execute_result",
     "data": {
      "text/plain": "       user_id  item_id    rating\n0         1238     2133  0.371867\n1         1238     1198  0.399217\n2         1238      346  0.439462\n3         1238     1156  0.488467\n4         1238     2620  0.517409\n...        ...      ...       ...\n11215     5309     1115  0.197690\n11216     5309     2130  0.171694\n11217     5309     2480  0.256298\n11218     5309     1505  0.161175\n11219     5309     1795  0.279239\n\n[11220 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>item_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1238</td>\n      <td>2133</td>\n      <td>0.371867</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1238</td>\n      <td>1198</td>\n      <td>0.399217</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1238</td>\n      <td>346</td>\n      <td>0.439462</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1238</td>\n      <td>1156</td>\n      <td>0.488467</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1238</td>\n      <td>2620</td>\n      <td>0.517409</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11215</th>\n      <td>5309</td>\n      <td>1115</td>\n      <td>0.197690</td>\n    </tr>\n    <tr>\n      <th>11216</th>\n      <td>5309</td>\n      <td>2130</td>\n      <td>0.171694</td>\n    </tr>\n    <tr>\n      <th>11217</th>\n      <td>5309</td>\n      <td>2480</td>\n      <td>0.256298</td>\n    </tr>\n    <tr>\n      <th>11218</th>\n      <td>5309</td>\n      <td>1505</td>\n      <td>0.161175</td>\n    </tr>\n    <tr>\n      <th>11219</th>\n      <td>5309</td>\n      <td>1795</td>\n      <td>0.279239</td>\n    </tr>\n  </tbody>\n</table>\n<p>11220 rows × 3 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We'll evaluate the prediction & test overlapping items using **hitrate**, to measure how well the model predicts at least one relevant recommendation for users. **NDCG**, for the evaluation of how well the model can correcly order the relevant items & **coverage** to measure how well the model predicts a range of items from all available items"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T20:02:42.799224Z",
     "iopub.execute_input": "2025-02-10T20:02:42.799504Z",
     "iopub.status.idle": "2025-02-10T20:02:42.803967Z",
     "shell.execute_reply.started": "2025-02-10T20:02:42.799474Z",
     "shell.execute_reply": "2025-02-10T20:02:42.803080Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "metrics = Experiment(\n    [NDCG(config.K), HitRate(config.K), Coverage(config.K)],\n    study.test,\n    study.train,\n    query_column=config.USER_COL, \n    item_column=config.ITEM_COL,\n)",
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-10T20:02:42.804812Z",
     "iopub.execute_input": "2025-02-10T20:02:42.805111Z",
     "iopub.status.idle": "2025-02-10T20:02:43.185473Z",
     "shell.execute_reply.started": "2025-02-10T20:02:42.805081Z",
     "shell.execute_reply": "2025-02-10T20:02:43.184707Z"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "metrics.add_result(\"dssm_model\", model_recommendations)\nmetrics.results",
   "execution_count": 26,
   "outputs": [
    {
     "execution_count": 26,
     "output_type": "execute_result",
     "data": {
      "text/plain": "             NDCG@10  HitRate@10  Coverage@10\ndssm_model  0.058016    0.317291     0.231294",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>NDCG@10</th>\n      <th>HitRate@10</th>\n      <th>Coverage@10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>dssm_model</th>\n      <td>0.058016</td>\n      <td>0.317291</td>\n      <td>0.231294</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}