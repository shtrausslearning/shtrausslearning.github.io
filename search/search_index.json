{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"blog/2023/08/10/named-entity-recognition-with-torch.html","title":"Named Entity Recognition with Torch Loop","text":"<p>In this notebook, we'll take a look at how we can utilise <code>HuggingFace</code> to easily load and use <code>BERT</code> for token classification. Whilst we are loading both the base model &amp; tokeniser from <code>HuggingFace</code>, we'll be using a custom <code>Torch</code> training loop and tail model customisation. The approach isn't the most straightforward but it is one way we can do it. We'll be utilising <code>Massive</code> dataset by Amazon and fine-tune the transformer encoder <code>BERT</code></p> <p></p>","tags":["huggingface","pytorch","ner"]},{"location":"blog/2023/08/10/named-entity-recognition-with-torch.html#background","title":"Background","text":"","tags":["huggingface","pytorch","ner"]},{"location":"blog/2023/08/10/named-entity-recognition-with-torch.html#what-is-ner","title":"What is NER?","text":"<ul> <li>NER is a natural language processing technique which identifies and extracts named entities from unstructured text</li> <li>Named entities refer to words or combination of words that represent specific objects, places etc, in principle it can be anything we define it to be</li> <li>NER algorithms use Machine or Deep Learning algorithms to analyse text and recognise pattens that indicate the presence of a named entity</li> </ul>","tags":["huggingface","pytorch","ner"]},{"location":"blog/2023/08/10/named-entity-recognition-with-torch.html#applications","title":"Applications","text":"<p>Named Entity Recognition has a wide range of applications in the field of Natural Language Processing and Information Retrieval. </p> <p>Few such examples have been listed below<sup>1</sup>:</p> <p>Classifying content for news providers: </p> <p>A large amount of online content is generated by the news and publishing houses on a daily basis and managing them correctly can be a challenging task for the human workers. Named Entity Recognition can automatically scan entire articles and help in identifying and retrieving major people, organizations, and places discussed in them. Thus articles are automatically categorized in defined hierarchies and the content is also much easily discovered. </p> <p>Automatically Summarizing Resumes: </p> <p>You might have come across various tools that scan your resume and retrieve important information such as Name, Address, Qualification, etc from them. The majority of such tools use the NER software which helps it to retrieve such information. Also one of the challenging tasks faced by the HR Departments across companies is to evaluate a gigantic pile of resumes to shortlist candidates. A lot of these resumes are excessively populated in detail, of which, most of the information is irrelevant to the evaluator. Using the NER model, the relevant information to the evaluator can be easily retrieved from them thereby simplifying the effort required in shortlisting candidates among a pile of resumes.</p> <p>Optimizing Search Engine Algorithms: </p> <p>When designing a search engine algorithm, It would be an inefficient and computational task to search for an entire query across the millions of articles and websites online, an alternate way is to run a NER model on the articles once and store the entities associated with them permanently. Thus for a quick and efficient search, the key tags in the search query can be compared with the tags associated with the website articles</p> <p>Powering  Recommendation systems: </p> <p>NER can be used in developing algorithms for recommender systems that make suggestions based on our search history or on our present activity. This is achieved by extracting the entities associated with the content in our history or previous activity and comparing them with the label assigned to other unseen content. Thus we frequently see the content of our interest.</p> <p>Simplifying Customer Support: </p> <p>Usually, a company gets tons of customer complaints and feedback on a daily basis, and going through each one of them and recognizing the concerned parties is not an easy task. Using NER we can recognize relevant entities in customer complaints and feedback such as Product specifications, department, or company branch location so that the feedback is classified accordingly and forwarded to the appropriate department responsible for the identified product.</p>","tags":["huggingface","pytorch","ner"]},{"location":"blog/2023/08/10/named-entity-recognition-with-torch.html#the-dataset","title":"The Dataset","text":"<p>To realise <code>NER</code> with Hugginface, we'll be utilising a muli-language dataset <code>massive</code></p>","tags":["huggingface","pytorch","ner"]},{"location":"blog/2023/08/10/named-entity-recognition-with-torch.html#massive-11","title":"Massive 1.1","text":"<p>Let's load our dataset MASSIVE; the dataset can be found on huggingface</p> <p>MASSIVE 1.1 is a parallel dataset of &gt; 1M utterances across 52 languages with annotations for the Natural Language Understanding tasks of intent prediction and slot annotation. Utterances span 60 intents and include 55 slot types. MASSIVE was created by localizing the SLURP dataset, composed of general Intelligent Voice Assistant single-shot interactions.</p> <p>We will be utilising this dataset for Named Entity Recognition, let's load our dataset, selecting only a subset of the data <code>ru-RU</code>, which is a subset of column <code>locale</code></p> <p>There are quite a number of ways we can extract data from this dataset, let's use the fastest way: - By specifying which <code>locale</code> group we want to load (so we don't load everything) - Define the subset of rows we want to use by using <code>split</code> (eg. train[10:100])</p> <pre><code>from datasets import load_dataset, Dataset\n\ntrain_dataset = load_dataset('AmazonScience/massive', \"ru-RU\",split=\"train[:100]\")\ntest_dataset = load_dataset('AmazonScience/massive', \"ru-RU\",split=\"test[:100]\")\n</code></pre>","tags":["huggingface","pytorch","ner"]},{"location":"blog/2023/08/10/named-entity-recognition-with-torch.html#relevant-columns","title":"Relevant Columns","text":"<p>Some samples from our documents, located in column <code>utt</code> and NER Annotations are located in <code>annot_utt</code>, which is in the format <code>[tag : tokens]</code></p> <pre><code>train_dataset['utt'][:10] \n</code></pre> <pre><code>['\u0440\u0430\u0437\u0431\u0443\u0434\u0438 \u043c\u0435\u043d\u044f \u0432 \u0434\u0435\u0432\u044f\u0442\u044c \u0443\u0442\u0440\u0430 \u0432 \u043f\u044f\u0442\u043d\u0438\u0446\u0443',\n'\u043f\u043e\u0441\u0442\u0430\u0432\u044c \u0431\u0443\u0434\u0438\u043b\u044c\u043d\u0438\u043a \u043d\u0430 \u0434\u0432\u0430 \u0447\u0430\u0441\u0430 \u0432\u043f\u0435\u0440\u0435\u0434',\n'\u043e\u043b\u043b\u0438 \u0442\u0438\u0445\u043e',\n'\u043e\u0442\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0441\u044c',\n'\u043e\u043b\u043b\u0438 \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0441\u044c \u043d\u0430 \u0434\u0435\u0441\u044f\u0442\u044c \u0441\u0435\u043a\u0443\u043d\u0434',\n'\u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0441\u044c \u043d\u0430 \u0434\u0435\u0441\u044f\u0442\u044c \u0441\u0435\u043a\u0443\u043d\u0434',\n'\u0441\u0434\u0435\u043b\u0430\u0439 \u043e\u0441\u0432\u0435\u0449\u0435\u043d\u0438\u0435 \u0437\u0434\u0435\u0441\u044c \u0447\u0443\u0442\u044c \u0431\u043e\u043b\u0435\u0435 \u0442\u0451\u043f\u043b\u044b\u043c',\n'\u043f\u043e\u0436\u0430\u043b\u0443\u0439\u0441\u0442\u0430 \u0441\u0434\u0435\u043b\u0430\u0439 \u0441\u0432\u0435\u0442 \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0449\u0438\u0439 \u0434\u043b\u044f \u0447\u0442\u0435\u043d\u0438\u044f',\n'\u0432\u0440\u0435\u043c\u044f \u0438\u0434\u0442\u0438 \u0441\u043f\u0430\u0442\u044c',\n'\u043e\u043b\u043b\u0438 \u0432\u0440\u0435\u043c\u044f \u0441\u043f\u0430\u0442\u044c']\n</code></pre> <pre><code>train_dataset['annot_utt'][:10]\n</code></pre> <pre><code>['\u0440\u0430\u0437\u0431\u0443\u0434\u0438 \u043c\u0435\u043d\u044f \u0432 [time : \u0434\u0435\u0432\u044f\u0442\u044c \u0443\u0442\u0440\u0430] \u0432 [date : \u043f\u044f\u0442\u043d\u0438\u0446\u0443]',\n'\u043f\u043e\u0441\u0442\u0430\u0432\u044c \u0431\u0443\u0434\u0438\u043b\u044c\u043d\u0438\u043a [time : \u043d\u0430 \u0434\u0432\u0430 \u0447\u0430\u0441\u0430 \u0432\u043f\u0435\u0440\u0435\u0434]',\n'\u043e\u043b\u043b\u0438 \u0442\u0438\u0445\u043e',\n'\u043e\u0442\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0441\u044c',\n'\u043e\u043b\u043b\u0438 \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0441\u044c \u043d\u0430 [time : \u0434\u0435\u0441\u044f\u0442\u044c \u0441\u0435\u043a\u0443\u043d\u0434]',\n'\u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0441\u044c \u043d\u0430 [time : \u0434\u0435\u0441\u044f\u0442\u044c \u0441\u0435\u043a\u0443\u043d\u0434]',\n'\u0441\u0434\u0435\u043b\u0430\u0439 \u043e\u0441\u0432\u0435\u0449\u0435\u043d\u0438\u0435 \u0437\u0434\u0435\u0441\u044c \u0447\u0443\u0442\u044c \u0431\u043e\u043b\u0435\u0435 [color_type : \u0442\u0451\u043f\u043b\u044b\u043c]',\n'\u043f\u043e\u0436\u0430\u043b\u0443\u0439\u0441\u0442\u0430 \u0441\u0434\u0435\u043b\u0430\u0439 \u0441\u0432\u0435\u0442 [color_type : \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0449\u0438\u0439 \u0434\u043b\u044f \u0447\u0442\u0435\u043d\u0438\u044f]',\n'\u0432\u0440\u0435\u043c\u044f \u0438\u0434\u0442\u0438 \u0441\u043f\u0430\u0442\u044c',\n'\u043e\u043b\u043b\u0438 \u0432\u0440\u0435\u043c\u044f \u0441\u043f\u0430\u0442\u044c']\n</code></pre>","tags":["huggingface","pytorch","ner"]},{"location":"blog/2023/08/10/named-entity-recognition-with-torch.html#text-preprocessing","title":"Text Preprocessing","text":"","tags":["huggingface","pytorch","ner"]},{"location":"blog/2023/08/10/named-entity-recognition-with-torch.html#define-tokeniser-model","title":"Define Tokeniser &amp; Model","text":"<p>For preprocessing we'll need a tokeniser, so let's define the tokeniser &amp; the base model</p> <pre><code>from transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruBert-base\")\nMODEL = AutoModel.from_pretrained(\"ai-forever/ruBert-base\")\n# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")    # multilingual                      \n# model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")            # multilingual\n# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")               # en only\n# model = AutoModel.from_pretrained(\"bert-base-uncased\")                       # en only\n</code></pre>","tags":["huggingface","pytorch","ner"]},{"location":"blog/2023/08/10/named-entity-recognition-with-torch.html#ner-format","title":"NER format","text":"<p>NER tagging formats can vary quite a bit, let's utilise class <code>Parser</code> to preprocess the NER format in our dataset</p> <p>Class <code>Parser</code> contains the relevant preprocessing steps we will take: - The class will be called when creating our <code>Dataset</code>, creating NER tags for each document - Its input will be the document &amp; annotated document     - The document itself is imported in order to create O tags     - Whilst the annotated document is used to extract the annotated tags     - Tag dictionaries are created <code>tag_to_id</code>, <code>id_to_tag</code></p> <pre><code>from typing import List\nimport regex as re\n\n'''\n\nPARSER FOR THE DATASET NER TAG FORMAT\n\n'''\n\nclass Parser:\n\n    # RE patterns for tag extraction\n    LABEL_PATTERN = r\"\\[(.*?)\\]\"\n    PUNCTUATION_PATTERN = r\"([.,\\/#!$%\\^&amp;\\*;:{}=\\-_`~()'\\\"\u2019\u00bf])\"\n\n    # initialise, first word/id tag is O (outside)\n    def __init__(self):\n        self.tag_to_id = {\n            \"O\": 0\n        }\n        self.id_to_tag = {\n            0: \"O\"\n        }\n\n    '''\n\n    CREATE TAGS\n\n    '''\n    # input : sentence, tagged sentence\n\n    def __call__(self, sentence: str, annotated: str) -&gt; List[str]:\n\n        ''' Create Dictionary of Identified Tags'''\n\n        # 1. set label B or I    \n\n        matches = re.findall(self.LABEL_PATTERN, annotated)\n        word_to_tag = {}\n        for match in matches:\n            tag, phrase = match.split(\" : \")\n            words = phrase.split(\" \") \n            word_to_tag[words[0]] = f\"B-{tag.upper()}\"\n            for w in words[1:]:\n                word_to_tag[w] = f\"I-{tag.upper()}\"\n\n        ''' Tokenise Sentence &amp; add tags to not tagged words (O)'''\n\n        # 2. add token tag to main tag dictionary\n\n        tags = []\n        sentence = re.sub(self.PUNCTUATION_PATTERN, r\" \\1 \", sentence)\n        for w in sentence.split():\n            if w not in word_to_tag:\n                tags.append(\"O\")\n            else:\n                tags.append(word_to_tag[w])\n                self.__add_tag(word_to_tag[w])\n\n        return tags\n\n    '''\n\n    TAG CONVERSION\n\n    '''\n    # to word2id (tag_to_id)\n    # to id2word (id_to_tag)\n\n    def __add_tag(self, tag: str):\n        if tag in self.tag_to_id:\n            return\n        id_ = len(self.tag_to_id)\n        self.tag_to_id[tag] = id_\n        self.id_to_tag[id_] = tag\n\n    ''' Get Tag Number ID '''\n    # or just number id for token\n\n    def get_id(self, tag: str):\n        return self.tag_to_id[tag]\n\n    ''' Get Tag Token from Number ID'''\n    # given id get its token\n\n    def get_label(self, id_: int):\n        return self.get_tag_label(id_)\n\nparser = Parser()\nparser(train_dataset[\"utt\"][0], train_dataset[\"annot_utt\"][0])\n</code></pre> <pre><code>['O', 'O', 'O', 'B-TIME', 'I-TIME', 'O', 'B-DATE']\n</code></pre>","tags":["huggingface","pytorch","ner"]},{"location":"blog/2023/08/10/named-entity-recognition-with-torch.html#create-dataset","title":"Create Dataset","text":"<p>The functions <code>NERDataset</code> is our Dataset class, it requires the <code>Parser</code> class instance </p> <p>Input into the <code>parser</code> orbjects are <code>utt</code> &amp; its annotated version <code>annot_utt</code></p> <p>The output from <code>parser</code> will be a list of target variable tags <code>BIO</code> tags</p> <pre><code>['O', 'O', 'O', 'B-TIME', 'I-TIME', 'O', 'B-DATE']\n['O', 'O', 'B-TIME', 'I-TIME', 'I-TIME', 'I-TIME']\n['O', 'O']\n['O']\n['O', 'O', 'O', 'B-TIME', 'I-TIME']\n['O', 'O', 'B-TIME', 'I-TIME']\n['O', 'O', 'O', 'O', 'O', 'B-COLOR_TYPE']\n['O', 'O', 'O', 'B-COLOR_TYPE', 'I-COLOR_TYPE', 'I-COLOR_TYPE']\n['O', 'O', 'O']\n</code></pre> <p>The <code>NERDataset</code> output each item (document) looks like the following:</p> <pre><code>tmp[idx] = {\n    **tokenizer_output,                             # BERT related token inputs\n    \"subword_group\": torch.tensor(subword_group),   # token/word association (start,number of tokens in word)\n    \"target\": torch.tensor(target)                  # word NER tag\n}\n</code></pre> <p>Now the class itself:</p> <pre><code>from torch.utils.data import Dataset, DataLoader\n\nclass NERDataset(Dataset):\n    def __init__(self, dataset, tokenizer):\n        self.tokenizer = tokenizer\n        self.processed_data = self.__preprocess(dataset)\n\n    def __len__(self):\n        return len(self.processed_data)\n\n    def __getitem__(self, idx):\n        return self.processed_data[idx]\n\n    def __preprocess(self, dataset):\n\n        tmp = {}\n        for idx in tqdm(range(len(dataset))):\n            item = dataset[idx]\n            tags = parser(item[\"utt\"], item[\"annot_utt\"])     # get list of tags\n            tokenizer_output = self.tokenizer(item[\"utt\"],    # tokenise document (incl. &lt;bos&gt;,&lt;eos&gt;)\n                                              padding=True, \n                                              truncation=True, \n                                              return_tensors='pt')\n\n            # token word identifier (each word can have multiple tokens)\n            word_ids = tokenizer_output.word_ids() \n\n            # for each word, how many subtokens are there (starts with 1 - first word)\n            subword_group = [\n                (key + 1, len(list(group))) \n                for key, group in itertools.groupby(word_ids) \n                    if key is not None\n            ] # index to aggregate tokens\n\n            # define bio tags for each word in numerical format using parser\n            target = [parser.get_id(t) for t in tags] \n\n            # group all relevant data that will be used in forward pass\n            tmp[idx] = {\n                **tokenizer_output,\n                \"subword_group\": torch.tensor(subword_group),\n                \"target\": torch.tensor(target)\n            }\n\n            # check consistency\n            try:\n                assert (len(subword_group) == len(target))\n            except:\n                print(item[\"annot_utt\"], subword_group, target)\n\n        return tmp\n\ntrain = NERDataset(train_dataset, tokenizer)\ntest = NERDataset(test_dataset, tokenizer)\n</code></pre> <pre><code>  0%|          | 0/1000 [00:00&lt;?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:00&lt;00:00, 1690.12it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:00&lt;00:00, 1598.76it/s]\n</code></pre>","tags":["huggingface","pytorch","ner"]},{"location":"blog/2023/08/10/named-entity-recognition-with-torch.html#bert-multiclass-classifier","title":"BERT Multiclass Classifier","text":"<p>We will use a rather custom approach to <code>NER</code>, we have loaded our <code>encoder</code> model, so we'll need to create a tail end for it, so we can use it for <code>NER</code> multiclass classification</p>","tags":["huggingface","pytorch","ner"]},{"location":"blog/2023/08/10/named-entity-recognition-with-torch.html#model-architecture","title":"Model Architecture","text":"<p>Let's create a custom training &amp; evaluation loop for our NER multiclass classification problem</p> <ul> <li>In total we have 34 classification tags (including O), so we can define the tail output dimension</li> <li>Training the BERT model using <code>input_ids</code> &amp; <code>attention_mask</code> (tokens) we extract the hidden state for all tokens in document</li> <li>Using the created <code>subword_group</code> data, we extract the token embeddings that correpospond to each word in the document</li> <li>For words which have more than one token, we average token embeddings to obtain word embeddings</li> <li>The word embeddings are passed to the linear tail classifier, which returns the logits for each word in the sentence</li> <li>To get the most probable tag for each word in the document, we use <code>torch.argmax</code></li> </ul> <p>To utilise our custom model with methods such as <code>.from_pretrained</code>, let's add <code>PreTrainedModel</code>, which contains <code>nn.Module</code></p> <pre><code>import torch.nn as nn\nfrom transformers import PreTrainedModel\nfrom transformers import PretrainedConfig\nfrom transformers import AutoModel, AutoConfig\n\nclass MyConfig(PretrainedConfig):\n    model_type = 'mymodel'\n    def __init__(self, important_param=42, **kwargs):\n        super().__init__(**kwargs)\n        self.important_param = important_param\n\n# PreTrainedModel has nn.Module\n\nclass NERClassifier(PreTrainedModel):\n\n    config_class = MyConfig\n    def __init__(self,config):\n        super().__init__(config)\n        self.bert = MODEL\n        self.seq = nn.Sequential(\n            nn.Linear(768, 256), \n            nn.ReLU(),\n            nn.Linear(256, CLASSES),\n        )\n\n    def forward(self, inputs): # returns list of targets\n\n        # standard inputs for BERT\n        bert_output = self.bert(\n            inputs[\"input_ids\"],\n            inputs[\"attention_mask\"]\n        )\n\n        # output of transformer encoder will be our hidden state for \n        # each input_ids \n        last_hidden_state = bert_output[\"last_hidden_state\"]\n\n        # tokens correspond tokenizer divisions \n        # each word can be split into multiple tokens\n        # ie. get the mean word embedding \n\n        target = []\n        for group in inputs[\"subword_group\"]:\n            b, e = group\n            word_embedding = last_hidden_state[:, b:b+e]       # get the token embeddings\n            agg_embedding = torch.mean(word_embedding, dim=1)  # mean word embeddings for tokens\n\n            # input mean word embedding (1,768) pass into nn.Sequential linear tail end\n            proba = self.__forward_one(agg_embedding)    # logits data \n            target.append(proba)\n\n        word_logits = torch.stack(target).squeeze(1)\n\n        return word_logits\n\n    def __forward_one(self, x):\n        logits = self.seq(x)\n        return logits\n</code></pre>","tags":["huggingface","pytorch","ner"]},{"location":"blog/2023/08/10/named-entity-recognition-with-torch.html#model-additions","title":"Model Additions","text":"<p>Aside from defining the classification model architecture for our base model BERT - We need to create dataloaders, instantiate our model (ie tail weights need to be initialised) - Define the model evaluation criterion (loss function)  - Define the relevant optimiser to update weights</p> <pre><code>import torch.optim as optim\n\n# define data loaders\ntrain_loader = DataLoader(train)\ntest_loader = DataLoader(test)\nCLASSES = len(parser.tag_to_id); print(f'{CLASSES} labels')\nconfig = MyConfig(4)\n\n# define classifier model, loss fucntion &amp; optimiser\nclf = NERClassifier(config).to(device)\ncriterion = nn.CrossEntropyLoss()  # for multiclass classification \noptimizer = optim.Adam(clf.parameters(), lr=1e-5)\n</code></pre> <p>In total, we have created 63 tags in <code>word_to_tag</code> </p> <pre><code>63 labels\n</code></pre>","tags":["huggingface","pytorch","ner"]},{"location":"blog/2023/08/10/named-entity-recognition-with-torch.html#train-model","title":"Train Model","text":"<ul> <li>Let's train our model for 20 epochs on <code>train_loader</code> &amp; validate the model using <code>test_loader</code> </li> <li>To check how well the model is performing during training, we will use the <code>accuracy</code> metric</li> </ul> <pre><code>from sklearn.metrics import accuracy_score\n\nfor epoch in range(20):\n\n    '''\n\n    (1) TRAINING LOOP\n\n    '''\n\n    loss_count, loss_sum = 0, 0\n    y_true, y_pred = [], []\n\n    # switch to training mode, ie backpropagation on\n    clf.train()\n    for data in tqdm(train_loader):\n\n        # move data to device\n        inputs = {\n            key: val.squeeze(0).to(device)\n            for key, val in data.items()\n        }\n\n        # logits of belonging to each of the tag class \n        # for all words in document\n        outputs = clf(inputs)\n\n        # predicted word tag \n        word_tag = torch.argmax(outputs, dim=1).tolist() \n\n        y_true.extend(inputs[\"target\"].tolist())\n        y_pred.extend(word_tag)        \n\n        # calcualate loss\n        loss = criterion(outputs, inputs[\"target\"])\n        loss_count += 1\n        loss_sum += loss.item()\n\n#         nn.utils.clip_grad_norm_(\n#             parameters=clf.parameters(), max_norm=20\n#         )\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch-{epoch + 1}: loss: {loss_sum / loss_count}; acc: {accuracy_score(y_true, y_pred)}\")\n\n    '''\n\n    (2) VALIDATION LOOP\n\n    '''\n\n    test_loss_sum, test_loss_count = 0, 0\n    test_true, test_pred = [], []\n\n    # switch to inference mode\n    with torch.no_grad():\n        for test_rows in tqdm(test_loader):\n\n            # move data to device\n            test_inputs = {\n                key: val.squeeze(0).to(device)\n                for key, val in test_rows.items()\n            }\n            test_outputs = clf(test_inputs)\n\n            # add metric data\n            test_true.extend(test_inputs[\"target\"].tolist())\n            test_pred.extend(torch.argmax(test_outputs, dim=1).tolist())\n\n            test_loss = criterion(test_outputs, test_inputs[\"target\"])\n            test_loss_count += 1\n            test_loss_sum += test_loss.item()\n\n\n    print(f\"Epoch-{epoch + 1}: loss: {loss_sum / loss_count}; acc: {accuracy_score(y_true, y_pred)},\\\n          val_loss: {test_loss_sum / test_loss_count}, val_acc: {accuracy_score(test_true, test_pred)}\")\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:02&lt;00:00, 16.09it/s]\nEpoch-1: loss: 1.6441781679093839; acc: 0.7374777271827361\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 88.19it/s]\nEpoch-1: loss: 1.6441781679093839; acc: 0.7374777271827361,          val_loss: 1.2796487891450525, val_acc: 0.7421347230264428\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:00&lt;00:00, 16.50it/s]\nEpoch-2: loss: 1.1323998833782971; acc: 0.7539101168085528\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 87.29it/s]\nEpoch-2: loss: 1.1323998833782971; acc: 0.7539101168085528,          val_loss: 1.065644938390702, val_acc: 0.7639451843273499\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:01&lt;00:00, 16.32it/s]\nEpoch-3: loss: 0.8694944252893329; acc: 0.7895466244308058\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 85.78it/s]\nEpoch-3: loss: 0.8694944252893329; acc: 0.7895466244308058,          val_loss: 0.9206145468372852, val_acc: 0.7815093611271955\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:58&lt;00:00, 16.95it/s]\nEpoch-4: loss: 0.6839374071029015; acc: 0.8204315977034251\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 84.64it/s]\nEpoch-4: loss: 0.6839374071029015; acc: 0.8204315977034251,          val_loss: 0.8384006800730712, val_acc: 0.7915460335842501\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.81it/s]\nEpoch-5: loss: 0.5615229318903293; acc: 0.8445852306473965\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 87.55it/s]\nEpoch-5: loss: 0.5615229318903293; acc: 0.8445852306473965,          val_loss: 0.7858886199912521, val_acc: 0.7979154603358425\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.78it/s]\nEpoch-6: loss: 0.4753220743876882; acc: 0.8643832904375371\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 88.00it/s]\nEpoch-6: loss: 0.4753220743876882; acc: 0.8643832904375371,          val_loss: 0.8105039822029648, val_acc: 0.7971434086083767\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.81it/s]\nEpoch-7: loss: 0.3950597488232888; acc: 0.8774500098990299\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 84.88it/s]\nEpoch-7: loss: 0.3950597488232888; acc: 0.8774500098990299,          val_loss: 0.7834225822311127, val_acc: 0.8062150164060992\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.88it/s]\nEpoch-8: loss: 0.33086000567564045; acc: 0.8948723025143536\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 84.73it/s]\nEpoch-8: loss: 0.33086000567564045; acc: 0.8948723025143536,          val_loss: 0.767055139105185, val_acc: 0.8096892491796951\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:00&lt;00:00, 16.50it/s]\nEpoch-9: loss: 0.28477367215882987; acc: 0.9103147891506632\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 85.45it/s]\nEpoch-9: loss: 0.28477367215882987; acc: 0.9103147891506632,          val_loss: 0.7778430652543611, val_acc: 0.8029337965643698\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.81it/s]\nEpoch-10: loss: 0.23926631732314127; acc: 0.9227875668184518\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 88.43it/s]\nEpoch-10: loss: 0.23926631732314127; acc: 0.9227875668184518,          val_loss: 0.7841836358316068, val_acc: 0.8120054043620922\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.82it/s]\nEpoch-11: loss: 0.20171416073056753; acc: 0.9362502474757474\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 84.28it/s]\nEpoch-11: loss: 0.20171416073056753; acc: 0.9362502474757474,          val_loss: 0.849082443419844, val_acc: 0.8104613009071607\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.83it/s]\nEpoch-12: loss: 0.17783702248180636; acc: 0.9459512967729162\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 87.20it/s]\nEpoch-12: loss: 0.17783702248180636; acc: 0.9459512967729162,          val_loss: 0.7976406391558412, val_acc: 0.8046709129511678\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.88it/s]\nEpoch-13: loss: 0.14918355378504203; acc: 0.9503068699267472\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 87.37it/s]\nEpoch-13: loss: 0.14918355378504203; acc: 0.9503068699267472,          val_loss: 0.8160117758086053, val_acc: 0.8118123914302259\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:00&lt;00:00, 16.60it/s]\nEpoch-14: loss: 0.1313198971484453; acc: 0.962383686398733\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 88.59it/s]\nEpoch-14: loss: 0.1313198971484453; acc: 0.962383686398733,          val_loss: 0.8072414025840553, val_acc: 0.815479637135688\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.84it/s]\nEpoch-15: loss: 0.11704726772185677; acc: 0.9633735893882399\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 85.19it/s]\nEpoch-15: loss: 0.11704726772185677; acc: 0.9633735893882399,          val_loss: 0.8006598546078457, val_acc: 0.8152866242038217\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.79it/s]\nEpoch-16: loss: 0.09824711217604636; acc: 0.9742625222728173\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 88.90it/s]\nEpoch-16: loss: 0.09824711217604636; acc: 0.9742625222728173,          val_loss: 0.8438674144655451, val_acc: 0.8177957923180853\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.79it/s]\nEpoch-17: loss: 0.0852395541092883; acc: 0.9750544446644229\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 88.00it/s]\nEpoch-17: loss: 0.0852395541092883; acc: 0.9750544446644229,          val_loss: 0.8193156978896586, val_acc: 0.8143215595444895\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.83it/s]\nEpoch-18: loss: 0.07844399727860218; acc: 0.9784201148287468\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 88.51it/s]\nEpoch-18: loss: 0.07844399727860218; acc: 0.9784201148287468,          val_loss: 0.8360713951853868, val_acc: 0.8251302837290099\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.86it/s]\nEpoch-19: loss: 0.07729116444718602; acc: 0.9788160760245496\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 85.81it/s]\nEpoch-19: loss: 0.07729116444718602; acc: 0.9788160760245496,          val_loss: 0.8346432074703807, val_acc: 0.8210770121598147\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.82it/s]\nEpoch-20: loss: 0.06690881398832789; acc: 0.9798059790140566\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 88.31it/s]\nEpoch-20: loss: 0.06690881398832789; acc: 0.9798059790140566,          val_loss: 0.8577917314651385, val_acc: 0.8183748311136846\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.80it/s]\nEpoch-21: loss: 0.05367027178355602; acc: 0.985349435755296\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 88.59it/s]\nEpoch-21: loss: 0.05367027178355602; acc: 0.985349435755296,          val_loss: 0.8853590850097044, val_acc: 0.8235861802740784\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.79it/s]\nEpoch-22: loss: 0.04562243979116829; acc: 0.9879231835280142\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 88.02it/s]\nEpoch-22: loss: 0.04562243979116829; acc: 0.9879231835280142,          val_loss: 0.8947100080056626, val_acc: 0.8251302837290099\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.80it/s]\nEpoch-23: loss: 0.04487149683444659; acc: 0.9875272223322115\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 83.96it/s]\nEpoch-23: loss: 0.04487149683444659; acc: 0.9875272223322115,          val_loss: 0.9012678128228526, val_acc: 0.8257093225246092\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.83it/s]\nEpoch-24: loss: 0.043597472023681805; acc: 0.9869332805385073\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 88.64it/s]\nEpoch-24: loss: 0.043597472023681805; acc: 0.9869332805385073,          val_loss: 0.9172096501152982, val_acc: 0.8272534259795407\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.78it/s]\nEpoch-25: loss: 0.039683220311884725; acc: 0.9879231835280142\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 89.10it/s]\nEpoch-25: loss: 0.039683220311884725; acc: 0.9879231835280142,          val_loss: 0.9694385796532524, val_acc: 0.8204979733642154\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.79it/s]\nEpoch-26: loss: 0.03646317584309327; acc: 0.9899029895070283\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 88.90it/s]\nEpoch-26: loss: 0.03646317584309327; acc: 0.9899029895070283,          val_loss: 0.9861661683519687, val_acc: 0.8266743871839414\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.75it/s]\nEpoch-27: loss: 0.03880691091310382; acc: 0.9893090477133241\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 84.44it/s]\nEpoch-27: loss: 0.03880691091310382; acc: 0.9893090477133241,          val_loss: 0.9058586812789617, val_acc: 0.8210770121598147\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.83it/s]\nEpoch-28: loss: 0.037079696985995725; acc: 0.9887151059196199\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 87.97it/s]\nEpoch-28: loss: 0.037079696985995725; acc: 0.9887151059196199,          val_loss: 0.925899648670832, val_acc: 0.8272534259795407\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.94it/s]\nEpoch-29: loss: 0.023646224649064154; acc: 0.9932686596713522\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 88.34it/s]\nEpoch-29: loss: 0.023646224649064154; acc: 0.9932686596713522,          val_loss: 0.9826786444298395, val_acc: 0.8230071414784791\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:59&lt;00:00, 16.94it/s]\nEpoch-30: loss: 0.030056740869996247; acc: 0.9914868342902395\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:11&lt;00:00, 88.16it/s]\nEpoch-30: loss: 0.030056740869996247; acc: 0.9914868342902395,          val_loss: 1.0284815851225262, val_acc: 0.8245512449334106\n</code></pre>","tags":["huggingface","pytorch","ner"]},{"location":"blog/2023/08/10/named-entity-recognition-with-torch.html#save-model","title":"Save Model","text":"<p>We can save the model for future use</p> <pre><code>clf.save_pretrained('./my_model_dir')\ntokenizer.save_pretrained('./my_model_dir')\n</code></pre> <pre><code>('./my_model_dir/tokenizer_config.json',\n './my_model_dir/special_tokens_map.json',\n './my_model_dir/vocab.txt',\n './my_model_dir/added_tokens.json',\n './my_model_dir/tokenizer.json')\n</code></pre>","tags":["huggingface","pytorch","ner"]},{"location":"blog/2023/08/10/named-entity-recognition-with-torch.html#using-our-ner-tagger","title":"Using our NER tagger","text":"<p>We can use our <code>NER</code> tagger by loading a stack of documents and creating a <code>dataset</code>, as we did for the <code>train</code> &amp; <code>test</code> subsets or we can use it for a single sentence, I'll be using a simple inference function.</p>","tags":["huggingface","pytorch","ner"]},{"location":"blog/2023/08/10/named-entity-recognition-with-torch.html#loading-model","title":"Loading Model","text":"<p>If we want to load the fine-tuned <code>model</code>, we can use <code>.from_pretrained</code> and specify the folder where ther model is located,  the <code>tokenizer</code> can be loaded in a similar way</p> <pre><code>new_model = NERClassifier.from_pretrained('./my_model_dir')\nnew_model\n</code></pre> <pre><code>NERClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(120138, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (seq): Sequential(\n    (0): Linear(in_features=768, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=256, out_features=63, bias=True)\n  )\n)\n</code></pre>","tags":["huggingface","pytorch","ner"]},{"location":"blog/2023/08/10/named-entity-recognition-with-torch.html#finding-ner-tags-in-a-document","title":"Finding NER tags in a document","text":"<p>We can use our model using something similar to the code below &amp; utilise the same <code>tokenizer</code>, the <code>parser</code> &amp; saved <code>model</code></p> <pre><code>def ner_inference(text,model):\n\n    # Tokenise input\n    tokenizer_output = tokenizer(text,    \n                                 padding=True, \n                                 truncation=True, \n                                 return_tensors='pt')\n\n    # token word identifier (each word can have multiple tokens)\n    word_ids = tokenizer_output.word_ids() \n\n    # for each word, how many subtokens are there (starts with 1 - first word)\n    subword_group = [\n        (key + 1, len(list(group))) \n        for key, group in itertools.groupby(word_ids) \n            if key is not None\n    ] # index to aggregate tokens\n\n    # group all relevant data that will be used in forward pass \n    output = {\n        **tokenizer_output,\n        \"subword_group\": torch.tensor(subword_group),\n    }\n\n    # get the highest logits value\n    tag_pred = torch.argmax(model(output),axis=1).tolist()\n    tag_pred \n\n    return tag_pred\n\nner_inference('\u0412 \u0434\u0435\u0432\u044f\u0442\u044c \u0443\u0442\u0440\u0430 \u044f \u0443\u043b\u0435\u0442\u0430\u044e \u0432 \u0417\u0438\u043c\u0431\u0430\u0431\u0432\u0435',new_model)\n</code></pre> <p>The model predicts the following tags, which we can map using <code>parser.id_to_tag</code></p> <pre><code>[0, 2, 0, 0, 0, 0, 0]\n</code></pre>","tags":["huggingface","pytorch","ner"]},{"location":"blog/2023/08/10/named-entity-recognition-with-torch.html#references","title":"REFERENCES","text":"<ol> <li> <p>https://www.mygreatlearning.com/blog/named-entity-recognition/#\u00a0\u21a9</p> </li> </ol>","tags":["huggingface","pytorch","ner"]},{"location":"blog/2023/08/20/data-preprocessing-with-pyspark.html","title":"Data Preprocessing with PySpark","text":"<p>In this post, we will introduce ourselves to <code>pyspark</code>, a framework that allows us to work with big data</p> <ul> <li>We'll look at how to start a <code>spark_session</code></li> <li>Setting up data types for the dataset using <code>StructType</code></li> <li>This post focuses on data preparation in the preprocessing state</li> </ul>","tags":["pyspark","preprocessing"]},{"location":"blog/2023/08/20/data-preprocessing-with-pyspark.html#introduction","title":"Introduction","text":"<p>In this post, we'll introduce ourselves to <code>pyspark</code>, working on a commonly used classification problem; the titanic. Our focus will be to learn the basics of how to work with <code>pyspark</code> when we work on machine learning projects. We'll split this little project into two parts; part I part will include data loading, data preprocessing (feature engineering and data cleaning). Second part, part II will include data preparation for machine learning and subsequent model training and evaluation</p>","tags":["pyspark","preprocessing"]},{"location":"blog/2023/08/20/data-preprocessing-with-pyspark.html#spark-over-pandas","title":"Spark over Pandas","text":"<p>In the previous post, we used <code>pandas</code> for working with tabular data. <code>pandas</code> is indeed quite convenient to use as it has a very rich functionality to work with tabular data, <code>pyspark</code> in comparison is much simplier, however it offers the user to work with <code>big data</code>, which <code>pandas</code> tends to strugle with.</p> <p>Some advantages of <code>pyspark</code> over <code>pandas</code>:</p> <ol> <li>Scalability: PySpark is designed to handle large datasets that cannot be processed on a single machine. It can distribute the processing of data across a cluster of machines, which makes it suitable for big data applications. Pandas, on the other hand, is limited by the memory of a single machine. </li> <li>Speed: PySpark is faster than pandas when dealing with large datasets. This is because PySpark uses distributed computing and can process data in parallel across multiple machines. Pandas, on the other hand, is limited by the processing power of a single machine. </li> <li>Machine learning: PySpark has built-in machine learning libraries such as MLlib and MLflow, which makes it easy to perform machine learning tasks on large datasets. Pandas does not have built-in machine learning libraries. </li> </ol> <p>Overall, <code>pyspark</code> is a better choice for big data applications that require distributed computing and machine learning capabilities. Pandas is suitable for smaller datasets that can be processed on a single machine.</p>","tags":["pyspark","preprocessing"]},{"location":"blog/2023/08/20/data-preprocessing-with-pyspark.html#start-spark-session","title":"Start Spark Session","text":"<p>First of all, we create a <code>spark session</code>, importing <code>SparkSession</code> from <code>pyspark.sql</code> and creating an instance, to which we will refence to when reading our data</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql import functions as f\n\n# create spark session\nspark = SparkSession.builder\\\n                    .appName('titanic')\\\n                    .getOrCreate()\n</code></pre>","tags":["pyspark","preprocessing"]},{"location":"blog/2023/08/20/data-preprocessing-with-pyspark.html#loading-data","title":"Loading Data","text":"<p>The dataset can be found from this source, which we can simply download using <code>wget</code> in Jupyter</p> <p><code>pyspark</code> supports a variety of input formats, to load a <code>csv</code> file, we can call <code>spark.read</code>. When reading our table, we should ideally specify the data types. We'll see below, which types are loaded when we don't specify them.  - Another alternative is to automaticaly detect suitable types for each column, we can do this by writing <code>inferSchema=True</code> in either <code>.csv</code> or <code>.options</code> - Or we can specify our own schema using <code>.schema(schema)</code>, like something shown below:</p> <pre><code>from pyspark.sql.types import StructTypes\n\nschema = StructType() \\\n      .add(\"PassengerId\",IntegerType(),True) \\\n      .add(\"Name\",StringType(),True) \\\n      .add(\"Fare\",DoubleType(),True) \\\n      .add(\"Decommisioned\",BooleanType(),True)\n</code></pre> <p>Let's read our dataset, that contains a header, which requires <code>header=True</code>, like in <code>pandas</code></p> <pre><code>df = spark.read.csv('train.csv',header=True)\ndf.show(1,vertical=True)\n\n# -RECORD 0---------------------------\n#  PassengerId | 1                    \n#  Survived    | 0                    \n#  Pclass      | 3                    \n#  Name        | Braund, Mr. Owen ... \n#  Sex         | male                 \n#  Age         | 22.0                 \n#  SibSp       | 1                    \n#  Parch       | 0                    \n#  Ticket      | A/5 21171            \n#  Fare        | 7.25                 \n#  Cabin       | null                 \n#  Embarked    | S                    \n# only showing top 1 row\n</code></pre>","tags":["pyspark","preprocessing"]},{"location":"blog/2023/08/20/data-preprocessing-with-pyspark.html#data-preprocessing","title":"Data Preprocessing","text":"","tags":["pyspark","preprocessing"]},{"location":"blog/2023/08/20/data-preprocessing-with-pyspark.html#dataframe-column-types","title":"DataFrame Column Types","text":"<p>Like in <code>pandas</code>, we can call the method <code>.dtypes</code>, to show the column types. Default column type interpretations aren't always ideal, so its useful to load your own <code>schema</code></p> <pre><code>df.dtypes\n\n# [('PassengerId', 'string'),\n#  ('Survived', 'string'),\n#  ('Pclass', 'string'),\n#  ('Name', 'string'),\n#  ('Sex', 'string'),\n#  ('Age', 'string'),\n#  ('SibSp', 'string'),\n#  ('Parch', 'string'),\n#  ('Ticket', 'string'),\n#  ('Fare', 'string'),\n#  ('Cabin', 'string'),\n#  ('Embarked', 'string')]\n</code></pre>","tags":["pyspark","preprocessing"]},{"location":"blog/2023/08/20/data-preprocessing-with-pyspark.html#dataframe-statistics","title":"DataFrame Statistics","text":"<p>Like in <code>pandas</code>, we can utilise method <code>describe</code>, in order to show column statistics. </p> <pre><code>df.describe(['Sex','Age']).show()\n\n# +-------+------+------------------+\n# |summary|   Sex|               Age|\n# +-------+------+------------------+\n# |  count|   891|               714|\n# |   mean|  null| 29.69911764705882|\n# | stddev|  null|14.526497332334035|\n# |    min|female|              0.42|\n# |    max|  male|                 9|\n# +-------+------+------------------+\n</code></pre>","tags":["pyspark","preprocessing"]},{"location":"blog/2023/08/20/data-preprocessing-with-pyspark.html#show-missing-data","title":"Show Missing Data","text":"<p>If we want to count the missing data in all our columns we can do the following:</p> <pre><code>df.select([f.count(f.when(f.isnan(c) | f.col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n\n# +-----------+--------+------+---+---+-----+-----+----+--------+-----------+-----+\n# |PassengerId|Survived|Pclass|Sex|Age|SibSp|Parch|Fare|Embarked|Family_Size|Alone|\n# +-----------+--------+------+---+---+-----+-----+----+--------+-----------+-----+\n# |          0|       0|     0|  0|  0|    0|    0|   0|       2|          0|    0|\n# +-----------+--------+------+---+---+-----+-----+----+--------+-----------+-----+\n</code></pre> <p>We can show rows with missing data using <code>.where</code> and <code>.f.col('column').isNull()</code></p> <pre><code>age_miss = df.where(f.col('Age').isNull())\nage_miss.show(5)\n\n# +-----------+--------+------+--------------------+------+----+-----+-----+------+------+-----+--------+\n# |PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|Ticket|  Fare|Cabin|Embarked|\n# +-----------+--------+------+--------------------+------+----+-----+-----+------+------+-----+--------+\n# |          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|330877|8.4583| null|       Q|\n# |         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|244373|  13.0| null|       S|\n# |         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|  2649| 7.225| null|       C|\n# |         27|       0|     3|Emir, Mr. Farred ...|  male|null|    0|    0|  2631| 7.225| null|       C|\n# |         29|       1|     3|\"O'Dwyer, Miss. E...|female|null|    0|    0|330959|7.8792| null|       Q|\n# +-----------+--------+------+--------------------+------+----+-----+-----+------+------+-----+--------+\n</code></pre>","tags":["pyspark","preprocessing"]},{"location":"blog/2023/08/20/data-preprocessing-with-pyspark.html#dropping-irrelovant-columns","title":"Dropping Irrelovant Columns","text":"<p>We can decide to remove columns that we won't be needing in our project by calling <code>.drop</code>, which is the same in <code>pandas</code></p> <pre><code>df = df.drop('Ticket','Name','Fare','Cabin')\ndf.show(5)\n\n# +-----------+--------+------+------+---+-----+-----+--------+\n# |PassengerId|Survived|Pclass|   Sex|Age|SibSp|Parch|Embarked|\n# +-----------+--------+------+------+---+-----+-----+--------+\n# |          1|       0|     3|  male| 22|    1|    0|       S|\n# |          2|       1|     1|female| 38|    1|    0|       C|\n# |          3|       1|     3|female| 26|    0|    0|       S|\n# |          4|       1|     1|female| 35|    1|    0|       S|\n# |          5|       0|     3|  male| 35|    0|    0|       S|\n# +-----------+--------+------+------+---+-----+-----+--------+\n</code></pre>","tags":["pyspark","preprocessing"]},{"location":"blog/2023/08/20/data-preprocessing-with-pyspark.html#adding-columns-to-dataframe","title":"Adding Columns to DataFrame","text":"<p>Column additions do however work a little differently, to add a column we add <code>.withColumn</code></p> <pre><code>df = df.withColumn('FamilySize',f.col('SibSp') + f.col('Parch') + 1)\ndf.show(5)\n\n# +-----------+--------+------+------+---+-----+-----+--------+----------+\n# |PassengerId|Survived|Pclass|   Sex|Age|SibSp|Parch|Embarked|FamilySize|\n# +-----------+--------+------+------+---+-----+-----+--------+----------+\n# |          1|       0|     3|  male| 22|    1|    0|       S|       2.0|\n# |          2|       1|     1|female| 38|    1|    0|       C|       2.0|\n# |          3|       1|     3|female| 26|    0|    0|       S|       1.0|\n# |          4|       1|     1|female| 35|    1|    0|       S|       2.0|\n# |          5|       0|     3|  male| 35|    0|    0|       S|       1.0|\n# +-----------+--------+------+------+---+-----+-----+--------+----------+\n</code></pre> <p>We can also define a condition, based on which we'll create a unique feature</p> <pre><code>ndf = ndf.withColumn('M',f.col('Sex') == 'male')\nndf = ndf.withColumn('F',f.col('Sex') == 'female')\nndf = ndf.drop('sex')\n</code></pre>","tags":["pyspark","preprocessing"]},{"location":"blog/2023/08/20/data-preprocessing-with-pyspark.html#data-imputation","title":"Data Imputation","text":"<p>Data imputation can be done via <code>fillna</code>, we pass a dictionary containing key,value pair for column name and value respectively </p> <pre><code>av_age = df.select(f.avg(f.col('age')))\nav_age.show()\n\n# +-----------------+\n# |         avg(age)|\n# +-----------------+\n# |29.69911764705882|\n# +-----------------+\n\n# To convert to python types, we can write:\nav_age.collect()[0][0]  # 29.69911764705882\n</code></pre> <pre><code>ndf = df.fillna({'age':av_age.collect()[0][0]})\nndf.show(5)\n\n# +-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+-----+--------+\n# |PassengerId|Survived|Pclass|                Name|   Sex|              Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n# +-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+-----+--------+\n# |          1|       0|     3|Braund, Mr. Owen ...|  male|             22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n# |          2|       1|     1|Cumings, Mrs. Joh...|female|             38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n# |          3|       1|     3|Heikkinen, Miss. ...|female|             26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n# |          4|       1|     1|Futrelle, Mrs. Ja...|female|             35.0|    1|    0|          113803|   53.1| C123|       S|\n# |          5|       0|     3|Allen, Mr. Willia...|  male|             35.0|    0|    0|          373450|   8.05| null|       S|\n# +-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+-----+--------+\n</code></pre>","tags":["pyspark","preprocessing"]},{"location":"blog/2023/08/20/data-preprocessing-with-pyspark.html#conclusion","title":"Conclusion","text":"<p>Let's review what we have covered in this post: - We learned how to drop columns, using <code>.drop</code> - We learned how to extract statistical data from our dataframe, using <code>.select</code> and functions <code>f.avg('column')</code> - We known how to fill missing data in different columns using a single value with a dictionary; <code>f.fillna({'column':'value'})</code> - Add or replace a column, using <code>f.withColumn</code> - <code>StringIndexer(inputCol,outputCol).fit(data)</code> - convert categorical into a numerical representation - Once we are done with our feature matrix, we can convert all the relevant features into a single feature that will be used as input into the model using <code>VectorAssembler(inputCols,outputCol).transform(data)</code></p>","tags":["pyspark","preprocessing"]},{"location":"blog/2023/08/21/creating-ml-models-with-pyspark.html","title":"Training ML Models with PySpark","text":"<p>In this post, we will introduce ourselves to <code>pyspark</code></p> <ul> <li>We are continuing on from the previous post PySpark Titanic Preprocessing, where we did some basic data preprocessing, here we will continue on with the modeling stage of our project</li> <li>We will be using <code>spark.ml.classification</code> to train binary classification models</li> <li>There are quite a number of differences from <code>pandas</code>, for example the formulation of a <code>VectorAssembler</code> columns, which combines all column features into one</li> </ul> <ul> <li> <p> Run on Colab</p> </li> <li> <p> Download dataset</p> </li> </ul>","tags":["pyspark","classification","modeling"]},{"location":"blog/2023/08/21/creating-ml-models-with-pyspark.html#introduction","title":"Introduction","text":"<p>We'll continue on where we left of PySpark Titanic Preprocessing</p> <ul> <li>In the last post, we focused on general preprocessing data, mostly data cleaning. </li> <li>In this post, we'll focus on finishing off data preprocessing, transformation steps that a required before passing the data to the model</li> </ul>","tags":["pyspark","classification","modeling"]},{"location":"blog/2023/08/21/creating-ml-models-with-pyspark.html#preprocessing-summary","title":"Preprocessing Summary","text":"<p>Let's summarise our preprocessing stages that we did last post:</p> <ul> <li>We learned how to drop columns that we won't be needing at all in our preprocessing using <code>.drop</code></li> <li>We learned how to extract statistical data from our dataframe, using <code>.select</code> and functions <code>f.avg('column')</code></li> <li>We known how to fill missing data in different columns using a single value with a dictionary; <code>f.fillna({'column':'value'})</code></li> <li>We know how to add or replace a column, using <code>f.withColumn</code></li> </ul> <pre><code>df = spark.read.csv('train.csv',header=True,inferSchema=True)\ndf = df.drop('Ticket','Name','Cabin')\nav_age = df.select(f.avg(f.col('age')))\ndf = df.fillna({'age':round(av_age.collect()[0][0],2)})\ndf = df.withColumn('Family_Size',f.col('SibSp') + f.col('Parch'))  # add values from two columns\ndf = df.withColumn('Alone',f.lit(0))  # fill all with 0\ndf = df.withColumn('Alone',f.when(df['Family_size'] == 0,1).otherwise(df['Alone'])) # conditional filling\ndf = df.drop('any')\ndf.show()\n\n# +-----------+--------+------+------+----+-----+-----+-------+--------+-----------+-----+\n# |PassengerId|Survived|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|Family_Size|Alone|\n# +-----------+--------+------+------+----+-----+-----+-------+--------+-----------+-----+\n# |          1|       0|     3|  male|22.0|    1|    0|   7.25|       S|          1|    0|\n# |          2|       1|     1|female|38.0|    1|    0|71.2833|       C|          1|    0|\n# |          3|       1|     3|female|26.0|    0|    0|  7.925|       S|          0|    1|\n# |          4|       1|     1|female|35.0|    1|    0|   53.1|       S|          1|    0|\n# |          5|       0|     3|  male|35.0|    0|    0|   8.05|       S|          0|    1|\n# +-----------+--------+------+------+----+-----+-----+-------+--------+-----------+-----+\n</code></pre>","tags":["pyspark","classification","modeling"]},{"location":"blog/2023/08/21/creating-ml-models-with-pyspark.html#string-indexing","title":"String Indexing","text":"<p>We have left two columns which contain categorical (string) data, with which we want to work with in our modeling process; <code>Sex</code>,<code>Embarked</code>. As we saw in an exploratory data analysis from a previous post, these two features do contain data distributions, which allow us to distinguish between whether a passenger survived or not, which means it probably would help a model improve its accuracy. However these features will need to be modified in order for us to use them in our model.</p> <ul> <li>In <code>sklearn</code> there is a method called LabelEncoder. </li> <li>In pyspark, there is a method called StringIndexer, which work in a similar way.</li> </ul> <pre><code>from pyspark.ml.feature import StringIndexer\nfrom pyspark.ml import Pipeline\n\nindexers = [StringIndexer(inputCol=column,outputCol=column+'_index').fit(df) for column in ['Sex','Embarked']]\npipeline = Pipeline(stages=indexers)\n\ndf = pipeline.fit(df).transform(df)\ndf = df.drop('PassengerId','Embarked','Sex')\ndf.show()\n\n# +--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+\n# |Survived|Pclass| Age|SibSp|Parch|   Fare|Family_Size|Alone|Sex_index|Embarked_index|\n# +--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+\n# |       0|     3|22.0|    1|    0|   7.25|          1|    0|      0.0|           0.0|\n# |       1|     1|38.0|    1|    0|71.2833|          1|    0|      1.0|           1.0|\n# |       1|     3|26.0|    0|    0|  7.925|          0|    1|      1.0|           0.0|\n# |       1|     1|35.0|    1|    0|   53.1|          1|    0|      1.0|           0.0|\n# |       0|     3|35.0|    0|    0|   8.05|          0|    1|      0.0|           0.0|\n# +--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+\n</code></pre> <p>Once we are done indexing string columns, we need to remove them!</p>","tags":["pyspark","classification","modeling"]},{"location":"blog/2023/08/21/creating-ml-models-with-pyspark.html#combine-features","title":"Combine Features","text":"<p>Once we are happy with all the features that we want to utilise in our model, we need to assemble them into a single column. </p> <ul> <li>To do so we need to utilise method <code>VectorAssembler</code>. </li> <li>We need to write the names of the input feature columns we want to use <code>inputCols</code> and define the output feature name <code>outputCol</code>, the resulting feature will be placed in the input dataframe.</li> </ul> <pre><code>from pyspark.ml.feature import VectorAssembler\n\nfeature = VectorAssembler(inputCols=df.columns[1:],\n                          outputCol='features')\nfeature_vector = feature.transform(df)\nfeature_vector.show()\n\n# +--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+--------------------+\n# |Survived|Pclass| Age|SibSp|Parch|   Fare|Family_Size|Alone|Sex_index|Embarked_index|            features|\n# +--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+--------------------+\n# |       0|     3|22.0|    1|    0|   7.25|          1|    0|      0.0|           0.0|[3.0,22.0,1.0,0.0...|\n# |       1|     1|38.0|    1|    0|71.2833|          1|    0|      1.0|           1.0|[1.0,38.0,1.0,0.0...|\n# |       1|     3|26.0|    0|    0|  7.925|          0|    1|      1.0|           0.0|[3.0,26.0,0.0,0.0...|\n# |       1|     1|35.0|    1|    0|   53.1|          1|    0|      1.0|           0.0|[1.0,35.0,1.0,0.0...|\n# |       0|     3|35.0|    0|    0|   8.05|          0|    1|      0.0|           0.0|(9,[0,1,4,6],[3.0...|\n# +--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+--------------------+\n</code></pre>","tags":["pyspark","classification","modeling"]},{"location":"blog/2023/08/21/creating-ml-models-with-pyspark.html#train-test-splitting","title":"Train-Test Splitting","text":"<p>Once our data is ready, we should think of a strategy to confirm the accuracy of our model</p> <ul> <li>Train-Test Splitting is a common strategy to verify how well a model generalises on data it wasn't trained on. In <code>spark</code>, we can reference to the dataframe itself to split it using <code>df.randomSplit</code></li> </ul> <pre><code>(training_data, test_data) = feature_vector.randomSplit([0.8,0.2],42)\n</code></pre>","tags":["pyspark","classification","modeling"]},{"location":"blog/2023/08/21/creating-ml-models-with-pyspark.html#training-evaluation","title":"Training &amp; Evaluation","text":"<p>Training &amp; evaluation of different models follow the same template of actions, the only thing that changes is we load different models from <code>spark.ml.classification</code></p>","tags":["pyspark","classification","modeling"]},{"location":"blog/2023/08/21/creating-ml-models-with-pyspark.html#logisticregression","title":"LogisticRegression","text":"<p>The first step is to load the relevant model from <code>.ml.classification</code>, in this case we start with a simplistic LogisticRegression model, which is named the same as in sklearn. Inputs into the model instance require us to specify the vectorised feature columns <code>featuresCol</code> and the target variable column, <code>labelCol</code></p> <p>The model should be <code>fit</code> on training data and saved into varaible <code>lrModel</code>, which is a little different to how you would do it in <code>sklearn</code></p> <pre><code>from pyspark.ml.classification import LogisticRegression\n\n# initialise model\nlr = LogisticRegression(labelCol='Survived',\n                        featuresCol='features')\n\n# returns a transformer which is our model\nlrModel = lr.fit(training_data)\n</code></pre> <p>Variable <code>lrModel</code> can then be used to make a prediction on the test set, to get its generalisation score on new data,  we can see which rows of data matches by using <code>.select</code></p> <pre><code># make prediction on test set\nlr_prediction = lrModel.transform(test_data)\nlr_prediction.select(['prediction','Survived']).show(5)\n\n# +----------+--------+\n# |prediction|Survived|\n# +----------+--------+\n# |       1.0|       0|\n# |       1.0|       0|\n# |       1.0|       0|\n# |       1.0|       0|\n# |       0.0|       0|\n# +----------+--------+\n</code></pre> <p>Finally, having the relevant prediction, we can evaluate the overall performance of the model using <code>MulticlassClassificationEvaluator</code></p> <p>One nuance that may seem odd is that we opted to use multiclass, even though our problem is a binary classification problem.  The reasoning can be explained by this post, which states that <code>MulticlassClassificationEvaluator</code> utilises class weighting</p> <pre><code>from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# evaluator\nevaluator = MulticlassClassificationEvaluator(labelCol='Survived',\n                                              predictionCol='prediction',\n                                              metricName='accuracy')\nevaluator.evaluate(lr_prediction)\n# 0.7586206896551724\n</code></pre>","tags":["pyspark","classification","modeling"]},{"location":"blog/2023/08/21/creating-ml-models-with-pyspark.html#decisiontree","title":"DecisionTree","text":"<p>A powerful binary tree based algorith, which is used by both gradient boosting and random forest:</p> <pre><code>from pyspark.ml.classification import DecisionTreeClassifier\n\n# initialise model\ndt = DecisionTreeClassifier(labelCol='Survived',\n                            featuresCol='features')\n\n# returns a transformer which is our model\ndtModel = dt.fit(training_data)   \ndt_prediction = dtModel.transform(test_data)\n\nevaluator = MulticlassClassificationEvaluator(labelCol='Survived',\n                                              predictionCol='prediction',\n                                              metricName='accuracy')\nevaluator.evaluate(dt_prediction)\n# 0.7448275862068966\n</code></pre>","tags":["pyspark","classification","modeling"]},{"location":"blog/2023/08/21/creating-ml-models-with-pyspark.html#randomforest","title":"RandomForest","text":"<p>One ensemble approach based on randomised generation of <code>DecisionTrees</code> we can try is <code>RandomForest</code>, which even is named the same as in <code>sklearn</code></p> <pre><code>from pyspark.ml.classification import RandomForestClassifier\n\n# initialise model\nrf = RandomForestClassifier(labelCol='Survived',\n                            featuresCol='features')\n\n# returns a transformer which is our model\nrfModel = rf.fit(training_data)   \nrf_prediction = rfModel.transform(test_data)\n\n# evaluator\nevaluator = MulticlassClassificationEvaluator(labelCol='Survived',\n                                              predictionCol='prediction',\n                                              metricName='accuracy')\nevaluator.evaluate(rf_prediction)\n# 0.7586206896551724\n</code></pre>","tags":["pyspark","classification","modeling"]},{"location":"blog/2023/08/21/creating-ml-models-with-pyspark.html#gradientboosting","title":"GradientBoosting","text":"<p>Another enseble method which uses <code>DecisionTrees</code> is Gradient Boosting, its name varies from that of <code>sklearn</code></p> <pre><code>from pyspark.ml.classification import GBTClassifier\n\n# initialise model\ngb = GBTClassifier(labelCol='Survived',\n                            featuresCol='features')\n\n# returns a transformer which is our model\ngbModel = gb.fit(training_data)   \ngb_prediction = gbModel.transform(test_data)\n\nevaluator = MulticlassClassificationEvaluator(labelCol='Survived',\n                                              predictionCol='prediction',\n                                              metricName='accuracy')\nevaluator.evaluate(gb_prediction)\n# 0.7517241379310344\n</code></pre>","tags":["pyspark","classification","modeling"]},{"location":"blog/2023/08/21/creating-ml-models-with-pyspark.html#saving-loading-model","title":"Saving &amp; Loading Model","text":"<p>We have tested different models and found the one which gives us the best metric, which in our case is <code>accuracy</code></p> <ul> <li>To save a model we need to save <code>model.fit</code>. The best performing model in our case was RandomForest, so let's save <code>rfModel</code></li> </ul> <pre><code>rfModel.save('rf_model')\n</code></pre> <p>To load the model, we need to load the relevant module from <code>classification</code>; <code>RandomForestClassificationModel</code>, which is different from <code>RandomForestClassifier</code>, and call the method <code>.load('folder')</code></p> <pre><code>from pyspark.ml.classification import RandomForestClassificationModel\n\nRandomForestClassificationModel.load('rf_model')\n# RandomForestClassificationModel: uid=RandomForestClassifier_f17b9c33fe1c, numTrees=20, numClasses=2, numFeatures=9\n</code></pre>","tags":["pyspark","classification","modeling"]},{"location":"blog/2023/08/21/creating-ml-models-with-pyspark.html#summary","title":"Summary","text":"<p>Let's review what we have covered in this post:</p> <ul> <li>We learned how to drop columns, using <code>.drop</code></li> <li>We learned how to extract statistical data from our dataframe, using <code>.select</code> and functions <code>f.avg('column')</code></li> <li>We known how to fill missing data in different columns using a single value with a dictionary; <code>f.fillna({'column':'value'})</code></li> <li>Add or replace a column, using <code>f.withColumn</code></li> <li><code>StringIndexer(inputCol,outputCol).fit(data)</code> - convert categorical into a numerical representation</li> <li>Once we are done with our feature matrix, we can convert all the relevant features into a single feature that will be used as input into the model using <code>VectorAssembler(inputCols,outputCol).transform(data)</code></li> <li>To split the data into a training &amp; validation dataset, we can use the dataframe method <code>df.randomSplit</code></li> </ul> <p>Training a model requires identical steps for whichever model we choose:</p> <ul> <li>Import the model class from <code>pyspark.ml.classification</code></li> <li>Instantiate the model by specifying <code>labelCol</code> and <code>featuresCol</code></li> <li>Train the model using <code>trained_model = model.fit(data)</code></li> <li>Use the model to make predictions using <code>y_pred = trained_model.transform(data)</code></li> <li>Once we have both a model prediction and training labels, we can make an evaluation using an evaluator <code>MulticlassClassificationEvaluator</code> with evaluator.evaluate(data)</li> <li>And to finish off our modeling state, we can save our model that we will use in production by saving <code>trained_model.save('name')</code> and load with the relevant <code>XModel.load()</code></li> </ul>","tags":["pyspark","classification","modeling"]},{"location":"blog/2023/08/22/hyperparameter-tuning-with-pipelines.html","title":"Hyperparameter Tuning with Pipelines","text":"<p>This post is the last of the three posts on the titanic classification problem in <code>pyspark</code></p> <ul> <li>In the last post, we started with a clearned dataset, which we prepared for machine learning, by utilising <code>StringIndexer</code> &amp; <code>VectorAssembler</code>, and then the model training stage itself. </li> <li>These steps are a series of stages in the construction of a model, which we can group into a single <code>pipline</code>. <code>pyspark</code> like <code>sklearn</code> has such pipeline classes that help us keep things organised</li> </ul> <ul> <li> <p> Run on Colab</p> </li> <li> <p> Download dataset</p> </li> </ul>","tags":["pyspark","pipeline","classification","hyperparameter tuning"]},{"location":"blog/2023/08/22/hyperparameter-tuning-with-pipelines.html#background","title":"Background","text":"<p><code>pyspark</code> pipeline is a tool for building and executing data processing workflows. It allows users to chain together multiple data processing tasks into a single workflow, making it easier to organize and manage complex data processing tasks. The pipeline provides a high-level API for building and executing these workflows, which makes it easier for users to quickly prototype and test new data processing pipelines.</p> <p>Having created a pipeline, we now need to tune the parameters and find the most optimal <code>hyperparameter</code> combination. This is necessary because the default hyperparameter seldom is most optimal, so we can test different combinations, and store a model, which for example shows the best generalisation performance.</p>","tags":["pyspark","pipeline","classification","hyperparameter tuning"]},{"location":"blog/2023/08/22/hyperparameter-tuning-with-pipelines.html#preprocessing-recap","title":"Preprocessing Recap","text":"<ul> <li>We learned how to drop columns that we won't be needing at all in our preprocessing using <code>.drop</code></li> <li>We learned how to extract statistical data from our dataframe, using <code>.select</code> and functions <code>f.avg('column')</code></li> <li>We known how to fill missing data in different columns using a single value with a dictionary; <code>f.fillna({'column':'value'})</code></li> <li>We know how to add or replace a column, using <code>f.withColumn</code></li> </ul> <pre><code>df = spark.read.csv('train.csv',header=True,inferSchema=True)\ndf = df.drop('Ticket','Name','Cabin')\nav_age = df.select(f.avg(f.col('age')))\ndf = df.fillna({'age':round(av_age.collect()[0][0],2)})\ndf = df.withColumn('Family_Size',f.col('SibSp') + f.col('Parch'))  # add values from two columns\ndf = df.withColumn('Alone',f.lit(0))  # fill all with 0\ndf = df.withColumn('Alone',f.when(df['Family_size'] == 0,1).otherwise(df['Alone'])) # conditional filling\ndf = df.drop('any')\ndf.show()\n\n# +-----------+--------+------+------+----+-----+-----+-------+--------+-----------+-----+\n# |PassengerId|Survived|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|Family_Size|Alone|\n# +-----------+--------+------+------+----+-----+-----+-------+--------+-----------+-----+\n# |          1|       0|     3|  male|22.0|    1|    0|   7.25|       S|          1|    0|\n# |          2|       1|     1|female|38.0|    1|    0|71.2833|       C|          1|    0|\n# |          3|       1|     3|female|26.0|    0|    0|  7.925|       S|          0|    1|\n# |          4|       1|     1|female|35.0|    1|    0|   53.1|       S|          1|    0|\n# |          5|       0|     3|  male|35.0|    0|    0|   8.05|       S|          0|    1|\n# +-----------+--------+------+------+----+-----+-----+-------+--------+-----------+-----+\n</code></pre>","tags":["pyspark","pipeline","classification","hyperparameter tuning"]},{"location":"blog/2023/08/22/hyperparameter-tuning-with-pipelines.html#creating-subsets","title":"Creating Subsets","text":"<p>The idea of utilising a <code>pipeline</code> is to process a given set of data using the same preprocessing steps as all input data on which we apply the pipeline on.  For our problem, our input data will look like the table above, lets first split the available data into two datasets; <code>train</code> &amp; <code>test</code>. We will construct our pipeline on the <code>training</code> data and then use the same pipeline on the <code>test</code> dataset.</p> <pre><code>train,test = ldf.randomSplit([0.8,0.2],42)\n</code></pre>","tags":["pyspark","pipeline","classification","hyperparameter tuning"]},{"location":"blog/2023/08/22/hyperparameter-tuning-with-pipelines.html#machine-learning-pipeline","title":"Machine Learning Pipeline","text":"","tags":["pyspark","pipeline","classification","hyperparameter tuning"]},{"location":"blog/2023/08/22/hyperparameter-tuning-with-pipelines.html#creating-a-pipeline","title":"Creating a Pipeline","text":"<p>To build a <code>pipeline</code>, we define the steps that make it up in a list <code>stages</code>, our pipeline consists of four steps:</p> <ul> <li><code>indexer_sex</code> : <code>StringIndexer</code></li> <li><code>indexer_embarked</code> : <code>StringIndexer</code></li> <li><code>feature</code> transformation<code>** : **</code>VectorAssembler`</li> <li><code>rf_classifier</code> : model which inputs the final result of all transformations above</li> </ul> <pre><code>from pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import VectorAssembler\n\n\n# Create indexer columns (step 1 &amp; step 2)\nindexer_sex = StringIndexer(inputCol='Sex',\n                            outputCol='Sex_index')\nindexer_embarked = StringIndexer(inputCol='Embarked',\n                                 outputCol='Embarked_index')\n\ntrain_sex = indexer_sex.fit(train).transform(train)\ntrain_embarked = indexer_embarked.fit(train_sex).transform(train_sex)\n\n# Assemble all features into a single column using VectorAssembler (step 3)\nfeature = VectorAssembler(inputCols=['Pclass','Age','SibSp','Parch','Fare',\n                                     'Family_Size','Embarked_index','Sex_index'],\n                          outputCol='features')\n\nresult = feature.transform(train_embarked)\n\n# Define model (step 4)\nrf_classifier = RandomForestClassifier(labelCol='Survived',\n                                       featuresCol='features')\n\n# merge all pipeline components\npipeline = Pipeline(stages=[indexer_sex,\n                            indexer_embarked,\n                            feature,\n                            rf_classifier])\n# Pipeline_67a25ab8838b\n</code></pre> <p>Let's use the <code>pipeline</code> model on the training data &amp; save it so we can use it on new data. Our pipeline model <code>pipeline</code> stores in itself information about what transformations we need to apply to any incoming data, so we won't need to call <code>pipeline</code> components (eg. <code>indexer_embarked</code>) if we want to use the model later.</p> <pre><code>p_model = pipeline.fit(train)\np_model.save('p_model')\n</code></pre> <p>Now let's load and use the model on new data (the <code>test</code> data, which we haven't touched yet). The <code>pipline</code> model can be loaded in the same way we load standard model, which had the ability to <code>.load</code>; <code>PipelineModel</code></p> <pre><code>from pyspark.ml.pipeline import PipelineModel\n\n# load pipeline model\nmodel = PipelineModel.load('p_model')\n</code></pre> <pre><code># transform model on new data\nyv_pred = model.transform(test)\nyv_pred.select(['features','rawPrediction','probability','prediction']).show(5)\n\n# +--------------------+--------------------+--------------------+----------+\n# |            features|       rawPrediction|         probability|prediction|\n# +--------------------+--------------------+--------------------+----------+\n# |(8,[0,1,4,7],[3.0...|[8.61510797966188...|[0.43075539898309...|       1.0|\n# |(8,[0,1,4],[1.0,5...|[15.1208402070771...|[0.75604201035385...|       0.0|\n# |[3.0,27.0,0.0,2.0...|[7.50139928738481...|[0.37506996436924...|       1.0|\n# |[3.0,39.0,1.0,5.0...|[16.7158036061856...|[0.83579018030928...|       0.0|\n# |[3.0,29.7,0.0,0.0...|[5.75011597218139...|[0.28750579860906...|       1.0|\n# +--------------------+--------------------+--------------------+----------+\n</code></pre> <p>Now let's evaluate the model prediction using <code>MulticlassClassificationEvaluator</code></p> <pre><code>from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# evaluator\nevaluator = MulticlassClassificationEvaluator(labelCol='Survived',\n                                              predictionCol='prediction',\n                                              metricName='accuracy')\nevaluator.evaluate(yv_pred)\n# 0.8275862068965517\n</code></pre>","tags":["pyspark","pipeline","classification","hyperparameter tuning"]},{"location":"blog/2023/08/22/hyperparameter-tuning-with-pipelines.html#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>Our pipeline contains a model <code>RandomForestClassifier</code> with default hyperparameters.  We can can find a better combination of hyperparameters which will give a better model utilising <code>TrainValidationSplit</code>, which is a lightweight generalisation evaluator in comparison to <code>CrossValidator</code></p> <p>First thigs first, we need to create a parametric grid, which will store all the hyperaparameter combinations we will test.</p> <pre><code>from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n\n# create a parameter grid\ngrid = ParamGridBuilder().addGrid(rf_classifier.maxDepth,[4,5,6])\\\n                         .addGrid(rf_classifier.maxBins,[3,4,5])\\\n                         .addGrid(rf_classifier.minInfoGain,[0.05,0.1,0.15]).build() \n\nprint(len(grid),'combinations')\n# 27 combinations*\n</code></pre> <p><code>TrainValidationSplit</code> requires an estimator; which is our <code>pipeline</code> model, the defined grid, evaluator and train-test split ratio</p> <pre><code># Similar to CrossValidator, but more lightweight\ngs = TrainValidationSplit(estimator=pipeline,\n                          estimatorParamMaps=grid,\n                          evaluator=evaluator,\n                          trainRatio=0.8)\n\nmodel = gs.fit(train)\n</code></pre> <p>To get the best model, we simply write model.bestModel, which has hyperparameters, which can be extracted as per below:</p> <pre><code>javobj = model.bestModel.stages[-1]._java_obj\nprint('maxdepth',javobj.getMaxDepth())\nprint('maxdepth',javobj.getMaxBins())\nprint('min_IG',javobj.getMinInfoGain())\n\n# maxdepth 5\n# maxdepth 5\n# min_IG 0.05\n</code></pre>","tags":["pyspark","pipeline","classification","hyperparameter tuning"]},{"location":"blog/2023/08/22/hyperparameter-tuning-with-pipelines.html#conclusion","title":"Conclusion","text":"<p>Important imports for <code>pipeline</code></p> <ul> <li><code>pyspark.ml import Pipeline</code> (create model)</li> <li><code>pyspark.ml.pipeline import PipelineModel</code> (load model)</li> </ul> <p>Important imports for finetuning</p> <ul> <li><code>pyspark.ml.tuning import ParamGridBuilder</code> (Create a parameter grid)</li> <li><code>pyspark.ml.tuning import TrainValidationSplit</code> (create an evaluator)</li> </ul> <p>\u276f\u276f <code>pipeline</code> allows us to group together preprocessing steps in one model</p> <ul> <li> <p>Merge all <code>pipeline</code> components together</p> </li> <li> <p>\u276f <code>Pipeline(stages=[steps1,steps2])</code></p> </li> <li> <p><code>Pipeline</code> then needs to be fit on training data</p> </li> <li> <p>\u276f <code>pipeline.fit(train)</code> and saved via <code>pipeline.save('path')</code></p> </li> <li> <p>To load the model;</p> </li> <li> <p>Load a <code>pipeline</code> model; from <code>PipelineModel</code>, which has the method <code>load(path)</code></p> </li> <li> <p>To utilise the <code>pipeline</code> model on new data;</p> </li> <li> <p><code>pipeline.transform(test)</code> </p> </li> </ul> <p>\u276f\u276f When we want to <code>finetune</code> our machine learning model, which is part of a <code>pipeline</code>:</p> <ul> <li> <p>Create a parameter grid using <code>pyspark.ml.tuning</code> ParamGridBuilder</p> </li> <li> <p><code>ParamGridBuilder().addGrid(model.maxDepth,params)</code> </p> </li> <li> <p>From <code>pyspark.ml.tuning</code> import <code>TrainValidationSplit</code> evaluator</p> </li> <li> <p><code>TrainValidationSplit(estimator,estimatorParamMaps,evaluator,trainRatio)</code></p> </li> <li> <p>Which the requires a fit <code>evaluator.fit(data)</code> </p> </li> <li> <p>Having fitted the evaluator:</p> </li> <li> <p>The best model can be obtained from <code>model.bestModel</code></p> </li> <li>Its parameters <code>model.bestModel.stages[-1]._java_obj</code> </li> </ul> <p>Thank you for reading!</p> <p>Any questions or comments about the above post can be addressed on the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>","tags":["pyspark","pipeline","classification","hyperparameter tuning"]},{"location":"blog/2023/08/19/named-entity-recognition-with-huggingface-trainer.html","title":"Named Entity Recognition with Huggingface Trainer","text":"<p>In a previous post we looked at how we can utilise Huggingface together with PyTorch in order to create a NER tagging classifier. We did this by loading a preset encoder model &amp; defined our own tail end model for our NER classification task. This required us to utilise Torch`, ie create more lower end code, which isn't the most beginner friendly, especially if you don't know Torch. In this post, we'll look at utilising only Huggingface, which simplifies the training &amp; inference steps quite a lot. We'll be using the trainer &amp; pipeline methods of the Huggingface library and will use a dataset used in mllibs, which includes tags for different words that can be identified as keywords to finding data source tokens, plot parameter tokens and function input parameter tokens.</p> <p></p>","tags":["huggingface","ner"]},{"location":"blog/2023/08/19/named-entity-recognition-with-huggingface-trainer.html#background","title":"Background","text":"","tags":["huggingface","ner"]},{"location":"blog/2023/08/19/named-entity-recognition-with-huggingface-trainer.html#huggingface-trainer","title":"Huggingface Trainer","text":"<p>A Huggingface <code>Trainer</code> is a high-level API provided by the Huggingface Transformers library that makes it easier to train, fine-tune, and evaluate various Natural Language Processing (NLP) models. It provides a simple and consistent interface for training and evaluating models using various techniques such as text classification, named entity recognition, question answering, and more. The Trainer API abstracts away many of the low-level details of model training and evaluation, like we did in the previous post, allowing users to focus on their specific NLP tasks.</p>","tags":["huggingface","ner"]},{"location":"blog/2023/08/19/named-entity-recognition-with-huggingface-trainer.html#why-do-we-need-ner-in-nlp","title":"Why do we need NER in NLP?","text":"<p>Named entity recognition (NER) is important because it helps to identify and extract important information from unstructured text data. This can be useful in a variety of applications such as information retrieval, sentiment analysis, and text summarization. NER can also help to improve the accuracy of machine learning models by providing additional features for classification tasks. Additionally, NER is important for natural language processing <code>NLP</code> tasks such as machine translation, speech recognition, and question answering systems.</p> <p>NER can be quite a helpful tool in a variety of NLP applications. </p>","tags":["huggingface","ner"]},{"location":"blog/2023/08/19/named-entity-recognition-with-huggingface-trainer.html#our-ner-application","title":"Our NER application","text":"<p>Our NER application in this example:</p> <p>We'll be using token classification in order to identify tokens that can be identified as function <code>input parameters</code>, <code>data sources</code> and <code>plot parameters</code>. Identifying them helps us extract information from input text and allow us to store &amp; use relevant data that is defined in a <code>string</code></p> <p>Let's look at an example:</p> Input Text: <p>visualise column kdeplot for hf hue author fill True alpha 0.1 mew 1 mec 1 s 10 stheme viridis bw 10</p> NER Tagged: <p>visualise column kdeplot <code>[source : for]</code> hf <code>[param : hue]</code> author <code>[pp : fill]</code> True <code>[pp : alpha]</code> 0.1 <code>[pp : mew]</code> 1 <code>[pp : mec]</code> 1 <code>[pp : s]</code> 10 <code>[pp : stheme]</code> viridis <code>[pp : bw]</code> 10</p>","tags":["huggingface","ner"]},{"location":"blog/2023/08/19/named-entity-recognition-with-huggingface-trainer.html#the-dataset","title":"The Dataset","text":"<p>We can define a tag parser that supports an input format: <code>[tag : name]</code> (the massive dataset format). First, lets initialise the parser; <code>parser</code>, which reads the above format every time it is called. To parse all data in our dataset, we loop through all data and store relevant mapping dictionaries, returning the tokenised text &amp; tokenised tags. </p> <p>Let's look at the input data format:</p> <pre><code># for demonstration only\nldf = pd.read_csv('ner_mp.csv')\ntext,annot = list(ldf['text'].values),list(ldf['annot'].values)\n\ntext[:4]\n# ['visualise column kdeplot for dbscan_labels hue outlier_dbscan',\n# 'visualise column kdeplot for hf hue author fill True alpha 0.1 mew 1 mec 1 s 10 stheme viridis bw 10',\n# 'create relplot using hf x pressure y mass_flux col geometry hue author',\n# 'pca dimensionality reduction using data mpg subset numerical columns only']\n\nannot[:4]\n# ['visualise column kdeplot [source : for] dbscan_labels [param : hue] outlier_dbscan',\n# 'visualise column kdeplot [source : for] hf [param : hue] author [pp : fill] True [pp : alpha] 0.1 [pp : mew] 1 [pp : mec] 1 [pp : s] 10 [pp : stheme] viridis [pp : bw] 10',\n# 'create relplot using hf [param : x] pressure [param : y] mass_flux [param : col] geometry [param : hue] author',\n# 'pca dimensionality reduction [source : using data] mpg [source : subset] numerical columns only']\n</code></pre>","tags":["huggingface","ner"]},{"location":"blog/2023/08/19/named-entity-recognition-with-huggingface-trainer.html#annotation-parser","title":"Annotation Parser","text":"<p>The above format can be parsed using <code>Parser</code>, we first need to initialise it</p> <pre><code>from typing import List\nimport regex as re\n\n# ner tag parser\nclass Parser:\n\n    LABEL_PATTERN = r\"\\[(.*?)\\]\"\n    PUNCTUATION_PATTERN = r\"([,\\/#!$%\\^&amp;\\*;:{}=\\-`~()'\\\"\u2019\u00bf])\"\n\n    def tokenise(self,text:str):\n        sentence = re.sub(self.PUNCTUATION_PATTERN, r\" \\1 \", text)\n        tokens = []\n        for w in sentence.split():\n            tokens.append(w)\n\n        return tokens\n\n    def __init__(self):\n        self.tag_to_id = {\n            \"O\": 0\n        }\n        self.id_to_tag = {\n            0: \"O\"\n        }\n\n    def __call__(self, sentence: str, annotated: str) -&gt; List[str]:\n        matches = re.findall(self.LABEL_PATTERN, annotated)\n        word_to_tag = {}\n        for match in matches:\n            tag, phrase = match.split(\" : \")\n            words = phrase.split(\" \") \n            word_to_tag[words[0]] = f\"B-{tag.upper()}\"\n            for w in words[1:]:\n                word_to_tag[w] = f\"I-{tag.upper()}\"\n\n        tags = []; txt = []\n        sentence = re.sub(self.PUNCTUATION_PATTERN, r\" \\1 \", sentence)\n        for w in sentence.split():\n\n            txt.append(w)\n            if w not in word_to_tag:\n                tags.append(\"O\")\n            else:\n                tags.append(word_to_tag[w])   # return tag for current [text,annot]\n                self.__add_tag(word_to_tag[w]) # add [tag] to tag_to_id/id_to_tag\n\n        # convert tags to numeric representation (if needed)\n\n        ttags = []\n        for tag in tags:\n            ttags.append(self.tag_to_id[tag])\n        tags = ttags\n\n        return tags\n\n    def __add_tag(self, tag: str):\n        if tag in self.tag_to_id:\n            return\n        id_ = len(self.tag_to_id)\n        self.tag_to_id[tag] = id_\n        self.id_to_tag[id_] = tag\n\n    def get_id(self, tag: str):\n        return self.tag_to_id[tag]\n\n# initialise parser\nparser = Parser()\n</code></pre>","tags":["huggingface","ner"]},{"location":"blog/2023/08/19/named-entity-recognition-with-huggingface-trainer.html#parse-dataset","title":"Parse Dataset","text":"<p>To use the parser on a dataset, we loop through all our rows, calling the parser, which fills out <code>tag_to_id</code>, <code>id_to_tag</code> and returns the tokenised <code>tags</code> when it is called</p> <pre><code># loop through dataset\nlst_tokens = []; lst_annot = []\nfor i,j in zip(text,annot):\n    tags = parser(i,j)                      # parse input ner format \n    lst_tokens.append(parser.tokenise(i))   # tokenise the input text\n    lst_annot.append(tags)             \n</code></pre> <p>We'll be using Huggingface to train our model, so we need our two lists <code>lst_tokens</code>,<code>lst_annot</code> to be converted into a huggingface <code>dataset</code>. To do this we create a <code>dataframe</code> and then use <code>from_pandas</code> method to convert the <code>dataframe</code> to a <code>dataset</code>. Each row in the <code>dataset</code> contains a list, which by default will not be registered correctly as the default datatype, so we need to define feature types (<code>features</code> argument) when calling <code>from_pandas</code>. A list is equivalent to a <code>Sequence</code>, whilst <code>ner_tags</code> are our labels, for which we need to define <code>ClassLabel</code>.</p> <pre><code>from datasets import Dataset,DatasetDict,Features,Value,ClassLabel,Sequence\n\n# create dataframe from token/tag pairs\ndf_raw_dataset = pd.DataFrame({'tokens':lst_tokens,\n                               'ner_tags':lst_annot})\n\n# custom data type\nclass_names = list(parser.tag_to_id.keys())\nft = Features({'tokens': Sequence(Value(\"string\")),\n               'ner_tags':Sequence(ClassLabel(names=class_names))})\n\n\n# dataset = Dataset.from_pandas(df_raw_dataset,features=ft) # convert dataframe to dataset\ndataset = Dataset.from_pandas(df_raw_dataset,\n                              features=ft) # convert dataframe to dataset\n\n# visualise a sample from the dataset\nraw_datasets = DatasetDict() # create dataset diction\nraw_datasets['train'] = dataset # register dataset training data\nraw_datasets['train'][0]\n</code></pre> <pre><code>{'tokens': ['visualise',\n  'column',\n  'kdeplot',\n  'for',\n  'dbscan_labels',\n  'hue',\n  'outlier_dbscan'],\n 'ner_tags': [0, 0, 0, 1, 0, 2, 0]}\n</code></pre> <p>Let's look at what data we have as a result of parsing input data <code>ldf</code> with <code>parser</code> </p> <pre><code># visualise tags\nwords = raw_datasets[\"train\"][0][\"tokens\"]\nlabels = raw_datasets[\"train\"][0][\"ner_tags\"]\nline1 = \"\"\nline2 = \"\"\nfor word, label in zip(words, labels):\n    full_label = label_names[label]\n    max_length = max(len(word), len(full_label))\n    line1 += word + \" \" * (max_length - len(word) + 1)\n    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n\nprint(line1)\nprint(line2)\n# visualise column kdeplot for      dbscan_labels hue     outlier_dbscan \n# O         O      O       B-SOURCE O             B-PARAM O              \n</code></pre>","tags":["huggingface","ner"]},{"location":"blog/2023/08/19/named-entity-recognition-with-huggingface-trainer.html#transformer-tokeniser","title":"Transformer Tokeniser","text":"<p>The above format of word/tag pair works fine if we use the pair for classification as it is. Our approach involves using a transformer encoder model (<code>bert-base-cased</code>), which has it own tokenisation approach, which means we need to create token tags for each subtoken that the tokeniser creates, this means we need to make a little adjustment to our input data</p> <p>First let's load our tokeniser using <code>form_pretrained</code></p> <pre><code>from transformers import AutoTokenizer\n\nmodel_checkpoint = \"bert-base-cased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n</code></pre> <p>To align the tags to correpond to subword tokens we can use a helper function <code>align_labels_with_tokens</code>; the function inputs the original labels (those with tags for words) and word_ids, which the <code>tokeniser</code> outputs representing the token-word association. As the tokeniser can split a word into parts, we can use this information to define the subtoken tags</p> <pre><code># helper function; convert word token labels to subword token labels\n# which will be fed into the model w/ input_ids\n\ndef align_labels_with_tokens(labels, word_ids):\n    new_labels = []\n    current_word = None\n    for word_id in word_ids:\n        if word_id != current_word:\n            # Start of a new word!\n            current_word = word_id\n            label = -100 if word_id is None else labels[word_id]\n            new_labels.append(label)\n        elif word_id is None:\n            # Special token\n            new_labels.append(-100)\n        else:\n            # Same word as previous token\n            label = labels[word_id]\n            # If the label is B-XXX we change it to I-XXX\n            if label % 2 == 1:\n                label += 1\n            new_labels.append(label)\n\n    return new_labels\n</code></pre>","tags":["huggingface","ner"]},{"location":"blog/2023/08/19/named-entity-recognition-with-huggingface-trainer.html#tokeniser-sample","title":"Tokeniser Sample","text":"<p>And let's look at an example of how the <code>tokeniser</code> actually splits the input text data, as well as the token-word association list; this information allows us to define subtoken tags based on their word tag from <code>word_ids</code> data</p> <pre><code># the model tokeniser format\ninputs = tokenizer(raw_datasets[\"train\"][10][\"tokens\"], is_split_into_words=True)\n\nprint('bert tokens')\nprint(inputs.tokens())\n\n# the tokeniser also keeps track of which tokens belong to which word\nprint('\\nbert word_ids')\nprint(inputs.word_ids())\n\n# bert tokens\n# ['[CLS]', 'create', 'sea', '##born', 'kernel', 'density', 'plot', 'using', 'h', '##f', 'x', 'x', '_', 'e', '_', 'out', '[SEP]']\n\n# bert word_id\n# [None, 0, 1, 1, 2, 3, 4, 5, 6, 6, 7, 8, 8, 8, 8, 8, None]\n</code></pre>","tags":["huggingface","ner"]},{"location":"blog/2023/08/19/named-entity-recognition-with-huggingface-trainer.html#subtoken-ner-tag-adjustment","title":"Subtoken NER tag adjustment","text":"<p>Having all relevant pieces, let's return to our original dataset <code>raw_datasets</code> which we created in parse dataset</p> <pre><code># Tokenise input text\ndef tokenize_and_align_labels(examples):\n\n    # tokenise \n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"], truncation=True, is_split_into_words=True\n    )\n    all_labels = examples[\"ner_tags\"]\n    new_labels = []\n    for i, labels in enumerate(all_labels):\n        word_ids = tokenized_inputs.word_ids(i)                        # subtoken word identifier\n        new_labels.append(align_labels_with_tokens(labels, word_ids))  # new ner labels for subtokens\n\n    tokenized_inputs[\"labels\"] = new_labels\n    return tokenized_inputs\n\ntokenized_datasets = raw_datasets.map(tokenize_and_align_labels,\n                                      batched=True,\n                                      remove_columns=raw_datasets[\"train\"].column_names)\n\ntokenized_datasets['train']\n# Dataset({\n#    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n#    num_rows: 51\n# })\n</code></pre>","tags":["huggingface","ner"]},{"location":"blog/2023/08/19/named-entity-recognition-with-huggingface-trainer.html#data-collator-adjustment","title":"Data Collator Adjustment","text":"<p>The final preprocessing step is the <code>data_collator</code></p> <pre><code>from transformers import DataCollatorForTokenClassification\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n\n# let's check what it does\nbatch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\nbatch[\"labels\"]\n#tensor([[-100,    0,    0,    0,    0,    0,    0,    0,    1,    0,    0,    0,\n#           0,    0,    2,    2,    0,    0,    0,    0,    0,    0, -100, -100,\n#        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n#        -100, -100],\n#       [-100,    0,    0,    0,    0,    0,    0,    0,    1,    0,    0,    2,\n#           2,    0,    3,    0,    3,    0,    0,    0,    3,    4,    0,    3,\n#           4,    0,    3,    0,    3,    4,    4,    0,    0,    0,    3,    4,\n#           0, -100]])\n</code></pre>","tags":["huggingface","ner"]},{"location":"blog/2023/08/19/named-entity-recognition-with-huggingface-trainer.html#prepare-for-training","title":"Prepare for Training","text":"","tags":["huggingface","ner"]},{"location":"blog/2023/08/19/named-entity-recognition-with-huggingface-trainer.html#evaluation-metrics","title":"Evaluation Metrics","text":"<p>The trainer allows us to add an evaluation function which when added to <code>compute_metrics</code> in the trainer return the model prediction, which are the <code>logits</code> and <code>labels</code>. Using this data we can then write an evaluation metric function which returns a dictionary of metric and its value pairs</p> <pre><code>import numpy as np\n\ndef compute_metrics(eval_preds):\n\n    model prediction\n    logits, labels = eval_preds\n    predictions = np.argmax(logits, axis=-1)\n\n    # Remove ignored index (special tokens) and convert to labels\n    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n    true_predictions = [\n        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": all_metrics[\"overall_precision\"],\n        \"recall\": all_metrics[\"overall_recall\"],\n        \"f1\": all_metrics[\"overall_f1\"],\n        \"accuracy\": all_metrics[\"overall_accuracy\"],\n    }\n</code></pre>","tags":["huggingface","ner"]},{"location":"blog/2023/08/19/named-entity-recognition-with-huggingface-trainer.html#define-model","title":"Define Model","text":"<p>Huggingface allows us to easily adjust the base model (<code>bert-base-cased</code>) for different tasks by loading the task type from the main library. For NER, we need to load <code>AutoModelForTokenClassification</code>, for which we then need to define two mapping dictionaries <code>id2label</code> &amp; <code>label2id</code></p> <pre><code># define subtoken-tag mapping dictionary\nid2label = {i: label for i, label in enumerate(label_names)}\nlabel2id = {v: k for k, v in id2label.items()}\n\nfrom transformers import AutoModelForTokenClassification\n\nmodel = AutoModelForTokenClassification.from_pretrained(model_checkpoint,\n                                                        id2label=id2label,\n                                                        label2id=label2id)\n</code></pre>","tags":["huggingface","ner"]},{"location":"blog/2023/08/19/named-entity-recognition-with-huggingface-trainer.html#train-model","title":"Train Model","text":"<p>Time to train our model, we'll train the model for 40 epochs, without an evaluation strategy (validation dataset), using a learning rate in our optimiser is set to 2e-5, which by default is the AdamW optimiser.</p> <pre><code>from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    \"bert-finetuned-ner2\",\n    evaluation_strategy=\"no\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    num_train_epochs=40,\n    weight_decay=0.01)\n</code></pre> <pre><code>from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,                       # define the model\n    args=args,                         # define the trainer parameters\n    train_dataset=tokenized_datasets[\"train\"],  # define the adjusted ner tag dataset\n    data_collator=data_collator,      # define the data collator\n    compute_metrics=compute_metrics,  # define the evaluation metrics function\n    tokenizer=tokenizer,              # define the tokeniser\n)\ntrainer.train()\n\n# TrainOutput(global_step=280, training_loss=0.009931504726409912, metrics={'train_runtime': 24.8186, 'train_samples_per_second': 82.197, 'train_steps_per_second': 11.282, 'total_flos': 27046414867968.0, 'train_loss': 0.009931504726409912, 'epoch': 40.0})\n</code></pre> <p>If we use <code>evaluation_strategy=\"steps\"</code>, <code>eval_steps=50</code> &amp; <code>eval_dataset</code> (just setting to train data) we get the following metrics so the model has memorised the inputs in our training set </p> <pre><code>Step    Training Loss   Validation Loss     Precision   Recall  F1  Accuracy\n50  No log  0.030486    0.994949    0.989950    0.992443    0.995763\n100     No log  0.006724    0.990000    0.994975    0.992481    0.995763\n150     No log  0.007272    0.994949    0.989950    0.992443    0.995763\n200     No log  0.007903    0.994949    0.989950    0.992443    0.995763\n250     No log  0.006493    0.990000    0.994975    0.992481    0.995763\n</code></pre>","tags":["huggingface","ner"]},{"location":"blog/2023/08/19/named-entity-recognition-with-huggingface-trainer.html#save-huggingface-trainer","title":"Save Huggingface Trainer","text":"<p>We can save our <code>trainer</code> using <code>.save_model</code> method, which saves all relevant needed to utilise the <code>pipeline</code> method.</p> <pre><code>trainer.save_model(\"bert-finetuned-ner2\")\n</code></pre>","tags":["huggingface","ner"]},{"location":"blog/2023/08/19/named-entity-recognition-with-huggingface-trainer.html#load-a-huggingface-pipeline","title":"Load a Huggingface Pipeline","text":"<p>Having saved our <code>trainer</code>, we simply call the relevant model checkpoint which we used in <code>.save_model</code> and set the <code>task</code> argument, which for our NER problem is <code>token-classification</code>, we also need to set an aggregation_strategy:</p> <p>The <code>aggregation_strategy</code> is a parameter in the Hugging Face Trainer API that specifies how to aggregate the predictions of multiple batches during evaluation. This is particularly useful when evaluating large datasets that cannot fit into memory all at once. The aggregation_strategy parameter can be set to one of several options, including \"mean\", \"max\", \"min\", and \"median\", which determine how the predictions should be combined. </p> <pre><code>from transformers import pipeline\n# Replace this with your own checkpoint\nmodel_checkpoint = \"bert-finetuned-ner2\"\ntoken_classifier = pipeline(\"token-classification\",\n                            model=model_checkpoint)\n</code></pre> <p>Let's try finding the relevant tokens in the following request</p> <p><pre><code>token_classifier(\"create a seaborn scatterplot using A, set x=B y: C (mew: 10, mec:20)\")\n</code></pre> <pre><code>[{'entity_group': 'SOURCE',\n  'score': 0.9986413,\n  'word': 'using',\n  'start': 29,\n  'end': 34},\n {'entity_group': 'PARAM',\n  'score': 0.9031896,\n  'word': 'x',\n  'start': 42,\n  'end': 43},\n {'entity_group': 'PARAM',\n  'score': 0.99833447,\n  'word': 'y',\n  'start': 46,\n  'end': 47},\n {'entity_group': 'PP',\n  'score': 0.74444526,\n  'word': 'mew',\n  'start': 52,\n  'end': 55},\n {'entity_group': 'SOURCE',\n  'score': 0.4806772,\n  'word': ',',\n  'start': 59,\n  'end': 60}]\n</code></pre></p> <p>All in all, we can see that the model predicts the relevant tags quite well, there are some issues with <code>mew</code> and <code>mec</code> tags probably because of the aggregation strategy doesn't work well with the subword tokens that the tokeniser creates for such a short word as it is likely present in other parts but has a different subword token tags, a problem for another day</p>","tags":["huggingface","ner"]},{"location":"blog/2023/08/19/named-entity-recognition-with-huggingface-trainer.html#summary","title":"Summary","text":"<p>In this post we looked at how we can use huggingface to do named entity recognition. Let's look at the step that we took:</p> <ul> <li> NER Task</li> <li> load massive format ner annotations</li> <li> parse the annotations &amp; create word/tag pairs <code>ldf</code> parse dataset</li> <li> convert parsed dataset to <code>dataframe</code> then <code>dataset</code> format <code>raw_dataset</code> parse dataset</li> <li> tokenise the input <code>text</code> from input dataset &amp; correct <code>raw_dataset</code> ner tags to coincide with subtokens subtoken ner tag adjustment</li> <li> defined a model &amp; data collator for token classification</li> <li> trained our token classifier which achieved an accuracy of 0.99+ on the training dataset</li> <li> defined a pipeline and tested our model on some input data, the pipeline returns <code>entity_group</code> &amp; relevant <code>score</code> for each tag it has identified</li> </ul> <p>So this wraps up our post. Such an approach definitely helps simplify things, a custom training loop like in a previous post is not needed because of the <code>trainer</code>, which is very convenient because we can save the trainer upon fine tuning &amp; easily do inference on new data using the pipeline method. Having used the massive dataset format) &amp; storing it in a <code>csv</code>, created some additional preprocessing steps, in addition to the adjustments we needed to make to each word token, so we would have data for subword tokens added some complexity. We also didn't experiment with the <code>aggregation_strategy</code> in pipeline since our results were more or less good, however its also something to consider when creating a NER tagger, you can find the different approaches here</p> <p>Any questions or comments about the above post can be addressed on the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>","tags":["huggingface","ner"]},{"location":"blog/2023/09/25/comparison-of-subsets.html","title":"Comparison of Subsets","text":"<p>An important concept in machine learning is model generalisation &amp; performance deterioration</p> <ul> <li>When we train a model, we perform an optimisation step, using metrics and/or loss values we can understand how well our model is understanding the relation between all data points and features in the input data we feed it</li> <li>Going through this process, we can tune a model so that it performs well on the data that we use to train it</li> </ul>","tags":["eda","machine learning","classification"]},{"location":"blog/2023/09/25/comparison-of-subsets.html#background","title":"Background","text":"<p>However, what if we receive some new data (for example after some time), which is also a subset of the entire dataset but it contains a slightly different data relation structure, our models may not perform as well as they did on the data it was trained on, by comparing the metrics of both dataset, we can get an idea of how similar our datasets are.</p> <p>The above approach to comparing the two datasets isn't the most ideal, but it should raise a question; are there any other approaches that we can use to compare different subsets of data? This is what we'll look at in this post!</p>","tags":["eda","machine learning","classification"]},{"location":"blog/2023/09/25/comparison-of-subsets.html#subset-comparison-approaches","title":"Subset Comparison Approaches","text":"<p>We'll look at these approaches:</p> <ul> <li><code>Violin plots</code> comparing the univariate distribution between two subsets</li> <li><code>Andrews Curves</code>, which helps us distinguish outliers that exist in the two datasets</li> <li><code>ANOVA</code>, to establish whether the difference between the two subsets is significant or not</li> <li>*<code>KS Statistic*</code> to check whether the each variable in train/test comes from the same distribution</li> <li><code>Machine Learning</code>, to classify the two subsets and compare their metrics</li> </ul>","tags":["eda","machine learning","classification"]},{"location":"blog/2023/09/25/comparison-of-subsets.html#the-dataset","title":"The Dataset","text":"<p>We'll use a favourite dataset of mine, which we can load very easily load via <code>seaborn</code>, it contains three categorical variables, so we'll use <code>LabelEncoder</code> to encode them. Then we'll create two subsets from this dataset</p> <pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom seaborn import load_dataset\nfrom sklearn.preprocessing import LabelEncoder\n\ndata = load_dataset('penguins')\ndata = data.dropna()\ndata['island'] = LabelEncoder().fit_transform(data['island'])\ndata['species'] = LabelEncoder().fit_transform(data['species'])\ndata['sex'] = LabelEncoder().fit_transform(data['sex'])\n</code></pre> <p>Then we create two subsets from the same dataset, creating <code>X_train</code> and <code>X_test</code>, we'll use these subsets as well as the main <code>data</code>, having created a new column <code>dataset</code> that indicates which subset the data belongs to</p> <pre><code># function \ndef PrepareData():\n\n    features = ['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g','island','species','sex']\n    X = data[features].copy()\n    y = data['sex']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                        test_size=0.20, \n                                                        random_state=42)\n\n    return X_train, X_test, y_train, y_test\n\n# prepare data\nX_train, X_test, y_train, y_test = PrepareData()\nX_train['dataset'] = 'train'; X_test['dataset'] = 'test'\ndata = X_train.append(X_test)\n</code></pre>","tags":["eda","machine learning","classification"]},{"location":"blog/2023/09/25/comparison-of-subsets.html#visualisation-approaches","title":"Visualisation Approaches","text":"","tags":["eda","machine learning","classification"]},{"location":"blog/2023/09/25/comparison-of-subsets.html#violin-plot","title":"Violin Plot","text":"<p>A violin plot is a type of data visualization that combines the features of a boxplot and a kernel density plot. It is used to display the distribution of a dataset and shows the <code>median</code>, <code>quartiles</code>, and <code>interquartile range</code> (IQR) like a boxplot, but also displays the shape of the distribution through a density curve. The width of the violin at any point represents the density or frequency of data at that point. Violin plots are useful for comparing the distributions of multiple datasets and identifying any differences or similarities between them</p> <pre><code>def violin_plots(data):\n\n    # create subplots\n    f, axes = plt.subplots(1,5,figsize = (10,3))\n    ax0 = axes.ravel()[0]\n\n    # set ticks to non\n    for ax in axes.ravel():\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    features = data.drop('sex', axis=1).columns[:-2]\n\n    # iterate over all variables and generate violin plot\n    for i, col in enumerate(features):\n        ax = axes.ravel()[i]\n        g = sns.violinplot(x='sex', y=col, hue='dataset',\n                           ax=ax, \n                           data=data, \n                           split=True, \n                           palette = ['#d0ec37', '#2c3e50'])\n        ax.set_title(col, fontsize=14, color='#0a3d62', fontfamily='monospace')\n        ax.set_ylabel('')\n        ax.set_xticks([])\n        ax.set_xlabel(''); \n        ax.get_legend().remove()\n        sns.despine(top=True, right=True, left=True, bottom=True)\n\n    plt.suptitle('UNIVARIATE DISTRIBUTIONS FOR BOTH SUBSETS', \n                 fontsize=16, \n                 color='#0a3d62', \n                 fontweight='bold', \n                 fontfamily='monospace')\n\n    plt.tight_layout()\n    plt.savefig('violin3.png')\n\nviolin_plots(data)\n</code></pre> <p></p>","tags":["eda","machine learning","classification"]},{"location":"blog/2023/09/25/comparison-of-subsets.html#andrew-curves","title":"Andrew Curves","text":"<p>Andrews curves are a type of data visualization that represent each observation in a dataset as a curve. The curve is generated by mapping the values of each feature in the dataset to a sine function, which is then plotted on a two-dimensional plane. Each curve represents one observation in the dataset, and the shape of the curve reflects the values of the features for that observation. It preserves means, distance, and variances. They curves are useful for visualizing the overall structure of a dataset and identifying any patterns or trends that may exist. They can also be used to compare the shapes of curves between different datasets to identify any differences or similarities</p> <p>It is given by formula:</p> \\[ T(n) = \\frac{x_1}{sqrt(2)} + x_2sin(n) + x_3 cos(n) + x_4 sin(2n) + x_5 cos(2n) + ... \\] <pre><code>from pandas.plotting import andrews_curves\n\ndata2['dataset'] = data2.dataset.replace({1:'TRAIN',2:'TEST'})\ndata3 = data2.drop('y', axis=1)\n\ndef plot_andrews(df):\n\n    f,ax = plt.subplots(1,1, figsize=(15,5))\n    andrews_curves(data3, \"dataset\", ax=ax, color = ['#2c3e50','#d0ec37'])\n    for s in ['top','right','bottom','left']:\n        ax.spines[s].set_visible(False)\n\n    plt.title('ANDREWS CURVES BY DATASET', fontsize=16, color='#0a3d62', fontfamily='monospace', fontweight='bold')\n    sns.despine(top=True, right=True, left=True, bottom=True)\n    plt.savefig('andrews.png')\n\nplot_andrews(data3)\n</code></pre> <p></p>","tags":["eda","machine learning","classification"]},{"location":"blog/2023/09/25/comparison-of-subsets.html#statistical-approaches","title":"Statistical Approaches","text":"","tags":["eda","machine learning","classification"]},{"location":"blog/2023/09/25/comparison-of-subsets.html#anova-test","title":"ANOVA Test","text":"<p>ANOVA (Analysis of Variance) is a statistical test used to determine whether there are significant differences between the means of two or more groups. It compares the variance within each group to the variance between the groups to determine if the differences in means are statistically significant. To use this approach, we'll use the <code>statsmodel</code> library, specifically the <code>ols</code> and <code>anova_lm</code> methods. The null hypothesis is that there is no significant difference between the groups being compared. More at statsmodels</p> <pre><code>import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nfrom statsmodels.stats.multicomp import MultiComparison\n\ndef anova_test(data):\n\n    # anova test\n    df_res = pd.DataFrame()\n    for col in data.columns[:-1]:\n        model = ols(f'{col} ~ dataset', data = data).fit()\n        res = sm.stats.anova_lm(model, typ=2)\n        df_res['var'] = col\n        df_res = anova_results.append(res)\n\n    # plot result for PR(&gt;F)\n\n    f, ax = plt.subplots(1,1,figsize=(12,4))\n    anova_results[['var','PR(&gt;F)']].dropna().set_index('var')['PR(&gt;F)'].plot(kind='bar', ax=ax, color = '#2c3e50', width=0.6)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=0, color='#2471a3', fontfamily='monospace', fontsize=12)\n    ax.set_xlabel('')\n    ax.set_yticks([])\n\n    for s in ['top','right','bottom','left']:\n        ax.spines[s].set_visible(False)\n    plt.axhline(y = 0.05, color = '#d0ec37', linestyle = '--', lw=3)\n\n    ax.text(-0.5,1.35,'P VALUES FOR EACH VARIABLE', fontsize=20, fontweight='bold', color='#0a3d62', fontfamily='monospace')\n    ax.text(-0.5,1.2,'P &lt; 0.05 : Variable is significantly different between two subsets', fontsize=15, color='#0a3d62', fontfamily='monospace')\n\n    for bar in ax.patches:\n        ax.annotate(\n            format(bar.get_height(), '.2f'), \n            (bar.get_x() + bar.get_width() / 2, bar.get_height()), \n            ha='center', va='bottom',\n            size=14, xytext=(0, 8), color = '#0a3d62',\n            textcoords='offset points'\n        )\n\n    plt.tight_layout()\n    plt.savefig('anova.png')\n\nanova_test(data2)\n</code></pre> <p></p> <p>The anova results show that for all features p&gt;0.05, thus the null hypothesis is accepted; there is no significant difference between the groups being compared</p>","tags":["eda","machine learning","classification"]},{"location":"blog/2023/09/25/comparison-of-subsets.html#kolmogorov-smirnov-test","title":"Kolmogorov-Smirnov Test","text":"<p>The Kolmogorov-Smirnov test is a statistical test used to determine whether a sample distribution differs significantly from a known or expected distribution. It compares the empirical cumulative distribution function (CDF) of the sample to the theoretical CDF of the expected distribution and calculates a test statistic that measures the maximum distance between the two functions. We'll be using the <code>scipy.stats.ks_2samp</code> module, which gives us the option to use three types of null &amp; alternative hypotheses, by default the <code>two-sided</code> option is used, so that will be our pair, so that's what we'll use. The null hypothesis is that the two distributions are identical, F(x)=G(x) for all x; the alternative is that they are not identical</p> <pre><code>from scipy.stats import ks_2samp\n\ndef ks_test(X_train,X_test):\n\n    ksdf = pd.DataFrame()\n    alpha = 0.05\n    for col in X_train.columns[:-2]:\n        s, p = ks_2samp(X_train[col], X_test[col])\n        ksdf = ksdf.append(pd.DataFrame({\n            'kstat' : [s],\n            'pval': [p],\n            'variable': [col],\n            'reject_null_hypo': [p&lt;alpha]\n        }), ignore_index=True)\n\n    f, ax = plt.subplots(1,1,figsize=(15,3))\n\n    ksdf[['variable','pval']].set_index('variable')['pval'].plot(kind='bar', ax=ax, color = '#2c3e50', width=0.6)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=0, color='#0a3d62', fontfamily='monospace', fontsize=12)\n    ax.set_xlabel('')\n    ax.set_yticks([])\n    for s in ['top','right','bottom','left']:\n        ax.spines[s].set_visible(False)\n    plt.axhline(y = alpha, color = '#d0ec37', linestyle = '--', lw=3)\n\n    ax.text(-0.5,1.35,'P VALUE FOR EACH VARIABLE', fontsize=20, fontweight='bold', color='#0a3d62', fontfamily='monospace')\n    ax.text(-0.5,1.2,f'P &gt; {alpha} : None of variables are significantly different between train and test', fontsize=15, color='#0a3d62', fontfamily='monospace')\n\n    for bar in ax.patches:\n        ax.annotate(\n            format(bar.get_height(), '.2f'), \n            (bar.get_x() + bar.get_width() / 2, bar.get_height()), \n            ha='center', va='bottom',\n            size=15, xytext=(0, 8), color = '#0a3d62',\n            textcoords='offset points')\n    plt.savefig('ks_stat.png')\n\nks_test(X_train,X_test)\n</code></pre> <p></p> <p>The Kolmogorov-Smirnov test results show that for all the features in the data p&gt;0.05, so the null hypothesis is accepted; for all the features the two subsets the two distributions are identical</p>","tags":["eda","machine learning","classification"]},{"location":"blog/2023/09/25/comparison-of-subsets.html#machine-learning-approach","title":"Machine Learning Approach","text":"<p>As we mentioned in the beginning we can utilise machine learning to compare different subsets, in fact this probably is the most common approach as it is a necessity everytime you train a new model, you need to validate your model. As a result, comparing metrics on different subsets of data is quite standard. However, what we will do here a little different. We will set our taget variable as <code>dataset</code> &amp; treat the problem as a binary classification problem, we'll use the <code>CatBoostClassifier</code> as our model with preset optimised hyperparameters</p> <p><pre><code>from sklearn.model_selection import train_test_split\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import roc_auc_score\n\ndef model_approach(X_train,X_test):\n\n    X_train['label'] = 0\n    X_test['label'] = 1\n    data = X_train.append(X_test)\n\n    X = data.drop('label',axis=1)\n    y = data.label\n\n    X_train, X_test, y_train, y_test = train_test_split(X,y, \n                                                        test_size=0.20, \n                                                        random_state=42)\n\n    # train &amp; predict on subsets\n    model = CatBoostClassifier(verbose=False)\n    model.fit(X_train,y_train.values)\n    ym_train = model.predict(X_train)\n    ym_test = model.predict(X_test)\n\n    # get AUC metric results\n    print('TRAIN AUC :',round(roc_auc_score(y_train.values, train_preds),4))\n    print('TEST AUC:',round(roc_auc_score(y_test.values, test_preds),4))\n\nmodel_approach(X_train,X_test)\n# TRAIN AUC : 0.50\n# TEST AUC: 0.50\n</code></pre> We aren\u2019t too concerned about the metric value because the features aren\u2019t not optimised for this problem, we\u2019re interested in the difference between the two models. The model performs the same on both subsets, which is a good indicator to assume both subsets have a very simular data structure, thus are more or less similar.</p>","tags":["eda","machine learning","classification"]},{"location":"blog/2023/09/25/comparison-of-subsets.html#conclusion","title":"Conclusion","text":"<p>In this post, we tried different approaches to compare datasets. We can do this in a number of ways, using visualisations, statistics &amp; machine learning approach. In our problem, we could conclude that the two subsets that we created at the start of the post There doesn't seem to be an ideal method which fits the problem at hand, so its good practice to do a number of checks when comparing datasets datasets</p> <p>Thank you for reading!</p> <p>Any questions or comments about the above post can be addressed on the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>","tags":["eda","machine learning","classification"]},{"location":"blog/2023/10/13/using-prophet-with-pyspark.html","title":"Utilising Prophet with PySpark","text":"<p>In this notebook, we look at how to use a popular machine learning library prophet with the pyspark architecture. pyspark itself unfortunatelly does not contain such an additive regression model, however we can utilise user defined functions, UDF, which allows us to utilise different functionality of different libraries that is not available in pyspark</p> <p>asa</p> <ul> <li> Run on Colab</li> <li> Download dataset</li> </ul>","tags":["machine learning","pyspark","prophet","regression","time series","eda"]},{"location":"blog/2023/10/13/using-prophet-with-pyspark.html#background","title":"Background","text":"","tags":["machine learning","pyspark","prophet","regression","time series","eda"]},{"location":"blog/2023/10/13/using-prophet-with-pyspark.html#prophet","title":"Prophet","text":"<p><code>Prophet</code> is a time series forecasting model. It is based on an additive regression model that takes into account trends, seasonality, and holidays. <code>Prophet</code> also allows for the inclusion of external regressors and can handle missing data and outliers. It uses Bayesian inference to estimate the parameters of the model and provides uncertainty intervals for the forecasts. Such a model is not available in the pyspark library, so we'll need to utilise user defined functions to utilise it with our dataset!</p>","tags":["machine learning","pyspark","prophet","regression","time series","eda"]},{"location":"blog/2023/10/13/using-prophet-with-pyspark.html#udf","title":"UDF","text":"<p>Pandas <code>UDFs</code> (User-Defined Functions) are one form of UDF that is available in pyspark. They allow you to apply a Python function that operates on pandas dataframes to Spark dataframes. This allows you to leverage the power of pandas, which is a popular data manipulation library in Python, in your PySpark applications. Pandas <code>UDFs</code> can take one or more input columns and return one or more output columns, which can be of any data type supported by Spark. With Pandas <code>UDFs</code>, you can perform complex data manipulations that are not possible using built-in Spark SQL functions. </p> <p>Of course, this is not a guide on UDF, nor are we going for the most optimal setup, it is simply an example of how we can use the rich user defined functionality of pyspark to integrate other functionalities not available in pyspark</p>","tags":["machine learning","pyspark","prophet","regression","time series","eda"]},{"location":"blog/2023/10/13/using-prophet-with-pyspark.html#avocado-price-prediction","title":"Avocado Price Prediction","text":"<p>Avocado price prediction is the process of using machine learning algorithms to forecast the future prices of avocados based on historical data and other relevant factors such as weather patterns, consumer demand, and supply chain disruptions. This can help stakeholders in the avocado industry make informed decisions about when and where to sell their avocados, as well as how much to charge for them. Avocado price prediction can also provide insights into the factors that affect avocado sales and help optimize the industry's efficiency and profitability.</p>","tags":["machine learning","pyspark","prophet","regression","time series","eda"]},{"location":"blog/2023/10/13/using-prophet-with-pyspark.html#the-dataset","title":"The Dataset","text":"<p>It is a well known fact that Millenials LOVE Avocado Toast. It's also a well known fact that all Millenials live in their parents basements.Clearly, they aren't buying home because they are buying too much Avocado Toast! But maybe there's hope\u2026 if a Millenial could find a city with cheap avocados, they could live out the Millenial American Dream.</p> <p>The dataset can be found on Kaggle &amp; its original source found here, it contains historical sales data for different avocado types in various states the United States</p>","tags":["machine learning","pyspark","prophet","regression","time series","eda"]},{"location":"blog/2023/10/13/using-prophet-with-pyspark.html#loading-data","title":"Loading data","text":"<p>To load the data, we start a spark session on our local machine</p> <pre><code>! pip install pyspark\n\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as f\nimport pandas as pd\n\n# Start spark session\nspark = SparkSession.builder\\\n                    .master(\"local\")\\\n                    .appName(\"prophet\")\\\n                    .getOrCreate()\n</code></pre> <p>To read the data, we'll use the <code>session.read.csv</code>, together with <code>inferSchema</code> method and look at the table schematics using <code>printSchema()</code> method to automatically assign types to table columns</p> <pre><code># read csv\nsales = spark.read.csv('/kaggle/input/avocado-prices/avocado.csv',header=True,inferSchema=True)\nsales.printSchema()\n</code></pre> <pre><code>root\n |-- _c0: integer (nullable = true)\n |-- Date: date (nullable = true)\n |-- AveragePrice: double (nullable = true)\n |-- Total Volume: double (nullable = true)\n |-- 4046: double (nullable = true)\n |-- 4225: double (nullable = true)\n |-- 4770: double (nullable = true)\n |-- Total Bags: double (nullable = true)\n |-- Small Bags: double (nullable = true)\n |-- Large Bags: double (nullable = true)\n |-- XLarge Bags: double (nullable = true)\n |-- type: string (nullable = true)\n |-- year: integer (nullable = true)\n |-- region: string (nullable = true)\n</code></pre>","tags":["machine learning","pyspark","prophet","regression","time series","eda"]},{"location":"blog/2023/10/13/using-prophet-with-pyspark.html#exploring-data","title":"Exploring Data","text":"<p>As with any dataset, its good to get a better understanding by exploring the data you are working with. Having loaded our data, first lets take a peek at our dataset, </p> <p>Let's use <code>select</code>,<code>orderBy</code> &amp; <code>show</code> methods to show our dataset</p> <p><pre><code>sales.select('Date','type','Total Volume','region')\\\n     .orderBy('Date')\\\n     .show(5)\n</code></pre> <pre><code>+----------+------------+------------+----------------+\n|      Date|        type|Total Volume|          region|\n+----------+------------+------------+----------------+\n|2015-01-04|conventional|   116253.44|BuffaloRochester|\n|2015-01-04|conventional|   158638.04|        Columbus|\n|2015-01-04|conventional|   5777334.9|      California|\n|2015-01-04|conventional|   435021.49|         Atlanta|\n|2015-01-04|conventional|   166006.29|       Charlotte|\n+----------+------------+------------+----------------+\n</code></pre></p> <p>The <code>Date</code> unique values can be called and checked, we have weekly data for different regions</p> <pre><code>sales.select(col('Date')).distinct().orderBy('Date').show(5)\n</code></pre> <pre><code>+----------+\n|      Date|\n+----------+\n|2015-01-04|\n|2015-01-11|\n|2015-01-18|\n|2015-01-25|\n|2015-02-01|\n+----------+\nonly showing top 5 rows\n</code></pre> <p>It's also good to know the range of the <code>date</code> of our dataset</p> <pre><code>sales.select(f.max(f.col('Date')).alias('last'),f.min(f.col('Date')).alias('first')).show()\n</code></pre> <pre><code>+----------+----------+\n|      last|     first|\n+----------+----------+\n|2018-03-25|2015-01-04|\n+----------+----------+\n</code></pre> <p>We will be using <code>Total Volume</code> as our target variable we'll be predicting. We also can note that we have different types <code>type</code> of avocados (organic and conventional)</p> <pre><code>sales.select(col('type')).distinct().show()\n</code></pre> <pre><code>+------------+\n|        type|\n+------------+\n|     organic|\n|conventional|\n+------------+\n</code></pre> <p>So what we'll be doing is creating a model to predict the sales for both of these types, which is something we'll need to incorporate into our <code>UDF</code></p> <p>We can also check the <code>region</code> limits for <code>Total Volume</code>, we can do this by using <code>agg</code> method with the <code>groupby</code> dataframe type (<code>pyspark.sql.group.GroupedData</code>):</p> <pre><code># min and maximum of sale volume\nsales.groupby('region').agg(f.max('Total Volume')).show()  # get max of column\nsales.groupby('region').agg(f.min('Total Volume')).show()  # get min of column\n</code></pre> <pre><code>+------------------+-----------------+\n|            region|max(Total Volume)|\n+------------------+-----------------+\n|     PhoenixTucson|       2200550.27|\n|       GrandRapids|        408921.57|\n|     SouthCarolina|        706098.15|\n|           TotalUS|    6.250564652E7|\n|  WestTexNewMexico|       1637554.42|\n|        Louisville|        169828.77|\n|      Philadelphia|         819224.3|\n|        Sacramento|         862337.1|\n|     DallasFtWorth|       1885401.44|\n|      Indianapolis|        335442.41|\n|          LasVegas|        680234.93|\n|         Nashville|        391780.25|\n|        GreatLakes|       7094764.73|\n|           Detroit|        880540.45|\n|            Albany|        216738.47|\n|          Portland|       1189151.17|\n|  CincinnatiDayton|        538518.77|\n|          SanDiego|        917660.79|\n|             Boise|        136377.55|\n|HarrisburgScranton|        395673.05|\n+------------------+-----------------+\nonly showing top 20 rows\n\n+------------------+-----------------+\n|            region|min(Total Volume)|\n+------------------+-----------------+\n|     PhoenixTucson|          4881.79|\n|       GrandRapids|           683.76|\n|     SouthCarolina|           2304.3|\n|           TotalUS|        501814.87|\n|  WestTexNewMexico|          4582.72|\n|        Louisville|           862.59|\n|      Philadelphia|           1699.0|\n|        Sacramento|          3562.52|\n|     DallasFtWorth|          6568.67|\n|      Indianapolis|           964.25|\n|          LasVegas|           2988.4|\n|         Nashville|          2892.29|\n|        GreatLakes|         56569.37|\n|           Detroit|          4973.92|\n|            Albany|            774.2|\n|          Portland|          7136.88|\n|  CincinnatiDayton|          6349.77|\n|          SanDiego|          5564.87|\n|             Boise|           562.64|\n|HarrisburgScranton|           971.81|\n+------------------+-----------------+\nonly showing top 20 rows\n</code></pre> <p>We can note that we have data for not only the different <code>regions</code>, but also for the entire country <code>TotalUS</code>. Also interesting to note is that the difference in <code>max</code> and <code>min</code> values is quite high.</p> <p>Let's find the locations (<code>region</code>) with the highest <code>total volumes</code> </p> <pre><code>from pyspark.sql.functions import desc,col\n\nby_volume = sales.orderBy(desc(\"Total Volume\"))\\\n                 .where(col('region') != 'TotalUS')\nby_volume.show(5)\n</code></pre> <pre><code>+---+----------+------------+-------------+----------+----------+---------+----------+----------+----------+-----------+------------+----+----------+\n|_c0|      Date|AveragePrice| Total Volume|      4046|      4225|     4770|Total Bags|Small Bags|Large Bags|XLarge Bags|        type|year|    region|\n+---+----------+------------+-------------+----------+----------+---------+----------+----------+----------+-----------+------------+----+----------+\n| 47|2017-02-05|        0.66|1.127474911E7|4377537.67|2558039.85|193764.89| 4145406.7|2508731.79|1627453.06|    9221.85|conventional|2017|      West|\n| 47|2017-02-05|        0.67|1.121359629E7|3986429.59|3550403.07|214137.93| 3462625.7|3403581.49|   7838.83|   51205.38|conventional|2017|California|\n|  7|2018-02-04|         0.8|1.089467777E7|4473811.63|4097591.67|146357.78|2176916.69|2072477.62|  34196.27|    70242.8|conventional|2018|California|\n|  7|2018-02-04|        0.83|1.056505641E7|3121272.58|3294335.87|142553.21|4006894.75|1151399.33|2838239.39|   17256.03|conventional|2018|      West|\n| 46|2016-02-07|         0.7|1.036169817E7|2930343.28|3950852.38| 424389.6|3056112.91|2693843.02| 344774.59|    17495.3|conventional|2016|California|\n+---+----------+------------+-------------+----------+----------+---------+----------+----------+----------+-----------+------------+----+----------+\nonly showing top 5 rows\n</code></pre> <p>We can note that <code>California</code> &amp; <code>West</code> regions have had the highest values for <code>Total Volume</code> on 2017-02-05</p> <p>Its also interest to note the difference in <code>Total Volume</code> for both types of avocado, so lets check that, lets just check the difference in <code>max</code> values</p> <pre><code>by_volume.groupby('type').agg(f.max('Total Volume')).show()\n</code></pre> <pre><code>+------------+-----------------+\n|        type|max(Total Volume)|\n+------------+-----------------+\n|     organic|        793464.77|\n|conventional|    1.127474911E7|\n+------------+-----------------+\n</code></pre> <p>So we can note that tehre is a significant diffence in <code>Total Volume</code>, let's also check when this actually occured:</p> <pre><code>by_volume.filter(f.col('Total Volume') == 1.127474911E7).show()\nby_volume.filter(f.col('Total Volume') == 793464.77).show()\n</code></pre> <pre><code>+---+----------+------------+-------------+----------+----------+---------+----------+----------+----------+-----------+------------+----+------+\n|_c0|      Date|AveragePrice| Total Volume|      4046|      4225|     4770|Total Bags|Small Bags|Large Bags|XLarge Bags|        type|year|region|\n+---+----------+------------+-------------+----------+----------+---------+----------+----------+----------+-----------+------------+----+------+\n| 47|2017-02-05|        0.66|1.127474911E7|4377537.67|2558039.85|193764.89| 4145406.7|2508731.79|1627453.06|    9221.85|conventional|2017|  West|\n+---+----------+------------+-------------+----------+----------+---------+----------+----------+----------+-----------+------------+----+------+\n</code></pre> <pre><code>+---+----------+------------+------------+--------+---------+-----+----------+----------+----------+-----------+-------+----+---------+\n|_c0|      Date|AveragePrice|Total Volume|    4046|     4225| 4770|Total Bags|Small Bags|Large Bags|XLarge Bags|   type|year|   region|\n+---+----------+------------+------------+--------+---------+-----+----------+----------+----------+-----------+-------+----+---------+\n|  5|2018-02-18|        1.39|   793464.77|150620.0|425616.86|874.9| 216353.01| 197949.51|   18403.5|        0.0|organic|2018|Northeast|\n+---+----------+------------+------------+--------+---------+-----+----------+----------+----------+-----------+-------+----+---------+\n</code></pre> <p>Let's also check how many regions there actually are:</p> <pre><code>sales.select('region').distinct().count()\n</code></pre> <pre><code>54\n</code></pre> <p>Which is interesting as there are only 50 states in the US, so perhaps <code>west</code> is a summation for all states on the west coast, lets check if there is also an east coast</p> <pre><code>sales.groupBy('region').count().show(100)\n</code></pre> <pre><code>+-------------------+-----+\n|             region|count|\n+-------------------+-----+\n|      PhoenixTucson|  338|\n|        GrandRapids|  338|\n|      SouthCarolina|  338|\n|            TotalUS|  338|\n|   WestTexNewMexico|  335|\n|         Louisville|  338|\n|       Philadelphia|  338|\n|         Sacramento|  338|\n|      DallasFtWorth|  338|\n|       Indianapolis|  338|\n|           LasVegas|  338|\n|          Nashville|  338|\n|         GreatLakes|  338|\n|            Detroit|  338|\n|             Albany|  338|\n|           Portland|  338|\n|   CincinnatiDayton|  338|\n|           SanDiego|  338|\n|              Boise|  338|\n| HarrisburgScranton|  338|\n|            StLouis|  338|\n|   NewOrleansMobile|  338|\n|           Columbus|  338|\n|         Pittsburgh|  338|\n|  MiamiFtLauderdale|  338|\n|       SouthCentral|  338|\n|            Chicago|  338|\n|   BuffaloRochester|  338|\n|              Tampa|  338|\n|          Southeast|  338|\n|             Plains|  338|\n|            Atlanta|  338|\n|BaltimoreWashington|  338|\n|            Seattle|  338|\n|       SanFrancisco|  338|\n|HartfordSpringfield|  338|\n|            Spokane|  338|\n| NorthernNewEngland|  338|\n|            Roanoke|  338|\n|         LosAngeles|  338|\n|            Houston|  338|\n|       Jacksonville|  338|\n|  RaleighGreensboro|  338|\n|               West|  338|\n|            NewYork|  338|\n|           Syracuse|  338|\n|         California|  338|\n|            Orlando|  338|\n|          Charlotte|  338|\n|           Midsouth|  338|\n|             Denver|  338|\n|             Boston|  338|\n|          Northeast|  338|\n|    RichmondNorfolk|  338|\n+-------------------+-----+\n</code></pre> <p>So as the name suggests, its a grouping that doesn't actually correspond to states, but rather are some general zones, mostly city specific regions, however we also have Northeast, West,Midsouth &amp; SouthCentral regions.</p> <p>Let's check how many values we have for each <code>region</code></p> <pre><code># value counts\nsales.groupBy('region').count().orderBy('count', ascending=True).show()\n</code></pre> <pre><code>+------------------+-----+\n|            region|count|\n+------------------+-----+\n|  WestTexNewMexico|  335|\n|     PhoenixTucson|  338|\n|       GrandRapids|  338|\n|     SouthCarolina|  338|\n|           TotalUS|  338|\n|        Louisville|  338|\n|      Philadelphia|  338|\n|        Sacramento|  338|\n|     DallasFtWorth|  338|\n|      Indianapolis|  338|\n|          LasVegas|  338|\n|         Nashville|  338|\n|        GreatLakes|  338|\n|           Detroit|  338|\n|            Albany|  338|\n|          Portland|  338|\n|  CincinnatiDayton|  338|\n|          SanDiego|  338|\n|             Boise|  338|\n|HarrisburgScranton|  338|\n+------------------+-----+\nonly showing top 20 rows\n</code></pre> <p>Looks like we mostly have 338 historical data points for each region, except for WestTexNewMexico.</p> <p>Turning our attention to only one region, let's check how the Houston region has been performing.</p> <pre><code># select only a subset of data\nsales.filter(f.col('region') == 'Houston').show()\n</code></pre> <pre><code>+---+----------+------------+------------+---------+---------+---------+----------+----------+----------+-----------+------------+----+-------+\n|_c0|      Date|AveragePrice|Total Volume|     4046|     4225|     4770|Total Bags|Small Bags|Large Bags|XLarge Bags|        type|year| region|\n+---+----------+------------+------------+---------+---------+---------+----------+----------+----------+-----------+------------+----+-------+\n|  0|2015-12-27|        0.78|   944506.54|389773.22|288003.62|126150.81| 140578.89|  73711.94|  36493.62|   30373.33|conventional|2015|Houston|\n|  1|2015-12-20|        0.75|   922355.67|382444.22|278067.11|127372.19| 134472.15|  72198.16|  31520.66|   30753.33|conventional|2015|Houston|\n|  2|2015-12-13|        0.73|   998752.95| 412187.8|386865.21| 81450.04|  118249.9|  69011.01|  48622.22|     616.67|conventional|2015|Houston|\n|  3|2015-12-06|        0.74|   989676.85|368528.91| 490805.0|  7041.19| 123301.75|  61020.31|  62281.44|        0.0|conventional|2015|Houston|\n|  4|2015-11-29|        0.79|   783225.98|391616.95|289533.68|  4334.89|  97740.46|  67880.28|  29860.18|        0.0|conventional|2015|Houston|\n|  5|2015-11-22|        0.73|   913002.96|402191.51|391110.76|  6924.43| 112776.26|  70785.25|  41991.01|        0.0|conventional|2015|Houston|\n|  6|2015-11-15|        0.72|   998801.78|530464.33|332541.11|   4611.0| 131185.34|  62414.66|  68770.68|        0.0|conventional|2015|Houston|\n|  7|2015-11-08|        0.75|   983909.85|427828.16|411365.91| 20404.29| 124311.49|  56573.89|   67737.6|        0.0|conventional|2015|Houston|\n|  8|2015-11-01|        0.77|  1007805.74| 395945.0|365506.02|111263.81| 135090.91|  56198.27|  78892.64|        0.0|conventional|2015|Houston|\n|  9|2015-10-25|        0.88|   933623.58|437329.85|313129.29| 81274.85| 101889.59|  57577.21|   44260.6|      51.78|conventional|2015|Houston|\n| 10|2015-10-18|         0.9|   847813.12| 436132.2|242842.91| 80895.03|  87942.98|  59835.83|  28107.15|        0.0|conventional|2015|Houston|\n| 11|2015-10-11|        0.79|  1036269.51|410949.18|385629.69|126765.96| 112924.68|  62638.13|  50286.55|        0.0|conventional|2015|Houston|\n| 12|2015-10-04|        0.82|  1019283.99|411727.49|435388.05| 43558.15|  128610.3|   59751.0|   68859.3|        0.0|conventional|2015|Houston|\n| 13|2015-09-27|        0.86|   968988.09|383218.43| 458982.9|  16780.9| 110005.86|  61098.51|  48907.35|        0.0|conventional|2015|Houston|\n| 14|2015-09-20|        0.83|   967228.05|417701.88|445473.03|   6959.5|  97093.64|  55198.32|  41895.32|        0.0|conventional|2015|Houston|\n| 15|2015-09-13|        0.89|  1095790.27|421533.83|540499.39|  5559.36| 128197.69|  55636.91|  72560.78|        0.0|conventional|2015|Houston|\n| 16|2015-09-06|        0.89|  1090493.39|460377.08|495487.54|   6230.7| 128398.07|  63724.96|  64673.11|        0.0|conventional|2015|Houston|\n| 17|2015-08-30|        0.88|   926124.93| 559048.7|260761.33|  4514.17| 101800.73|   72264.6|  29536.13|        0.0|conventional|2015|Houston|\n| 18|2015-08-23|        0.89|   933166.17|509472.17| 321616.5|  4323.68|  97753.82|  62733.47|  35020.35|        0.0|conventional|2015|Houston|\n| 19|2015-08-16|        0.92|   968899.09|565965.79|297434.65|  3479.25|  102019.4|  65764.02|  36255.38|        0.0|conventional|2015|Houston|\n+---+----------+------------+------------+---------+---------+---------+----------+----------+----------+-----------+------------+----+-------+\n</code></pre>","tags":["machine learning","pyspark","prophet","regression","time series","eda"]},{"location":"blog/2023/10/13/using-prophet-with-pyspark.html#preparing-data-for-modeling","title":"Preparing data for modeling","text":"<p>Since we don't have any missing data points for the Houston region, let's use it for our model example, let's define a subset <code>houston_df</code>, which will contain only avocado sales data for the Houston region</p> <pre><code># select houson \nhouston_df = sales.filter(f.col('region')=='Houston')\n</code></pre> <p>Let's select only the relevant features which will be used to train our model</p> <pre><code>houston_df = houston_df.groupBy(['Region','type','Date']).agg(f.sum('Total Volume').alias('y'))\nhouston_df.show(5)\n</code></pre> <pre><code>+-------+------------+----------+----------+\n| Region|        type|      Date|         y|\n+-------+------------+----------+----------+\n|Houston|conventional|2016-03-06|1091432.18|\n|Houston|conventional|2017-02-05|1977923.65|\n|Houston|conventional|2018-02-04|2381742.59|\n|Houston|     organic|2015-05-24|  12358.51|\n|Houston|     organic|2017-07-23|  40100.89|\n+-------+------------+----------+----------+\nonly showing top 5 rows\n</code></pre> <p><pre><code># change column datatype\nhouston_df = houston_df.withColumn('y',f.round('y',2))\nhouston_df = houston_df.withColumn('ds',f.to_date('Date'))\nhouston_df_final = houston_df.select(['Region','type','ds','y'])\nhouston_df_final.show(4)\n</code></pre> <pre><code>+-------+------------+----------+----------+\n| Region|        type|        ds|         y|\n+-------+------------+----------+----------+\n|Houston|conventional|2016-03-06|1091432.18|\n|Houston|conventional|2017-02-05|1977923.65|\n|Houston|conventional|2018-02-04|2381742.59|\n|Houston|     organic|2015-05-24|  12358.51|\n+-------+------------+----------+----------+\nonly showing top 4 rows\n</code></pre></p> <p>As we will be utilising prophet, we can define the different settings of features we will be utilising in the modeling process, like <code>weekly_seasonality</code> and <code>yearly_seasonality</code>, we don't really need to explicitly create features by ourselves. For this study, we'll limit ourselves with these features provided in module.</p>","tags":["machine learning","pyspark","prophet","regression","time series","eda"]},{"location":"blog/2023/10/13/using-prophet-with-pyspark.html#creating-a-model","title":"Creating a model","text":"","tags":["machine learning","pyspark","prophet","regression","time series","eda"]},{"location":"blog/2023/10/13/using-prophet-with-pyspark.html#defining-scheme","title":"Defining Scheme","text":"<p>Like any UDF, we need to define the output type of our data, let's prepare the data format scheme for the outputs of our <code>UDF</code>. We will need to use <code>StructType</code> and the relevant type improted from <code>sql.types</code></p> <pre><code>import pyspark.sql.types  as ty\n\nschema = ty.StructType([\n                     ty.StructField('Region', ty.StringType()),     # our main features\n                     ty.StructField('type', ty. StringType()),      # \n                     ty.StructField('ds', ty.TimestampType()),      # \n                     ty.StructField('y', ty.FloatType()),           #\n                     ty.StructField('yhat', ty.DoubleType()),\n                     ty.StructField('yhat_upper', ty.DoubleType()),\n                     ty.StructField('yhat_lower', ty.DoubleType()),\n                     ]) \n</code></pre>","tags":["machine learning","pyspark","prophet","regression","time series","eda"]},{"location":"blog/2023/10/13/using-prophet-with-pyspark.html#udf_1","title":"UDF","text":"<p>Our <code>UDF</code> will be slightly involved than our typical UDF, we will be using <code>PandasUDFType.GROUPED_MAP</code> so it should be called with <code>groupby</code> &amp; <code>apply</code></p> <p><code>PandasUDFType.GROUPED_MAP</code> is a type of user-defined function (UDF) in PySpark that allows for the application of a Pandas function to each group of data within a Spark DataFrame. This UDF type is useful when working with grouped data, such as when aggregating data by a certain column or grouping data by time intervals.</p> <pre><code>from prophet import Prophet\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\n\n@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\ndef apply_model(store_pd):\n\n  # instantiate the model and set parameters\n    model = Prophet(\n      interval_width=0.1,\n      growth='linear',\n      daily_seasonality=False,\n      weekly_seasonality=True,\n      yearly_seasonality=True,\n      seasonality_mode='multiplicative'\n    )\n\n    # fit the model to historical data\n    model.fit(store_pd)\n\n    # Create a data frame that lists 90 dates starting from Jan 1 2018\n    future = model.make_future_dataframe(\n      periods=6,\n      freq='w',\n      include_history=True)\n\n    # Out of sample prediction\n    future = model.predict(future)\n\n    # Create a data frame that contains store, item, y, and yhat\n    f_pd = future[['ds', 'yhat', 'yhat_upper', 'yhat_lower']].copy()\n    st_pd = store_pd[['ds', 'Region','y']].copy()\n    f_pd.loc[:,'ds'] = pd.to_datetime(f_pd['ds'])\n    st_pd.loc[:,'ds'] = pd.to_datetime(st_pd['ds'])\n    result_pd = f_pd.join(st_pd.set_index('ds'), on='ds', how='left')\n\n    result_pd.loc[:,'Region'] = store_pd['Region'].iloc[0]\n    result_pd.loc[:,'type'] = store_pd['type'].iloc[0]\n\n    return result_pd[['Region','type', 'ds', 'y', 'yhat',\n                    'yhat_upper', 'yhat_lower']]\n</code></pre>","tags":["machine learning","pyspark","prophet","regression","time series","eda"]},{"location":"blog/2023/10/13/using-prophet-with-pyspark.html#training-process","title":"Training process","text":"<p>Our model setup will be the following hyperparameters:</p> <pre><code>model = Prophet(\n interval_width=0.1,\n growth='linear',\n daily_seasonality=False,\n weekly_seasonality=True,\n yearly_seasonality=True,\n seasonality_mode='multiplicative'\n)\n</code></pre> <p>Having fitted the model on our data (which runs from 2015-01-04 to 2018-03-25), we'll be making a prediction using <code>model.make_future_dataframe</code>, in which we'll be specifying that we want to make a prediction for 6 weeks in advanced, setting the model prediction parameters. Then we simply call <code>model.predict</code> to actually make the prediction.</p> <p>To create models for both region (which in our case will only be Houston) &amp; type (organic &amp; convensional), we'll call the <code>apply</code> method for the pyspark dataframe</p> <pre><code>results = houston_df_final.groupby(['Region','type']).apply(apply_model)\nresults.show()\n</code></pre> <pre><code>+-------+------------+-------------------+----------+------------------+------------------+------------------+\n| Region|        type|                 ds|         y|              yhat|        yhat_upper|        yhat_lower|\n+-------+------------+-------------------+----------+------------------+------------------+------------------+\n|Houston|conventional|2015-01-04 00:00:00| 1062990.6| 967605.2625150415| 987966.9732773455| 952305.7433327858|\n|Houston|conventional|2015-01-11 00:00:00| 1062071.6| 993363.6182296885| 995012.7884165086| 964197.9475039787|\n|Houston|conventional|2015-01-18 00:00:00| 1017854.2| 1015966.900950469|1025256.0703683371| 986191.4013155274|\n|Houston|conventional|2015-01-25 00:00:00| 983910.94| 1074995.886186154|1101222.7673731335| 1062121.771442082|\n|Houston|conventional|2015-02-01 00:00:00| 1280364.0|1167900.5325763628| 1188681.132535528|  1145180.90581613|\n|Houston|conventional|2015-02-08 00:00:00| 1180723.0|1232065.4408061658|1239550.0307806057|1198043.2132719166|\n|Houston|conventional|2015-02-15 00:00:00| 1062387.8|1199450.2180199602|1223060.5212891083|1176312.1838248386|\n|Houston|conventional|2015-02-22 00:00:00| 978807.75|1073369.2242733259|1080957.9300315154|1040938.8331911729|\n|Houston|conventional|2015-03-01 00:00:00| 991328.44| 941094.8884144317| 959840.4691880762| 919962.6290055071|\n|Houston|conventional|2015-03-08 00:00:00| 1166055.9| 898815.9800298921| 918134.9334112646| 880629.0129257607|\n|Houston|conventional|2015-03-15 00:00:00|1043172.75| 962628.9504384592| 984672.2940272797|  941114.615399528|\n|Houston|conventional|2015-03-22 00:00:00|1036663.75|1059573.4445409325|1079371.1160897035|1042058.3879815078|\n|Houston|conventional|2015-03-29 00:00:00| 1116225.2|1108114.6730682629| 1128250.035336084| 1084141.715137374|\n|Houston|conventional|2015-04-05 00:00:00| 1249645.2|1100408.8290516583| 1116313.220982885| 1080221.717887945|\n|Houston|conventional|2015-04-12 00:00:00| 1096075.0|1098549.7905255936|1117497.0118921385|1076220.1122365214|\n|Houston|conventional|2015-04-19 00:00:00|  933033.2| 1153840.801604051|1185673.7940084708|1142908.6811052496|\n|Houston|conventional|2015-04-26 00:00:00| 1082231.9|1242221.1196405245| 1269836.200588267|1229695.3593816601|\n|Houston|conventional|2015-05-03 00:00:00| 1296308.0|1287101.4463574803|1309988.6590584682| 1264268.574431952|\n|Houston|conventional|2015-05-10 00:00:00| 1201673.1|1241601.8571321142|1263813.3510391738|1222570.1340492044|\n|Houston|conventional|2015-05-17 00:00:00|1025659.56| 1139019.362908239|1164737.2354728119|1124616.7852877746|\n+-------+------------+-------------------+----------+------------------+------------------+------------------+\nonly showing top 20 rows\n</code></pre>","tags":["machine learning","pyspark","prophet","regression","time series","eda"]},{"location":"blog/2023/10/13/using-prophet-with-pyspark.html#model-prediction-visualisation","title":"Model Prediction Visualisation","text":"<p>Now to visualise the results, lets do some preparation</p> <pre><code>import pyspark.pandas as ps\norganic_data = results.filter(f.col('type')=='organic').pandas_api()\norganic_data = organic_data.set_index(['ds'])\nconventional_data = results.filter(f.col('type')=='conventional').pandas_api()\nconventional_data = conventional_data.set_index(['ds'])\n</code></pre> <p>Let's visualise the <code>organic</code> subset model predictions</p> <pre><code>organic_data[['y','yhat']].plot.line(backend='plotly')\n</code></pre> <p></p> <p>And now the <code>convensional</code> subset model</p> <pre><code>conventional_data[['y','yhat']].plot.line(backend='plotly')\n</code></pre> <p></p>","tags":["machine learning","pyspark","prophet","regression","time series","eda"]},{"location":"blog/2023/10/13/using-prophet-with-pyspark.html#concluding-remarks","title":"Concluding remarks","text":"<p>In this post we looked how to utilise pyspark together with a common time series prediction library prophet. We achieved this with the use of pyspark user defined functionality which it offers to allow us to continue working with pyspark dataframe inputs. Of course such functionality would result in performance drops when using large datasets, so it must be used with caution, especially pandas UDF. Our goal was to do some data exploration with pyspark &amp; predict the total required volume of avocadoes for 6 weeks in advanced, for two different types of avocados and for the Houston region. We used the entire available dataset without any validation techniques to confirm the accuracy of our prediction, so that is something you can definitely try out next, validation of models is of course very important!</p> <p>Thank you for reading!</p> <p>Any questions or comments about the above post can be addressed on the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>","tags":["machine learning","pyspark","prophet","regression","time series","eda"]},{"location":"blog/2023/10/21/gene-classification.html","title":"Gene Classification","text":"<p>In this notebook, we look at how to work with biological sequence data, by venturing into a classification problem, in which we will be classifying between seven different genes groups common to three different species (human,chimpanzee &amp; dog)</p> <ul> <li> Open Colab Notebook</li> <li> Kaggle Dataset</li> </ul>","tags":["classification","bioinformatics","machine learning"]},{"location":"blog/2023/10/21/gene-classification.html#background","title":"Background","text":"","tags":["classification","bioinformatics","machine learning"]},{"location":"blog/2023/10/21/gene-classification.html#gene-classification-problem","title":"Gene Classification Problem","text":"<p>Gene classification using machine learning is the process of using algorithms and statistical models to analyze large datasets of genetic information and predict the function or characteristics of different genes. Machine learning techniques can be used to identify patterns in gene expression data, classify genes into different functional categories, or predict the likelihood of a gene being associated with a particular disease or phenotype. This approach can help researchers to better understand the complex relationships between genes, their functions, and their interactions with other biological systems.</p>","tags":["classification","bioinformatics","machine learning"]},{"location":"blog/2023/10/21/gene-classification.html#genes","title":"Genes","text":"<p>The dataset that we'll be using contains the following gene classes for three different species, human, chimpanzee &amp; dog, it is added more as a reference for those interested in what each class represents in our data</p> <p>Gene families are groups of related genes that share a common ancestor. Members of gene families may be paralogs or orthologs. Gene paralogs are genes with similar sequences from within the same species while gene orthologs are genes with similar sequences in different species.</p> <p>Gene Families</p> <p>\"G Protein-Coupled Receptors\"</p> <p>The G protein-coupled receptors (GPCRs) gene family is a large and diverse group of genes that encode proteins involved in cellular signaling pathways. These receptors are located on the surface of cells and are activated by a wide range of ligands, including hormones, neurotransmitters, and environmental stimuli. GPCRs play critical roles in regulating many physiological processes, such as sensory perception, hormone secretion, and immune response. Dysregulation of GPCR signaling has been implicated in a variety of diseases, including cancer, diabetes, and cardiovascular disorders.</p> <p>\"Tyrosine Kinase\"</p> <p>The tyrosine kinase gene family is a group of genes that encode proteins involved in cellular signaling pathways. These proteins are enzymes that add phosphate groups to specific tyrosine residues on target proteins, thereby regulating their activity. Tyrosine kinases play critical roles in many physiological processes, such as cell growth, differentiation, and survival. Dysregulation of tyrosine kinase signaling has been implicated in a variety of diseases, including cancer, autoimmune disorders, and developmental disorders. Examples of tyrosine kinase genes include EGFR, HER2, and BCR-ABL.</p> <p>\"Tyrosine Phosphatase\"</p> <p>The tyrosine phosphatase gene family is a group of genes that encode proteins involved in cellular signaling pathways. These proteins are enzymes that remove phosphate groups from specific tyrosine residues on target proteins, thereby regulating their activity. Tyrosine phosphatases play critical roles in many physiological processes, such as cell growth, differentiation, and survival. Dysregulation of tyrosine phosphatase signaling has also been implicated in a variety of diseases, including cancer, autoimmune disorders, and developmental disorders. Examples of tyrosine phosphatase genes include PTPN1, PTPN6, and PTPN11.</p> <p>\"Synthetase\"</p> <p>The synthetase gene family is a group of genes that encode for enzymes called aminoacyl-tRNA synthetases. These enzymes are responsible for attaching specific amino acids to their corresponding tRNA molecules during protein synthesis. There are 20 different aminoacyl-tRNA synthetases, one for each amino acid, and each enzyme recognizes and binds to its specific amino acid and tRNA molecule. The synthetase gene family is highly conserved across all living organisms and mutations in these genes can lead to various genetic disorders.</p> <p>\"Synthase\"</p> <p>The synthase gene family is a group of genes that encode enzymes responsible for synthesizing various molecules within cells. These enzymes are involved in a wide range of biological processes, including the synthesis of lipids, nucleotides, and amino acids. Different members of the synthase gene family may be involved in different aspects of these processes, and mutations in these genes can lead to a variety of diseases and disorders. Examples of synthase genes include fatty acid synthase, which is involved in the synthesis of fatty acids, and adenylyl cyclase, which synthesizes the signaling molecule cyclic AMP.</p> <p>\"Ion Channel\"</p> <p>The ion channel gene family is a group of genes that encode proteins responsible for the transport of ions across cell membranes. These proteins are integral membrane proteins that form pores or channels in the lipid bilayer of the cell membrane, allowing the selective movement of ions such as sodium, potassium, calcium, and chloride. Ion channels are critical for many physiological processes, including muscle contraction, nerve signaling, and hormone secretion. Dysregulation of ion channel activity has been implicated in a variety of diseases, including epilepsy, cardiac arrhythmias, and cystic fibrosis. Examples of ion channel genes include SCN1A, KCNQ1, and CFTR.</p> <p>\"Transcription Factors\"</p> <p>The transcription factor gene family is a group of genes that encode proteins responsible for regulating the expression of other genes. These proteins bind to DNA and control the rate at which genes are transcribed into mRNA, which is then translated into proteins. Transcription factors are involved in a wide range of biological processes, including development, differentiation, and response to environmental stimuli. Different members of the transcription factor gene family may have different target genes and regulatory mechanisms, allowing for precise control of gene expression. Mutations in these genes can lead to a variety of diseases and disorders, including cancer and developmental disorders. Examples of transcription factor genes include homeobox genes, which regulate embryonic development, and p53, which regulates cell cycle progression and DNA repair</p>","tags":["classification","bioinformatics","machine learning"]},{"location":"blog/2023/10/21/gene-classification.html#dataset","title":"Dataset","text":"<p>We're loading three DNA datasets, our dataset is in the form of a <code>sequence</code> &amp; subsequent gene family label, <code>class</code></p> <pre><code>import pandas as pd\n\nhuman_dna = pd.read_table('../input/dna-sequence-dataset/human.txt')\nchimp_dna = pd.read_table('../input/dna-sequence-dataset/chimpanzee.txt')\ndog_dna = pd.read_table('../input/dna-sequence-dataset/dog.txt')\n</code></pre>","tags":["classification","bioinformatics","machine learning"]},{"location":"blog/2023/10/21/gene-classification.html#dna-encoding","title":"DNA Encoding","text":"<p>Biological sequences come in the format:</p> <p>GTGCCCAGGTTCAGTGAGTGACACAGGCAG</p> <p>This mimics a standard NLP based problem, in which we need to convert text into numerical representation before we can feed this data into our models</p> <p>There are 3 general approaches to encode biological sequence data:</p> <ol> <li>Ordinal encoding DNA Sequence</li> <li>One-Hot encoding DNA Sequence</li> <li>DNA sequence as a \u201clanguage\u201d, known as <code>k-mer</code> counting</li> </ol> <p>So let us implement each of them and see which gives us the perfect input features.</p>","tags":["classification","bioinformatics","machine learning"]},{"location":"blog/2023/10/21/gene-classification.html#encoding-sequences","title":"Encoding Sequences","text":"","tags":["classification","bioinformatics","machine learning"]},{"location":"blog/2023/10/21/gene-classification.html#1-ordinal-encoding","title":"(1) Ordinal Encoding","text":"<pre><code># encode list of strings\ndef ordinal_encoder(my_array:list[str]):\n    integer_encoded = label_encoder.transform(my_array)\n    float_encoded = integer_encoded.astype(float)\n    float_encoded[float_encoded == 0] = 0.25 # A\n    float_encoded[float_encoded == 1] = 0.50 # C\n    float_encoded[float_encoded == 2] = 0.75 # G\n    float_encoded[float_encoded == 3] = 1.00 # T\n    float_encoded[float_encoded == 4] = 0.00 # anything else\n    return float_encoded\n\n# let\u2019s try it out with a simple sequence\nseq_test = 'TTCAGCCAGTG'\nordinal_encoder(lst_string(seq_test))\n</code></pre> <pre><code>array([1.  , 1.  , 0.5 , 0.25, 0.75, 0.5 , 0.5 , 0.25, 0.75, 1.  , 0.75])\n</code></pre> <p>One slight issue with such an approach is that if we have biological sequences of different length, we won't be able to concatenate them together without truncation or padding</p>","tags":["classification","bioinformatics","machine learning"]},{"location":"blog/2023/10/21/gene-classification.html#2-one-hot-encoding","title":"(2) One-Hot Encoding","text":"<p>Another approach is to use one-hot encoding to represent the DNA sequence. For example, \u201cATGC\u201d would become [0,0,0,1], [0,0,1,0], [0,1,0,0], [1,0,0,0] vectors &amp; these one-hot encoded vectors are then concatenated into 2-dimensional arrays. Ie. each vector represents the presence or absence of a particular nucleotides in the sequence, the total length then becomes the total number of nucleotides x nucleotide absence/present vector.</p> <p><pre><code>from sklearn.preprocessing import OneHotEncoder\n\ndef ohe(seq_string:str):\n    seq_string = lst_string(seq_string)\n    int_encoded = label_encoder.transform(seq_string)\n    onehot_encoder = OneHotEncoder(sparse_output=True,dtype=int)\n    onehot_encoded = onehot_encoder.fit_transform(int_encoded[:,None])\n    return onehot_encoded.toarray()\n\n# let\u2019s try it out with a simple sequence\nseq_test = 'GAATTCTCGAA'\nohe(seq_test)\n</code></pre> <pre><code>array([[0, 0, 1, 0],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 0, 1],\n       [0, 1, 0, 0],\n       [0, 0, 0, 1],\n       [0, 1, 0, 0],\n       [0, 0, 1, 0],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0]])\n</code></pre></p> <p>The size of these matrices will be directly proptional to their total nucleotide count. If we have sequences of different length, we would need to resort to either padding or truncation again. If we had used unitigs as in this notebooks, this problem would not exist</p>","tags":["classification","bioinformatics","machine learning"]},{"location":"blog/2023/10/21/gene-classification.html#3-k-mer-counting","title":"(3) K-MER Counting","text":"<p>DNA and protein sequences can be seen as the language of life. The language encodes instructions as well as functions for the molecules that are found in all life forms. The sequence language resemblance continues with the genome as the book, subsequences (genes and gene families) are sentences and chapters, k-mers and peptides are words, and nucleotide bases and amino acids are the alphabets. Since the relationship seems so likely, it stands to reason that the natural language processing(NLP) should also implement the natural language of DNA and protein sequences.</p> <p>The method we use here is manageable and easy. We first take the long biological sequence and break it down into k-mer length overlapping \u201cwords\u201d. For example, if we use \u201cwords\u201d of length 6 (hexamers), \u201cATGCATGCA\u201d becomes: \u2018ATGCAT\u2019, \u2018TGCATG\u2019, \u2018GCATGC\u2019, \u2018CATGCA\u2019. Hence our example sequence is broken down into 4 hexamer words.</p> <pre><code>def kmers_count(seq:str, size:int) -&gt; list:\n    return [seq[x:x+size].lower() for x in range(len(seq) - size + 1)]\n\n# let\u2019s try it out with a simple sequence\nmySeq = 'GTGCCCAGGTTCAGTGAGTGACACAGGCAG'\nkmers_count(mySeq, size=7)\n</code></pre> <pre><code>['gtgccc',\n 'tgccca',\n 'gcccag',\n 'cccagg',\n 'ccaggt',\n 'caggtt',\n 'aggttc',\n 'ggttca',\n 'gttcag',\n 'ttcagt',\n 'tcagtg',\n 'cagtga',\n 'agtgag',\n 'gtgagt',\n 'tgagtg',\n 'gagtga',\n 'agtgac',\n 'gtgaca',\n 'tgacac',\n 'gacaca',\n 'acacag',\n 'cacagg',\n 'acaggc',\n 'caggca',\n 'aggcag']\n</code></pre> <p><code>kmers_count</code> returns a list of sequences (<code>k-mer</code> words), which then can be joined together into a single <code>string</code>. Once we have this split, we can use NLP encoding/embedding methods (eg. <code>CountVectorizer</code>) to generate numerical representations of these <code>k-mer</code> words</p> <pre><code>joined_sentence = ' '.join(words)\njoined_sentence\n</code></pre> <pre><code>'gtgccc tgccca gcccag cccagg ccaggt caggtt aggttc ggttca gttcag ttcagt tcagtg cagtga agtgag gtgagt tgagtg gagtga agtgac gtgaca tgacac gacaca acacag cacagg acaggc caggca aggcag'\n</code></pre> <p>Having a \"corpus\" of sequences &amp; their labels, we need to merge them together into a single array, for example if we only have two sequences, even of different length:</p> <pre><code>mySeq1 = 'TCTCACACATGTGCCAATCACTGTCACCC'\nmySeq2 = 'GTGCCCAGGTTCAGTGAGTGACACAGGCAG'\nsentence1 = ' '.join(kmers_count(mySeq1, size=6))\nsentence2 = ' '.join(kmers_count(mySeq2, size=6))\n</code></pre> <p>Fitting a Bag of Words model, we generate a fixed dictionary size of <code>kmers</code>, thus all data will have a dimensionality proportional to the dictionary count. Similar to OHE, the content will be (1/0), corresponding to either being present in the string or not.</p> <pre><code># Creating the Bag of Words model:\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer()\nX = cv.fit_transform([sentence1, sentence2]).toarray()\n</code></pre> <pre><code>array([[1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n        1, 0, 1, 1, 0],\n       [0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n        0, 1, 0, 0, 1]])\n</code></pre>","tags":["classification","bioinformatics","machine learning"]},{"location":"blog/2023/10/21/gene-classification.html#choice-of-encoding","title":"Choice of Encoding","text":"<p>For DNA sequence classification methods, its more logical to utilise the <code>kmers</code> approach since <code>kmers</code> are commonly used in sequence analysis and genome assembly, as they can provide information about the composition and structure of the sequence. Which is on top of the non uniform sequence length issue addessed above.</p>","tags":["classification","bioinformatics","machine learning"]},{"location":"blog/2023/10/21/gene-classification.html#objective","title":"Objective","text":"<p>Our objective is to train a classification model that is trained on the human DNA sequence and can predict a gene family based on the DNA sequence of the coding sequence. To test the model, we will use the DNA sequence of humans, dogs, and chimpanzees and compare model accuracies.</p>","tags":["classification","bioinformatics","machine learning"]},{"location":"blog/2023/10/21/gene-classification.html#preprocessing","title":"Preprocessing","text":"<p>Having already loaded our dataset &amp; define our problem statement, lets create prepare our dataset for training, we'll simply using the <code>apply</code> method &amp; store the list of kmers in our dataframe</p> <p><pre><code>def kmers_count(seq:str, size:int) -&gt; list:\n    return [seq[x:x+size].lower() for x in range(len(seq) - size + 1)]\n\nhuman_dna['kmers'] = human_dna.apply(lambda x: kmers_count(x['sequence']), axis=1)\nhuman_dna = human_dna.drop('sequence', axis=1)\n\nchimp_dna['kmers'] = chimp_dna.apply(lambda x: kmers_count(x['sequence']), axis=1)\nchimp_dna = chimp_dna.drop('sequence', axis=1)\n\ndog_dna['kmers'] = dog_dna.apply(lambda x: kmers_count(x['sequence']), axis=1)\ndog_dna = dog_dna.drop('sequence', axis=1)\n</code></pre> <pre><code>     class     words\n0    4    [atgccc, tgcccc, gcccca, ccccaa, cccaac, ccaac...\n1    4    [atgaac, tgaacg, gaacga, aacgaa, acgaaa, cgaaa...\n2    3    [atgtgt, tgtgtg, gtgtgg, tgtggc, gtggca, tggca...\n3    3    [atgtgt, tgtgtg, gtgtgg, tgtggc, gtggca, tggca...\n4    3    [atgcaa, tgcaac, gcaaca, caacag, aacagc, acagc...\n</code></pre></p> <p>Let's create a list containg the kmers string for each row in the dataset (which can simply be using with <code>fit</code> in <code>CountVectorizer</code>) like we did in the example &amp; its related label:</p> <pre><code>human_texts = list(human_dna['kmers'])\nfor item in range(len(human_texts)):\n    human_texts[item] = ' '.join(human_texts[item])\n#separate labels\ny_human = human_dna.iloc[:, 0].values # y_human for human_dna\n\nchimp_texts = list(chimp_dna['kmers'])\nfor item in range(len(chimp_texts)):\n    chimp_texts[item] = ' '.join(chimp_texts[item])\n#separate labels\ny_chim = chimp_dna.iloc[:, 0].values # y_chim for chimp_dna\n\ndog_texts = list(dog_dna['kmers'])\nfor item in range(len(dog_texts)):\n    dog_texts[item] = ' '.join(dog_texts[item])\n#separate labels\ny_dog = dog_dna.iloc[:, 0].values  # y_dog for dog_dna\n</code></pre> <pre><code>human_texts[0]\n</code></pre> <pre><code>'atgccc tgcccc gcccca ccccaa cccaac ccaact caacta aactaa actaaa ctaaat taaata aaatac aatact atacta tactac actacc ctaccg taccgt accgta ccgtat cgtatg gtatgg tatggc atggcc tggccc ggccca gcccac cccacc ccacca caccat accata ccataa cataat ataatt taatta aattac attacc ttaccc tacccc accccc ccccca ccccat cccata ccatac catact atactc tactcc actcct ctcctt tcctta ccttac cttaca ttacac tacact acacta cactat actatt ctattc tattcc attcct ttcctc tcctca cctcat ctcatc tcatca catcac atcacc tcaccc caccca acccaa cccaac ccaact caacta aactaa actaaa ctaaaa taaaaa aaaaat aaaata aaatat aatatt atatta tattaa attaaa ttaaac taaaca aaacac aacaca acacaa cacaaa acaaac caaact aaacta aactac actacc ctacca taccac accacc ccacct caccta acctac cctacc ctacct tacctc acctcc cctccc ctccct tccctc ccctca cctcac ctcacc tcacca caccaa accaaa ccaaag caaagc aaagcc aagccc agccca gcccat cccata ccataa cataaa ataaaa taaaaa aaaaat aaaata aaataa aataaa ataaaa taaaaa aaaaaa aaaaat aaaatt aaatta aattat attata ttataa tataac ataaca taacaa aacaaa acaaac caaacc aaaccc aaccct accctg ccctga cctgag ctgaga tgagaa gagaac agaacc gaacca aaccaa accaaa ccaaaa caaaat aaaatg aaatga aatgaa atgaac tgaacg gaacga aacgaa acgaaa cgaaaa gaaaat aaaatc aaatct aatctg atctgt tctgtt ctgttc tgttcg gttcgc ttcgct tcgctt cgcttc gcttca cttcat ttcatt tcattc cattca attcat ttcatt tcattg cattgc attgcc ttgccc tgcccc gccccc ccccca ccccac cccaca ccacaa cacaat acaatc caatcc aatcct atccta tcctag'\n</code></pre> <p><code>CountVectorizer</code> allows us to control groupings of these kmers, lets use <code>ngram_range</code> of 4. We'll call <code>fit</code> on the human dataset &amp; <code>transform</code> all datasets:</p> <pre><code>from sklearn.feature_extraction.text import CountVectorizer\nvectoriser = CountVectorizer(ngram_range=(4,4)) #The n-gram size of 4 is previously determined \n\n# fit &amp; transform\nX = vectoriser.fit_transform(human_texts)\nX_chimp = vectoriser.transform(chimp_texts)\nX_dog = vectoriser.transform(dog_texts)\n</code></pre> <p>This will give us the following dataset size, our dictionary for <code>ngram=4</code> gives us 232414 features for our model:</p> <pre><code>print(X.shape)\nprint(X_chimp.shape)\nprint(X_dog.shape)\n</code></pre> <pre><code>(4380, 232414)\n(1682, 232414)\n(820, 232414)\n</code></pre> <p>So, for humans we have 4380 genes converted into uniform length feature vectors of 4-gram k-mer (length 6) counts. For chimpanzee and dogs, we have the same number of features with 1682 and 820 genes respectively since we <code>fit</code> on the human dataset.</p>","tags":["classification","bioinformatics","machine learning"]},{"location":"blog/2023/10/21/gene-classification.html#training-model","title":"Training Model","text":"<p>So now that we know how to transform our DNA sequences into uniform length numerical vectors in the form of k-mer counts and ngrams, we can now go ahead and build a classification model that can predict the DNA sequence function based only on the sequence itself.</p> <p>Here we will use the human data to train the model, holding out 20% of the human data to test/evaluation the model. Then we can challenge the model\u2019s generalizability by trying to predict sequence function in other species (the chimpanzee and dog).</p> <p>Next, train/test split human dataset and build simple multinomial naive Bayes classifier</p> <pre><code># Splitting the human dataset into the training set and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y_human, \n                                                    test_size = 0.20, \n                                                    random_state=42)\n</code></pre> <pre><code>from sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB(alpha=0.1)\nclassifier.fit(X_train, y_train)\n</code></pre>","tags":["classification","bioinformatics","machine learning"]},{"location":"blog/2023/10/21/gene-classification.html#evaluation","title":"Evaluation","text":"<p>For evaluation, we'll be checking the confusion matrix as well as some other metrics like f1_score for three different subsets of generalisation data:</p>","tags":["classification","bioinformatics","machine learning"]},{"location":"blog/2023/10/21/gene-classification.html#human-dataset-prediction","title":"Human Dataset Prediction","text":"<p>Let's check how well our model performs on the hold out human dataset:</p> <pre><code>from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\nprint(pd.crosstab(pd.Series(y_test, name='Actual'), pd.Series(y_pred, name='Predicted')))\ndef get_metrics(y_test, y_predicted):\n    accuracy = accuracy_score(y_test, y_predicted)\n    precision = precision_score(y_test, y_predicted, average='weighted')\n    recall = recall_score(y_test, y_predicted, average='weighted')\n    f1 = f1_score(y_test, y_predicted, average='weighted')\n    return accuracy, precision, recall, f1\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\nprint(\"accuracy = %.3f \\nprecision = %.3f \\nrecall = %.3f \\nf1 = %.3f\" % (accuracy, precision, recall, f1))\n</code></pre> <pre><code>Confusion matrix for predictions on human test DNA sequence\n\nPredicted   0    1   2    3    4   5    6\nActual                                   \n0          99    0   0    0    1   0    2\n1           0  104   0    0    0   0    2\n2           0    0  78    0    0   0    0\n3           0    0   0  124    0   0    1\n4           1    0   0    0  143   0    5\n5           0    0   0    0    0  51    0\n6           1    0   0    1    0   0  263\naccuracy = 0.984 \nprecision = 0.984 \nrecall = 0.984 \nf1 = 0.984\n</code></pre>","tags":["classification","bioinformatics","machine learning"]},{"location":"blog/2023/10/21/gene-classification.html#chimpanzee-dataset-prediction","title":"Chimpanzee Dataset Prediction","text":"<p>Let's check how well the model performs on the chimpanzee dataset:</p> <pre><code>print(pd.crosstab(pd.Series(y_chim, name='Actual'), pd.Series(y_pred_chimp, name='Predicted')))\naccuracy, precision, recall, f1 = get_metrics(y_chim, y_pred_chimp)\nprint(\"accuracy = %.3f \\nprecision = %.3f \\nrecall = %.3f \\nf1 = %.3f\" % (accuracy, precision, recall, f1))\n</code></pre> <pre><code>Confusion matrix for predictions on Chimpanzee test DNA sequence\n\nPredicted    0    1    2    3    4    5    6\nActual                                      \n0          232    0    0    0    0    0    2\n1            0  184    0    0    0    0    1\n2            0    0  144    0    0    0    0\n3            0    0    0  227    0    0    1\n4            2    0    0    0  254    0    5\n5            0    0    0    0    0  109    0\n6            0    0    0    0    0    0  521\naccuracy = 0.993 \nprecision = 0.994 \nrecall = 0.993 \nf1 = 0.993\n</code></pre>","tags":["classification","bioinformatics","machine learning"]},{"location":"blog/2023/10/21/gene-classification.html#dog-dataset-prediction","title":"Dog Dataset Prediction","text":"<p>Let's check the classification performance on the dog test dataset:</p> <pre><code>print(pd.crosstab(pd.Series(y_dog, name='Actual'), pd.Series(y_pred_dog, name='Predicted')))\naccuracy, precision, recall, f1 = get_metrics(y_dog, y_pred_dog)\nprint(\"accuracy = %.3f \\nprecision = %.3f \\nrecall = %.3f \\nf1 = %.3f\" % (accuracy, precision, recall, f1))\n</code></pre> <pre><code>Predicted    0   1   2   3    4   5    6\nActual                                  \n0          127   0   0   0    0   0    4\n1            0  63   0   0    1   0   11\n2            0   0  49   0    1   0   14\n3            1   0   0  81    2   0   11\n4            4   0   0   1  126   0    4\n5            4   0   0   0    1  53    2\n6            0   0   0   0    0   0  260\naccuracy = 0.926 \nprecision = 0.934 \nrecall = 0.926 \nf1 = 0.925\n</code></pre>","tags":["classification","bioinformatics","machine learning"]},{"location":"blog/2023/10/21/gene-classification.html#conclusion","title":"Conclusion","text":"<p>For all gene family data, the model is able to produce good results. It also does on Chimpanzee which is because the chimpanzee and humans share the same genetic hierarchy structure. However, the performance on the dog dataset (in comparison) was not quite as good, probably because dogs and human share less common genes.</p>","tags":["classification","bioinformatics","machine learning"]},{"location":"blog/2023/10/21/gene-classification.html#concluding-remarks","title":"Concluding remarks","text":"<p>In this post we looked at an interesting machine learning applicaiton in the field of bioinformatics. We started with a marked dataset for three difference species, containing labelled gene classes of DNA segments extracted from the genome of these species. </p> <p>Our goal was to create a machine learning model that was able to classify input DNA segments into one of the specified classes. For this we utilised a kmer preprocessing approach. This preprocessing step probably was the most difficult part of the entire project:</p> <ul> <li>Our gene classes contain biological sequences of various lengths, as a result, we needed to pay attention to how we would go about encoding the text data into numerical format, so we could train a classifier. </li> <li>For this reason we resorted to kmer subset groupings &amp; created a dictionary of these subset, thus the dataset ended up representing data that specified whether this kmer subset of the data was present in the input DNA sequence or not.</li> </ul> <p>The resulting model was able to very convinsingly carry out this task without any significant problems, even on non human datasets (chimpanzee &amp; dogs), which is very promising.</p>","tags":["classification","bioinformatics","machine learning"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html","title":"Prediction of customer stable funds volume","text":"<p>\u0422\u0432\u043e\u0435\u0439 \u0441\u0435\u0433\u043e\u0434\u043d\u044f\u0448\u043d\u0435\u0439 \u0437\u0430\u0434\u0430\u0447\u0435\u0439 \u043a\u0430\u043a \u0441\u0442\u0430\u0436\u0435\u0440\u0430 \u043d\u0430\u0448\u0435\u0433\u043e \u043e\u0442\u0434\u0435\u043b\u0430 \u0431\u0443\u0434\u0435\u0442 \u043d\u0430\u0443\u0447\u0438\u0442\u044c\u0441\u044f \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043e\u0431\u044a\u0435\u043c \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u044b\u0445 \u0441\u0440\u0435\u0434\u0441\u0442\u0432 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432 \u0431\u0435\u0437 \u0441\u0440\u043e\u043a\u043e\u0432 \u043f\u043e\u0433\u0430\u0448\u0435\u043d\u0438\u044f, \u0432 \u0434\u0430\u043d\u043d\u043e\u043c \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u044d\u0442\u043e \u0440\u0430\u0441\u0447\u0435\u0442\u043d\u044b\u0435 \u0441\u0447\u0435\u0442\u0430 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432.</p> <ul> <li>\u041f\u043e\u0447\u0435\u043c\u0443 \u044d\u0442\u043e \u0432\u0430\u0436\u043d\u043e? \u041d\u043e\u043c\u0438\u043d\u0430\u043b\u044c\u043d\u043e, \u0432\u0441\u0435 \u0441\u0440\u0435\u0434\u0441\u0442\u0432\u0430 \u043d\u0430 \u0440\u0430\u0441\u0447\u0435\u0442\u043d\u044b\u0445 \u0441\u0447\u0435\u0442\u0430\u0445 \u043a\u043b\u0438\u0435\u043d\u0442\u044b \u043c\u043e\u0433\u0443\u0442 \u0432 \u043b\u044e\u0431\u043e\u0439 \u043c\u043e\u043c\u0435\u043d\u0442 \u0437\u0430\u0431\u0440\u0430\u0442\u044c \u0438\u0437 \u0411\u0430\u043d\u043a\u0430, \u0430 \u0432 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0438 \u044d\u0442\u043e\u0433\u043e \u0411\u0430\u043d\u043a \u043d\u0435 \u043c\u043e\u0436\u0435\u0442 \u0438\u0445 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0432 \u0434\u043e\u043b\u0433\u043e\u0441\u0440\u043e\u0447\u043d\u043e\u043c / \u0441\u0440\u0435\u0434\u043d\u0435\u0441\u0440\u043e\u0447\u043d\u043e\u043c \u043f\u043b\u0430\u043d\u0435 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0434\u043b\u044f \u0432\u044b\u0434\u0430\u0447\u0438 \u043a\u0440\u0435\u0434\u0438\u0442\u043e\u0432)</li> <li>\u041f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f, \u0447\u0442\u043e \u0432 \u0442\u0430\u043a\u043e\u0439 \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u0438 \u0411\u0430\u043d\u043a \u043d\u0438\u0447\u0435\u0433\u043e \u043d\u0435 \u0437\u0430\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0435\u0442, \u043d\u043e \u043f\u043b\u0430\u0442\u0438\u0442 \u043a\u043b\u0438\u0435\u043d\u0442\u0430\u043c \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u044b \u043f\u043e \u0441\u0440\u0435\u0434\u0441\u0442\u0432\u0430\u043c \u043d\u0430 \u0438\u0445 \u0441\u0447\u0435\u0442\u0430\u0445, \u043f\u0443\u0441\u0442\u044c \u0438 \u043d\u0435 \u0432\u044b\u0441\u043e\u043a\u0438\u0435, \u043d\u043e \u0432 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0430\u0445 \u0431\u0438\u0437\u043d\u0435\u0441\u0430 \u0411\u0430\u043d\u043a\u0430 \u044d\u0442\u0438 \u0443\u0431\u044b\u0442\u043a\u0438 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u044b</li> </ul> <ul> <li> Open Colab Notebook</li> <li> GitHub Repository</li> </ul>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#background","title":"Background","text":"<p>\u041d\u043e \u0432 \u0440\u0435\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432 \u0434\u0440\u0443\u0433\u043e\u0435, \u043e\u043d\u043e \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u043c\u043d\u043e\u0433\u0438\u0445 \u0444\u0430\u043a\u0442\u043e\u0440\u043e\u0432 (\u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0447\u0435\u0441\u043a\u0438\u0445, \u043c\u0430\u043a\u0440\u043e\u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0445, \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439 \u043a\u043e\u043d\u043a\u0443\u0440\u0435\u043d\u0442\u043e\u0432 \u0438 \u0442.\u0434.). \u041a\u043b\u0438\u0435\u043d\u0442\u044b \u043d\u0435 \u0437\u0430\u0431\u0438\u0440\u0430\u044e\u0442 \u0441\u0440\u0430\u0437\u0443 \u0432\u0441\u0435 \u0441\u0432\u043e\u0438 \u0441\u0440\u0435\u0434\u0441\u0442\u0432\u0430 \u0441 \u0440\u0430\u0441\u0447\u0435\u0442\u043d\u044b\u0445 \u0441\u0447\u0435\u0442\u043e\u0432, \u0430 \u043a\u0430\u043a\u043e\u0435-\u0442\u043e \u0432\u0440\u0435\u043c\u044f \u0438\u0445 \u0442\u0430\u043c \u0445\u0440\u0430\u043d\u044f\u0442, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0441\u0443\u043c\u043c\u0430\u0440\u043d\u043e \u043f\u043e \u0432\u0441\u0435\u043c \u043a\u043b\u0438\u0435\u043d\u0442\u0430\u043c \u043d\u0430 \u0438\u0445 \u0440\u0430\u0441\u0447\u0435\u0442\u043d\u044b\u0445 \u0441\u0447\u0435\u0442\u0430\u0445 \u0432\u0441\u0435\u0433\u0434\u0430 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u043a\u0430\u043a\u043e\u0439-\u0442\u043e \u043e\u0431\u044a\u0435\u043c \u0441\u0440\u0435\u0434\u0441\u0442\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0445\u043e\u0442\u044c \u0438 \u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f \u0441\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0435\u043c, \u043d\u043e \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0440\u0430\u0441\u0446\u0435\u043d\u0435\u043d \u0411\u0430\u043d\u043a\u043e\u043c \u043a\u0430\u043a \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u044b\u0439 \u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d \u0434\u043b\u044f \u0432\u044b\u0434\u0430\u0447\u0438 \u043a\u0440\u0435\u0434\u0438\u0442\u043e\u0432 (\u0430 \u0411\u0430\u043d\u043a \u043d\u0430 \u044d\u0442\u043e\u043c \u0437\u0430\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0435\u0442)</p>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#importance-of-model-accuracy","title":"Importance of model accuracy","text":"<p>\u0423\u043c\u0435\u043d\u0438\u0435 \u0442\u043e\u0447\u043d\u043e \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043e\u0431\u044a\u0435\u043c \u0438 \u0434\u0438\u043d\u0430\u043c\u0438\u043a\u0443 \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u043e\u0433\u043e \u043e\u0441\u0442\u0430\u0442\u043a\u0430 \u0441\u0440\u0435\u0434\u0441\u0442\u0432 \u043d\u0430 \u0440\u0430\u0441\u0447\u0435\u0442\u043d\u044b\u0445 \u0441\u0447\u0435\u0442\u0430\u0445 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0411\u0430\u043d\u043a\u0443 \u0437\u0430\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0442\u044c \u043d\u0430 \u043a\u0440\u0435\u0434\u0438\u0442\u043e\u0432\u0430\u043d\u0438\u0438, \u043d\u043e \u043f\u0440\u0438 \u044d\u0442\u043e\u043c \u0434\u0435\u0440\u0436\u0430\u0442\u044c \u0432 \u043f\u0440\u0438\u0435\u043c\u043b\u0435\u043c\u044b\u0445 \u0440\u0430\u043c\u043a\u0430\u0445 \u0440\u0438\u0441\u043a \u0442\u043e\u0433\u043e, \u0447\u0442\u043e \u043a\u043b\u0438\u0435\u043d\u0442\u044b \u043c\u043e\u0433\u0443\u0442 \u0432 \u043b\u044e\u0431\u043e\u0439 \u043c\u043e\u043c\u0435\u043d\u0442 \u043f\u043e\u0442\u0440\u0435\u0431\u043e\u0432\u0430\u0442\u044c \u044d\u0442\u0438 \u0441\u0440\u0435\u0434\u0441\u0442\u0432\u0430 \u043d\u0430\u0437\u0430\u0434 \u2013 \u044d\u0442\u043e \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f \"\u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0440\u0438\u0441\u043a\u043e\u043c \u043b\u0438\u043a\u0432\u0438\u0434\u043d\u043e\u0441\u0442\u0438\". \u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0441\u0442\u0440\u043e\u0438\u0442\u0441\u044f ML \u043c\u043e\u0434\u0435\u043b\u044c \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u043e\u0433\u043e \u043e\u0441\u0442\u0430\u0442\u043a\u0430 \u0441\u0440\u0435\u0434\u0441\u0442\u0432 \u043d\u0430 \u0440\u0430\u0441\u0447\u0435\u0442\u043d\u044b\u0445 \u0441\u0447\u0435\u0442\u0430\u0445 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432, \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u0430\u044f \u0441 \u043c\u043e\u0434\u0435\u043b\u044f\u043c\u0438 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0430 \u0440\u044b\u043d\u043a\u043e\u0432, \u043c\u0430\u043a\u0440\u043e\u044d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0438 \u0438 \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u044f \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432</p> <p>\u041c\u0430\u0441\u0448\u0442\u0430\u0431\u044b \u0431\u0438\u0437\u043d\u0435\u0441\u0430 \u0411\u0430\u043d\u043a\u0430 \u043f\u043e\u0440\u0430\u0437\u0438\u0442\u0435\u043b\u044c\u043d\u044b: \u043f\u043e \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u043c \u0431\u0430\u043d\u043a\u043e\u0432\u0441\u043a\u0438\u043c \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430\u043c \u0434\u043e\u043b\u044f \u0434\u043e\u0441\u0442\u0438\u0433\u0430\u0435\u0442 30-50%, \u0430 \u044d\u0442\u043e \u0434\u0435\u0441\u044f\u0442\u043a\u0438 \u043c\u0438\u043b\u043b\u0438\u043e\u043d\u043e\u0432 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432, \u0442\u0440\u0438\u043b\u043b\u0438\u043e\u043d\u044b \u0440\u0443\u0431\u043b\u0435\u0439 \u043e\u0431\u044a\u0435\u043c\u043e\u0432. \u041f\u043e\u0432\u044b\u0448\u0435\u043d\u0438\u0435 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0430 \u0432\u0441\u0435\u0433\u043e \u043e\u0434\u043d\u043e\u0439 \u0442\u0430\u043a\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438, \u043a \u043f\u0440\u0438\u043c\u0435\u0440\u0443 \u043d\u0430 5% \u0438\u043b\u0438 \u0432 \u043f\u0435\u0440\u0435\u0441\u0447\u0435\u0442\u0435 \u043d\u0430 \u0434\u0435\u043d\u044c\u0433\u0438 \u043d\u0430 50 \u043c\u043b\u0440\u0434 \u0440\u0443\u0431., \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u0442 \u0411\u0430\u043d\u043a\u0443 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0437\u0430\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c 1 \u043c\u043b\u0440\u0434 \u0440\u0443\u0431. \u0432 \u0433\u043e\u0434 (\u0432 \u043f\u0440\u0435\u0434\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0438 5% \u043c\u0430\u0440\u0436\u0438 \u0431\u0430\u043d\u043a\u043e\u0432\u0441\u043a\u043e\u0433\u043e \u0431\u0438\u0437\u043d\u0435\u0441\u0430)</p>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#customer-funds-data","title":"Customer funds data","text":"<p>\u0412 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u043d\u043e\u043c \u0444\u0430\u0439\u043b\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u043f\u043e\u0434\u043d\u0435\u0432\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043e\u0431\u044a\u0435\u043c\u0430 \u0440\u0430\u0441\u0447\u0435\u0442\u043d\u044b\u0445 \u0441\u0447\u0435\u0442\u043e\u0432 \u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043b\u0438\u0446. \u0412 \u043e\u0442\u043b\u0438\u0447\u0438\u0435 \u043e\u0442 \u0434\u0435\u043f\u043e\u0437\u0438\u0442\u0430, \u043a\u043b\u0438\u0435\u043d\u0442 \u043c\u043e\u0436\u0435\u0442 \u0441\u043d\u044f\u0442\u044c \u0432\u0441\u044e \u0441\u0443\u043c\u043c\u0443 \u0441 \u0440\u0430\u0441\u0447\u0435\u0442\u043d\u043e\u0433\u043e \u0441\u0447\u0435\u0442\u0430 \u0432 \u043b\u044e\u0431\u043e\u0439 \u043c\u043e\u043c\u0435\u043d\u0442 \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0431\u0435\u0437 \u043a\u0430\u043a\u0438\u0445-\u043b\u0438\u0431\u043e \u00ab\u0448\u0442\u0440\u0430\u0444\u043e\u0432\u00bb. \u0422\u0430\u043a\u043e\u0439 \u043f\u0440\u043e\u0434\u0443\u043a\u0442 \u043d\u0430\u0437\u044b\u0432\u0430\u044e\u0442 Undefined Maturity Product \u2013 UMP). \u041e\u0434\u043d\u0430\u043a\u043e \u043c\u0430\u043b\u043e\u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e, \u0447\u0442\u043e \u0432\u0441\u0435 \u043a\u043b\u0438\u0435\u043d\u0442\u044b \u0440\u0430\u0437\u043e\u043c \u0437\u0430\u043a\u0440\u043e\u044e\u0442 \u0441\u0432\u043e\u0438 \u0441\u0447\u0435\u0442\u0430 \u0432 \u0411\u0430\u043d\u043a\u0435. \u0412\u0441\u0435\u0433\u0434\u0430 \u043a\u0442\u043e-\u0442\u043e \u0441\u043d\u0438\u043c\u0430\u0435\u0442 \u0434\u0435\u043d\u044c\u0433\u0438, \u0430 \u043a\u0442\u043e-\u0442\u043e \u043f\u043e\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u0441\u0447\u0435\u0442 \u2013 \u0435\u0441\u0442\u044c \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u044b\u0439 \u0443\u0440\u043e\u0432\u0435\u043d\u044c, \u043d\u0438\u0436\u0435 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u043d\u0435 \u043e\u043f\u0443\u0441\u0442\u0438\u0442\u0441\u044f \u0441\u0443\u043c\u043c\u0430\u0440\u043d\u044b\u0439 \u043e\u0431\u044a\u0435\u043c \u0440\u0430\u0441\u0447\u0435\u0442\u043d\u044b\u0445 \u0441\u0447\u0435\u0442\u043e\u0432.</p>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#study-aim","title":"Study Aim","text":"<p>\u0417\u0430\u0434\u0430\u0447\u0430 \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c \u043e\u0431\u044a\u0435\u043c \u044d\u0442\u043e\u0439 \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u043e\u0439 \u0447\u0430\u0441\u0442\u0438 \u0441\u0440\u0435\u0434\u0441\u0442\u0432</p> <p>\u0414\u043e\u043b\u044f \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430 \u043d\u0430 \u0440\u0430\u0437\u043d\u044b\u0445 \u0440\u044b\u043d\u043a\u043e\u0432 \u0434\u043e\u0441\u0442\u0438\u0433\u0430\u0435\u0442 30+%, 10\u043c \u043a\u043b\u0438\u0435\u0442\u043d\u043e\u0432/\u0442\u0440\u0438\u043b\u043b\u0438\u043e\u043d \u0440\u0443\u0431 \u041f\u043e\u0432\u044b\u0448\u0435\u043d\u0438\u044f \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0430 \u043d\u0430 5% \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0431\u0430\u043d\u043a\u0443 \u0437\u0430\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0442\u044c 10\u043c \u0440\u0443\u0431</p> <p>\u041f\u043e\u043d\u0438\u043c\u0430\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043f\u043e\u043f\u044b\u0442\u0430\u0435\u043c\u0441\u044f \u043f\u043e\u0432\u044b\u0441\u0438\u0442\u044c \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043e\u0431\u043e\u0431\u0449\u0430\u044e\u0448\u0435\u0439 \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u0435\u0439</p>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#feature-engineering","title":"Feature Engineering","text":"<p>\u0414\u0430\u0432\u0430\u0439\u0442\u0435 \u0441\u0440\u0430\u0437\u0443 \u043e\u0442\u043c\u0435\u0442\u0438\u043c \u043f\u043e\u0434\u0445\u043e\u0434\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u0443\u0434\u0435\u043c \u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c \u0432 \u044d\u0442\u043e\u043c \u043f\u0440\u043e\u0435\u043a\u0442\u0435. \u041a \u0437\u0430\u0434\u0430\u0447\u0435 feature engineering \u0434\u043b\u044f \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0440\u044f\u0434\u043e\u0432 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0434\u043e\u0439\u0442\u0438 \u043f\u043e \u0440\u0430\u0437\u043d\u043e\u043c\u0443, \u043e\u0442\u043c\u0435\u0442\u0438\u043c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u043e\u0434\u0445\u043b\u0434\u043e\u0432:</p> <p>Tip</p> <ul> <li>\u0418\u043c\u0435\u044f \u0432\u0440\u0435\u043c\u043c\u0435\u043d\u043d\u044b\u0439 \u0440\u044f\u0434 \u043c\u043e\u0436\u043d\u043e \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0431\u044b\u0441\u0442\u0440\u043e \u043d\u0430\u0447\u0430\u0442\u044c \u0441 \u0444\u0438\u0447 \u0447\u0435\u0440\u0435\u0437 <code>dt</code> \u0432 pandas, \u043f\u043e\u043b\u0443\u0447\u0438\u0432 <code>\u0434\u0435\u043d\u044c</code>, <code>\u043c\u0435\u0441\u044f\u0446</code>, <code>\u0433\u043e\u0434</code> \u0438 <code>\u0434\u0435\u043d\u044c \u043d\u0435\u0434\u0435\u043b\u0438</code> \u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u044d\u0442\u043e \u043a\u0430\u043a \u0431\u0430\u0437\u043e\u0432\u043e\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u0435. </li> <li>\u0421\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u044e\u0442 \u0438 \u0434\u0440\u0443\u0433\u0438\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u043a\u0430\u043a \u043c\u0435\u0442\u043e\u0434 \u0441 \u043f\u043e\u043d\u0438\u0436\u0435\u043d\u0438\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 (eg. NMF \u0434\u0435\u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u044f), \u043b\u0438\u0431\u043e \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u043d\u0430 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0440\u044f\u0434\u0430\u0445, \u043b\u0438\u0431\u043e \u0447\u0435\u0440\u0435\u0437 \u0424\u0443\u0440\u044c\u0435 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435).</li> <li>\u0412 \u0434\u0430\u043d\u043d\u043e\u043c \u0441\u043b\u0443\u043c\u0430\u0435 \u043c\u044b \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044f \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u043c \u0433\u0434\u0435 \u043c\u044b \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u0432 \u0440\u0443\u0447\u043d\u0443\u044e \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0438\u043d\u0434\u0438\u043a\u0430\u0442\u043e\u0440\u044b, \u0447\u0442\u043e \u0442\u043e\u0436\u0435 \u0447\u0430\u0441\u0442\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u0447 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0440\u044f\u0434\u043e\u0432</li> </ul>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#time-series-features","title":"Time Series Features","text":"<p>\u041c\u044b \u043c\u043e\u0436\u0435\u043c \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u043d\u043e\u0432\u044b \u0444\u0438\u0447\u0438 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e pandas's <code>dt</code></p> <pre><code>data['dow'] = data['REPORTDATE'].dt.dayofweek\ndata['y']=data['REPORTDATE'].dt.year\ndata['d']=data['REPORTDATE'].dt.day\ndata['m']=data['REPORTDATE'].dt.month\ndata.head()\n</code></pre> <pre><code>REPORTDATE  VALUE   dow     y   d   m\n0   2013-12-30  3457625638  0   2013    30  12\n1   2013-12-31  3417092149  1   2013    31  12\n2   2014-01-01  3417092149  2   2014    1   1\n3   2014-01-02  3417092149  3   2014    2   1\n4   2014-01-03  3417092149  4   2014    3   1\n</code></pre>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#technical-indicators","title":"Technical Indicators","text":"<p>\u0427\u0430\u0441\u0442\u043e \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0449\u0438\u0439\u0441\u044f \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u0442\u043a\u0438\u0435 \u0438\u043d\u0434\u0438\u043a\u0430\u0442\u043e\u0440\u044b (eg. \u0441\u043a\u043e\u043b\u044c\u0437\u044f\u0449\u0435e \u0441\u0440\u0435\u0434\u043d\u0435e, \u044d\u043a\u0441\u043f\u043e\u043d\u0435\u043d\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0435 \u0441\u043a\u043e\u043b\u044c\u0437\u044f\u0449\u0435e \u0441\u0440\u0435\u0434\u043d\u0435e...) </p> <pre><code>#Calculation of moving average\ndef ma(df,column,n):\n    return pd.Series(df[column].rolling(n, min_periods=n).mean(), name='MA_' + str(n))\n\n# exponentially weighted moving average\ndef ema(df,column,n):\n    return pd.Series(df[column].ewm(span=n,min_periods=n).mean(), name='EMA_' + str(n))\n\n#Calculation of price momentum\ndef mom(df,column,n):\n    return pd.Series(df[column].diff(n), name='Momentum_' + str(n))\n\n# rate of change\ndef roc(df, column, n):\n    M = df[column].diff(n - 1) ; N = df[column].shift(n - 1)\n    return pd.Series(((M / N) * 100), name = 'ROC_' + str(n))\n\n# relative strength index\ndef rsi(df, period):\n    delta = df.diff().dropna()\n    u = delta * 0; d = u.copy()\n    u[delta &gt; 0] = delta[delta &gt; 0]; d[delta &lt; 0] = -delta[delta &lt; 0]\n    u[u.index[period-1]] = np.mean( u[:period] ) #first value is sum of avg gains\n    u = u.drop(u.index[:(period-1)])\n    d[d.index[period-1]] = np.mean( d[:period] ) #first value is sum of avg losses\n    d = d.drop(d.index[:(period-1)])\n    rs = u.ewm(com=period-1, adjust=False).mean() / d.ewm(com=period-1, adjust=False).mean()\n    return 100 - 100 / (1 + rs)\n\n# stochastic oscillators slow &amp; fast\ndef sto(close, low, high, n,id):\n    stok = ((close - low.rolling(n).min()) / (high.rolling(n).max() - low.rolling(n).min())) * 100\n    if(id is 0):\n        return stok\n    else:\n        return stok.rolling(3).mean()\n</code></pre>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#exploratory-data-analysis","title":"Exploratory Data Analysis","text":"","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#statistical-boxplots-of-target-variable","title":"Statistical Boxplots of Target Variable","text":"<p>\u041f\u0440\u0435\u0436\u0434\u0435 \u0432\u0441\u0435\u0433\u043e \u043d\u0443\u0436\u043d\u043e \u0434\u0443\u043c\u0430\u0442\u044c \u043e \u0442\u043e\u043c \u043a\u0430\u043a\u0438\u0435 \u0444\u0438\u0447\u0438 \u043c\u043e\u0433\u0443\u0442 \u043f\u043e\u043c\u043e\u0447\u044c \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u0435\u043b\u0430\u0442\u044c \u0431\u043e\u043b\u0435\u0435 \u0442\u043e\u0447\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u044b, \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u0432 \u0433\u0440\u0430\u0444\u0438\u043a <code>boxplot</code>, \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0443\u0432\u0438\u0434\u0438\u0442\u044c \u0447\u0442\u043e \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0445\u043e\u0440\u043e\u0448\u043e \u0440\u0430\u0437\u0434\u0435\u043b\u044f\u044e\u0442\u0441\u044f \u043f\u043e \u0433\u043e\u0434\u0430\u043c, \u0431\u043b\u0430\u0433\u043e\u0434\u043e\u0440\u044f \u0435\u0436\u0435\u0433\u043e\u0434\u043d\u043e\u043c \u043f\u0440\u0438\u0440\u043e\u0441\u0442\u043e\u043c \u0434\u0435\u043d\u0435\u0436\u043d\u044b\u0445 \u0441\u0442\u0440\u0435\u0434\u0441\u0442\u0432 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432. \u041e\u0434\u043d\u043e\u0433\u043e \u0442\u0430\u043a\u043e\u0433\u043e \u0433\u0440\u0430\u0444\u0444\u0438\u043a\u0430 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c pandas's <code>dt</code> \u0444\u0446\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c, \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u0438\u0437\u0432\u043b\u0435\u0447\u044c \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0444\u0438\u0447\u0438.</p> <p></p>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#univariate-distribution-of-target-variable","title":"Univariate Distribution of Target Variable","text":"<p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0443\u043d\u0438\u0442\u0430\u0440\u043d\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u044b</p> <p></p>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#time-features-correlation-to-target-variable","title":"Time Features Correlation to Target Variable","text":"<p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u044e \u043d\u043e\u0432\u044b\u0445 \u0444\u0438\u0447 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438 \u0447\u0435\u0440\u0435\u0437 pandas's <code>dt</code></p> <pre><code># Plot Correlation to Target Variable only\ndef corrMat(df,target='demand',figsize=(4,0.5),ret_id=False):\n\n    corr_mat = df.corr().round(2);shape = corr_mat.shape[0]\n    corr_mat = corr_mat.transpose()\n    corr = corr_mat.loc[:, df.columns == target].transpose().copy()\n    return corr\n\n# What are the correlations like\nsubset = data[['dow','y','d','m','VALUE']]\ncorrMat(subset,target='VALUE')\n</code></pre> <pre><code>         dow       y        d     m   VALUE\nVALUE   0.01    0.94    -0.05   0.1     1.0\n</code></pre> <p>\u0412 \u0446\u0435\u043b\u043e\u043c, \u043c\u044b \u0442\u043e\u043b\u044c\u043a\u043e \u0438 \u043f\u043e\u0434\u0442\u0432\u0435\u0440\u0434\u0438\u043b\u0438 \u0447\u0442\u043e \u0431\u044b\u043b\u043e \u0432\u0438\u0434\u043d\u043e \u0432 \u0433\u0440\u0430\u0444\u0438\u043a\u0435 boxplot, <code>y</code> \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u044b\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u043c.</p>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#stationarity-testing","title":"Stationarity Testing","text":"<p>\u0422\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0440\u044f\u0434\u043e\u0432 \u043d\u0430 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0441\u0432\u043e\u0439\u0441\u0442\u0432\u0430 \u0432\u0430\u0436\u043d\u043e \u0434\u043b\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0441\u0442\u0430\u0446\u0438\u043e\u043d\u0430\u0440\u043d\u043e\u0441\u0442\u0438 \u0434\u0430\u043d\u043d\u044b\u0445. \u0421\u0442\u0430\u0446\u0438\u043e\u043d\u0430\u0440\u043d\u043e\u0441\u0442\u044c \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442, \u0447\u0442\u043e \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0441\u0432\u043e\u0439\u0441\u0442\u0432\u0430 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0433\u043e \u0440\u044f\u0434\u0430 \u043d\u0435 \u043c\u0435\u043d\u044f\u044e\u0442\u0441\u044f \u0441\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0435\u043c, \u0442\u0430\u043a\u0438\u0435 \u043a\u0430\u043a \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435, \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u044f \u0438 \u0430\u0432\u0442\u043e\u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u044f. \u042d\u0442\u043e \u0432\u0430\u0436\u043d\u043e \u0434\u043b\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0442\u043e\u0447\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f, \u0442\u0430\u043a \u043a\u0430\u043a \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u0440\u0435\u0434\u043f\u043e\u043b\u0430\u0433\u0430\u044e\u0442 \u0441\u0442\u0430\u0446\u0438\u043e\u043d\u0430\u0440\u043d\u043e\u0441\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0445 (\u041d\u0435 \u0432\u0441\u0435 \u043a\u043e\u043d\u0435\u0447\u043d\u043e). \u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043d\u0430 \u0441\u0442\u0430\u0446\u0438\u043e\u043d\u0430\u0440\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u043c\u043e\u0433\u0430\u0435\u0442 \u0443\u0431\u0435\u0434\u0438\u0442\u044c\u0441\u044f, \u0447\u0442\u043e \u0434\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0442 \u0434\u043b\u044f \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.</p> <p>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044f \u0434\u0432\u0443\u043c\u044f \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044f\u043c\u0438 \u0434\u043b\u044f \u0435\u0435 \u043e\u0446\u0435\u043d\u043a\u0438:</p> <ul> <li> <p>\u041c\u0435\u0442\u0440\u0438\u043a\u0430 <code>\u0430\u0432\u0442\u043e\u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0438</code> \u0438\u0437\u043c\u0435\u0440\u044f\u0435\u0442 \u0441\u0442\u0435\u043f\u0435\u043d\u044c \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0438 \u043c\u0435\u0436\u0434\u0443 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0433\u043e \u0440\u044f\u0434\u0430 \u0438 \u0435\u0433\u043e \u043e\u0442\u0441\u0442\u0430\u044e\u0449\u0438\u043c\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438. \u041e\u043d\u0430 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c, \u0435\u0441\u0442\u044c \u043b\u0438 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u044c \u043c\u0435\u0436\u0434\u0443 \u0442\u0435\u043a\u0443\u0449\u0438\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\u043c \u0440\u044f\u0434\u0430 \u0438 \u0435\u0433\u043e \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u043c\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438. \u0410\u0432\u0442\u043e\u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u044f \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439, \u0435\u0441\u043b\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0440\u044f\u0434\u0430 \u0438\u0437\u043c\u0435\u043d\u044f\u044e\u0442\u0441\u044f \u0432\u043c\u0435\u0441\u0442\u0435, \u0438\u043b\u0438 \u043e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0439, \u0435\u0441\u043b\u0438 \u043e\u043d\u0438 \u0438\u0437\u043c\u0435\u043d\u044f\u044e\u0442\u0441\u044f \u0432 \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u043f\u043e\u043b\u043e\u0436\u043d\u044b\u0445 \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f\u0445. </p> </li> <li> <p>\u0422\u0435\u0441\u0442 <code>\u0414\u0438\u043a\u0438-\u0424\u0443\u043b\u043b\u0435\u0440\u0430</code> (Dickey-Fuller test) - \u044d\u0442\u043e \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0442\u0435\u0441\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u043d\u0430\u043b\u0438\u0447\u0438\u044f \u0435\u0434\u0438\u043d\u0438\u0447\u043d\u044b\u0445 \u043a\u043e\u0440\u043d\u0435\u0439 \u0432\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0440\u044f\u0434\u0430\u0445. \u0415\u0434\u0438\u043d\u0438\u0447\u043d\u044b\u0435 \u043a\u043e\u0440\u043d\u0438 \u043c\u043e\u0433\u0443\u0442 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u043d\u0430 \u043d\u0435\u0441\u0442\u0430\u0446\u0438\u043e\u043d\u0430\u0440\u043d\u043e\u0441\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0445, \u0447\u0442\u043e \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442, \u0447\u0442\u043e \u0438\u0445 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0441\u0432\u043e\u0439\u0441\u0442\u0432\u0430 \u043c\u0435\u043d\u044f\u044e\u0442\u0441\u044f \u0441\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0435\u043c. \u0422\u0435\u0441\u0442 \u0414\u0438\u043a\u0438-\u0424\u0443\u043b\u043b\u0435\u0440\u0430 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c, \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043b\u0438 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u0440\u044f\u0434 \u0441\u0442\u0430\u0446\u0438\u043e\u043d\u0430\u0440\u043d\u044b\u043c \u0438\u043b\u0438 \u043d\u0435\u0442. \u0415\u0441\u043b\u0438 \u043d\u0443\u043b\u0435\u0432\u0430\u044f \u0433\u0438\u043f\u043e\u0442\u0435\u0437\u0430 \u0442\u0435\u0441\u0442\u0430 \u043e\u0442\u0432\u0435\u0440\u0433\u0430\u0435\u0442\u0441\u044f, \u0442\u043e \u043c\u043e\u0436\u043d\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0432\u044b\u0432\u043e\u0434 \u043e \u0441\u0442\u0430\u0446\u0438\u043e\u043d\u0430\u0440\u043d\u043e\u0441\u0442\u0438 \u0440\u044f\u0434\u0430</p> </li> </ul>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#helper-function","title":"Helper function","text":"<p>Both metrics can be found in the <code>statsmodels</code> library (<code>smt.graphics.plot_pacf</code> and <code>sm.tsa.stattools.adfuller</code>)</p> <pre><code>import statsmodels.tsa.api as smt\nimport statsmodels.api as sm\n\ndef tsplot(y, lags=None, figsize=(12, 7), style='bmh'):\n\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n\n    with plt.style.context(style):\n\n        fig = plt.figure(figsize=figsize)\n        layout = (2, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n\n        y.plot(ax=ts_ax)\n        ts_ax.set_title('Time series analysis')\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)\n\n        criterion = sm.tsa.stattools.adfuller(y)[1]\n        print(f\"Dickey-Fuller criterion: p={criterion}\")\n        plt.tight_layout()\n        plt.show()\n</code></pre>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#baseline-time-series","title":"Baseline Time-Series","text":"<p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0433\u043e \u0440\u044f\u0434\u0430</p> <pre><code>tsplot(data['VALUE'])\n# Dickey-Fuller criterion: p=0.934988760701965\n</code></pre> <p>\u041e\u0431\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0433\u043e\u0432\u043e\u0440\u044f\u0442 \u043d\u0430\u043c \u043e \u0442\u043e\u043c \u0447\u0442\u043e \u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u0440\u0438\u0441\u0443\u0434\u0441\u0442\u0432\u0443\u0435\u0442 \u043d\u0435 \u0441\u0442\u0430\u0446\u0438\u043e\u043d\u0430\u0440\u043d\u043e\u0441\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0445</p> <p></p>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#box-ox-transformation","title":"Box-\u0421ox transformation","text":"<p>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 box-cox (<code>scipy.stats.boxcox</code>); \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445, \u043d\u0435 \u0438\u043c\u0435\u044e\u0449\u0438\u0445 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435, \u0432 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435, \u043a\u0430\u043a \u043c\u044b \u0432\u0438\u043c\u0438\u043c \u043f\u043e boxplot, \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0441\u0434\u0432\u0438\u043d\u0443\u043b\u0430\u0441\u0442\u044c. </p> <pre><code>import scipy.stats as scs\n\ndata['VALUE_box'], lmbda = scs.boxcox(data['VALUE'])\npx.histogram(data,x='VALUE_box',height=400,width=800,template='plotly_white',marginal='box')\n</code></pre> <pre><code>tsplot(data['VALUE_box'])\nprint(\"The optimal parameter of the Box-Cox transformation is: %f\" % lmbda)\n# The optimal parameter of the Box-Cox transformation is: 0.488251\n# Dickey-Fuller criterion: p=0.33953657978375956\n</code></pre> <p></p> <p></p> <p>\u0421\u0443\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0435 \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u0435 \u0432 \u043c\u0435\u0442\u0440\u0438\u043a\u0435 Dicker-Fuller \u0435\u0441\u0442\u044c, \u043d\u043e \u043f\u043e\u043a\u0430 \u043d\u0443\u043b\u0435\u0432\u0430\u044f \u0433\u0438\u043f\u043e\u0442\u0435\u0437\u0430 \u043e \u0441\u0442\u0430\u0446\u0438\u043e\u043d\u0430\u0440\u043d\u043e\u0441\u0442\u0438 \u043e\u0442\u0432\u0435\u0440\u0433\u0430\u0435\u0442\u0441\u044f</p>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#removing-seasonality","title":"Removing seasonality","text":"<p>\u041c\u044b \u043c\u043e\u0436\u0435\u043c \u0432\u0437\u044f\u0442\u044c \u0440\u0430\u0437\u043d\u0438\u0446\u0443 VALUE \u0438\u0437 \u043f\u0440\u043e\u0448\u043b\u043e\u0433\u043e (ie. \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0443\u0431\u0440\u0430\u0442\u044c \u044d\u043b\u0435\u043c\u0435\u043d\u0442 \u0441\u0435\u0437\u043e\u043d\u043d\u043e\u0441\u0442\u0438), \u0432\u043e\u0437\u043c\u0435\u043c \u043f\u0435\u0440\u0438\u043e\u0434 30 \u0434\u043d\u0435\u0439</p> <pre><code>data['VALUE_box_season'] = data['VALUE_box'] - data['VALUE_box'].shift(30)\ntsplot(data.VALUE_box_season[30:])\n# Dickey-Fuller criterion: p=1.3329526107395548e-13\n</code></pre> <p></p> <p>\u041d\u0443\u043b\u0435\u0432\u0430\u044f \u0433\u0438\u043f\u043e\u0442\u0435\u0437\u0430 Dicker-Fuller \u0442\u0435\u0441\u0442\u0430 \u043e \u043d\u0435\u0441\u0442\u0430\u0446\u0438\u043e\u043d\u0430\u0440\u043d\u043e\u0441\u0442\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0435 \u043e\u0442\u0432\u0435\u0440\u0433\u0430\u0435\u0442\u0441\u044f, \u043d\u043e \u0433\u0440\u0430\u0444\u0438\u043a \u0430\u0432\u0442\u043e\u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0438 \u0433\u043e\u0432\u043e\u0440\u0438\u0442 \u043d\u0430\u043c \u043e \u0442\u043e\u043c \u0447\u0442\u043e \u0447\u0442\u043e \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0435\u0449\u0435 \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u0441\u0434\u0432\u0438\u0433 </p>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#removing-seasonality-final-tweak","title":"Removing seasonality (final tweak)","text":"<p>\u0414\u043e\u0431\u0430\u0432\u0438\u043c \u0435\u0449\u0435 \u043e\u0434\u0438\u043d \u0434\u0435\u043d\u044c!</p> <pre><code>data['VALUE_box_season_diff'] = data.VALUE_box_season - data['VALUE_box_season'].shift(1)\ntsplot(data['VALUE_box_season_diff'][30+1:])\n# Dickey-Fuller criterion: p=6.437091549657279e-20\n</code></pre> <p></p> <p>\u0412\u0441\u0435 \u043e\u0442\u043b\u0438\u0447\u043d\u043e! \u041e\u0431\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0435 \u0433\u043e\u0432\u043e\u0440\u044f\u0442 \u043d\u0430\u043c \u043e \u0442\u043e\u043c \u0447\u0442\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0439 \u0440\u044f\u0434 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0441\u0442\u0430\u0446\u0438\u043e\u043d\u0430\u0440\u043d\u044b\u043c. \u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044f \u044d\u0442\u0438\u043c\u0438 \u0444\u0438\u0447\u0430\u043c\u0438 \u043a\u043e\u0433\u0434\u0430 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043f\u043e\u0434\u0445\u043e\u0434 feature engineering II</p> <pre><code>REPORTDATE  VALUE   dow     y   d   m   VALUE_box   VALUE_box_season    VALUE_box_season_diff\n0   2013-12-30  3457625638  0   2013    30  12  93039.660413    NaN     NaN\n1   2013-12-31  3417092149  1   2013    31  12  92505.507470    NaN     NaN\n2   2014-01-01  3417092149  2   2014    1   1   92505.507470    NaN     NaN\n3   2014-01-02  3417092149  3   2014    2   1   92505.507470    NaN     NaN\n4   2014-01-03  3417092149  4   2014    3   1   92505.507470    NaN     NaN\n</code></pre>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#training-models","title":"Training Models","text":"<p>\u041e\u0442\u043b\u0438\u0447\u043d\u043e, \u043c\u044b \u043f\u0440\u0438\u043c\u0435\u0432\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u043a \u043d\u0443\u0436\u043d\u044b\u0435 \u043d\u0430\u043c \u0444\u043e\u0440\u043c\u0430\u0442\u0435, \u0442\u0435\u043f\u0435\u0440\u044c \u043f\u0440\u0438\u0441\u0442\u0443\u043f\u0438\u043c \u043a \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044e. \u0411\u0443\u0434\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c 3 \u043c\u0435\u0441\u0435\u0447\u043d\u044b\u0439 \u0438 6 \u043c\u0435\u0441\u0435\u0447\u043d\u044b\u0439 \u043f\u0435\u0440\u0438\u043e\u0434, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0434\u0432\u0430 \u043f\u043e\u0434\u0445\u043e\u0434\u0430 feature engineering.</p> <ul> <li>\u0412 \u043f\u0435\u0440\u0432\u043e\u043c \u043f\u043e\u0434\u0445\u043e\u0434\u0435 \u043c\u044b \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0444\u0438\u0447\u0430\u043c\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438 \u0447\u0435\u0440\u0435\u0437 <code>dt</code>,  </li> <li>\u0412\u043e \u0432\u0442\u043e\u0440\u043e\u043c \u043f\u043e\u0434\u0445\u043e\u0434\u0435 \u043c\u044b \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u043c\u0438 \u0438\u043d\u0434\u0438\u043a\u0430\u0442\u043e\u0440\u0430\u043c\u0438!</li> </ul>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#_1","title":"\u0412\u044b\u0431\u043e\u0440 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043e\u0446\u0435\u043d\u043a\u0438","text":"<p>\u0414\u043b\u044f \u044d\u0442\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u043c \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u043e\u043c catboost, \u044d\u0442\u0443 \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0435 \u0440\u0435\u0434\u043a\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0431\u0443\u0434\u0443\u0439\u0449\u0435\u0433\u043e, \u0432 \u0442\u043e\u043c \u0447\u0438\u0441\u043b\u0435 \u0432 \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u044f\u0445 kaggle. \u0414\u043b\u044f \u043e\u0446\u0435\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u0431\u0443\u0434\u0435\u043c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0441\u044f \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0439 \u043c\u0435\u0442\u0440\u0438\u043a\u043e\u0439 RMSE</p>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#_2","title":"\u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432","text":"<p>CatBoost \u0445\u043e\u0442\u044c \u0438 \u0443\u043c\u0435\u0435\u0442 \u0443\u043c\u043d\u043e \u0430\u0434\u0430\u043f\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043f\u043e\u0434 \u0432\u044b\u0431\u043e\u0440\u043a\u0443 (\u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e), \u043d\u0430\u043c \u0432\u0441\u0435 \u0440\u0430\u0432\u043d\u043e \u0431\u0443\u0434\u0435\u0442 \u0432\u0430\u0436\u043d\u043e \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043d\u0430\u0448\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u043b\u044f \u0440\u0430\u0437\u043d\u044b\u0445 \u0441\u0440\u043e\u043a\u043e\u0432 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0438. \u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u043c\u043e\u0434\u0443\u043b\u0435\u043c Optuna. \u041f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0441\u044f \u0438\u043c \u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u0441\u043b\u043e\u0436\u043d\u043e, \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u0446\u0435\u043b\u0435\u0432\u0443\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u044e, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0431\u0443\u0434\u0435\u0442 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0442\u044c \u043c\u0435\u0442\u0440\u0438\u043a\u0443 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f. \u0412 \u0444\u0443\u043d\u043a\u0446\u0438\u0438, \u043c\u044b \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438, \u0441\u0430\u043c\u0443 \u043c\u043e\u0434\u0435\u043b\u044c \u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f.</p> <p>\u041f\u043e\u0447\u0435\u043c\u0443 \u0438\u043c\u0435\u043d\u043d\u043e \u044d\u0442\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b?</p> <ul> <li>learning_rate \u043a\u043e\u043d\u0435\u0447\u043d\u043e \u043e\u0434\u043d\u0430 \u0438\u0437 \u0432\u0430\u0436\u043d\u0435\u0439\u0448\u0438\u0445 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0432 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u043c \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u0435</li> <li>depth \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0438\u0437\u0443\u0447\u0438\u0442\u044c \u043a\u0430\u043a\u043e\u0435 \u0432\u043b\u0438\u044f\u043d\u0438\u0435 \u0438\u043c\u0435\u0435\u0442 \u0433\u043b\u0443\u0431\u0438\u043d\u0430 \u0431\u0430\u0437\u043e\u0432\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439</li> <li>l2_leaf_reg \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u043e\u0442\u0432\u0435\u0447\u0430\u0435\u0442 \u0437\u0430 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438 L2 \u0434\u043b\u044f \u043b\u0438\u0441\u0442\u043e\u0432\u044b\u0445 \u0443\u0437\u043b\u043e\u0432 \u0434\u0435\u0440\u0435\u0432\u0430, \u0447\u0435\u043c \u0431\u043e\u043b\u044c\u0448\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440, \u0442\u0435\u043c \u0431\u043e\u043b\u044c\u0448\u0435 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438, \u043e\u043d \u043d\u0443\u0436\u0435\u0442 \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u043f\u0440\u0435\u0434\u043e\u0442\u0432\u0440\u0430\u0449\u0430\u0442\u044c \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043f\u0443\u0442\u0435\u043c \u0448\u0442\u0440\u0430\u0444\u043e\u0432\u0430\u043d\u0438\u044f \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u0432\u0435\u0441\u043e\u0432 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432</li> </ul> <pre><code># Define objective function for Optuna\ndef objective(trial):\n\n    # Define hyperparameters to optimize\n    params = {\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),\n        'depth': trial.suggest_int('depth', 4, 10),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10)\n    }\n\n    # Initialize CatBoostRegressor with the hyperparameters\n    model = CatBoostRegressor(**params, verbose=0)\n    model.fit(X_train, y_train, eval_set=(X_val, y_val))\n    y_pred = model.predict(X_val)\n    mse = mean_squared_error(y_val, y_pred)\n\n    return mse\n</code></pre> <p>\u0421\u043e\u0437\u0434\u0430\u0432 \u0446\u0435\u043b\u0435\u0432\u0443\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u044e, \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u043d\u0430\u0447\u0430\u0442\u044c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u043e\u043d\u044b\u0439 \u0446\u0438\u043a\u043b</p> <pre><code>study = optuna.create_study(direction='minimize',)\nstudy.optimize(objective, n_trials=100,show_progress_bar=True)\n</code></pre>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#helper-functions","title":"Helper Functions","text":"<p><code>prepare_output</code> \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043e\u044c\u044a\u044f\u0434\u0438\u043d\u0435\u043d\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435, \u0447\u0442\u043e\u0431\u044b \u043c\u043e\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c plotly express. <code>get_split</code> \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u0440\u0430\u0437\u0431\u0438\u0442\u044c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 \u0434\u0432\u0435 \u0431\u044b\u0431\u043e\u0440\u043a\u0438</p> <pre><code>from catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# helper function for plotting in plotly\ndef prepare_output(model,X_train,y_train,X_test,y_test):\n\n    # predict &amp; merge\n    y_pred_tr = pd.Series(model.predict(X_train),y_train.index,name='pred').to_frame()\n    y_pred_tr['ground'] = y_train['VALUE']\n    y_pred_tr['subset'] = 'train'\n    y_pred = model.predict(X_test)\n    y_pred_te = pd.Series(y_pred,y_test.index,name='pred').to_frame()\n    y_pred_te['ground'] = y_test['VALUE']\n    y_pred_te['subset'] = 'test'\n    print('test rmse:',f'{mean_squared_error(y_pred,y_test,squared=False):.4e}')\n\n    all = pd.concat([y_pred_tr,y_pred_te],axis=0)\n    all['diff'] = abs(all['pred'] - all['ground'])\n    return all\n\n# helper function to split the dataset by days\ndef get_split(n:int):\n    X_train = X[:-30*n]\n    y_train = y[:-30*n]\n    X_test = X[-30*n:]\n    y_test = y[-30*n:]\n    max_tr = max(list(X_train.index))\n    return X_train,y_train,X_test,y_test,max_tr\n</code></pre>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#feature-engineering-i","title":"Feature Engineering I","text":"<p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u043a \u0441\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0431\u0430\u0437\u043e\u0432\u044b\u0435 \u0444\u0438\u0447\u0438 \u0434\u043b\u044f \u043a\u0440\u0430\u0442\u043a\u043e\u0441\u0440\u043e\u0447\u043d\u043e\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f (3 \u043c\u0435\u0441\u044f\u0446\u0430) \u0438 \u0434\u0430\u043b\u044c\u043d\u043e\u0441\u0440\u043e\u0447\u043d\u043e\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f (6 \u043c\u0435\u0441\u044f\u0446\u0430)</p> 3-Month Forecast <p>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044f \u0431\u0430\u0437\u043e\u0432\u044b\u043c\u0438 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 \u0434\u043b\u044f \u043a\u043e\u0440\u043e\u0442\u043a\u043e\u0433\u043e \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f; 3 \u043c\u0435\u0441\u044f\u0446\u0435\u0432</p> <pre><code># 3 month split for test set\nX_train,y_train,X_test,y_test,max_tr = get_split(3)\n\n# train\nmodel = CatBoostRegressor(silent=True)\nmodel.fit(X_train,y_train)\n\nall = prepare_output(model,X_train,y_train,X_test,y_test)\nfig = px.line(all,all.index,y=['pred','ground','diff'],template='plotly_white',width=800,height=300,title='3 month forecast model')\nfig.update_xaxes(range = [1500,2100])\nfig.add_vrect(x0=0, x1=max_tr, line_width=0, fillcolor=\"yellow\", opacity=0.1)\nfig.add_vline(x=max_tr)\n# test rmse: 5.1401e+09\n</code></pre> <p></p> 6-Month Forecast <p>\u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0431\u043e\u043b\u0435\u0435 \u0434\u043b\u0438\u043d\u043d\u043e\u0433\u043e \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f; 6 \u043c\u0435\u0441\u044f\u0446\u0435\u0432</p> <pre><code># 6 month split for test set\nX_train,y_train,X_test,y_test,max_tr = get_split(6)\n\n# train\nmodel = CatBoostRegressor(silent=True)\nmodel.fit(X_train,y_train)\n\nall = prepare_output(model,X_train,y_train,X_test,y_test)\nfig = px.line(all,all.index,y=['pred','ground','diff'],template='plotly_white',width=800,height=300,title='6 month forecast model')\nfig.update_xaxes(range = [1500,2100])\nfig.add_vrect(x0=0, x1=max_tr, line_width=0, fillcolor=\"yellow\", opacity=0.1)\nfig.add_vline(x=max_tr)\n# test rmse: 1.6883e+10\n</code></pre> <p></p> 3-Month Forecast (Hyperparameter Tuning) <p>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044f \u0431\u0430\u0437\u043e\u0432\u044b\u043c\u0438 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 \u0434\u043b\u044f \u043a\u043e\u0440\u043e\u0442\u043a\u043e\u0433\u043e \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f; 3 \u043c\u0435\u0441\u044f\u0446\u0435\u0432</p> <pre><code>X_train,y_train,X_val,y_val,max_tr = get_split(3)\n\n# Create study object and optimize the objective function\nstudy = optuna.create_study(direction='minimize',)\nstudy.optimize(objective, n_trials=100,show_progress_bar=True)\n\n# Get the best hyperparameters found by Optuna\nbest_params = study.best_params\nprint(\"Best hyperparameters:\", best_params)\n# Best hyperparameters: {'learning_rate': 0.2688362880517153, 'depth': 4, 'l2_leaf_reg': 9.45603461263423}\n</code></pre> <pre><code>best_model = CatBoostRegressor(**best_params, verbose=0)\nbest_model.fit(X_train,y_train)\ny_pred = best_model.predict(X_val)\n\nall = prepare_output(best_model,X_train,y_train,X_val,y_val)\nfig = px.line(all,all.index,y=['pred','ground','diff'],template='plotly_white',width=800,height=300,title='3 month forecast model (optimised hyperparameters)')\nfig.update_xaxes(range = [1500,2100])\nfig.add_vrect(x0=0, x1=max_tr, line_width=0, fillcolor=\"yellow\", opacity=0.1)\nfig.add_vline(x=max_tr)\n# test rmse: 1.6375e+09\n</code></pre> <p></p> 6-Month Forecast (Hyperparameter Tuning) <p>\u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0431\u043e\u043b\u0435\u0435 \u0434\u043b\u0438\u043d\u043d\u043e\u0433\u043e \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f; 6 \u043c\u0435\u0441\u044f\u0446\u0435\u0432</p> <pre><code>X_train,y_train,X_val,y_val,max_tr = get_split(6)\n\n# Create study object and optimize the objective function\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100,show_progress_bar=True)\n\n# Get the best hyperparameters found by Optuna\nbest_params = study.best_params\nprint(\"Best hyperparameters:\", best_params)\n# Best hyperparameters: {'learning_rate': 0.09317938390639577, 'depth': 4, 'l2_leaf_reg': 1.4161107817520655}\n</code></pre> <pre><code>best_model = CatBoostRegressor(**best_params, verbose=0)\nbest_model.fit(X_train,y_train)\ny_pred = best_model.predict(X_val)\n\nall = prepare_output(best_model,X_train,y_train,X_val,y_val)\nfig = px.line(all,all.index,y=['pred','ground','diff'],template='plotly_white',width=800,height=300,title='6 month forecast model (optimised hyperparameters)')\nfig.update_xaxes(range = [1500,2100])\nfig.add_vrect(x0=0, x1=max_tr, line_width=0, fillcolor=\"yellow\", opacity=0.1)\nfig.add_vline(x=max_tr)\n# test rmse: 1.2348e+10\n</code></pre> <p></p>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#feature-engineering-ii","title":"Feature Engineering II","text":"<p>\u0412 \u0446\u0435\u043b\u043e\u0432 \u043c\u044b \u0443\u0432\u0438\u0434\u0438\u043b\u0438 \u0447\u0442\u043e \u0434\u043b\u044f \u043a\u0440\u0430\u0442\u043a\u043e\u0441\u0440\u043e\u0447\u043d\u043e\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f, feature engineering I \u0444\u0438\u0447\u0438 \u043d\u0435 \u043f\u043b\u043e\u0445\u043e \u0441\u0440\u0430\u0432\u043d\u044f\u044e\u0442\u0441\u044f, \u043d\u043e \u0434\u043b\u044f 6 \u043c\u0435\u0441\u044f\u0447\u043d\u044b\u0445 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0438 \u043d\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0432\u0435\u043d\u043d\u043e \u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c \u0432\u0442\u043e\u0440\u043e\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u0441 feature engineering II \u0444\u0438\u0447\u0430\u043c\u0438, \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u043a \u043e\u043d\u0438 \u0441\u043f\u0440\u0430\u0432\u044f\u0442\u0441\u044f</p> <p>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043a\u0435\u043c\u044f \u0434\u0432\u0443\u043c\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0430\u043c\u0438 momentum \u0438 rate of change. \u042d\u0442\u043e \u043e\u0431\u043e\u0441\u043d\u043e\u0432\u0430\u0435\u0442\u0441\u044f \u0442\u0435\u043c \u0447\u0442\u043e \u043e\u0431\u0430 <code>ma</code>, <code>ema</code> \u0441\u043e\u0437\u0434\u0430\u044e\u0442 \u0444\u0438\u0447\u0438 \u043a\u043e\u0442\u043e\u0440\u0435 \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0441\u0438\u043b\u044c\u043d\u043e \u043a\u043e\u0440\u0440\u0435\u043b\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u0441 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 (0.9-0.98). \u041c\u0443\u043b\u044c\u0442\u0438\u043a\u043e\u0440\u0440\u0435\u043b\u0438\u0430\u0440\u043d\u043e\u0441\u0442\u044c \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u0447 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0440\u044f\u0434\u043e\u0432 \u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u0436\u0435\u043b\u0430\u0442\u0435\u043b\u044c\u043d\u043e.</p> Selecting the subset of data <p>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044f \u0438\u043d\u0442\u0435\u0440\u0432\u0430\u043b\u0430\u043c\u0438 100,200 \u0438 300 \u0434\u043d\u0435\u0439</p> <pre><code>data['MOM100'] = mom(data,'VALUE',100)\ndata['MOM200'] = mom(data,'VALUE',200)\ndata['MOM300'] = mom(data,'VALUE',300)\n\ndata['ROC100'] = roc(data,'VALUE',100)\ndata['ROC200'] = roc(data,'VALUE',200)\ndata['ROC300'] = roc(data,'VALUE',300)\n</code></pre> <pre><code>subset = data[['dow','y','d','m','VALUE','MOM100','MOM200','MOM300','VALUE_box_season_diff','VALUE_box_season','ROC100','ROC200','ROC300']]\nprint(subset.shape)\nsubset = subset.dropna()\nprint(subset.shape)\ncorrMat(subset,target='VALUE')\n\n# (2111, 13)\n# (1811, 13)\n</code></pre> 6-Month Forecast <p>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044f \u0431\u0430\u0437\u043e\u0432\u044b\u043c\u0438 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 \u0434\u043b\u044f \u0431\u043e\u043b\u0435\u0435 \u0434\u043b\u0438\u043d\u043d\u043e\u0433\u043e \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f; 6 \u043c\u0435\u0441\u044f\u0446\u0435\u0432</p> <pre><code># 6 month split for test set\nX_train,y_train,X_test,y_test,max_tr = get_split(6)\n\n# train\nmodel = CatBoostRegressor(silent=True)\nmodel.fit(X_train,y_train)\n\nall = prepare_output(model,X_train,y_train,X_test,y_test)\nfig = px.line(all,all.index,y=['pred','ground','diff'],template='plotly_white',width=800,height=300,title='6 month forecast model')\nfig.update_xaxes(range = [1500,2100])\nfig.add_vrect(x0=0, x1=max_tr, line_width=0, fillcolor=\"yellow\", opacity=0.1)\nfig.add_vline(x=max_tr)\n# test rmse: 1.0816e+10\n</code></pre> <p></p> 6-Month Forecast (Hyperparameter Tuning) <p>\u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0431\u043e\u043b\u0435\u0435 \u0434\u043b\u0438\u043d\u043d\u043e\u0433\u043e \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f; 6 \u043c\u0435\u0441\u044f\u0446\u0435\u0432</p> <pre><code>X_train,y_train,X_val,y_val,max_tr = get_split(6)\n\n# Create study object and optimize the objective function\nstudy = optuna.create_study(direction='minimize',)\nstudy.optimize(objective, n_trials=100,show_progress_bar=True)\n\n# Get the best hyperparameters found by Optuna\nbest_params = study.best_params\nprint(\"Best hyperparameters:\", best_params)\n# Best hyperparameters: {'learning_rate': 0.4956253048007977, 'depth': 4, 'l2_leaf_reg': 1.3139821695288907}\n</code></pre> <pre><code>best_model = CatBoostRegressor(**best_params, verbose=0)\nbest_model.fit(X_train,y_train)\ny_pred = best_model.predict(X_val)\n\nall = prepare_output(best_model,X_train,y_train,X_val,y_val)\nfig = px.line(all,all.index,y=['pred','ground','diff'],template='plotly_white',width=800,height=300,title='6 month forecast model (optimised hyperparameters)')\nfig.update_xaxes(range = [1500,2100])\nfig.add_vrect(x0=0, x1=max_tr, line_width=0, fillcolor=\"yellow\", opacity=0.1)\nfig.add_vline(x=max_tr)\n# test rmse: 5.1134e+09\n</code></pre> <p></p> <p>\u041f\u0440\u0438\u0435\u043c\u0443\u0449\u0435\u0441\u0442\u0432\u043e catboost (\u0434\u0430 \u0438 \u0432 \u0446\u0435\u043b\u043e\u0432 \u043c\u0435\u0442\u043e\u0434 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432) \u043a\u043e\u043d\u0435\u0447\u043d\u043e \u0432 \u0442\u043e\u043c \u0447\u0442\u043e \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u043f\u043e\u043b\u0443\u0447\u0430\u0442\u044c \u043e\u0431\u0440\u0430\u0442\u043d\u0443\u044e \u0441\u0432\u044f\u0437\u044c \u043e \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u0432 \u0434\u0430\u043d\u043d\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435\u043c \u043c\u044b \u0432\u0438\u0434\u0438\u043c \u0447\u0442\u043e \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0444\u0438\u0447\u0438 \u043d\u0430\u043c\u043d\u043e\u0433\u043e \u0432\u0430\u0436\u043d\u0435\u0435 <code>VALUE_box_season_diff</code>, <code>VALUE_box_season</code>, <code>ROC</code> \u0442\u043e\u0436\u0435 \u043d\u0435 \u043e\u0441\u043e\u0431\u043e \u0432\u043b\u0438\u044f\u0435\u0442 \u043d\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442. \u042d\u0442\u043e \u0433\u043e\u0432\u043e\u0440\u0438\u0442 \u043e \u0442\u043e\u043c \u0447\u0442\u043e \u0431\u043e\u043b\u044c\u0448\u043e\u0433\u043e \u0441\u043c\u044b\u0441\u043b\u0430 \u0432 \u0441\u0442\u0430\u0446\u0438\u043e\u043d\u0430\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0435\u0431\u044b\u043b\u043e \u043f\u0440\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0444\u0438\u0447. </p> <pre><code>importances = best_model.get_feature_importance(type='PredictionValuesChange')\nfeature_importances = pd.Series(importances, index=X.columns,name='fi').sort_values()\npx.bar(feature_importances,template='plotly_white',height=300,width=800,title='Feature importance')\n</code></pre> <p></p>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#model-testing-result","title":"Model Testing Result","text":"<p>\u041f\u043e\u0434\u0432\u0435\u0434\u0435\u043c \u0438\u0442\u043e\u0433\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0432\u0441\u0435\u0445 \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439, \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u0435 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0431\u0430\u0437\u043e\u0432\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 (baseline)</p> Feature Engineering Time Interval RMSE Improvement Approach I 3 Months (baseline) 5.1401e+09 - Approach I 6 Months (baseline) 1.6883e+10 - Approach I 3 Months (optuna) 1.6375e+09 68% Approach I 6 Months (optuna) 1.2348e+10 26% Approach II 6 Months 1.0816e+10 35% Approach II 6 Months (optuna) 5.1134e+09 67%","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#concluding-remarks","title":"Concluding Remarks","text":"<p>\u041f\u043e\u0434\u0432\u0435\u0434\u0435\u043c \u0438\u0442\u043e\u0433\u0438, \u0446\u0435\u043b\u044c \u043d\u0430\u0448\u0435\u0433\u043e \u043f\u0440\u043e\u0435\u043a\u0442\u0430 \u0431\u044b\u043b\u0430 \u0432 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0431\u044b \u0434\u0430\u043b\u0430 \u043d\u0430\u043c \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043e\u0431\u044a\u0435\u043c \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u044b\u0445 \u0441\u0440\u0435\u0434\u0441\u0442\u0432 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432. </p> <ul> <li>\u0414\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u0430 \u043a \u044d\u0442\u043e\u043c\u0443 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044e \u043c\u043e\u0436\u043d\u043e \u043e\u0442\u043d\u0435\u0441\u0442\u0438 \u0441\u0430\u043c\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u043e\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438. </li> <li>\u041c\u044b \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043b\u0438 \u0434\u0432\u0430 \u0441\u0440\u043e\u043a\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u043f\u043e\u043b\u0435\u0437\u043d\u044b\u043c (3 \u0438 6 \u043c\u0435\u0441\u044f\u0446\u0435\u0432) \u0438 \u0441\u043e\u0437\u0434\u0430\u043b\u0438 \u0432\u0441\u0435 \u0433\u0440\u0443\u043f\u043f \u0444\u0438\u0447 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0433\u0443\u0442 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0445\u043e\u0440\u043e\u0448\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u043d\u0430 \u044d\u0442\u043e\u043c \u0438\u043d\u0442\u0435\u0440\u0432\u0430\u043b\u043e\u043c. \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u0438 \u0444\u0438\u0447\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u0438\u0437 \u0441\u0430\u043c\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0433\u043e \u0440\u044f\u0434\u0430, \u0430 \u0442\u0430\u043a \u0436\u0435 \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u0438\u0441\u044c \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u043c\u0438 \u043c\u0435\u0442\u0440\u0438\u043a\u0430\u043c\u0438. </li> <li>\u041c\u044b \u0442\u0430\u043a \u0436\u0435 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043b\u0438 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0439 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0439 \u0440\u044f\u0434 \u0432 \u0441\u0442\u0430\u0446\u0438\u043e\u0440\u0430\u0440\u043d\u0443\u044e \u0444\u043e\u0440\u043c\u0443, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0438 autocorrelation \u0438 dicker-fuller, \u043d\u043e \u0432 \u0438\u0442\u043e\u0436\u0435 \u043a\u043e\u0433\u0434\u0430 \u043c\u044b \u0434\u043e\u0431\u0430\u0432\u0438\u043b\u0438 \u0438\u0445 \u043a \u0434\u0440\u0443\u0433\u0438\u043c \u0444\u0438\u0447\u0430\u043c, \u043e\u0441\u043e\u0431\u043e\u0433\u043e \u0432\u043b\u0438\u044f\u043d\u0438\u044f \u043e\u043d\u0438 \u043d\u0435 \u043e\u043a\u0430\u0437\u0430\u043b\u0438. </li> <li>\u0412 \u044d\u0442\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438, \u043c\u043e\u0434\u0435\u043b\u044c \u0431\u043e\u043b\u044c\u0448\u0435 \u043e\u043f\u0438\u0440\u0430\u043b\u0430\u0441\u044c \u043d\u0430 \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 momentum. \u0421 \u043f\u043e\u043c\u043e\u0448\u044c\u044e Optuna \u043c\u044b \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043b\u0438 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b catboost \u0438 \u0441\u043c\u043e\u0433\u043b\u0438 \u0441\u043d\u0438\u0437\u0438\u0442\u044c \u043e\u0448\u0438\u0431\u043a\u0443 RMSE \u043d\u0430 68% \u0438 67%. </li> <li>\u042d\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u043c\u043e\u0436\u043d\u043e \u0434\u0430\u043b\u0435\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u043f\u0440\u0438\u043d\u044f\u0442\u0438\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u0438 \u043e \u0442\u043e\u043c \u043a\u0430\u043a\u043e\u0439 \u043e\u0431\u044a\u0435\u043c \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u044b\u0445 \u0441\u0440\u0435\u0434\u0441\u0442\u0432 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432 \u043c\u043e\u0436\u043d\u043e \u043e\u0442\u043a\u043b\u0430\u0434\u044b\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u0432\u044b\u0434\u0430\u0447\u0438 \u043a\u0440\u0435\u0434\u0438\u0442\u043e\u0432. </li> </ul>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#_3","title":"\u041a\u0430\u043a \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u043c\u0435\u0442\u0440\u0438\u043a\u0438","text":"<p>\u041c\u043e\u0434\u0435\u043b\u0438 \u043d\u0443\u0436\u043d\u043e \u043f\u043e\u0441\u0442\u043e\u044f\u043d\u043d\u043e \u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0442\u044c \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u043e\u043d\u0438 \u043d\u0435 \u0442\u0435\u0440\u044f\u043b\u0438 \u0430\u043a\u0442\u0443\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c, \u0438\u0434\u0435\u043b\u044c\u043d\u043e \u0431\u044b\u043b\u043e \u0431\u044b \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0435\u0449\u0435 \u0431\u043e\u043b\u0435\u0435 \u0434\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f (1 \u0433\u043e\u0434). </p> <ul> <li>\u0422\u0430\u043a \u0436\u0435 \u043d\u0443\u0436\u043d\u043e \u043d\u0430\u0439\u0442\u0438 \u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0449\u0438\u0435 \u0444\u0438\u0447\u0438 \u0434\u043b\u044f \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u0441\u043d\u0438\u0436\u0435\u043d\u0438\u044f RMSE. </li> <li>\u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0435 \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u043e\u0432\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0443\u0434\u043e\u0431\u044b\u043d\u0435 \u0442\u0435\u043c \u0447\u0442\u043e \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0442\u044c \u043a\u0430\u043a\u0438\u0435 \u0444\u0438\u0447\u0438 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0432\u043b\u0438\u044f\u044e\u0442 \u043d\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435. </li> <li>\u0412 \u0431\u0443\u0434\u0443\u0439\u0448\u0435\u043c \u043c\u043e\u0436\u043d\u043e \u0434\u0430\u0436\u0435 \u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c \u0442\u0430\u043a\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043a\u0430\u043a LSTM \u0438 GRU \u0438\u043b\u0438 \u0434\u0430\u0436\u0435 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438.</li> </ul>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/17/prediction-of-customer-stable-funds-volume.html#_4","title":"\u0427\u0442\u043e \u0443\u0437\u043d\u0430\u043b\u0438 \u043d\u043e\u0432\u043e\u0433\u043e","text":"<p>\u0412\u0430\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u0432\u0441\u043f\u043e\u043c\u043d\u0438\u0442\u044c \u043a\u0430\u043a\u0438\u0435 \u043f\u0440\u0435\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043c\u043e\u0436\u043d\u043e \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0442\u044c \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0439 \u0440\u044f\u0434 \u0441\u0442\u0430\u0446\u0438\u043e\u043d\u0430\u0440\u043d\u0443\u044e \u0444\u043e\u0440\u043c\u0443, \u044d\u0442\u043e \u043c\u043e\u0436\u043d\u043e \u0434\u0435\u043b\u0430\u0442\u044c \u0441 \u043f\u043e\u043c\u043e\u0448\u044c\u044e \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043c\u0435\u0442\u0440\u0438\u043a autocorrelation \u0438 dicker-fuller \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u044f\u043c\u0438. \u0418\u043d\u0442\u0443\u0438\u0442\u0438\u0432\u043d\u043e \u044f \u043e\u0431\u044b\u0447\u043d\u043e \u043f\u0440\u043e\u0441\u0442\u043e \u0441\u0440\u0430\u0437\u0443 \u043f\u0440\u0438\u0441\u0442\u0443\u043f\u0430\u044e \u0443 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044e \u0444\u0438\u0447 \u0438 \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f. \u0412 \u0434\u0430\u043d\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 \u044d\u0442\u0438 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u043d\u0435 \u0432\u043d\u0435\u0441\u043b\u0438 \u0432\u043a\u043b\u0430\u0434\u0430 \u043d\u043e \u0432\u0441\u0435 \u0440\u0430\u0432\u043d\u043e \u044d\u0442\u043e \u0432\u0430\u0436\u043d\u043e \u0434\u0435\u043b\u0430\u0442\u044c \u0432 \u0440\u0430\u043c\u043a\u0430\u0445 \u0438\u0441\u043f\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u044f \u0432\u043b\u0438\u044f\u043d\u0438\u044f \u0441\u0442\u0430\u0446\u0438\u043e\u043d\u0430\u0440\u043d\u043e\u0441\u0442\u0438.</p>","tags":["machine learning","financial analysis","catboost","optuna","time series"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html","title":"Prediction of Product Stock Levels","text":"<p>In this project, we work with a client Gala Groceries, who has contacted Cognizant for logistics advice about product storage </p> <ul> <li>Specifically, they are interested in wanting to know how better stock the items that they sell. </li> <li>Our role is to take on this project as a data scientist and understand what the client actually needs. This will result in the formulation/confirmation of a new project statement, in which we will be focusing on predicting stock levels of products. </li> <li>Such a model would enable the client to estimate their product stock levels at a given time &amp; make subsequent business decisions in a more effective manner reducing understocking and overstocking losses.</li> </ul> <ul> <li> GitHub Repository</li> <li> Cognizant Internship (Discontinued)</li> </ul>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#background","title":"Background","text":"","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#project-statement","title":"Project Statement","text":"<p>In this project, we aim to help Gala Groceries who have approached Cognizant to help them with supply chain issues. Specifically, they are interested in wanting to know how better stock the items that they sell</p> <p>Can we accurately predict the stock levels of products based on sales data and sensor data on an hourly basis in order to more intelligently procure products from our suppliers?\u201d </p>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#gala-groceries","title":"Gala Groceries","text":"<p>More information about the client:</p> <ul> <li>Gala Groceries is a technology-led grocery store chain based in the USA. They rely heavily on new technologies, such as IoT to give them a competitive edge over other grocery stores. </li> <li>They pride themselves on providing the best quality, fresh produce from locally sourced suppliers. However, this comes with many challenges to consistently deliver on this objective year-round.</li> </ul> <p>Gala Groceries approached Cognizant to help them with a supply chain issue. Groceries are highly perishable items. If you overstock, you are wasting money on excessive storage and waste, but if you understock, then you risk losing customers. They want to know how to better stock the items that they sell.</p> <p>This is a high-level business problem and will require you to dive into the data in order to formulate some questions and recommendations to the client about what else we need in order to answer that question</p>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#clients-dataset","title":"Clients dataset","text":"<p>The client has agreed to share data in the form of <code>sensor data</code>. - They use sensors to measure temperature storage facilities where products are stored in the warehouse, - And they also use stock levels within the refrigerators and freezers in store</p>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#preview","title":"Preview","text":"<p>Initially, the Data Engineering team is able to extract one weeks worth of sales data for one of the Gala Groceries stores, this will allow us to find some insights into the data and in general feel more confident with the data that we will be using in future machine learning modeling. Our task at hand is to summarise what we have learned from the data, as well as make some suggestions about what we will be needing in order to fulfill the business requirement of the client.</p> <p>Let's take a look at our sample dataset:</p> <pre><code>+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n|      transaction_id|          timestamp|          product_id|category|customer_type|unit_price|quantity|total|payment_type|\n+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n|a1c82654-c52c-45b...|2022-03-02 09:51:38|3bc6c1ea-0198-46d...|   fruit|         gold|      3.99|       2| 7.98|    e-wallet|\n|931ad550-09e8-4da...|2022-03-06 10:33:59|ad81b46c-bf38-41c...|   fruit|     standard|      3.99|       1| 3.99|    e-wallet|\n|ae133534-6f61-4cd...|2022-03-04 17:20:21|7c55cbd4-f306-4c0...|   fruit|      premium|      0.19|       2| 0.38|    e-wallet|\n|157cebd9-aaf0-475...|2022-03-02 17:23:58|80da8348-1707-403...|   fruit|         gold|      0.19|       4| 0.76|    e-wallet|\n|a81a6cd3-5e0c-44a...|2022-03-05 14:32:43|7f5e86e6-f06f-45f...|   fruit|        basic|      4.49|       2| 8.98|  debit card|\n+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n</code></pre> <p>We have the following features</p> <ul> <li>transaction_id : represents a unique transaction identified</li> <li>timestamp : time at which a product was purchased</li> <li>product_id : the unique identifier of the product which was purchased</li> <li>category : the category of product which was purchased</li> <li>unit_price : the price of the purchased product</li> <li>quantity : the ammount of product purchased</li> <li>total : the total ammount of product purchased</li> <li>payment_type : the type of payment which the client used to purchase the product</li> </ul>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#exploratory-data-analysis","title":"Exploratory Data Analysis","text":"","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#statistical-tests","title":"Statistical Tests","text":"<p>Statistical tests are quite useful to check some hypothesis that can arise, let's use chi2 test to analyse the relationship between two categorical variables</p> <pre><code>import pingouin as pg\n\ndef crosstables(df,cat1,cat2):\n\n    print('&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Frequencies &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;')\n    print('')\n    w=pd.crosstab(df[cat1],df[cat2])\n    print(w)\n    print('')\n    print('&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Chi-square test &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;')\n    e,o,st=pg.chi2_independence(data=df,x=cat1,y=cat2)\n    print(f\"Chi-Square Statistic:\")\n    print(spark.createDataFrame(st.round(3)).show())\n\ncrosstables(df,'customer_type','quantity')\ncrosstables(df,'customer_type','payment_type')\ncrosstables(df,'payment_type','quantity')\ncrosstables(df,'category','customer_type')\ncrosstables(df,'category','quantity')\n</code></pre> <p>Let's check is there is any relationship between customer_type &amp; quantities purchased</p> <pre><code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Frequencies &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\n\nquantity         1    2    3    4\ncustomer_type                    \nbasic          405  350  395  376\ngold           379  383  374  381\nnon-member     420  395  378  408\npremium        384  397  422  387\nstandard       391  395  385  424\n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Chi-square test &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\nChi-Square Statistic:\n+------------------+------+------+----+-----+------+-----+\n|              test|lambda|  chi2| dof| pval|cramer|power|\n+------------------+------+------+----+-----+------+-----+\n|           pearson|   1.0| 10.52|12.0| 0.57| 0.021|0.175|\n|      cressie-read| 0.667| 10.52|12.0| 0.57| 0.021|0.175|\n|    log-likelihood|   0.0| 10.52|12.0| 0.57| 0.021|0.175|\n|     freeman-tukey|  -0.5|10.522|12.0| 0.57| 0.021|0.175|\n|mod-log-likelihood|  -1.0|10.525|12.0| 0.57| 0.021|0.175|\n|            neyman|  -2.0|10.534|12.0|0.569| 0.021|0.176|\n+------------------+------+------+----+-----+------+-----+\n</code></pre> <p>Since the P value(pearson) is greater than 0.05 so there is no actual relationship between the customer_type and quantities**</p> <p>Let's check is there is any relationship between customer_type &amp; payment_type purchased</p> <pre><code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Frequencies &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\n\npayment_type   cash  credit card  debit card  e-wallet\ncustomer_type                                         \nbasic           373          386         391       376\ngold            358          389         352       418\nnon-member      434          396         402       369\npremium         434          382         388       386\nstandard        428          396         385       386\n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Chi-square test &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\nChi-Square Statistic:\n+------------------+------+------+----+-----+------+-----+\n|              test|lambda|  chi2| dof| pval|cramer|power|\n+------------------+------+------+----+-----+------+-----+\n|           pearson|   1.0|17.135|12.0|0.145| 0.027|0.282|\n|      cressie-read| 0.667|17.116|12.0|0.145| 0.027|0.282|\n|    log-likelihood|   0.0|17.084|12.0|0.146| 0.027|0.281|\n|     freeman-tukey|  -0.5|17.066|12.0|0.147| 0.027|0.281|\n|mod-log-likelihood|  -1.0|17.051|12.0|0.148| 0.027| 0.28|\n|            neyman|  -2.0|17.036|12.0|0.148| 0.027| 0.28|\n+------------------+------+------+----+-----+------+-----+\n</code></pre> <p>Since the P value(pearson) is greater than 0.05 so there is no actual relationship between the customer_type and quantities**</p> <p>Let's check is there is any relationship between payment_type &amp; payment_type purchased</p> <pre><code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Frequencies &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\n\nquantity        1    2    3    4\npayment_type                    \ncash          480  502  495  550\ncredit card   506  485  460  498\ndebit card    483  451  488  496\ne-wallet      510  482  511  432\n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Chi-square test &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\nChi-Square Statistic:\n+------------------+------+------+---+-----+------+-----+\n|              test|lambda|  chi2|dof| pval|cramer|power|\n+------------------+------+------+---+-----+------+-----+\n|           pearson|   1.0|17.613|9.0| 0.04| 0.027|0.331|\n|      cressie-read| 0.667|17.687|9.0|0.039| 0.027|0.333|\n|    log-likelihood|   0.0|17.843|9.0|0.037| 0.028|0.336|\n|     freeman-tukey|  -0.5|17.966|9.0|0.036| 0.028|0.338|\n|mod-log-likelihood|  -1.0|18.095|9.0|0.034| 0.028|0.341|\n|            neyman|  -2.0|18.371|9.0|0.031| 0.028|0.346|\n+------------------+------+------+---+-----+------+-----+\n</code></pre> <p>Since all the p value(pearson) are lower than 0.05 so there is a relationship between payment_type  and quantity. However, there is a very weak association between the payment type and quantity variables</p> <p>Let's check is there is any relationship between category &amp; customer_type purchased</p> <pre><code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Frequencies &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\n\ncustomer_type          basic  gold  non-member  premium  standard\ncategory                                                         \nbaby products             49    46          45       39        45\nbaked goods               87    73          85       89       109\nbaking                    53    55          47       55        54\nbeverages                 66    47          57       63        68\ncanned foods              89    77          88       93        84\ncheese                    42    63          77       60        51\ncleaning products         57    56          67       63        49\ncondiments and sauces     35    37          37       37        35\ndairy                     71    71          82       76        75\nfrozen                    45    59          48       55        56\nfruit                    186   197         187      206       222\nkitchen                   85    79          64       76        78\nmeat                      78    82          73       77        72\nmedicine                  47    41          63       52        40\npackaged foods           106    94         101      104       102\npersonal care             30    45          33       31        38\npets                      26    34          36       36        29\nrefrigerated items        90    73         102       76        84\nseafood                   44    50          54       55        50\nsnacks                    59    50          52       44        58\nspices and herbs          21    14          26       21        43\nvegetables               160   174         177      182       153\n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Chi-square test &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\nChi-Square Statistic:\n+------------------+------+------+----+-----+------+-----+\n|              test|lambda|  chi2| dof| pval|cramer|power|\n+------------------+------+------+----+-----+------+-----+\n|           pearson|   1.0|87.391|84.0|0.378| 0.053|0.467|\n|      cressie-read| 0.667|86.838|84.0|0.394| 0.053|0.463|\n|    log-likelihood|   0.0|86.121|84.0|0.415| 0.052|0.459|\n|     freeman-tukey|  -0.5|85.907|84.0|0.422| 0.052|0.458|\n|mod-log-likelihood|  -1.0|85.964|84.0| 0.42| 0.052|0.458|\n|            neyman|  -2.0|86.886|84.0|0.393| 0.053|0.464|\n+------------------+------+------+----+-----+------+-----+\n</code></pre> <p>Since all the p value(pearson) are greater than 0.05 so there is no relationship between customer_type and category</p> <p>Let's check is there is any relationship between category &amp; quantity purchased</p> <pre><code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Frequencies &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\n\nquantity                 1    2    3    4\ncategory                                 \nbaby products           57   67   48   52\nbaked goods            122  107  103  111\nbaking                  73   63   63   65\nbeverages               69   71   79   82\ncanned foods           106  110  104  111\ncheese                  57   66   86   84\ncleaning products       77   75   62   78\ncondiments and sauces   43   48   49   41\ndairy                   92   89   96   98\nfrozen                  65   61   75   62\nfruit                  271  251  238  238\nkitchen                 93   98   98   93\nmeat                   107   87   94   94\nmedicine                60   65   60   58\npackaged foods         129  118  125  135\npersonal care           39   43   50   45\npets                    39   51   30   41\nrefrigerated items     112  100   99  114\nseafood                 54   58   65   76\nsnacks                  69   62   69   63\nspices and herbs        34   28   38   25\nvegetables             211  202  223  210\n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Chi-square test &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\nChi-Square Statistic:\n+------------------+------+------+----+-----+------+-----+\n|              test|lambda|  chi2| dof| pval|cramer|power|\n+------------------+------+------+----+-----+------+-----+\n|           pearson|   1.0| 46.05|63.0|0.946| 0.044|0.361|\n|      cressie-read| 0.667|46.011|63.0|0.947| 0.044| 0.36|\n|    log-likelihood|   0.0|46.008|63.0|0.947| 0.044| 0.36|\n|     freeman-tukey|  -0.5|46.071|63.0|0.946| 0.044|0.361|\n|mod-log-likelihood|  -1.0|46.191|63.0|0.945| 0.044|0.362|\n|            neyman|  -2.0| 46.61|63.0|0.939| 0.045|0.366|\n+------------------+------+------+----+-----+------+-----+\n</code></pre> <p>Since all the p value(pearson) are greater than 0.05 so there is no relationship between category and quantity</p> <p>From most tests we can conclude that there is only a relationship between payment_type and quantity amongst the tested categorical column comparison cases</p>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#visualisations","title":"Visualisations","text":"<p>Visualisation &amp; utilisation of data wrangling to understand our data a little more and feel more comfortable with it is an important part of a DS project, some interesting findings are included below:</p>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#category-based-statistics","title":"Category based statistics","text":"<p>Meat, kitchen, seafood &amp; medicine are amongst the highely prices units, snacks &amp; fruit are amongst the more cheaper items sold at the store</p> <p></p> <p>The total sum of purchases gives us the following information: </p> <p></p>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#customer-type-visualisations","title":"Customer Type Visualisations","text":"<p>Whilst we will be more focusing on the task of creating a model that will predict the estimated stock levels, some interesting insights into customer purchases are still interesting to explore, for example, on average non members tend to buy more beverages, gold members tend to procure more cheese products, premium members tend to produre more personalised case products</p> <p></p> <p>We can also visualise the mean producure ment values for each category, premium and basic members tend to spend more on baby products</p> <p></p> <p>We can divide our data into different days and visualise the procurement mean of each customer_type. premium and non-members tend to spend the most</p> <p></p> <p>Let's look at the breakdown of the contibution to the total procurement of each customer type across each category</p> <p></p> <p>Of course there are many more things we can explore, let's focus on the business task at hand and write an email to the DS lead about the client's request.</p>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#framing-the-problem-statement","title":"Framing the Problem Statement","text":"","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#letter-to-ds-lead","title":"Letter to DS lead","text":"<p>We have to email the findings of our data exploration &amp; discuss our ideas about the client's needs</p> <pre><code>    Dear [insert name of recipient]\n\n    I received the sample dataset from the Data Engineering team and I\u2019ve been analysing the sample on behalf of the Data Science team.\n\n    I found the following insights as part of the analysis:\n\n    - Fruit &amp; vegetables are the 2 most frequently bought product categories\n    - Non-members are the most frequent buyers within the store\n    - Cash is the most frequently used payment method\n    - 11am is the busiest hour with regards to number of transactions\n    - On average non-members overspent gold, standard and basic members &amp; premium members spent the most\n\n    As a reminder, the client has indicated that they wanted to know the following: \u201cHow to better stock the items that they sell.\u201d\n\n    With respect to this business question, my recommendations are the following:\n\n    As this is a very broad statement, we need to identify a specific problem statement that the business would like to solve, so as an example, we can focus on predicting the store demand on an hourly basis, in order to smooth out the procurement logistics of each store so that they procure only the necessary product quantities on a weekly basis. We obviously need more data, the current sample only covers a week worth of data at a single store.\n\n    Based on the problem statement that we move forward with, we will need more informative dataset features related to stocking of products, for example, if we\u2019re modelling demand for products, we may want to include information about stock levels or weather conditions, customer spending habits, all these feature combinations must be tested.\n\n    Kind Regards, \n\n    [name of sender]\n</code></pre>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#model-diagram","title":"Model Diagram","text":"<p>Having clarified the problem statement, the data engineering team has provided a data model of the available tables of data that has been provided by the client, based on different sensor readings</p> <p>sales table is the same as in our initial Initial Data Exploration (with the exception that we have more data now). </p> <p>Additional IoT data (as per our request):</p> <ul> <li>The client has sensors that monitor the estimated stock of each product <code>product_id</code>, <code>estimated_stock_pct</code> (this will be our target variable), this is stored in sensor_stock_levels table.</li> <li>sensor data is also available to us, this data monitors the storage facility temperature data, this is stored in table sensor_storage_temperature</li> <li>As before we have the sales data stored in the sales table</li> </ul> <p></p>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#stategic-plan","title":"Stategic Plan","text":"<p>Lets define a plan as to how we'll use the data to solve the problem statement that the client has positioned. This plan will be used to describe to the client how we are planning to complete the remaining work and to build trust with the client as a domain expert</p> <p></p> <p></p>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#baseline-model","title":"Baseline Model","text":"<p>Modeling is an iterative process, let's begin with a general baseline, upon which we will try to improve, by considering a much larger range of preprocessing &amp; model options. As defined in the strategic plan, we will go through most of the steps, however we'll keep things a little more simple at first, and do more testing in subsequent iterations. </p> <ul> <li>Preprocessing: Filling NaN, Adding Date Features, Adding one-hot-encoding of category</li> <li>Modeling: RandomForest with default hyperparameters, we test 10 random splits and average the MAE</li> </ul>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#datasets","title":"Datasets","text":"<p>Samples from the three datasets are defined above can be visualised below:</p> <pre><code>+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n|      transaction_id|          timestamp|          product_id|category|customer_type|unit_price|quantity|total|payment_type|\n+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n|a1c82654-c52c-45b...|2022-03-02 09:51:38|3bc6c1ea-0198-46d...|   fruit|         gold|      3.99|       2| 7.98|    e-wallet|\n|931ad550-09e8-4da...|2022-03-06 10:33:59|ad81b46c-bf38-41c...|   fruit|     standard|      3.99|       1| 3.99|    e-wallet|\n|ae133534-6f61-4cd...|2022-03-04 17:20:21|7c55cbd4-f306-4c0...|   fruit|      premium|      0.19|       2| 0.38|    e-wallet|\n|157cebd9-aaf0-475...|2022-03-02 17:23:58|80da8348-1707-403...|   fruit|         gold|      0.19|       4| 0.76|    e-wallet|\n|a81a6cd3-5e0c-44a...|2022-03-05 14:32:43|7f5e86e6-f06f-45f...|   fruit|        basic|      4.49|       2| 8.98|  debit card|\n+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n</code></pre> <pre><code>+--------------------+-------------------+--------------------+-------------------+\n|                  id|          timestamp|          product_id|estimated_stock_pct|\n+--------------------+-------------------+--------------------+-------------------+\n|4220e505-c247-478...|2022-03-07 12:13:02|f658605e-75f3-4fe...|               0.75|\n|f2612b26-fc82-49e...|2022-03-07 16:39:46|de06083a-f5c0-451...|               0.48|\n|989a287f-67e6-447...|2022-03-01 18:17:43|ce8f3a04-d1a4-43b...|               0.58|\n|af8e5683-d247-46a...|2022-03-02 14:29:09|c21e3ba9-92a3-474...|               0.79|\n|08a32247-3f44-400...|2022-03-02 13:46:18|7f478817-aa5b-44e...|               0.22|\n+--------------------+-------------------+--------------------+-------------------+\n</code></pre> <pre><code>+--------------------+-------------------+-----------+\n|                  id|          timestamp|temperature|\n+--------------------+-------------------+-----------+\n|d1ca1ef8-0eac-42f...|2022-03-07 15:55:20|       2.96|\n|4b8a66c4-0f3a-4f1...|2022-03-01 09:18:22|       1.88|\n|3d47a0c7-1e72-451...|2022-03-04 15:12:26|       1.78|\n|9500357b-ce15-424...|2022-03-02 12:30:42|       2.18|\n|c4b61fec-99c2-4c6...|2022-03-05 09:09:33|       1.38|\n+--------------------+-------------------+-----------+\n</code></pre>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#preprocessing","title":"Preprocessing","text":"","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#converting-to-datetime","title":"Converting to datetime","text":"<p>We first need to convert the str format columns into datetime columns</p> <pre><code>def convert_to_datetime(data: pd.DataFrame = None, column: str = None):\n\n  dummy = data.copy()\n  dummy[column] = pd.to_datetime(dummy[column], format='%Y-%m-%d %H:%M:%S')\n  return dummy\n\nsales_df = convert_to_datetime(sales_df, 'timestamp')\nstock_df = convert_to_datetime(stock_df, 'timestamp')\ntemp_df = convert_to_datetime(temp_df, 'timestamp')\n</code></pre>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#converting-to-datetime_1","title":"Converting to datetime","text":"<p>If we revisit the problem statement: </p> <pre><code>\u201cCan we accurately predict the stock levels of products, based on sales data and sensor data, \non an hourly basis in order to more intelligently procure products from our suppliers.\u201d\n</code></pre> <ul> <li>The client indicates that they want the model to predict on an hourly basis. </li> <li>Looking at the data model, we can see that only column that we can use to merge the 3 datasets together is timestamp</li> <li>So, we must first transform the timestamp column in all 3 datasets to be based on the hour of the day, then we can merge the datasets together</li> </ul> <pre><code>from datetime import datetime\n\n# helper function to convert datetime to desired format\ndef convert_timestamp_to_hourly(data: pd.DataFrame = None, column: str = None):\n  dummy = data.copy()\n  new_ts = dummy[column].tolist() # timestamp list [Timestamp(),Timestamp(),...]\n  new_ts = [i.strftime('%Y-%m-%d %H:00:00') for i in new_ts] # change the value of timestamp\n  new_ts = [datetime.strptime(i, '%Y-%m-%d %H:00:00') for i in new_ts] # change to datetime\n  dummy[column] = new_ts # replace\n  return dummy\n\nsales_df = convert_timestamp_to_hourly(sales_df, 'timestamp')\nstock_df = convert_timestamp_to_hourly(stock_df, 'timestamp')\ntemp_df = convert_timestamp_to_hourly(temp_df, 'timestamp')\n</code></pre>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#aggregations","title":"Aggregations","text":"<p>For the sales data, we want to group the data by timestamp but also by product_id. When we aggregate, since the client is interested in hourly product estimates. We must choose which columns to aggregate by the grouping. For now, let's aggregate quantity and get the total sum.</p> <pre><code>sales_agg = sales_df.groupby(['timestamp', 'product_id'],as_index=False).agg({'quantity': 'sum'})\n\n# +-------------------+--------------------+--------+\n# |          timestamp|          product_id|quantity|\n# +-------------------+--------------------+--------+\n# |2022-03-01 09:00:00|00e120bb-89d6-4df...|       3|\n# |2022-03-01 09:00:00|01f3cdd9-8e9e-4df...|       3|\n# |2022-03-01 09:00:00|03a2557a-aa12-4ad...|       3|\n# |2022-03-01 09:00:00|049b2171-0eeb-4a3...|       7|\n# |2022-03-01 09:00:00|04da844d-8dba-447...|      11|\n# +-------------------+--------------------+--------+\n\nstock_agg = stock_df.groupby(['timestamp', 'product_id'],as_index=False).agg({'estimated_stock_pct': 'mean'})\n\n# +-------------------+--------------------+-------------------+\n# |          timestamp|          product_id|estimated_stock_pct|\n# +-------------------+--------------------+-------------------+\n# |2022-03-01 09:00:00|00e120bb-89d6-4df...|               0.89|\n# |2022-03-01 09:00:00|01f3cdd9-8e9e-4df...|               0.14|\n# |2022-03-01 09:00:00|01ff0803-ae73-423...|               0.67|\n# |2022-03-01 09:00:00|0363eb21-8c74-47e...|               0.82|\n# |2022-03-01 09:00:00|03f0b20e-3b5b-444...|               0.05|\n# +-------------------+--------------------+-------------------+\n\ntemp_agg = temp_df.groupby(['timestamp'],as_index=False).agg({'temperature': 'mean'})\n\n# +-------------------+--------------------+\n# |          timestamp|         temperature|\n# +-------------------+--------------------+\n# |2022-03-01 09:00:00|-0.02884984025559...|\n# |2022-03-01 10:00:00|  1.2843137254901962|\n# |2022-03-01 11:00:00|               -0.56|\n# |2022-03-01 12:00:00| -0.5377210884353741|\n# |2022-03-01 13:00:00|-0.18873417721518987|\n# +-------------------+--------------------+\n</code></pre> <p><code>sales_agg</code> : We now have an aggregated sales data where each row represents a unique combination of hour during which the sales took place from that weeks worth of data and the product_id. We summed the quantity and we took the mean average of the unit_price</p> <p><code>stock_agg</code> : This shows us the average stock percentage of each product at unique hours within the week of sample data</p> <p><code>temp_agg</code> : This gives us the average temperature of the storage facility where the produce is stored in the warehouse by unique hours during the week</p>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#merging-data","title":"Merging Data","text":"<p>Currently we have 3 datasets. In order to include all of this data within a predictive model, we need to merge them together into 1 dataframe. </p> <pre><code># merge sales &amp; stock \nmerged_df = stock_agg.merge(sales_agg, on=['timestamp', 'product_id'], how='left')\nmerged_df = merged_df.merge(temp_agg, on='timestamp', how='left')\nmerged_df.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 10845 entries, 0 to 10844\nData columns (total 5 columns):\n #   Column               Non-Null Count  Dtype         \n---  ------               --------------  -----         \n 0   timestamp            10845 non-null  datetime64[ns]\n 1   product_id           10845 non-null  object        \n 2   estimated_stock_pct  10845 non-null  float64       \n 3   quantity             3067 non-null   float64       \n 4   temperature          10845 non-null  float64       \ndtypes: datetime64[ns](1), float64(3), object(1)\nmemory usage: 508.4+ KB\n</code></pre> <p>We can see from the <code>.info()</code> method that we have some null values. These need to be treated before we can build a predictive model. The column that features some null values is quantity. We can assume that if there is a null value for this column, it represents that there were 0 sales of this product within this hour. So, lets fill this columns null values with 0</p> <pre><code>merged_df['quantity'] = merged_df['quantity'].fillna(0)\n</code></pre>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#adding-additional-features","title":"Adding Additional Features","text":"<p>Next, we can add the category &amp; unit_price to each of the rows by creating unique <code>product_id</code> tables</p> <pre><code>product_categories = sales_df[['product_id', 'category']]\nproduct_categories = product_categories.drop_duplicates()\nproduct_price = sales_df[['product_id', 'unit_price']]\nproduct_price = product_price.drop_duplicates()\n</code></pre> <pre><code>merged_df = merged_df.merge(product_categories, on=\"product_id\", how=\"left\")\nmerged_df = merged_df.merge(product_price, on=\"product_id\", how=\"left\")\nmerged_df.head()\n</code></pre> <pre><code>+-------------------+--------------------+-------------------+--------+--------------------+-------------+------------+------------+----------+\n|          timestamp|          product_id|estimated_stock_pct|quantity|         temperature|     category|unit_price_x|unit_price_y|unit_price|\n+-------------------+--------------------+-------------------+--------+--------------------+-------------+------------+------------+----------+\n|2022-03-01 09:00:00|00e120bb-89d6-4df...|               0.89|     3.0|-0.02884984025559...|      kitchen|       11.19|       11.19|     11.19|\n|2022-03-01 09:00:00|01f3cdd9-8e9e-4df...|               0.14|     3.0|-0.02884984025559...|   vegetables|        1.49|        1.49|      1.49|\n|2022-03-01 09:00:00|01ff0803-ae73-423...|               0.67|     0.0|-0.02884984025559...|baby products|       14.19|       14.19|     14.19|\n|2022-03-01 09:00:00|0363eb21-8c74-47e...|               0.82|     0.0|-0.02884984025559...|    beverages|       20.19|       20.19|     20.19|\n|2022-03-01 09:00:00|03f0b20e-3b5b-444...|               0.05|     0.0|-0.02884984025559...|         pets|        8.19|        8.19|      8.19|\n+-------------------+--------------------+-------------------+--------+--------------------+-------------+------------+------------+----------+\n</code></pre>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#feature-engineering","title":"Feature Engineering","text":"","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#time-based-features","title":"Time based features","text":"<p>Intuitively, time based features often have has significant relevance</p> <pre><code>merged_df['day'] = merged_df['timestamp'].dt.day\nmerged_df['dow'] = merged_df['timestamp'].dt.dayofweek\nmerged_df['hour'] = merged_df['timestamp'].dt.hour\nmerged_df.drop(columns=['timestamp'], inplace=True)\n</code></pre> <pre><code>+--------------------+-------------------+--------+--------------------+-------------+----------+---+---+----+\n|          product_id|estimated_stock_pct|quantity|         temperature|     category|unit_price|day|dow|hour|\n+--------------------+-------------------+--------+--------------------+-------------+----------+---+---+----+\n|00e120bb-89d6-4df...|               0.89|     3.0|-0.02884984025559...|      kitchen|     11.19|  1|  1|   9|\n|01f3cdd9-8e9e-4df...|               0.14|     3.0|-0.02884984025559...|   vegetables|      1.49|  1|  1|   9|\n|01ff0803-ae73-423...|               0.67|     0.0|-0.02884984025559...|baby products|     14.19|  1|  1|   9|\n|0363eb21-8c74-47e...|               0.82|     0.0|-0.02884984025559...|    beverages|     20.19|  1|  1|   9|\n|03f0b20e-3b5b-444...|               0.05|     0.0|-0.02884984025559...|         pets|      8.19|  1|  1|   9|\n+--------------------+-------------------+--------+--------------------+-------------+----------+---+---+----+\n</code></pre>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#one-hot-encoding","title":"One-Hot Encoding","text":"<p>We have a few categorical features, which we need to preprocess if they are to be used in our model, lets start with one hot encoding of <code>category</code></p> <pre><code>merged_df = pd.get_dummies(merged_df, columns=['category'])\nmerged_df.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 10845 entries, 0 to 10844\nData columns (total 31 columns):\n #   Column                          Non-Null Count  Dtype         \n---  ------                          --------------  -----         \n 0   timestamp                       10845 non-null  datetime64[ns]\n 1   product_id                      10845 non-null  object        \n 2   estimated_stock_pct             10845 non-null  float64       \n 3   quantity                        10845 non-null  float64       \n 4   temperature                     10845 non-null  float64       \n 5   unit_price                      10845 non-null  float64       \n 6   day                             10845 non-null  int64         \n 7   dow                             10845 non-null  int64         \n 8   hour                            10845 non-null  int64         \n 9   category_baby products          10845 non-null  uint8         \n 10  category_baked goods            10845 non-null  uint8         \n 11  category_baking                 10845 non-null  uint8         \n 12  category_beverages              10845 non-null  uint8         \n 13  category_canned foods           10845 non-null  uint8         \n 14  category_cheese                 10845 non-null  uint8         \n 15  category_cleaning products      10845 non-null  uint8         \n 16  category_condiments and sauces  10845 non-null  uint8         \n 17  category_dairy                  10845 non-null  uint8         \n 18  category_frozen                 10845 non-null  uint8         \n 19  category_fruit                  10845 non-null  uint8         \n 20  category_kitchen                10845 non-null  uint8         \n 21  category_meat                   10845 non-null  uint8         \n 22  category_medicine               10845 non-null  uint8         \n 23  category_packaged foods         10845 non-null  uint8         \n 24  category_personal care          10845 non-null  uint8         \n 25  category_pets                   10845 non-null  uint8         \n 26  category_refrigerated items     10845 non-null  uint8         \n 27  category_seafood                10845 non-null  uint8         \n 28  category_snacks                 10845 non-null  uint8         \n 29  category_spices and herbs       10845 non-null  uint8         \n 30  category_vegetables             10845 non-null  uint8         \ndtypes: datetime64[ns](1), float64(4), int64(3), object(1), uint8(22)\nmemory usage: 1.1+ MB\n</code></pre> <p>Okay, now that we have assembled our dataset, lets understand what we are actually modeling; our aim is to train a model that will be able to predict the <code>estimated_stock_pct</code></p>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#modeling","title":"Modeling","text":"<p>Time to do some modeling! <code>estimated_stock_pct</code> is our target variable.</p> <pre><code>X = merged_df.drop(columns=['estimated_stock_pct'])\ny = merged_df['estimated_stock_pct']\nprint(X.shape)\nprint(y.shape)\n# (10845, 29)\n# (10845,)\n</code></pre> <p>This shows that we have 29 predictor variables that we will train our machine learning model on and 10845 rows of data. Now let's define how many folds we want to complete during training, and how much of the dataset to assign to training, leaving the rest for test. Let's create a loop to train K models with a 75/25% random split of the data each time between training and test samples.</p> <p>We repeat the training process 10 times and average the MAE across the different test subsets</p> <pre><code>from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.metrics import mean_absolute_error\n\nK = 10\nsplit = 0.75\n\naccuracy = []\n\nfor fold in range(0, K):\n\n  model = RandomForestRegressor()\n\n  X_train, X_test, y_train, y_test = tts(X, y, \n                                         train_size=0.75,\n                                         random_state=42)\n\n  trained_model = model.fit(X_train, y_train)\n  y_pred = trained_model.predict(X_test)\n\n  mae = mean_absolute_error(y_test,y_pred)\n  accuracy.append(mae)\n  print(f\"Fold {fold + 1}: MAE = {mae:.3f}\")\n\nprint(f\"Average MAE: {(sum(accuracy) / len(accuracy)):.2f}\")\n</code></pre> <pre><code>Fold 1: MAE = 0.236\nFold 2: MAE = 0.236\nFold 3: MAE = 0.237\nFold 4: MAE = 0.237\nFold 5: MAE = 0.236\nFold 6: MAE = 0.237\nFold 7: MAE = 0.236\nFold 8: MAE = 0.236\nFold 9: MAE = 0.236\nFold 10: MAE = 0.236\nAverage MAE: 0.24\n</code></pre> <p>We can see that the mean absolute error (MAE) is almost exactly the same each time, averaged to 0.24. This is a good sign, it shows that the performance of the model is consistent across different random samples of the data, which is what we want. In other words, it shows a robust nature.</p> <p>MAE was chosen as a performance metric because it describes how closely the machine learning model was able to predict the exact value of estimated_stock_pct</p> <p>Even though the model is predicting robustly, this value for MAE is not so good, since the average value of the target variable is around 0.51, meaning that the accuracy as a percentage was around 50%. In an ideal world, we would want the MAE to be as low as possible.</p> <pre><code>import plotly.express as px\n\nfeatures = [i.split(\"__\")[0] for i in X.columns]\nfeat_map = dict(zip([i for i in range(0,len(features))],features))\n\nimportances = model.feature_importances_[:10]\nindices = np.argsort(importances)[:10]\nfeature = list(map(feat_map.get,indices))\n\nldf = pd.DataFrame({'feature':feature,\n                   'importance':importances})\nldf = ldf.sort_values(by='importance',ascending=False)\n\npx.bar(ldf,x='feature',y='importance',template='plotly_white',height=300,width=700)\n</code></pre> <p></p> <p>Despite not having an optimised model, we can still visualise the features that impact the model predictions. We can see that baked goods and baby products categories are important, however we should first optimise the model.</p>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#model-optimisation","title":"Model Optimisation","text":"<p>Having a baseline, lets focus some attention to feature transformations as they can impact model accuracy. We'll also pay more attention to variations of machine learning models and utilise our knowledge of hyperaparameters and gridsearch optimisation to find the most optimal hyperparameters</p> <ul> <li>Preprocessing: Filling NaN, Adding Date Features, Logarithmic Feature Transformation, Normalisation of features, label encoding with and without numerical column normalisation</li> <li>Modeling: For modeling we investigate how different models perform using a gridsearch optimisation cycle for different models</li> </ul>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#recap-preprocessing","title":"Recap Preprocessing","text":"<p>Not a bad start start, however the client won't be satisfied with a model that performs this poorly, we need to make at least explore how well this model performs compared to other models for a start. We also need to spend more time on data preparation</p> <pre><code># read datasets\nsales_df = pd.read_csv(f\"sales.csv\")\nsales_df.drop(columns=[\"Unnamed: 0\"], inplace=True, errors='ignore')\nstock_df = pd.read_csv(f\"sensor_stock_levels.csv\")\nstock_df.drop(columns=[\"Unnamed: 0\"], inplace=True, errors='ignore')\ntemp_df = pd.read_csv(f\"sensor_storage_temperature.csv\")\ntemp_df.drop(columns=[\"Unnamed: 0\"], inplace=True, errors='ignore')\n\ndef convert_to_datetime(data: pd.DataFrame = None, column: str = None):\n  dummy = data.copy()\n  dummy[column] = pd.to_datetime(dummy[column], format='%Y-%m-%d %H:%M:%S')\n  return dummy\n\n# convert str to datetime\nsales_df = convert_to_datetime(sales_df, 'timestamp')\nstock_df = convert_to_datetime(stock_df, 'timestamp')\ntemp_df = convert_to_datetime(temp_df, 'timestamp')\n\nfrom datetime import datetime\n\n# helper function to convert datetime to desired format\ndef convert_timestamp_to_hourly(data: pd.DataFrame = None, column: str = None):\n  dummy = data.copy()\n  new_ts = dummy[column].tolist() # timestamp list [Timestamp(),Timestamp(),...]\n  new_ts = [i.strftime('%Y-%m-%d %H:00:00') for i in new_ts] # change the value of timestamp\n  new_ts = [datetime.strptime(i, '%Y-%m-%d %H:00:00') for i in new_ts] # change to datetime\n  dummy[column] = new_ts # replace\n  return dummy\n\n# convert datetime to hour approximation\nsales_df = convert_timestamp_to_hourly(sales_df, 'timestamp')\nstock_df = convert_timestamp_to_hourly(stock_df, 'timestamp')\ntemp_df = convert_timestamp_to_hourly(temp_df, 'timestamp')\n\n# aggregate data based on time &amp; product ID\n# total sales &amp; mean estimated stock percentage\n# add temperature aggregations for timestamp\nsales_agg = sales_df.groupby(['timestamp', 'product_id'],as_index=False).agg({'quantity': 'sum'})\nstock_agg = stock_df.groupby(['timestamp', 'product_id'],as_index=False).agg({'estimated_stock_pct': 'mean'})\ntemp_agg = temp_df.groupby(['timestamp'],as_index=False).agg(temp_max=('temperature',\"mean\"),\n                                                             temp_min=('temperature','min'),\n                                                             temp_median=('temperature','median'),\n                                                             temp_mean=('temperature','mean'))\n\nmerged_df = stock_agg.merge(sales_agg, on=['timestamp', 'product_id'], how='left')\nmerged_df = merged_df.merge(temp_agg, on='timestamp', how='left')\nmerged_df['quantity'] = merged_df['quantity'].fillna(0)\n\n# add features to aggregated dataframe \nproduct_categories = sales_df[['product_id', 'category']]\nproduct_categories = product_categories.drop_duplicates()\nproduct_price = sales_df[['product_id', 'unit_price']]\nproduct_price = product_price.drop_duplicates()\nmerged_df = merged_df.merge(product_categories, on=\"product_id\", how=\"left\")\nmerged_df = merged_df.merge(product_price, on=\"product_id\", how=\"left\")\n\n# add time based features\nmerged_df['day'] = merged_df['timestamp'].dt.day\nmerged_df['dow'] = merged_df['timestamp'].dt.dayofweek\nmerged_df['hour'] = merged_df['timestamp'].dt.hour\nspark.createDataFrame(merged_df.tail()).show()\n</code></pre> <pre><code>+-------------------+--------------------+-------------------+--------+--------------------+--------+-----------+--------------------+--------------+----------+---+---+----+\n|          timestamp|          product_id|estimated_stock_pct|quantity|            temp_max|temp_min|temp_median|           temp_mean|      category|unit_price|day|dow|hour|\n+-------------------+--------------------+-------------------+--------+--------------------+--------+-----------+--------------------+--------------+----------+---+---+----+\n|2022-03-07 19:00:00|ecac012c-1dec-41d...|                0.5|     4.0|-0.16507739938080493|  -30.58|       0.18|-0.16507739938080493|         fruit|      4.99|  7|  0|  19|\n|2022-03-07 19:00:00|ed7f6b14-67c9-42a...|               0.26|     0.0|-0.16507739938080493|  -30.58|       0.18|-0.16507739938080493|          meat|     19.99|  7|  0|  19|\n|2022-03-07 19:00:00|edf4ac93-4e14-4a3...|               0.78|     3.0|-0.16507739938080493|  -30.58|       0.18|-0.16507739938080493|packaged foods|      6.99|  7|  0|  19|\n|2022-03-07 19:00:00|f01b189c-6345-463...|               0.92|     3.0|-0.16507739938080493|  -30.58|       0.18|-0.16507739938080493|          meat|     14.99|  7|  0|  19|\n|2022-03-07 19:00:00|f3bec808-bee0-459...|               0.01|     2.0|-0.16507739938080493|  -30.58|       0.18|-0.16507739938080493|     beverages|      5.19|  7|  0|  19|\n+-------------------+--------------------+-------------------+--------+--------------------+--------+-----------+--------------------+--------------+----------+---+---+----+\n</code></pre>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#feature-transformations","title":"Feature Transformations","text":"<p>One of the important things to notice is whether the distribution itself is having an influence on the model evaluation metrics, so let's look into two forms for preprocessing; column transformation &amp; normalisation &amp; label encoding</p> <p>In the following section, we'll be creating different dataframe variations of <code>merged_df</code>: - <code>merged_df_tr</code> : Which will contain only column transformations &amp; label encoding - <code>merged_df_tr_minmax</code> : The same transformations as <code>merged_df_tr</code> but with the addition of normalisation</p>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#column-transformations","title":"Column Transformations","text":"<p>Let us now check the skewness values for each of the columns</p> <pre><code>from scipy.stats import skew,kurtosis\n\nskew_data = merged_df[c].apply(lambda x: skew(x),axis=0)\n\n# estimated_stock_pct    0.006773\n# quantity               2.384249\n# temp_median            0.068542\n# unit_price             0.507440\n# day                   -0.002082\n# dow                    0.002152\n# hour                  -0.001072\n# dtype: float64\n</code></pre> <p>Our skewness value for quantity is rather high, lets use logarithmic transformation to modify the univariate distribution &amp; compare the boxplot distributions for the numeric columns</p> <pre><code># column transformations\ndef log_column(df,columns):\n    df[columns] = df[columns].apply(lambda x: np.log(x + 1))\n    return df\n\nmerged_df_tr = log_column(merged_df,'quantity')\n\npx.box(merged_df_tr[c],\n       y=merged_df[c].columns,\n       template='plotly_white',\n       width=600,height=400,\n       title='univariate feature distribution')\n</code></pre>  ![](images/before_box.png){width=\"300\"} ![](images/after_box.png){width=\"300\"}  <p>Looks much better now, lets also check the skewness metric once again to confirm we have a numeric improvement</p> <pre><code>skew_data = merged_df[c].apply(lambda x: skew(x),axis=0)\n\n# estimated_stock_pct    0.006773\n# quantity               1.357097\n# temp_median            0.068542\n# unit_price             0.507440\n# day                   -0.002082\n# dow                    0.002152\n# hour                  -0.001072\n# dtype: float64\n</code></pre>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#normalisation-of-columns","title":"Normalisation of columns","text":"<p>Another often important column transformation is normalisation, let's utilise MinMax normalisation for numerical columns</p> <pre><code>from sklearn.preprocessing import MaxAbsScaler,Normalizer\n\n# column normalisation\ndef normalise_columns(df,columns,norm_id):\n    normaliser = Normalizer(norm=norm_id)\n    df[columns] = normaliser.fit_transform(df[columns])\n    return df\n\n# transformation &amp; minmax\nmerged_df_tr_minmax = normalise_columns(merged_df_tr,numerical,'max')\n</code></pre> <pre><code>from sklearn.preprocessing import LabelEncoder,OneHotEncoder\nimport seaborn as sns\n\n# encoding transformations\nle=LabelEncoder()\nmerged_df_tr['category']=le.fit_transform(merged_df_tr['category'])\nmerged_df_tr_minmax['category']=le.fit_transform(merged_df_tr_minmax['category'])\n</code></pre>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#helper-functions","title":"Helper Functions","text":"<p>Its good practice to start grouping things that will allow us to automate the machine learning training loop</p> <ul> <li><code>split_data</code> will be used to remove irrelovant columns (this will be standard for all incoming dataframes) &amp; do a train/test split of the dataset, so we can do both evaluation &amp; test metric accessments (via GridSearch + Test set)</li> <li><code>p</code> &amp; <code>pt</code> functions are used for cross validation &amp; train/test split metric assessment using MAE</li> <li>We'll try a variety of models:<ul> <li>HistGradientBoostingRegressor (More efficient Gradient Boosting)</li> <li>AdaBoostRegressor (Simple Gradient Boosting)</li> <li>RandomForestRegressor (Decision Tree Ensemble)</li> <li>SGDRegressor (Optimisable LR w/ regularisation options in gradient boosting etc)</li> <li>ARDRegression (LR that automatically determines the relevance of each feature in making predictions)</li> </ul> </li> </ul> <p>And we'll conclude which performs best</p> <pre><code>from sklearn.model_selection import train_test_split as tts\n\ndef split_data(df):\n\n    # remove product_id, category\n    df = df[df.columns.difference(['product_id','category'])]\n    y=df.estimated_stock_pct\n    x=df.drop(columns='estimated_stock_pct')\n\n    xtrain , xtest, ytrain, ytest = tts(x,y,shuffle=True,train_size=.75)\n    print(f\"xtrain: {xtrain.shape} and xtest: {xtest.shape}\")\n    print(f\"ytrain: {ytrain.shape} and ytest: {ytest.shape}\")\n    return xtrain,xtest,ytrain,ytest\n</code></pre> <pre><code># randomised gridsearch cross validation (7 kfolds)\ndef p(g,model,name):\n    pt=RandomizedSearchCV(estimator=model,cv=7,\n                          param_distributions=g,\n                          n_jobs=-1,\n                          random_state=344)\n\n    pt.fit(xtrain,ytrain)\n    best = pt.best_estimator_\n    best.fit(xtrain,ytrain)\n    ypred = best.predict(xtest)\n\n    pk.dump(best, open(f'\"{name}.pkl\"', 'wb'))\n    return name,mean_absolute_error(ytest,ypred)\n\n# Standard Train/Test evaluation\ndef pt(model,name):\n    model.fit(xtrain,ytrain)\n    ypred = model.predict(xtest)\n    pk.dump(model, open(f'\"{name}.pkl\"', 'wb'))\n    return name,mean_absolute_error(ytest,ypred)\n</code></pre> <pre><code>def check_models():\n\n    results = {'name':[],'mae':[]}\n\n    # Histogram Gradient Boosting\n    h=HistGradientBoostingRegressor(random_state=34563,\n                                    max_bins=244,\n                                    max_depth=30)\n\n    g={'learning_rate':[0.1,0.01],\n    'max_iter':[100,200,500,600,800,900],\n    'max_leaf_nodes':[20,30],\n    'l2_regularization':[1,0.01],\n    'tol':[1e-7,1e-8]}\n\n    name,mae = p(g,h,'histgrdbstreg')\n    results['name'].append(name); results['mae'].append(mae)\n\n    # Adaboost Gradient Boosting\n    ada=AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=16))\n\n    grid={'n_estimators':[7,8,10],\n        'learning_rate':[1.2,1.6,2],\n        'loss':['linear', 'square', 'exponential']}\n\n    name,mae = p(grid,ada,'adabstreg')\n    results['name'].append(name); results['mae'].append(mae)\n\n    # Bagging Regressor\n    bag=BaggingRegressor(estimator=DecisionTreeRegressor(max_depth=17),\n                        oob_score=False,\n                        n_jobs=-1)\n\n    bgrid={'n_estimators':[10,13,16]}\n    name, mae = p(bgrid,bag,'bagreg')\n    results['name'].append(name); results['mae'].append(mae)\n\n    # Random Forest\n    r=RandomForestRegressor(n_jobs=-1,oob_score=True)\n\n    rgrid={'max_depth':[170,190,200,210],\n        'max_features':['sqrt', 'log2'],\n        'max_samples':[30,100,150,200],\n        'max_leaf_nodes':[20,40,60,100]}\n\n    name,mae = p(rgrid,r,'randfrstreg')\n    results['name'].append(name); results['mae'].append(mae)\n\n    # Stochastic Gradient Regressor\n    sgd=SGDRegressor()\n    sgdg={'penalty':['l2', 'l1', 'elasticnet', None],\n        'max_iter':[100,400,800],\n        'tol':[1e-3,1e-5,1e-8],\n        'alpha':[0.1,.001,0.0001,1],\n        'learning_rate':['constant','optimal','invscaling','adaptive']\n        }\n    name, mae = p(sgdg,sgd,'sgdreg')\n    results['name'].append(name); results['mae'].append(mae)\n\n    # Bayesian ARD regression.\n    a=ARDRegression()\n    gg={'alpha_1':[1e-3,1e-5,1e-7,1e-9],\n    'alpha_2':[1e-3,1e-5,1e-7],\n    'lambda_1':[1e-1,1e-3,1e-5,1e-7],\n    'n_iter':[100,200,300],\n    'lambda_2':[1e-3,1e-5,1e-7,1e-9],\n    'tol':[1e-3,1e-5,1e-7,1e-9]}\n\n    name, mae = p(gg,a,'ardreg')\n    results['name'].append(name); results['mae'].append(mae)\n\n    return results\n</code></pre>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#training-models","title":"Training Models","text":"<p>Having defined all the helper functions above, the actual training code is very minimal, we'll store the MAE metrics of all tested models for both datasets that we're testing in results_tr and merged_df_tr_minmax</p> <p>First of all, our baseline with gridsearchcv:</p> <pre><code>xtrain,xtest,ytrain,ytest = split_data(merged_df)\npt(RandomForestRegressor(),'randomforest')\n# ('randomforest', 0.25014228782291775)\n</code></pre> <p>Our feature engineering modifications:</p> <pre><code># column transformation\nxtrain,xtest,ytrain,ytest = split_data(merged_df_tr)\nresults_tr = check_models()\nnp.mean(results_tr_minmax['mae']) # 0.2253\n\n# column transformation + normalisation\nxtrain,xtest,ytrain,ytest = split_data(merged_df_tr_minmax)\nresults_tr_minmax = check_models()\nnp.mean(results_tr_minmax['mae'])  # 0.2268\n</code></pre>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#contact-with-client","title":"Contact with Client","text":"<p>Most importantly, once the modeling process is complete, we need you to communicate your work and analysis in the form of a single PowerPoint slide, so that we can present the results back to the business. The key here is to use business-friendly language and to explain your results in a way that the business will understand. For example, ensure that when you\u2019re summarizing the performance of the results you don\u2019t use technical metrics, but rather convert it into numbers that they\u2019ll understand. </p> <p>To summarise the results of the modeling phase, it is suggested to utilise two figures:</p> <ul> <li> <p>Feature importance using tuned tree based model</p> <ul> <li>importance will allow us to convey the importance of different factors that affect the results </li> </ul> </li> <li> <p>Relative metric results of all models</p> <ul> <li>The data used for training the models may not be diverse enough or may not contain enough relevant features to effectively differentiate between different models. It could also indicate that the models are not capturing the complexity of the underlying data, or that the models are not being trained with enough variability in the input data. In such cases, it may be necessary to re-evaluate the feature selection process, gather more diverse and relevant data,</li> </ul> </li> </ul> <p></p> <p>In response to the presentation:</p> <p>Gala Groceries saw the results of the machine learning model as promising and believe that with more data and time, it can add real value to the business.</p>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/20/prediction-of-product-stock-levels.html#concluding-remarks","title":"Concluding Remarks","text":"<p>This was a rather interesting project focusing on how a client tries to improve their business by utilising artificial intelligence</p> <ul> <li>The client had a general idea of what they wanted, and in the process of completing the project a specific project goal was created</li> <li>Based on the sales and sensor data that the client has integrated into their business, we were able to create a production ready model which the DevOps team has implemented. </li> <li>The early feedback implied that the model was rather underperforming because it was most likely underfitting the data, we simply haven't gathered enough data for the model to start performing optimally.</li> <li>Nevertheless, we went through the entire data scientist cycle and obtained optimised models that with the help of data preprocessing were able to score MAE in the region of 0.22, which is about 10% less that the initial baseline that we tried.</li> </ul>","tags":["regression","business","internship","cognizant"]},{"location":"blog/2023/11/25/linear-regression.html","title":"Coding Linear Regression","text":"<p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043d\u0435\u043a\u0438\u0439 \u043e\u0431\u0437\u043e\u0440 \u0433\u043b\u0430\u0432\u043d\u044b\u0445 \u043c\u043e\u043c\u0435\u043d\u0442\u043e\u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u0430\u0434\u0443\u0442 \u043d\u0430\u043c \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0432 python \u0438 numpy. \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u043a \u043e\u0442\u043b\u0438\u0447\u0430\u0435\u0442\u0441\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u043e\u0442 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439, \u0438 \u043a\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0442\u044c \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044e \u0434\u043b\u044f \u044d\u0442\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439, \u0447\u0442\u043e\u0431\u044b \u043c\u043e\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043e\u0431\u043e\u0431\u0449\u0430\u044e\u0449\u0438\u044e \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438. \u0412 \u044d\u0442\u043e\u043c \u0440\u0430\u0437\u0434\u0435\u043b\u0435 \u0444\u043e\u043a\u0443\u0441 \u043d\u0430 <code>\u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438</code></p> <ul> <li> GitHub Repository</li> </ul> <p>\u0412 \u044d\u0442\u043e\u043c \u0440\u0430\u0437\u0434\u0435\u043b\u0435 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0432 python \u0438 numpy. \u0415\u0441\u0442\u044c \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044f \u0434\u043b\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u0437\u0430\u0434\u0430\u0447\u0438 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438, \u043c\u044b \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u0445 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 \u0438 \u0441 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u043c \u0441\u043f\u0443\u0441\u043a\u043e\u043c. \u0422\u0430\u043a \u0436\u0435 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u043a \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0442\u044c \u0433\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044e \u0434\u043b\u044f \u044d\u0442\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438</p>","tags":["regression","linear model","code model"]},{"location":"blog/2023/11/25/linear-regression.html#_1","title":"\u0417\u0430\u0434\u0430\u0447\u0430 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432","text":"<p>\u0421\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432 \u0434\u043b\u044f \u0432\u044b\u0431\u043e\u0440\u0430 \u0433\u0438\u043f\u0435\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438</p> <ul> <li>\u0427\u0435\u0440\u0435\u0437 \u043f\u0440\u044f\u043c\u044b\u0435 \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u0435 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438</li> <li>\u0427\u0435\u0440\u0435\u0437 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b \u0441\u043f\u0443\u0441\u043a</li> </ul> <p>\u042f \u0434\u0443\u043c\u0430\u044e \u0432\u0430\u0436\u043d\u043e \u0437\u043d\u0430\u0442\u044c \u043e \u043e\u0431\u043e\u0438\u0445 \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u0432 \u0438 \u043a\u0430\u043a \u043e\u043d\u0438 \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u044e\u0442\u0441\u044f</p>","tags":["regression","linear model","code model"]},{"location":"blog/2023/11/25/linear-regression.html#_2","title":"\u0420\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u041c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u0445 \u041e\u043f\u0435\u0440\u0430\u0446\u0438\u0438","text":"<p>\u041b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u0432\u044b\u0440\u0430\u0436\u0430\u0435\u0442\u0441\u044f \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0439 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u044c\u044e:</p> \\[y=X\\theta+\\epsilon\\] <ul> <li>\\(X\\) \u2014 \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043e\u0431\u044a\u0435\u043a\u0442\u044b-\u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438</li> <li>\\(y\\) \u2014 \u0432\u0435\u043a\u0442\u043e\u0440 \u0446\u0435\u043b\u0435\u0432\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439</li> </ul> <p>C\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445:</p> <ul> <li>\\(X\\), \\(\\theta\\) \u2014 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438, \\(\\epsilon\\) \u2014 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0448\u0443\u043c</li> </ul> <p>\u041d\u0430\u0448\u0430 \u0437\u0430\u0434\u0430\u0447\u0430, \u043c\u0438\u043d\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441\u0440\u0435\u0434\u043d\u0435\u043a\u0432\u0430\u0434\u0440\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u043e\u0448\u0438\u0431\u043a\u0443 \u043c\u0435\u0436\u0434\u0443 \\(y\\) \u0438 \\(X\\theta\\) (\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u044f Least Squares Method)</p> <p>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u0443\u044e \u0444\u043e\u0440\u043c\u0443\u043b\u0435\u0440\u043e\u0432\u043a\u0443, \u0441\u043b\u0435\u0434\u0443\u0435\u0442 \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \\(\\theta\\) \u043a\u0430\u043a:</p> \\[X^Ty=X^TX\\theta \\rightarrow \\theta=(X^TX)^{-1}X^Ty\\] <p>\u041a\u043e\u0442\u043e\u0440\u044b\u0439 \u043d\u0430\u043c \u0434\u0430\u0435\u0442 \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \\(\\theta\\) \u0432 \u044d\u0442\u043e\u0439 \u043f\u043e\u0441\u0442\u043e\u043d\u043e\u0432\u043a\u0435 \u0437\u0430\u0434\u0430\u0447\u0438, \u0440\u0435\u0448\u0430\u044f \\(\\theta\\) \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \\(y=X\\theta+\\epsilon\\)</p> <pre><code>def linreg(X,y):\n    lsm = inv(np.dot(X.T,X))\n    Xt = np.dot(X.T,y)\n    theta = np.dot(lsm,Xt)\n    return theta\n</code></pre>","tags":["regression","linear model","code model"]},{"location":"blog/2023/11/25/linear-regression.html#_3","title":"\u0420\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0421\u043f\u0443\u0441\u043a\u0430","text":"<p>\u0410\u043b\u0442\u0435\u0440\u043d\u0430\u0442\u0438\u0432\u043d\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u0434\u043b\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u044d\u0442\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 (\u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432) \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0441 \u043c\u0435\u0442\u043e\u0434\u043e\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430</p> <p>\u0414\u043b\u044f \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043e\u0448\u0438\u0431\u043a\u0438 \u0441\u0440\u0435\u0434\u043d\u0435\u0433\u043e \u043a\u0432\u0430\u0434\u0440\u0430\u0442\u0438\u0447\u043d\u043e\u0433\u043e, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0432\u044b\u043f\u0443\u043a\u043b\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0435\u0439 \u0432 n-\u043c\u0435\u0440\u043d\u043e\u043c \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0435 \\(\\mathbb{R}^n\\) \u0438 \u0432 \u043e\u0431\u0449\u0435\u043c \u0432\u0438\u0434\u0435 \u0432\u044b\u0433\u043b\u044f\u0434\u0438\u0442 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c:</p> \\[MSE = \\frac{1}{n} * \\sum_{i=1}^{n}{(y_i - a(x_i))^2}\\] <ul> <li>\\(x_i\\) \u2014 \u0432\u0435\u043a\u0442\u043e\u0440-\u043f\u0440\u0438\u0437\u043d\u0430\u043a \\(i\\)-\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438</li> <li>\\(y_i\\) \u2014 \u0438\u0441\u0442\u0438\u043d\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \\(i\\)-\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430,</li> <li>\\(a(x)\\) \u2014 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c, \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0449\u0438\u0439 \u0434\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \\(x\\) \u0446\u0435\u043b\u0435\u0432\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435</li> <li>\\(n\\) \u2014 \u043a\u043e\u043b-\u0432\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0435</li> </ul> <p>\u0412 \u0441\u043b\u0443\u0447\u0430\u0435 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \\(MSE\\) \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043a\u0430\u043a:</p> \\[MSE(X, y, \\theta) = \\frac{1}{2n} * \\sum_{i=1}^{n}{(y_i - \\theta^Tx_i)^2} = \\frac{1}{2n} \\lVert{y - X\\theta}\\rVert_{2}^{2}=\\frac{1}{2n} (y - X\\theta)^T(y - X\\theta)\\] <ul> <li>\\(\\theta\\) \u2014 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u043c\u043e\u0434\u0435\u043b\u0438 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438</li> <li>\\(X\\) \u2014 \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043e\u0431\u044a\u0435\u043a\u0442\u044b-\u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438</li> <li>\\(y\\) - \u0432\u0435\u043a\u0442\u043e\u0440 \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \\(X\\)</li> </ul> <p>\u0412\u043e\u0437\u044c\u043c\u0435\u043c \u043f\u0435\u0440\u0432\u044b\u0439 \u0432\u0430\u0440\u0438\u0430\u043d\u0442 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u0448\u0438\u0431\u043a\u0438 \u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0435\u0435 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u043f\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0443 \\(\\theta\\), \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u0435\u0440\u0435\u0438\u043c\u0435\u043d\u043e\u0432\u0430\u0432 \\(MSE\\) \u0432 \\(L\\):</p> \\[L=\\frac{1}{2n} * \\sum_{i=1}^{n}{(y_i - \\theta^Tx_i)^2}\\] \\[\\nabla L = \\frac{1}{n}\\sum_{i=1}^{n}{(\\theta^Tx_i - y_i) \\cdot x_i} = \\frac{1}{n}X^T(X\\theta - y)\\] <pre><code>def mse_grad(X,theta):\n    n = X.shape[0]\n    grad = (1/n) * X.T.dot(X.dot(theta) - y)\n    return grad\n</code></pre> <p>\u041d\u0430\u043c \u0435\u0449\u0435 \u043d\u0443\u0436\u043d\u044b \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0434\u043b\u044f \u0448\u0430\u0433\u0430 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430 \u0438 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u0446\u0438\u043a\u043b</p> <pre><code># \u0448\u0430\u0433 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430\ndef grad_step(theta,theta_grad,alpha):\n    return theta - alpha*theta_grad\n\n# \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u0446\u0438\u043a\u043b\ndef optimise(X,theta,n_iters):\n\n    # theta0\n    theta = start_theta.copy()\n\n    # \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u0446\u0438\u043a\u043b\n    for i in range(n_iters):\n        theta_grad = mse_grad(X,theta)\n        theta = grad_step(theta,theta_grad,alpha)\n\n    return theta\n</code></pre> <p>\u041c\u044b \u043d\u0430\u0447\u0438\u043d\u0430\u0435\u043c \u0441 \u043d\u0430\u0447\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0443\u0441\u043b\u043e\u0432\u0438\u044f theta \u0438\u0442\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u043e \u043c\u0435\u043d\u044f\u0435\u043c \u0435\u0433\u043e \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0430, \u043c\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0441\u043f\u0443\u0441\u043a \u0434\u043b\u044f \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u044f \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0438 (\u0432\u0435\u0441\u043e\u0432) \u0432 \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0438, \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u043f\u043e\u043b\u043e\u0436\u043d\u043e\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0443 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c, \u0447\u0442\u043e\u0431\u044b \u043c\u0438\u043d\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043e\u0448\u0438\u0431\u043a\u0443 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f</p>","tags":["regression","linear model","code model"]},{"location":"blog/2023/11/25/linear-regression.html#_4","title":"\u0420\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044f","text":"<p>\u0414\u0430\u043b\u0435\u0435 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044e \u0432 \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e, \u044d\u0442\u043e \u0432\u0430\u0436\u043d\u043e\u0435 \u043f\u043e\u043d\u044f\u0442\u0438\u0435 \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u043e\u043d\u043e \u0434\u0430\u0441\u0442 \u043d\u0430\u043c \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043e\u0431\u043e\u0431\u0449\u0430\u044e\u0448\u0438\u044e \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438, \u0434\u0430\u0432\u0430\u044f \u043d\u0430\u043c \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c \u043d\u0430\u0434 \u0432\u043b\u0438\u044f\u043d\u0438\u0435\u043c \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u0440\u0438 \u0435\u0435 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438. \u0421\u0443\u0449\u0435\u0441\u0442\u0432\u0435\u0442 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u0432, L1 \u0438 L2 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044f:</p> <ul> <li>\u0414\u043b\u044f L1 \u043d\u043e\u0440\u043c\u0430\u043b\u0442\u0437\u0430\u0446\u0438\u0438: \u0420\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u0442\u0430\u043a\u0436\u0435 \u0448\u0442\u0440\u0430\u0444\u0443\u0435\u0442 \u0431\u043e\u043b\u044c\u0448\u0438\u0435 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u044b, \u0438 \u043c\u043e\u0436\u0435\u0442 \u0441\u0432\u0435\u0441\u0442\u0438 \u0438\u0445 \u043a \u043d\u0443\u043b\u044e!</li> <li>\u0414\u043b\u044f L2 \u043d\u043e\u0440\u043c\u0430\u043b\u0442\u0437\u0430\u0446\u0438\u0438: \u0420\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u0448\u0442\u0440\u0430\u0444\u0443\u0435\u0442 \u0431\u043e\u043b\u044c\u0448\u0438\u0435 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u044b, \u043d\u043e \u043d\u0435 \u043e\u0431\u043d\u0443\u043b\u044f\u0435\u0442 \u0438\u0445, \u0430 \u043b\u0438\u0448\u044c \u0434\u0435\u043b\u0430\u0435\u0442 \u0438\u0445 \u0431\u043b\u0438\u0436\u0435 \u043a \u043d\u0443\u043b\u044e. \u042d\u0442\u043e \u043f\u043e\u043c\u043e\u0433\u0430\u0435\u0442 \u0443\u043c\u0435\u043d\u044c\u0448\u0438\u0442\u044c \u0432\u0430\u0440\u0438\u0430\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0435\u0451 \u0431\u043e\u043b\u0435\u0435 \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u043e\u0439</li> </ul> <p>\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u043a \u044d\u0442\u043e \u043c\u043e\u0436\u043d\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430</p> <p>\u041f\u043e\u0441\u043b\u0435 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043e\u0448\u0438\u0431\u043a\u0438 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u0431\u0443\u0434\u0435\u0442 \u0432\u044b\u0433\u043b\u044f\u0434\u0435\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c:</p> \\[L=\\frac{1}{2n} * \\sum_{i=1}^{n}{(y_i - \\theta^Tx_i)^2} + \\frac{\\lambda}{2m}\\sum_{j}^{m}{\\theta_j^2}\\] <p>\u0410 \u0435\u0435 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u043f\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0443 \\(\\theta\\):</p> \\[\\nabla L = \\frac{1}{n}\\sum_{i=1}^{n}{(\\theta^Tx_i - y_i) \\cdot x_i} + \\frac{\\lambda}{m}\\sum_{j=1}^{m}{\\theta_j} = \\frac{1}{n}X^T(X\\theta - y) + \\frac{\\lambda}{m}\\sum_{j=1}^{m}{\\theta_j}\\] <p>\u0427\u0442\u043e \u043c\u044b \u0434\u0435\u043b\u0430\u0435\u043c, \u043c\u044b \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0442\u0435\u0440\u043c\u0438\u043d \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u0440\u043e\u043f\u043e\u0440\u0446\u0438\u043e\u043d\u0430\u043b\u0435\u043d \u043a\u0432\u0430\u0434\u0440\u0430\u0442\u0443 \u0432\u0435\u043b\u0438\u0447\u0438\u043d\u044b \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u043e\u0432 \u0432 \u043a\u0432\u0430\u0434\u0440\u0430\u0442\u043d\u0443\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043f\u043e\u0442\u0435\u0440\u044c (\u043d\u0430 \u043f\u043e\u0434\u043e\u0431\u0438\u0435 RidgeRegressor)</p> <pre><code># \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u043a\u0432\u0430\u0434\u0440\u0430\u0442\u043d\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c\ndef mse_grad_reg(X,theta):\n    n = X.shape[0]\n    grad = (1/n) * X.T.dot(X.dot(theta) - y) # \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0439 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c\n    grad_temp = lambd * np.mean(theta)  # \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0442\u0435\u0440\u043c\u0438\u043d\n    return grad + grad_temp\n</code></pre>","tags":["regression","linear model","code model"]},{"location":"blog/2023/11/25/linear-regression.html#linearregression","title":"*\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0441 LinearRegression^","text":"<p>\u0418\u043c\u0435\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \\(\\theta\\), \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0434\u0435\u043b\u0430\u0438\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044e</p> <pre><code>def predict(X,theta):\n     y_pred = X.dot(self.theta)\n</code></pre>","tags":["regression","linear model","code model"]},{"location":"blog/2023/11/25/linear-regression.html#_5","title":"\u041f\u043e\u0434\u0432\u0435\u0434\u0435\u043c \u0418\u0442\u043e\u0433\u0438","text":"<p>\u0412 \u044d\u0442\u043e\u043c \u043f\u043e\u0441\u0442\u0435 \u043c\u044b \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0435\u043b\u0438 \u043a\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e \u0432 \u043f\u0440\u043e\u0441\u0442\u043e\u043c \u0432\u0438\u0434\u0435. \u0422\u0430\u043a \u0436\u0435 \u043e\u0442\u043c\u0435\u0442\u0438\u043b\u0438 \u043a\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044e \u0434\u043b\u044f \u043e\u0431\u043e\u0438\u0445 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432. \u0414\u043b\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0438\u0438 \u043c\u044b \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u043b\u0438 \u0434\u0432\u0430 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430 \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439**. \u041f\u043e\u0434\u0445\u043e\u0434 \u0433\u0440\u0430\u0434\u0438\u0435\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0447\u0430\u0449\u0435 \u0432\u0441\u0435\u0433\u043e \u043d\u0430 \u043f\u0440\u0430\u043a\u0442\u0438\u043a\u0435, \u0442\u0430\u043a \u043a\u0430\u043a \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u043e\u0431\u0440\u0430\u0442\u043d\u043e\u0439 \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0442\u0440\u0443\u0434\u043e\u0437\u0430\u0442\u0440\u0430\u0442\u043d\u044b\u043c \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u043c. </p> <p>\u0414\u043b\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u0437\u0430\u0434\u0430\u0447\u0438 \u0432\u044b\u0431\u043e\u0440\u0430 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0438\u0438 \u0441 \u043f\u043e\u043c\u043e\u0448\u044c\u044e \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430 \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0437\u043d\u0430\u0442\u044c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0441\u0440\u0435\u0434\u043d\u0435\u043a\u0432\u0430\u0434\u0440\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043e\u0448\u0438\u0431\u043a\u0438, \u0442\u0430\u043a \u0436\u0435 \u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0438\u0438\u0438 \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0437\u043d\u0430\u0442\u044c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0431\u0438\u043d\u0430\u0440\u043d\u043e\u0439 \u043a\u0440\u043e\u0441\u0441 \u044d\u043d\u0442\u0440\u043e\u043f\u0438\u0438, \u044d\u0442\u043e \u043f\u043e\u0436\u0430\u043b\u0443\u0439 \u043e\u0441\u0442\u043d\u043e\u0432\u043d\u043e\u0439 \u043c\u043e\u043c\u0435\u043d\u0442. \u0417\u043d\u0430\u044f \u0444\u043e\u0440\u043c\u0443\u043b\u044b \u0434\u043b\u044f \\(\\frac{dL}{d\\theta}\\), \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u043c \u0441\u043f\u0443\u0441\u043a\u043e\u043c \u0434\u043b\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432.</p> <p>\u0422\u0430\u043a \u0436\u0435 \u043c\u044b \u0443\u0432\u0438\u0434\u0438\u043b\u0438 \u043a\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0442\u044c \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044e, \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u044f \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0442\u0435\u0440\u043c\u0438\u043d \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043f\u043e\u0442\u0435\u0440\u044c, \u0434\u0430\u0435\u0442 \u043d\u0430\u043c \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0448\u0442\u0440\u0430\u0444\u043e\u0432\u0430\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0432\u0435\u0441\u043e\u0432, \u0447\u0435\u043c \u0431\u043e\u043b\u044c\u0448\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \\(\\alpha\\), \u0442\u0435\u043c \u0431\u043e\u043b\u044c\u0448\u0435 \u044d\u0444\u0444\u0435\u043a\u0442 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0438 \u0432\u0435\u0441\u0430 \u0431\u0443\u0434\u0443\u0442 \u0443\u043c\u0435\u043d\u044c\u0448\u0430\u0442\u0441\u044f \u0434\u043b\u044f \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0438.</p>","tags":["regression","linear model","code model"]},{"location":"blog/2023/11/25/logistic-regression.html","title":"Coding Logistic Regression","text":"<p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043d\u0435\u043a\u0438\u0439 \u043e\u0431\u0437\u043e\u0440 \u0433\u043b\u0430\u0432\u043d\u044b\u0445 \u043c\u043e\u043c\u0435\u043d\u0442\u043e\u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u0430\u0434\u0443\u0442 \u043d\u0430\u043c \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0432 python \u0438 numpy. \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u043a \u043e\u0442\u043b\u0438\u0447\u0430\u0435\u0442\u0441\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u043e\u0442 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439, \u0438 \u043a\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0442\u044c \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044e \u0434\u043b\u044f \u044d\u0442\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439, \u0447\u0442\u043e\u0431\u044b \u043c\u043e\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043e\u0431\u043e\u0431\u0449\u0430\u044e\u0449\u0438\u044e \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438. \u0412 \u044d\u0442\u043e\u043c \u0440\u0430\u0437\u0434\u0435\u043b\u0435 \u0444\u043e\u043a\u0443\u0441 \u043d\u0430 <code>\u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438</code></p> <ul> <li> GitHub Repository</li> </ul>","tags":["classification","linear model","code model"]},{"location":"blog/2023/11/25/logistic-regression.html#_1","title":"\u0417\u0430\u0434\u0430\u0447\u0430 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432","text":"<p>\u0422\u0435\u043f\u0435\u0440\u044c \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e \u0432 \u044d\u0442\u043e\u043c \u0440\u0430\u0437\u0434\u0435\u043b\u0435, \u0435\u0435 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e \u0432 python \u0438 numpy. \u0417\u0430\u0434\u0430\u0447\u0430 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0441 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u043e\u0442\u043b\u0438\u0447\u0430\u0435\u0442\u0441\u044f \u043e\u0442 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438. \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u043a \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0435\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0433\u0440\u0430\u0434\u0438\u0435\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430, \u0438 \u0442\u0430\u043a \u0436\u0435 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043f\u0440\u0438\u043c\u0435\u0440 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438.</p>","tags":["classification","linear model","code model"]},{"location":"blog/2023/11/25/logistic-regression.html#_2","title":"\u0420\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0421\u043f\u0443\u0441\u043a\u0430","text":"<p>\u0412 \u0441\u043b\u0443\u0447\u0430\u0435 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0435\u0439, \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0441\u043f\u0443\u0441\u043a, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u043d\u0435\u0442 \u044f\u0432\u043d\u043e\u0433\u043e \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u043e\u0433\u043e \u0441\u043f\u043e\u0441\u043e\u0431\u0430 \u043d\u0430\u0439\u0442\u0438 \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0435 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u044b</p> <p>\u041d\u0430\u0447\u043d\u0435\u043c \u0441 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0441\u0438\u0433\u043c\u043e\u0439\u0434\u0430, sigmoid \u0444\u0443\u043d\u043a\u0446\u0438\u044f:</p> \\[h_{\\theta}(x)=\\frac{1}{1+\\exp^{-\\theta x}},\\] <ul> <li>\\(\\theta\\) \u2014 \u0432\u0435\u043a\u0442\u043e\u0440 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438</li> <li>\\(x\\) - \u0432\u0435\u043a\u0442\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u0438\u0437 \u0432\u044b\u0431\u043e\u0440\u043a\u0438</li> </ul> <p>\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043e\u0448\u0438\u0431\u043a\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u0432 \u0441\u043b\u0443\u0447\u0430\u0435 \u0431\u0438\u043d\u0430\u0440\u043d\u043e\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0431\u0438\u043d\u0430\u0440\u043d\u043e\u0439 \u043a\u0440\u043e\u0441\u0441-\u044d\u043d\u0442\u0440\u043e\u043f\u0438\u0435\u0439 (log loss) \u0438 \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c:</p> \\[L=-\\frac{1}{n}(y_i \\log h_{\\theta}(x_i) + (1-y_i) \\log(1-h_{\\theta}(x_i)))\\] <p>\u0414\u043b\u044f \u0433\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430, \u043d\u0430\u043c \u043d\u0443\u0434\u0435\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c</p> <p>\u0421\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0439 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u0448\u0438\u0431\u043a\u0438 \u0440\u0430\u0432\u0435\u043d:</p> \\[\\nabla L=\\frac{1}{n}\\sum_{i=1}^{n}{(h_{\\theta}(x_i)-y_i)x_i}\\] <p>\u0420\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0441\u0438\u0433\u043c\u043e\u0439\u0434\u0430</p> <pre><code># \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0421\u0438\u0433\u043c\u043e\u0439\u0434\u0430\ndef sigmoid(X, theta):\n    return 1.0 / (1.0 + np.exp(-X.dot(theta)))\n</code></pre> <p>\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043e\u0448\u0438\u0431\u043a\u0438 logloss, \u0440\u0430\u0437\u043d\u0438\u0446\u0430 \u0441 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u0442\u043e\u043b\u044c\u043a\u043e \u0432 sigmoid(X,theta), \u0432 \u0440\u0435\u0433\u0440\u0435\u0441\u0438\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c X.theta</p> <pre><code># \u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0431\u0438\u043d\u0430\u0440\u043d\u043e\u0439 \u043a\u0440\u043e\u0441\u0441 \u044d\u043d\u0442\u0440\u043e\u043f\u0438\u0438\n# sigmoid(X,theta) - \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438\n# X.theta - \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438\ndef bin_crossentropy_grad(X, y, theta):\n    n = X.shape[0]\n    grad = (1.0/n) * X.T.dot(sigmoid(X,theta) - y )\n    return grad\n</code></pre> <p>\u041a\u0430\u043a \u0438 \u0440\u0430\u043d\u0435\u0435, \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u0430 \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u043e\u0434\u0438\u043d \u0448\u0430\u0433 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430, \u0430 \u0442\u0430\u043a \u0436\u0435 \u0432\u0435\u0441 \u0446\u0438\u043a\u043b \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \\(\\theta\\)</p> <pre><code># \u0428\u0430\u0433 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430\ndef gradient_step(theta, theta_grad, alpha):\n    return theta - alpha * theta_grad\n\n# \u0413\u043b\u0430\u0432\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f \ndef optimize(X, y, grad_func, start_theta, alpha, n_iters):\n\n    theta = start_theta.copy()\n\n    for i in range(n_iters):\n        theta_grad = grad_func(X, y, theta)\n        theta = gradient_step(theta, theta_grad, alpha)\n\n    return theta\n</code></pre>","tags":["classification","linear model","code model"]},{"location":"blog/2023/11/25/logistic-regression.html#_3","title":"\u0420\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044f","text":"<p>\u041f\u0440\u043e\u0446\u0435\u0441\u0441 \u0438\u0434\u0435\u043d\u0442\u0438\u0447\u043d\u044b\u0439, \u043a\u0430\u043a \u0438 \u0432 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438, \u043c\u044b \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0442\u0435\u0440\u043c\u0438\u043d \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043f\u043e\u0442\u0435\u0440\u044c, \u0432 \u044d\u0442\u043e\u0442 \u0440\u0430\u0437 \u044d\u0442\u043e \u0431\u0438\u043d\u0430\u0440\u043d\u0430\u044f \u043a\u0440\u043e\u0441\u0441 \u044d\u043d\u0442\u0440\u043e\u043f\u0438\u044f (logloss)</p> <p>\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043e\u0448\u0438\u0431\u043a\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u0432 \u0441\u043b\u0443\u0447\u0430\u0435 \u0431\u0438\u043d\u0430\u0440\u043d\u043e\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0441 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0442\u043e\u0440\u043e\u043c \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c:</p> \\[L=-\\frac{1}{n}(y_i \\log h_{\\theta}(x_i) + (1-y_i) \\log(1-h_{\\theta}(x_i)))+\\frac{\\lambda}{2m}\\sum_{j}^{m}{\\theta_j^2}\\] <ul> <li>\\(x_i\\) \u2014 \u0432\u0435\u043a\u0442\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432</li> <li>\\(i\\)-\u0433\u043e \u043f\u0440\u0438\u043c\u0435\u0440\u0430 \u0438\u0437 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438</li> <li>\\(y_i\\) \u2014 \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043a\u043b\u0430\u0441\u0441 \u0434\u043b\u044f \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0433\u043e \u043f\u0440\u0438\u043c\u0435\u0440\u0430 (0 \u0438\u043b\u0438 1),- \\(n\\) \u2014 \u0447\u0438\u0441\u043b\u043e \u043f\u0440\u0438\u043c\u0435\u0440\u043e\u0432 \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435</li> <li>\\(m\\) \u2014 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432</li> <li>\\(\\lambda\\) \u2014 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438</li> <li>\\(h_{\\theta}(x)\\) \u2014 sigmoid \u0444\u0443\u043d\u043a\u0446\u0438\u044f, \u0440\u0430\u0432\u043d\u0430\u044f:</li> </ul> \\[h_{\\theta}(x)=\\frac{1}{1+\\exp^{-\\theta x}}\\] <ul> <li>\\(\\theta\\) \u2014 \u0432\u0435\u043a\u0442\u043e\u0440 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438</li> <li>\\(x\\) - \u0432\u0435\u043a\u0442\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u0438\u0437 \u0432\u044b\u0431\u043e\u0440\u043a\u0438</li> </ul> <p>\u0421\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0439 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u0448\u0438\u0431\u043a\u0438 \u0440\u0430\u0432\u0435\u043d:</p> \\[\\nabla L=\\frac{1}{n}\\sum_{i=1}^{n}{(h_{\\theta}(x_i)-y_i)x_i}+\\frac{\\lambda}{m}\\sum_{j}^{m}{\\theta_j}\\] <p>\u0418 \u0435\u0435 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f:</p> <pre><code># \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0431\u0438\u043d\u0430\u0440\u043d\u043e\u0439 \u043a\u0440\u043e\u0441\u0441 \u044d\u043d\u0442\u0440\u043e\u043f\u0438\u0438\ndef logloss_reg(X,y,theta):\n   n = X.shape[0]\n   grad = (1.0/n) * X.T.dot(sigmoid(X,theta) - y)\n   grad_term = lambd * np.mean(theta)\n   return grad + grad_term\n</code></pre>","tags":["classification","linear model","code model"]},{"location":"blog/2023/11/25/logistic-regression.html#logisticregression","title":"\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0441 LogisticRegression","text":"<p>\u041d\u0430\u0439\u0434\u044f \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0435 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \\(\\theta\\) , \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0441 \u043f\u043e\u043c\u043e\u0448\u044c\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0441\u0438\u0433\u043c\u043e\u0439\u0434\u044b. \u041e\u043d\u0430 \u0434\u0430\u0435\u0442 \u043d\u0430\u043c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043a \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u043c\u0443 \u043a\u043b\u0430\u0441\u0441\u0443 \u0432 \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0435 [0,1]. \u041c\u044b \u043c\u043e\u0436\u0435\u043c \u043a\u0430\u043a \u0438 \u0432 sklearn \u0430\u043d\u043e\u043b\u0430\u0433\u0438\u0447\u043d\u043e \u0438\u043c\u0435\u043d\u043e\u0432\u0430\u0442\u044c \u0438\u0445 predict &amp; predict_proba</p> <pre><code># \u0412\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043a\u043b\u0430\u0441\u0441\u0430 (0/1)\ndef predict_proba(X,theta):\n   return self.sigmoid(X,theta)\n\n# \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0441 \u043f\u043e\u0440\u043e\u0433\u043e\u043c 0.5\ndef predict(X):\n   y_pred = self.predict_proba(X) &gt; 0.5\n</code></pre>","tags":["classification","linear model","code model"]},{"location":"blog/2023/11/25/logistic-regression.html#_4","title":"\u041f\u043e\u0434\u0432\u0435\u0434\u0435\u043c \u0418\u0442\u043e\u0433\u0438","text":"<p>\u0412 \u044d\u0442\u043e\u043c \u043f\u043e\u0441\u0442\u0435 \u043c\u044b \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0435\u043b\u0438 \u043a\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0438 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e \u0432 \u043f\u0440\u043e\u0441\u0442\u043e\u043c \u0432\u0438\u0434\u0435. \u0422\u0430\u043a \u0436\u0435 \u043e\u0442\u043c\u0435\u0442\u0438\u043b\u0438 \u043a\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044e \u0434\u043b\u044f \u043e\u0431\u043e\u0438\u0445 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432. \u0414\u043b\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0438\u0438 \u043c\u044b \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u043b\u0438 \u0434\u0432\u0430 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430 \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439. \u041f\u043e\u0434\u0445\u043e\u0434 \u0433\u0440\u0430\u0434\u0438\u0435\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0447\u0430\u0449\u0435 \u0432\u0441\u0435\u0433\u043e \u043d\u0430 \u043f\u0440\u0430\u043a\u0442\u0438\u043a\u0435, \u0442\u0430\u043a \u043a\u0430\u043a \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u043e\u0431\u0440\u0430\u0442\u043d\u043e\u0439 \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0442\u0440\u0443\u0434\u043e\u0437\u0430\u0442\u0440\u0430\u0442\u043d\u044b\u043c \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u043c. </p> <p>\u0414\u043b\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u0437\u0430\u0434\u0430\u0447\u0438 \u0432\u044b\u0431\u043e\u0440\u0430 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0438\u0438 \u0441 \u043f\u043e\u043c\u043e\u0448\u044c\u044e \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430 \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0437\u043d\u0430\u0442\u044c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0441\u0440\u0435\u0434\u043d\u0435\u043a\u0432\u0430\u0434\u0440\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043e\u0448\u0438\u0431\u043a\u0438, \u0442\u0430\u043a \u0436\u0435 \u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0438\u0438\u0438 \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0437\u043d\u0430\u0442\u044c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0431\u0438\u043d\u0430\u0440\u043d\u043e\u0439 \u043a\u0440\u043e\u0441\u0441 \u044d\u043d\u0442\u0440\u043e\u043f\u0438\u0438, \u044d\u0442\u043e \u043f\u043e\u0436\u0430\u043b\u0443\u0439 \u043e\u0441\u0442\u043d\u043e\u0432\u043d\u043e\u0439 \u043c\u043e\u043c\u0435\u043d\u0442. \u0417\u043d\u0430\u044f \u0444\u043e\u0440\u043c\u0443\u043b\u044b \u0434\u043b\u044f \\(\\frac{dL}{d\\theta}\\), \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u043c \u0441\u043f\u0443\u0441\u043a\u043e\u043c \u0434\u043b\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432.</p> <p>\u0422\u0430\u043a \u0436\u0435 \u043c\u044b \u0443\u0432\u0438\u0434\u0438\u043b\u0438 \u043a\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0442\u044c \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044e, \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u044f \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0442\u0435\u0440\u043c\u0438\u043d \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043f\u043e\u0442\u0435\u0440\u044c, \u0434\u0430\u0435\u0442 \u043d\u0430\u043c \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0448\u0442\u0440\u0430\u0444\u043e\u0432\u0430\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0432\u0435\u0441\u043e\u0432, \u0447\u0435\u043c \u0431\u043e\u043b\u044c\u0448\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \\(\\alpha\\), \u0442\u0435\u043c \u0431\u043e\u043b\u044c\u0448\u0435 \u044d\u0444\u0444\u0435\u043a\u0442 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0438 \u0432\u0435\u0441\u0430 \u0431\u0443\u0434\u0443\u0442 \u0443\u043c\u0435\u043d\u044c\u0448\u0430\u0442\u0441\u044f \u0434\u043b\u044f \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0438.</p>","tags":["classification","linear model","code model"]},{"location":"blog/2023/12/10/pyspark-daily-summary-1.html","title":"PySpark Daily Summary I","text":"<p>Something I decided would be fun to do on a daily basis; write pyspark code everyday and post about it, this is mainly because I don't use it as often as I would like, so this is my motivation. If you too want to join in, just fork the notebook (on Kaggle) and practice various pyspark codings everyday! Visit my telegram channel if you have any questions or just post them here!</p> <p>Here I will be posting summaries that cover roughtly 10 days worth of posts that I make on Kaggle, so that would equate to three posts a month</p>","tags":["pyspark"]},{"location":"blog/2023/12/10/pyspark-daily-summary-1.html#setting-data-types-via-custom-schema","title":"Setting data types via custom schema","text":"<p>Today's post is about schemes. PySpark tends to mimic a lot of SQL database aspects. Its standard practice to define a table scheme for our dataframe when either creating a dataframe or reading files</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n\n# Create a Spark session\nspark = SparkSession.builder.appName(\"schema\").getOrCreate()\n\n# Define the schema using StructType and StructField\nschema = StructType([\n    StructField(\"name\", StringType(), True),\n    StructField(\"age\", IntegerType(), True),\n    StructField(\"salary\", FloatType(), True)\n])\n\n# Create a DataFrame with the defined schema\ndata = [(\"Alice\", 28, 100000.0),\n        (\"Bob\", 35, 120000.0)]\ndf = spark.createDataFrame(data, schema)\n\n# Show the DataFrame with the defined schema\ndf.show()\n\n# +-----+---+--------+\n# | name|age|  salary|\n# +-----+---+--------+\n# |Alice| 28|100000.0|\n# |  Bob| 35|120000.0|\n# +-----+---+--------+\n</code></pre>","tags":["pyspark"]},{"location":"blog/2023/12/10/pyspark-daily-summary-1.html#creating-table-view-for-spark-dataframe","title":"Creating Table View for Spark DataFrame","text":"<p>Using SQL requests via spark.sql, you can work with the data the same way you would when working with databases, this is convenient for people who are more used to SQL based notation when working with data</p> <pre><code># Create a Spark session\nspark = SparkSession.builder.appName(\"schema\").getOrCreate()\n\n# Create a Pyspark DataFrame from a list of tuples\ndata = [\n    (\"2020-01-01\", 10),\n    (\"2020-01-02\", 20),\n    (\"2020-01-03\", 30),\n    (\"2020-01-04\", 40),\n    (\"2020-01-05\", 50)\n]\ndf = spark.createDataFrame(data, [\"date\", \"value\"])\n\n# Register the DataFrame as a temporary table\ndf.createOrReplaceTempView(\"date_table\")\n\n# lets preview our table\nspark.sql('select * from date_table').show()\n\n# +----------+-----+\n# |      date|value|\n# +----------+-----+\n# |2020-01-01|   10|\n# |2020-01-02|   20|\n# |2020-01-03|   30|\n# |2020-01-04|   40|\n# |2020-01-05|   50|\n# +----------+-----+\n</code></pre> <pre><code># Perform the rolling mean calculation using SQL notation\nrequest = \"\"\"\nSELECT date,\n       value,\n       AVG(value) OVER (ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) AS rolling_mean\nFROM date_table\nORDER BY date\n\"\"\"\n\nresult = spark.sql(request)\nresult.show()\n\n# +----------+-----+------------+\n# |      date|value|rolling_mean|\n# +----------+-----+------------+\n# |2020-01-01|   10|        15.0|\n# |2020-01-02|   20|        20.0|\n# |2020-01-03|   30|        30.0|\n# |2020-01-04|   40|        40.0|\n# |2020-01-05|   50|        45.0|\n# +----------+-----+------------+\n</code></pre> <ul> <li>If you wanted to replicate the same request using pyspark functions, you'd need to know what functionality to import</li> <li>For this problem we need to import from sql.functions and sql.window, so SQL notation is definitely convenient</li> <li>This is a big positive for pyspark, because you can do data analysis using big data without needing to know the library component imports (such as pyspark.sql.window import Window)</li> </ul> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import avg\nfrom pyspark.sql.window import Window\n\nspark = SparkSession.builder.getOrCreate()\n\n# Assuming you have a DataFrame called 'df' with columns 'value' and 'timestamp'\nwindowSpec = Window.orderBy(\"date\").rowsBetween(-1, 1)\nrollingMean = avg(df[\"value\"]).over(windowSpec)\n\nresult = df.select(df[\"date\"], df[\"value\"], rollingMean.alias(\"rolling_mean\"))\nresult.show()\n\n# +----------+-----+------------+\n# |      date|value|rolling_mean|\n# +----------+-----+------------+\n# |2020-01-01|   10|        15.0|\n# |2020-01-02|   20|        20.0|\n# |2020-01-03|   30|        30.0|\n# |2020-01-04|   40|        40.0|\n# |2020-01-05|   50|        45.0|\n# +----------+-----+------------+\n</code></pre>","tags":["pyspark"]},{"location":"blog/2023/12/10/pyspark-daily-summary-1.html#reading-simple-csv-files","title":"Reading Simple CSV files","text":"<p>As mentioned before, PySpark assigns StringType to each column when reading csv file. Having played with read.csv and pandas' read_csv, I definitely would say that pandas offers much more options when importing. I tend to actually prefer to read the data with default settings and make adjustments after import. </p> <pre><code>spark = SparkSession.builder.getOrCreate()\n\nspark.read.csv('/kaggle/input/cognizant-artificial-intelligence/sample_sales_data (1).csv') # defaults to string types\n#   DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string]\n</code></pre> <p>Some useful things to note when reading csv file:</p> <ul> <li>If your data contains a header; set header=True</li> <li>If you want to automatically determine column types and set them; set inferSchema=True</li> <li>To add an option to <code>.csv</code>, add it before <code>.csv</code> by using <code>.option</code>, we can set different settings for reading csv files here</li> <li>Set the delimiter, eg. via <code>.option('delimiter',';')</code> if you data is separated by ';'</li> </ul> <pre><code># a header is present in the data\nspark.read.csv('/kaggle/input/cognizant-artificial-intelligence/sample_sales_data (1).csv',header=True).show(1)\n\n# +---+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n# |_c0|      transaction_id|          timestamp|          product_id|category|customer_type|unit_price|quantity|total|payment_type|\n# +---+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n# |  0|a1c82654-c52c-45b...|2022-03-02 09:51:38|3bc6c1ea-0198-46d...|   fruit|         gold|      3.99|       2| 7.98|    e-wallet|\n# +---+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n\n# automatically assign data types to columns\nspark.read.csv('/kaggle/input/cognizant-artificial-intelligence/sample_sales_data (1).csv',header=True,inferSchema=True)\n\n# slightly different format, specify the delimiter that splits columns\nspark.read.option('delimiter',',')\\\n          .option('header',True)\\\n          .option('inferSchema',True)\\\n          .csv('/kaggle/input/cognizant-artificial-intelligence/sample_sales_data (1).csv').show(5)\n\n# +---+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n# |_c0|      transaction_id|          timestamp|          product_id|category|customer_type|unit_price|quantity|total|payment_type|\n# +---+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n# |  0|a1c82654-c52c-45b...|2022-03-02 09:51:38|3bc6c1ea-0198-46d...|   fruit|         gold|      3.99|       2| 7.98|    e-wallet|\n# |  1|931ad550-09e8-4da...|2022-03-06 10:33:59|ad81b46c-bf38-41c...|   fruit|     standard|      3.99|       1| 3.99|    e-wallet|\n# |  2|ae133534-6f61-4cd...|2022-03-04 17:20:21|7c55cbd4-f306-4c0...|   fruit|      premium|      0.19|       2| 0.38|    e-wallet|\n# |  3|157cebd9-aaf0-475...|2022-03-02 17:23:58|80da8348-1707-403...|   fruit|         gold|      0.19|       4| 0.76|    e-wallet|\n# |  4|a81a6cd3-5e0c-44a...|2022-03-05 14:32:43|7f5e86e6-f06f-45f...|   fruit|        basic|      4.49|       2| 8.98|  debit card|\n# +---+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n# only showing top 5 rows\n\n# limit the number of loaded rows of data\nspark.read.option('delimiter',',')\\\n          .option('header',True)\\\n          .option('inferSchema',True)\\\n          .csv('/kaggle/input/cognizant-artificial-intelligence/sample_sales_data (1).csv')\\\n          .limit(10).show()\n\n# +---+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n# |_c0|      transaction_id|          timestamp|          product_id|category|customer_type|unit_price|quantity|total|payment_type|\n# +---+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n# |  0|a1c82654-c52c-45b...|2022-03-02 09:51:38|3bc6c1ea-0198-46d...|   fruit|         gold|      3.99|       2| 7.98|    e-wallet|\n# |  1|931ad550-09e8-4da...|2022-03-06 10:33:59|ad81b46c-bf38-41c...|   fruit|     standard|      3.99|       1| 3.99|    e-wallet|\n# |  2|ae133534-6f61-4cd...|2022-03-04 17:20:21|7c55cbd4-f306-4c0...|   fruit|      \n# ...\n# +---+--------------------+-------------------+--------------------+--------+-------------+----------+--------+-----+------------+\n</code></pre> <pre><code>from pyspark.sql.types import DateType, StringType, FloatType, IntegerType, TimestampType\n\n# Define the schema using StructType and StructField\nschema = StructType([\n    StructField(\"_c0\", IntegerType(), True),            # as the data has a , at the start of each row\n    StructField(\"transaction_id\", StringType(), True),\n    StructField(\"timestamp\", DateType(), True),         # read the column as a DateType, not TimestampType\n    StructField(\"product_id\", StringType(), True),\n    StructField(\"category\", StringType(), True),\n    StructField(\"customer_type\", StringType(), True),\n    StructField(\"unit_price\", FloatType(), True),\n    StructField(\"quantity\", IntegerType(), True),\n    StructField(\"total\", FloatType(), True),\n    StructField(\"payment_type\", StringType(), True)\n])\n\ndf = spark.read.csv('/kaggle/input/cognizant-artificial-intelligence/sample_sales_data (1).csv',header=True,inferSchema=False,schema=schema)\ndf.show()\n\n# +---+--------------------+----------+--------------------+--------+-------------+----------+--------+-----+------------+\n# |_c0|      transaction_id| timestamp|          product_id|category|customer_type|unit_price|quantity|total|payment_type|\n# +---+--------------------+----------+--------------------+--------+-------------+----------+--------+-----+------------+\n# |  0|a1c82654-c52c-45b...|2022-03-02|3bc6c1ea-0198-46d...|   fruit|         gold|      3.99|       2| 7.98|    e-wallet|\n# |  1|931ad550-09e8-4da...|2022-03-06|ad81b46c-bf38-41c...|   fruit|     standard|      3.99|       1| 3.99|    e-wallet|\n# |  2|ae133534-6f61-4cd...|2022-03-04|7c55cbd4-f306-4c0...|   fruit|      premium|      0.19|       2| 0.38|    e-wallet|\n# ...\n# +---+--------------------+----------+--------------------+--------+-------------+----------+--------+-----+------------+\n</code></pre>","tags":["pyspark"]},{"location":"blog/2023/12/10/pyspark-daily-summary-1.html#knowing-your-pyspark-types","title":"Knowing your PySpark Types","text":"<p>To set StructFields and define a type, we should know which types are available to us in pyspark</p> <ol> <li>StringType: Represents string values.</li> <li>IntegerType: Represents integer values.</li> <li>LongType: Represents long integer values.</li> <li>FloatType: Represents float values.</li> <li>DoubleType: Represents double values.</li> <li>BooleanType: Represents boolean values.</li> <li>DateType: Represents date values.</li> <li>TimestampType: Represents timestamp values.</li> <li>ArrayType: Represents arrays of elements with a specific data type.</li> <li>MapType: Represents key-value pairs with specific data types for keys and values.</li> <li>StructType: Represents a structure or record with multiple fields.</li> </ol> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import MapType, StringType\n\n# Create a SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n# Create a DataFrame with a column of MapType\ndata = [(1, {\"name\": \"John\", \"age\": \"30\"}), \n        (2, {\"name\": \"Jane\", \"age\": \"25\"})]\n\ndf = spark.createDataFrame(data, [\"id\", \"info\"])\n# DataFrame[id: bigint, info: map&lt;string,string&gt;]\n</code></pre> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.types import ArrayType, StringType\n\n# Create a SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n# Sample data\ndata = [(\"Alice\", [\"apple\", \"banana\", \"orange\"]),\n        (\"Bob\", [\"grape\", \"kiwi\"]),\n        (\"Charlie\", [\"watermelon\"])]\n\n# Define the schema with ArrayType\nspark.createDataFrame(data, [\"name\", \"fruits\"])\n# DataFrame[name: string, fruits: array&lt;string&gt;]\n</code></pre>","tags":["pyspark"]},{"location":"blog/2023/12/10/pyspark-daily-summary-1.html#timestamp-zone-consideration","title":"Timestamp Zone Consideration","text":"<p>If your column is of type datetime (TimestampType), here's how you can use it with different timezones, so you can make the necessary adjustments if needed</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import from_utc_timestamp, to_utc_timestamp\n\n# Create a SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n# Create a DataFrame with a timestamp column (UTC time)\ndata = [(\"2022-01-01 12:00:00\",)]\ndf = spark.createDataFrame(data, [\"timestamp\"])\n\n# if timestamp is UTC\ndf_tz = df.withColumn(\"timestamp_with_ny\", from_utc_timestamp(df.timestamp, \"America/New_York\"))\ndf_tz = df_tz.withColumn(\"timestamp_with_moscow\", from_utc_timestamp(df.timestamp, \"Europe/Moscow\"))\ndf_tz.show()\n\n# if timestamp is local\ndf_utc = df_tz.withColumn(\"timestamp_utc_ny\", to_utc_timestamp(df_tz.timestamp_with_ny, \"America/New_York\"))\ndf_utc = df_tz.withColumn(\"timestamp_utc_moscow\", to_utc_timestamp(df_tz.timestamp_with_moscow, \"Europe/Moscow\"))\ndf_utc.show()\n\n# +-------------------+-------------------+---------------------+\n# |          timestamp|  timestamp_with_ny|timestamp_with_moscow|\n# +-------------------+-------------------+---------------------+\n# |2022-01-01 12:00:00|2022-01-01 07:00:00|  2022-01-01 15:00:00|\n# +-------------------+-------------------+---------------------+\n\n# +-------------------+-------------------+---------------------+--------------------+\n# |          timestamp|  timestamp_with_ny|timestamp_with_moscow|timestamp_utc_moscow|\n# +-------------------+-------------------+---------------------+--------------------+\n# |2022-01-01 12:00:00|2022-01-01 07:00:00|  2022-01-01 15:00:00| 2022-01-01 12:00:00|\n# +-------------------+-------------------+---------------------+--------------------+\n</code></pre>","tags":["pyspark"]},{"location":"blog/2023/12/10/pyspark-daily-summary-1.html#training-a-linear-model","title":"Training a linear model","text":"<p>The process of creating models differs a little bit to how one would go about it in sklearn. Once we have a dataframe that contains all our features &amp; target variable <code>df</code>, we need to assemble them into a vectorised format using VectorAssembler, to do so we need to define the inputCols and outputCol (which will assemble all our input feature data). Loading the relevant model from the library pyspark.ml, we then need to define inputCols (which is the ouput column of the VectorAssembler) and outputCol arguments</p> <p>Initialised the model (LinearRegression), we call the method fit and define it as a variable (which is different to sklearn). To use the model for prediction, we need to transform the new data into the same vectorised format using the assembler to create new_data, and use model.transform(new_data) to make the prediction</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.feature import VectorAssembler\n\n# Create a SparkSession\nspark = SparkSession.builder.appName(\"LinearRegressionExample\").getOrCreate()\n\n# Sample dataset (two features &amp; target variable)\ndata = [(1, 2, 3), \n        (2, 4, 6), \n        (3, 6, 9), \n        (4, 8,12), \n        (5,10,15)]\ndf = spark.createDataFrame(data, [\"feature1\", \"feature2\", \"target\"])\n\n# Prepare the data for modeling\nassembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"features\")\ndf = assembler.transform(df)\ndf\n\n# Create and fit the linear regression model\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"target\")\nmodel = lr.fit(df)\n\n# Make predictions on new data\nnew_data = spark.createDataFrame([(6, 12)], [\"feature1\", \"feature2\"])\nnew_data = assembler.transform(new_data)\npredictions = model.transform(new_data)\npredictions.show()\n\n# +--------+--------+----------+----------+\n# |feature1|feature2|  features|prediction|\n# +--------+--------+----------+----------+\n# |       6|      12|[6.0,12.0]|      18.0|\n# +--------+--------+----------+----------+\n</code></pre>","tags":["pyspark"]},{"location":"blog/2023/12/10/pyspark-daily-summary-1.html#filter-rows-that-contain-item-in-array-column","title":"Filter rows that contain item in array column","text":"<p>PySpark contains a special function array_contains which allows you to check if a specified value exists in an array column. It returns a boolean value indicating whether the array contains the specified value</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import array_contains\n\n# Create a SparkSession\nspark = SparkSession.builder.appName(\"filter_rows\").getOrCreate()\n\n# Example of ArrayType\ndata = [(\"Alice\", [\"apple\", \"banana\", \"orange\"]),\n        (\"Bob\", [\"grape\", \"kiwi\"]),\n        (\"Charlie\", [\"watermelon\"])]\n\n# Define the schema with ArrayType\ndf = spark.createDataFrame(data, [\"name\", \"fruits\"])\n# DataFrame[name: string, fruits: array&lt;string&gt;]\n\n# Filter rows where the array column contains a specific element\nfiltered_df = df.where(array_contains(df.fruits, \"orange\"))\n\n# Show the filtered DataFrame\nfiltered_df.show()\n\n# +-----+--------------------+\n# | name|              fruits|\n# +-----+--------------------+\n# |Alice|[apple, banana, o...|\n# +-----+--------------------+\n</code></pre> <p>We can add a new column and confirm what array_contains does</p> <pre><code>test = df.withColumn('contains',array_contains(df.fruits, \"orange\")).show()\n\n# +-------+--------------------+--------+\n# |   name|              fruits|contains|\n# +-------+--------------------+--------+\n# |  Alice|[apple, banana, o...|    true|\n# |    Bob|       [grape, kiwi]|   false|\n# |Charlie|        [watermelon]|   false|\n# +-------+--------------------+--------+\n</code></pre>","tags":["pyspark"]},{"location":"blog/2023/12/10/pyspark-daily-summary-1.html#sql-like-functions-select","title":"SQL like functions (SELECT)","text":"<p>Select columns from PySpark DataFrame, similar to SELECT in SQL</p> <pre><code>from pyspark.sql import SparkSession\n\n# Create a SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n# Create a DataFrame\ndata = [(\"Alice\", 25, \"New York\"),\n        (\"Bob\", 30, \"Los Angeles\"),\n        (\"Charlie\", 35, \"San Francisco\")]\n\ndf = spark.createDataFrame(data, [\"Name\", \"Age\", \"City\"])\ndf.show()\n\n# +-------+---+-------------+\n# |   Name|Age|         City|\n# +-------+---+-------------+\n# |  Alice| 25|     New York|\n# |    Bob| 30|  Los Angeles|\n# |Charlie| 35|San Francisco|\n# +-------+---+-------------+\n\n# Select specific columns from the DataFrame\nselected_df = df.select(\"Name\", \"City\")\nselected_df.show()\n\n# +-------+-------------+\n# |   Name|         City|\n# +-------+-------------+\n# |  Alice|     New York|\n# |    Bob|  Los Angeles|\n# |Charlie|San Francisco|\n# +-------+-------------+\n</code></pre>","tags":["pyspark"]},{"location":"blog/2023/12/10/pyspark-daily-summary-1.html#sql-like-functions-where","title":"SQL like functions (WHERE)","text":"<p>Filter rows in PySpark DataFrame, similar to WHERE in SQL</p> <pre><code>from pyspark.sql import SparkSession\n\n# Create a SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n# Create a DataFrame\ndata = [(\"Alice\", 25, \"New York\"),\n        (\"Bob\", 30, \"Los Angeles\"),\n        (\"Charlie\", 35, \"San Francisco\")]\n\ndf = spark.createDataFrame(data, [\"Name\", \"Age\", \"City\"])\ndf.show()\n\n# +-------+---+-------------+\n# |   Name|Age|         City|\n# +-------+---+-------------+\n# |  Alice| 25|     New York|\n# |    Bob| 30|  Los Angeles|\n# |Charlie| 35|San Francisco|\n# +-------+---+-------------+\n</code></pre> <pre><code>from pyspark.sql import functions as f\n\n# Filter rows based on a condition (any of the following notations)\n# filtered_df = df.filter(df.Age &gt; 30)\n# filtered_df = df.filter(df['Age'] &gt; 30)\n# filtered_df = df.filter(f.col('Age') &gt; 30)\nfiltered_df = df.filter((f.col('Age') &gt; 30) | (df.Age == 'Charlie'))\n\nfiltered_df.show()\n\n# +-------+---+-------------+\n# |   Name|Age|         City|\n# +-------+---+-------------+\n# |Charlie| 35|San Francisco|\n# +-------+---+-------------+\n</code></pre>","tags":["pyspark"]},{"location":"blog/2023/12/10/pyspark-daily-summary-1.html#sql-like-functions-group-by","title":"SQL like functions (GROUP BY)","text":"<p>Simple single column based group by operations with agg functionality options</p> <pre><code>from pyspark.sql import SparkSession\n\n# Create a SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n# Create a DataFrame\ndata = [(\"Alice\", 25, \"New York\"),\n        (\"Bob\", 30, \"London\"),\n        (\"Charlie\", 35, \"New York\"),\n        (\"Dave\", 40, \"London\")]\n\ndf = spark.createDataFrame(data, [\"name\", \"age\", \"city\"])\ndf.show()\n\n# +-------+---+--------+\n# |   name|age|    city|\n# +-------+---+--------+\n# |  Alice| 25|New York|\n# |    Bob| 30|  London|\n# |Charlie| 35|New York|\n# |   Dave| 40|  London|\n# +-------+---+--------+\n</code></pre> <pre><code>from pyspark.sql.functions import avg, count, expr\n\n# Group the DataFrame by the 'city' column\ngrouped_df = df.groupBy(\"city\")\n\n# Perform aggregation on the grouped DataFrame\n# result = grouped_df.agg({\"age\": \"avg\"}) # dictionary format (single)\n# result = grouped_df.agg({\"age\": \"avg\", \"name\": \"count\"}) # dictionary format (multiple)\n# result = grouped_df.agg(avg(df.age), count(df.name)) # column object format\n# result = grouped_df.agg(expr(\"avg(age)\"), expr(\"count(name)\")) # sql expression format\n\n# giving alias\nresult = grouped_df.agg(avg(df.age).alias(\"average_age\"), \n                        count(df.name).alias(\"name_count\"))\n\n# Show the result\nresult.show()\n\n# +--------+-----------+----------+\n# |    city|average_age|name_count|\n# +--------+-----------+----------+\n# |New York|       30.0|         2|\n# |  London|       35.0|         2|\n# +--------+-----------+----------+\n</code></pre>","tags":["pyspark"]},{"location":"blog/2023/12/10/pyspark-daily-summary-1.html#summary-pyspark-daily-december-i","title":"Summary | PySpark Daily December I","text":"<p>Lets try to summarise everything important from these posts</p> <ul> <li>Reading a CSV file using spark.read.options(X).csv('data.csv')</li> <li>Define a custom schema with spark.read.csv('data.csv',schema=schema)</li> <li>Scheme format: schema = StructType([StructField(\"name\",Type, True)])</li> <li> <p>Some important Types, which can be imported from pyspark.sql.types</p> <ul> <li>StringType: Represents string values</li> <li>IntegerType: Represents integer values</li> <li>FloatType: Represents float values</li> <li>BooleanType: Represents boolean values</li> <li>DateType: Represents date values.</li> <li>TimestampType: Represents timestamp values.</li> <li>StructType: Represents a structure or record with multiple fields</li> </ul> </li> <li> <p>Automatically define column types using spark.read.csv('data.csv',inferSchema=True)</p> </li> <li>Create SQL table view using spark.createOrReplaceTempView('name') </li> <li>Interact with data using SQL via spark.sql(query)</li> <li>Selecting columns in dataframe df.select(X), works with aggregations like in SQL (see window functions)</li> <li>Filtering dataframe based on column condition df.where(X) | df.filter(X)</li> <li>Adding a new column: df.withColumn('name',X)</li> <li>Renaming columns: df.withColumnRenamed('A','B')</li> <li>Rearranging index index, same as pandas notation df.agg({\"col\": \"aggregation\"})</li> </ul> <p>Training models:</p> <ul> <li> <p>Create a vectorised assembly of features</p> <ul> <li>assembler = VectorAssembler(inputCols=[columns], outputCol=\"output\")</li> <li>df = assembler.transform(df)</li> </ul> </li> <li> <p>Train Model </p> <ul> <li>lr = LinearRegression(featuresCol=\"output\", labelCol=\"target\")</li> <li>model = lr.fit(df)</li> </ul> </li> <li> <p>Use model for prediction </p> <ul> <li>new_data = assembler.transform(new_data)</li> <li>predictions = model.transform(new_data)</li> </ul> </li> </ul> <p>Window Functions:</p> <ul> <li>Import from pyspark.sql.window import Window</li> <li>Define a window Window.rowsBetween(-1, 1)</li> <li>Aggregation over a window rollingMean = avg(data).over(windowSpec)</li> <li>Use aggregation with select: df.select(df[\"date\"], df[\"value\"], rollingMean) or withColumn etc</li> </ul> <p>Thank you for reading!</p> <p>Any questions or comments about the above post can be addressed on the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning or simply below!</p>","tags":["pyspark"]},{"location":"blog/2023/12/20/pyspark-daily-summary-2.html","title":"PySpark Daily Summary II","text":"<p>Continuing on where we left off last post, I'll be exploring pypspark on a daily basis, just to get more used to it. Here I will be posting summaries that cover roughtly 10 days worth of posts that I make on Kaggle, so that would equate to three posts a month</p>","tags":["pyspark"]},{"location":"blog/2023/12/20/pyspark-daily-summary-2.html#sql-like-functions-order-by","title":"SQL like functions (ORDER BY)","text":"<p>Ordering a column using orderBy based on ascending f.col.asc() or descending order f.col.desc()</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\n# Create a SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n# Create a DataFrame\ndata = [(\"Alice\", 25), \n        (\"Bob\", 30), \n        (\"Charlie\", 35)]\ndf = spark.createDataFrame(data, [\"name\", \"age\"])\ndf.show()\n\n+-------+---+\n|   name|age|\n+-------+---+\n|  Alice| 25|\n|    Bob| 30|\n|Charlie| 35|\n+-------+---+\n</code></pre> <pre><code># Order DataFrame by age in descending order\ndf.orderBy(f.col('age').desc()).show()\n\n+-------+---+\n|   name|age|\n+-------+---+\n|Charlie| 35|\n|    Bob| 30|\n|  Alice| 25|\n+-------+---+\n</code></pre> <p>If we want to sort with two columns, which is useful when we have the same values in the first column, we would do something like this:</p> <pre><code># Create a DataFrame\ndata = [(\"Alice\", 25, 180), \n        (\"Bob\", 25, 150), \n        (\"Charlie\", 35, 167)]\ndf = spark.createDataFrame(data, [\"name\", \"age\",\"height\"])\ndf.show()\n</code></pre> <pre><code># Order DataFrame by age in descending order\ndf.orderBy(f.col('age').desc(),f.col('height').asc()).show()\n\n+-------+---+------+\n|   name|age|height|\n+-------+---+------+\n|Charlie| 35|   167|\n|    Bob| 25|   150|\n|  Alice| 25|   180|\n+-------+---+------+\n</code></pre>","tags":["pyspark"]},{"location":"blog/2023/12/20/pyspark-daily-summary-2.html#sql-like-functions-join","title":"SQL like functions (JOIN)","text":"<p>Joining dataframes is of course an important part of data analysis:</p> <ul> <li>Using pyspark.sql, we can join dataframes with the notation shown in 01/12/2023, however pyspark dataframe has its own method for joining dataframe tables join()</li> <li>As with the pandas notation of merge df1.merge(df2), we can join dataframes using the a similar notation df1.join(df2,'on','how')</li> </ul> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\n# Create a Spark session\nspark = SparkSession.builder.getOrCreate()\n\n# Create sample data for dataset1 (January)\ndataset1 = spark.createDataFrame([\n    (\"2021-01-01\", 10),\n    (\"2021-01-02\", 20),\n    (\"2021-01-03\", 30),\n    (\"2021-01-04\", 70)\n], [\"date\", \"value1\"])\n\n# Create sample data for dataset2 (February)\ndataset2 = spark.createDataFrame([\n    (\"2021-02-01\", 40),\n    (\"2021-02-02\", 50),\n    (\"2021-02-03\", 60),\n    (\"2021-02-05\", 70)\n], [\"date\", \"value2\"])\n\n# Show the joined dataframe\ndataset1.show()\ndataset2.show()\n\n+----------+------+\n|      date|value1|\n+----------+------+\n|2021-01-01|    10|\n|2021-01-02|    20|\n|2021-01-03|    30|\n|2021-01-04|    70|\n+----------+------+\n\n+----------+------+\n|      date|value2|\n+----------+------+\n|2021-02-01|    40|\n|2021-02-02|    50|\n|2021-02-03|    60|\n|2021-02-05|    70|\n+----------+------+\n</code></pre> <p>If we choose to inner join, we will be left with only three rows, since they both share them</p> <pre><code>dataset1.join(dataset2,on='date',how='inner').show()\n\n+----------+------+------+\n|      date|value1|value2|\n+----------+------+------+\n|2021-01-01|    10|    40|\n|2021-01-02|    20|    50|\n|2021-01-03|    30|    60|\n+----------+------+------+\n</code></pre> <p>LEFT JOIN uses all the values in the left table, if some data is missing in the right it will be replaced with NULL</p> <pre><code>dataset1.join(dataset2,on='date',how='inner').show()\n\n+----------+------+------+\n|      date|value1|value2|\n+----------+------+------+\n|2021-01-01|    10|    40|\n|2021-01-02|    20|    50|\n|2021-01-03|    30|    60|\n|2021-01-04|    70|  NULL|\n+----------+------+------+\n</code></pre> <p>Similar to RIGHT JOIN</p> <pre><code>dataset1.join(dataset2,on='date',how='right').show()\n\n+----------+------+------+\n|      date|value1|value2|\n+----------+------+------+\n|2021-01-01|    10|    40|\n|2021-01-02|    20|    50|\n|2021-01-03|    30|    60|\n|2021-01-05|  NULL|    70|\n+----------+------+------+\n</code></pre> <p>And FULL OUTER join as well:</p> <pre><code>dataset1.join(dataset2,on='date',how='outer').show()\n\n+----------+------+------+\n|      date|value1|value2|\n+----------+------+------+\n|2021-01-01|    10|    40|\n|2021-01-02|    20|    50|\n|2021-01-03|    30|    60|\n|2021-01-04|    70|  NULL|\n|2021-01-05|  NULL|    70|\n+----------+------+------+\n</code></pre>","tags":["pyspark"]},{"location":"blog/2023/12/20/pyspark-daily-summary-2.html#pyspark-udf-standard-udf","title":"PySpark UDF (Standard UDF)","text":"<p>PySpark UDFs are custom functions that can be created and applied to DataFrame columns in PySpark. They allow users to perform custom computations or transformations on DataFrame data by defining their own functions and applying them to specific columns, </p> <p>A UDF example which takes one column value, the only thing to note is that we need to define the output type of the function:</p> <pre><code>from pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\n\nspark = SparkSession.builder.getOrCreate()\n\n# Create a DataFrame\ndata = [(\"Alice\", 25), \n        (\"Bob\", 30), \n        (\"Charlie\", 35)]\ndf = spark.createDataFrame(data, [\"name\", \"age\"])\n\n# UDF function which accepts one column &amp; multiplies it by itself\ndef square(num):\n    return num * num\n\n# register UDF\nsquare_udf = udf(square, IntegerType())\n\n# Apply the UDF to a DataFrame column\nnew_df = df.withColumn(\"square\", square_udf(df[\"age\"]))\nnew_df.show()\n\n+-------+---+------+\n|   name|age|square|\n+-------+---+------+\n|  Alice| 25|   625|\n|    Bob| 30|   900|\n|Charlie| 35|  1225|\n+-------+---+------+\n</code></pre> <p>We can also use more than one column of data</p> <pre><code>def add(num1,num2):\n    return num1 + num2\n\nadd_udf = udf(add,IntegerType())\n\ndf = df.withColumn(\"added2\",add_udf(df['age'],df['added2']))\ndf.show()\n\n+-------+---+------+\n|   name|age|added2|\n+-------+---+------+\n|  Alice| 25|    50|\n|    Bob| 30|    60|\n|Charlie| 35|    70|\n+-------+---+------+\n</code></pre> <p>Lambda functions can also be used instead of standard python functions</p> <pre><code>add_udf = udf(lambda x,y : x + y,IntegerType())\ndf = df.withColumn(\"added3\",add_udf(df['age'],df['age']))\ndf.show()\n\n+-------+---+------+\n|   name|age|added3|\n+-------+---+------+\n|  Alice| 25|    50|\n|    Bob| 30|    60|\n|Charlie| 35|    70|\n+-------+---+------+\n</code></pre>","tags":["pyspark"]},{"location":"blog/2023/12/20/pyspark-daily-summary-2.html#pyspark-udf-to-create-features-for-modeling","title":"PySpark UDF to create features for modeling","text":"<p>UDF can be used like apply in pandas dataframes, allowing custom logic modifications to columns values. </p> <p>In this example we'll create a new feature (new_column) based on the row values of other columns, and use it as feature input into our model LinearRegression. To do this, we will need to utilise the previously visited VectorAssembler as the inputCols</p> <p>Suppose we have two features x1,x2 and we want to add an extra feature that is created from using the values from these two columns. To do this we create custom_udf and define the operations and output type DoubleType</p> <p>To add a new column with the new value we simply use withColumn and call the UDF function with column values as input.</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\n# Create a Spark session\nspark = SparkSession.builder.getOrCreate()\n\n# Sample data\ndata = [(1, 2, 5), \n        (4, 1, 10), \n        (7, 3, 15)]\ndf = spark.createDataFrame(data, [\"x1\", \"x2\", \"y\"])\n\n# Define a UDF to perform a custom operation\ndef custom_operation(x1,x2):\n    return x1 * 2.0 + x2\n\ncustom_udf = udf(custom_operation, DoubleType())\n\n# Apply the UDF to create a new column\ndf = df.withColumn(\"new_column\", custom_udf(df[\"x1\"], df[\"x2\"]))\ndf.show()\n\n# Assemble features into a vector\nassembler = VectorAssembler(inputCols=[\"x1\",\"new_column\"], outputCol=\"features\")\ndf = assembler.transform(df)\ndf.show()\n\n# # Train a linear regression model\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"y\")\nmodel = lr.fit(df)\nspark.stop()\n</code></pre>","tags":["pyspark"]},{"location":"blog/2023/12/20/pyspark-daily-summary-2.html#pyspark-udf-to-create-features-for-modeling_1","title":"PySpark UDF to create features for modeling","text":"<p>PySpark offers another type of UDF, PandasUDF, which can be imported from functions as well like udf. Unlike UDF, we need to specify which type of pandasUDF we will be using (functionType), one of these types is PandasUDFType.SCALAR. </p> <p>PandasUDFType.SCALAR is a constant in PySpark that represents the type of pandas UDF (SCALAR)</p> <ul> <li>A scalar pandas UDF takes one or more columns as input and returns a single column as output</li> <li>It operates on a single row at a time and can be used to apply arbitrary Python functions to the data in a DataFrame</li> </ul> <pre><code>from pyspark.sql import SparkSession\n\n# Create a Spark session\nspark = SparkSession.builder.getOrCreate()\n\n# Create a sample DataFrame\ndata = [(\"alice\", 25), \n        (\"bob\", 30), \n        (\"charlie\", 35)]\n\ndf = spark.createDataFrame(data, [\"Name\", \"Age\"])\ndf.show()\n\n+-------+---+\n|   Name|Age|\n+-------+---+\n|  alice| 25|\n|    bob| 30|\n|charlie| 35|\n+-------+---+\n</code></pre> <p>We can define the UDF as a decorator @pandas_udf, we can add a simple functionality to capitalise the input string column using capitalise_name</p> <pre><code>from pyspark.sql.functions import pandas_udf, PandasUDFType\n\n# Define a pandas UDF\n# return type &amp; function type\n\n@pandas_udf(returnType=\"string\", functionType=PandasUDFType.SCALAR)\ndef capitalise_name(name):\n    return name.str.capitalize()\n\n# Add new column Apply the pandas UDF on the DataFrame\ndf = df.withColumn(\"Capitalised\", capitalise_name(df[\"Name\"]))\ndf.show()\n\n+-------+---+-----------+\n|   Name|Age|Capitalised|\n+-------+---+-----------+\n|  alice| 25|      Alice|\n|    bob| 30|        Bob|\n|charlie| 35|    Charlie|\n+-------+---+-----------+\n</code></pre> <p>Thank you for reading!</p> <p>Any questions or comments about the above post can be addressed on the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning or simply below!</p>","tags":["pyspark"]},{"location":"blog/2024/03/24/sql-analytics-problem.html","title":"SQL Analytics Problem","text":"<p>An interview question related to SQL knowledge from a financial bank which I thought was interesting so decided to share</p> <ul> <li>The first part consists of standard SQL knowledge questions </li> <li>The second part consists of a problem in which we will need to create some code for monitoring the number of hours an employee has worked, which we will be doing with <code>python</code> and <code>posgres</code></li> </ul> <ul> <li> GitHub Repository</li> </ul>","tags":["sql","interview"]},{"location":"blog/2024/03/24/sql-analytics-problem.html#questions","title":"Questions","text":"","tags":["sql","interview"]},{"location":"blog/2024/03/24/sql-analytics-problem.html#elementary-questions","title":"Elementary Questions","text":"<p>These are quite standard simple questions about general understanding of SQL</p> <ul> <li>1) \u0412\u044b\u0432\u0435\u0441\u0442\u0438 \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432, \u043f\u043e\u043b\u0443\u0447\u0430\u044e\u0449\u0438\u0445 \u0437\u0430\u0440\u043e\u0431\u043e\u0442\u043d\u0443\u044e \u043f\u043b\u0430\u0442\u0443 \u0431\u043e\u043b\u044c\u0448\u0435 \u0447\u0435\u043c \u0443 \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044f</li> <li>2) \u0412\u044b\u0432\u0435\u0441\u0442\u0438 \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432, \u043f\u043e\u043b\u0443\u0447\u0430\u044e\u0449\u0438\u0445 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u0443\u044e \u0437\u0430\u0440\u043e\u0431\u043e\u0442\u043d\u0443\u044e \u043f\u043b\u0430\u0442\u0443 \u0432 \u0441\u0432\u043e\u0435\u043c \u043e\u0442\u0434\u0435\u043b\u0435</li> <li>3) \u0412\u044b\u0432\u0435\u0441\u0442\u0438 \u0441\u043f\u0438\u0441\u043e\u043a ID \u043e\u0442\u0434\u0435\u043b\u043e\u0432, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432 \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043d\u0435 \u043f\u0440\u0435\u0432\u044b\u0448\u0430\u0435\u0442 3 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430</li> <li>4) \u0412\u044b\u0432\u0435\u0441\u0442\u0438 \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432, \u043d\u0435 \u0438\u043c\u0435\u044e\u0449\u0438\u0445 \u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d\u043d\u043e\u0433\u043e \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044f, \u0440\u0430\u0431\u043e\u0442\u0443\u044e\u0449\u0435\u0433\u043e \u0432 \u0442\u043e\u043c \u0436\u0435 \u043e\u0442\u0434\u0435\u043b\u0435</li> <li>5) \u041d\u0430\u0439\u0442\u0438 \u0441\u043f\u0438\u0441\u043e\u043a ID \u043e\u0442\u0434\u0435\u043b\u043e\u0432 \u0441 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0441\u0443\u043c\u043c\u0430\u0440\u043d\u043e\u0439 \u0437\u0430\u0440\u043f\u043b\u0430\u0442\u043e\u0439 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432</li> </ul>","tags":["sql","interview"]},{"location":"blog/2024/03/24/sql-analytics-problem.html#main-problem","title":"Main Problem","text":"<p>\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u044c \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0443 \u043d\u043e\u0432\u044b\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0432 \u0441\u0432\u044f\u0437\u0438 \u0441 \u0437\u0430\u0434\u0430\u0447\u0435\u0439 \u0444\u0438\u043a\u0441\u0430\u0446\u0438\u0438 \u043e\u0442\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438. \u0423\u0441\u043b\u043e\u0432\u0438\u044f:</p> <ul> <li>\u041a\u0430\u0436\u0434\u044b\u0439 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0432 \u0441\u0432\u043e\u0435\u043c \u0447\u0430\u0441\u043e\u0432\u043e\u043c \u043f\u043e\u044f\u0441\u0435, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u044f\u0432\u043d\u043e \u043f\u0440\u043e\u043f\u0438\u0441\u0430\u043d \u0438 \u043d\u0435 \u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f. \u0421\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0439 \u0433\u0440\u0430\u0444\u0438\u043a 8 \u0447\u0430\u0441\u043e\u0432 \u0441 9 \u0443\u0442\u0440\u0430 \u043f\u043e \u0435\u0433\u043e \u0447\u0430\u0441\u043e\u0432\u043e\u043c\u0443 \u043f\u043e\u044f\u0441\u0443 (\u041e\u0431\u0435\u0434 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u0432 \u0440\u0430\u043c\u043a\u0430\u0445 \u0434\u0430\u043d\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438)</li> <li>\u0420\u0430\u0431\u043e\u0442\u043d\u0438\u043a \u043a \u043a\u043e\u043d\u0446\u0443 \u043d\u0435\u0434\u0435\u043b\u0438 \u0434\u043e\u043b\u0436\u0435\u043d \u0437\u0430\u043f\u043e\u043b\u043d\u0438\u0442\u044c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0442\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u044b\u0445 \u0447\u0430\u0441\u043e\u0432 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0434\u043d\u044f. \u041e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u0447\u0430\u0441\u043e\u0432 \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u0430 \u043d\u0430 \u0440\u0430\u0431\u043e\u0447\u0435\u043c \u043c\u0435\u0441\u0442\u0435. \u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441\u043a\u0440\u0438\u043f\u0442\u044b \u0434\u043b\u044f \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u044f \u0431\u0430\u0437\u044b \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043e\u0442\u0447\u0435\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u0442 \u043e\u0442\u0441\u043b\u0435\u0434\u0438\u0442\u044c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0447\u0430\u0441\u0430 \u0440\u0430\u0431\u043e\u0447\u0435\u0439 \u043d\u0435\u0434\u0435\u043b\u0438. \u0414\u043e\u043f\u0443\u0441\u043a\u0430\u0435\u0442\u0441\u044f \u0432\u044b\u0432\u043e\u0434\u0438\u0442\u044c 0 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432.</li> </ul> <p>\u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0432 \u0432\u0438\u0434\u0435:</p> <ul> <li>\u0414\u0430\u0442\u0430 </li> <li>\u0412\u0440\u0435\u043c\u044f</li> <li>\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432</li> </ul> <p>So we know that a company has employees working in different time zones, and doesn't change in the context of the problem. The standard working hours are 8 hours from 9 am. </p> <p>Each employee must fill in the hours spent working for each day. This data will be used to determine how many people are working at the same period in time, taking into consideration this shift in timezones.</p>","tags":["sql","interview"]},{"location":"blog/2024/03/24/sql-analytics-problem.html#available-data","title":"Available Data","text":"<p>We are given two tables, which contain information about the department for employees, containing their name and identifier, and another table containing employee information, as shown below: </p> <p>DEPARTMENT</p> <ul> <li>ID NUMBER  <li>NAME VARCHAR2(100) </li> <p>EMPLOYEE </p> <ul> <li>ID NUMBER  <li>DEPARTMENT_ID NUMBER </li> <li>CHIEF_ID NUMBER  <li>NAME VARCHAR2(100)  <li>SALARY NUMBER </li>","tags":["sql","interview"]},{"location":"blog/2024/03/24/sql-analytics-problem.html#tools-data","title":"Tools &amp; Data","text":"<p>We'll need some sample data, I'll be using posgres, pyspark is mainly for presentation purposes. I've created only two Departments (Business and Analytics) and 7 employees work across both of these departments as shown below:</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType\n\n'''\n\n\u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043f\u0440\u0438\u043c\u0435\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0442\u0430\u0431\u043b\u0438\u0446 EMPLOYEE, DEPARTAMENT\n\n'''\n\n# Create a Spark session\nspark = SparkSession.builder.getOrCreate()\n\n'''\n\nCreate Departament Table\n\n'''\n\n# Sample data for the Department table\ndata = [\n        (1, \"Business\"),\n        (2, \"Analytics\")]\n\n# Define the schema for the Department table\nschema = StructType([\n    StructField(\"ID\", IntegerType(), True),\n    StructField(\"NAME\", StringType(), True),\n])\n\n# Create a DataFrame for the Department table\ndepartament = spark.createDataFrame(data, schema)\ndepartament.createOrReplaceTempView(\"DEPARTAMENT\")\n\n'''\n\nCreate Employee Table\n\n'''\n\n# Sample data for the Department table\ndata = [\n        (1,1,None,\"\u0422\u0430\u043c\u0430\u0440\u0430\",100000.0),\n        (2,2,3,\"\u041d\u0430\u0442\u0430\u0448\u0430\",150000.0),\n        (3,2,None,\"\u0414\u0430\u0432\u0438\u0434\",110000.0),\n        (4,1,1, \"\u0410\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440\",70000.0),\n        (5,2,3,\"\u041f\u0430\u0432\u0435\u043b\",200000.0),\n        (6,1,1,\"\u041c\u0438\u0445\u0430\u0438\u043b\",50000.0),\n         (7,1,1,\"\u0415\u043a\u0430\u0442\u0435\u0440\u0438\u043d\u0430\",90000.0)]\n\n# chief 1 : Tamara, chief 2 : David\n\n# Define the schema for the Department table\nschema = StructType([\n    StructField(\"ID\", IntegerType(), True),\n    StructField(\"DEPARTAMENT_ID\", IntegerType(), True),\n    StructField(\"CHIEF_ID\", IntegerType(), True),\n    StructField(\"NAME\", StringType(), True),\n    StructField(\"SALARY\", FloatType(), True)\n])\n\n# Create a DataFrame for the Department table\nemployee = spark.createDataFrame(data, schema)\nemployee.createOrReplaceTempView(\"EMPLOYEE\")\n</code></pre>","tags":["sql","interview"]},{"location":"blog/2024/03/24/sql-analytics-problem.html#elementary-questions_1","title":"Elementary Questions","text":"<p>Some basic concepts in SQL, joining, subqueries and CTE. Some of the more interesting things that one might not come across in SQL problems often is a WHERE condition format such as:</p> <pre><code>WHERE\n    (DEPARTAMENT_ID, SALARY) IN (SELECT \n                                    DEPARTAMENT_ID,\n                                    MAX(SALARY)\n                                 FROM EMPLOYEE\n                                 GROUP BY DEPARTAMENT_ID)\n</code></pre> <p>The format allows us to select users with maximum salaries in each department</p> <pre><code># 1) \u0412\u044b\u0432\u0435\u0441\u0442\u0438 \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432, \u043f\u043e\u043b\u0443\u0447\u0430\u044e\u0449\u0438\u0445 \u0437\u0430\u0440\u043e\u0431\u043e\u0442\u043d\u0443\u044e \u043f\u043b\u0430\u0442\u0443 \u0431\u043e\u043b\u044c\u0448\u0435 \u0447\u0435\u043c \u0443 \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044f\n\n# \u0425\u043e\u0434 \u043c\u044b\u0448\u043b\u0435\u043d\u0438\u044f:\n# - \u041d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0441\u0440\u0430\u0432\u043d\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u0432\u043d\u0443\u0442\u0440\u0438 \u0442\u0430\u0431\u043b\u0438\u0446\u044b; \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442 \u0438 \u0441\u043e\u0435\u0434\u0435\u043d\u0438\u043c \u0441 JOIN\n# - \u0414\u043e\u0431\u0430\u0432\u0438\u043c \u0443\u0441\u043b\u043e\u0432\u0438\u0435 \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u0433\u043e\u0432\u043e\u0440\u0438\u0442 \u043e \u0442\u043e\u043c \u0447\u0442\u043e salary \u0434\u043e\u043b\u0436\u043d\u043e \u0431\u044b\u0442\u044c \u043c\u0435\u043d\u044c\u0448\u0435 \u0447\u0435\u043c \u0443 \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044f\n\nquery1 = \"\"\"\nSELECT e2.NAME\nFROM EMPLOYEE e1\nINNER JOIN EMPLOYEE e2 ON e1.ID = e2.CHIEF_ID\nWHERE e1.SALARY &lt; e2.SALARY\n\"\"\"\n\n# Now you can use SQL to query the table\nspark.sql(query1).show()\n</code></pre> <pre><code>+------+\n|  NAME|\n+------+\n|\u041d\u0430\u0442\u0430\u0448\u0430|\n| \u041f\u0430\u0432\u0435\u043b|\n+------+\n</code></pre> <pre><code># 2) \u0412\u044b\u0432\u0435\u0441\u0442\u0438 \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432, \u043f\u043e\u043b\u0443\u0447\u0430\u044e\u0449\u0438\u0445 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u0443\u044e \u0437\u0430\u0440\u043e\u0431\u043e\u0442\u043d\u0443\u044e \u043f\u043b\u0430\u0442\u0443 \u0432 \u0441\u0432\u043e\u0435\u043c \u043e\u0442\u0434\u0435\u043b\u0435\n\n# \u0425\u043e\u0434 \u043c\u044b\u0448\u043b\u0435\u043d\u0438\u044f:\n# - \u041d\u0430\u0434\u043e \u043d\u0430\u0439\u0442\u0438 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043e\u0442\u0434\u0435\u043b\u043e\u0432, \u043c\u043e\u0436\u043d\u043e \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0441\u044f\n#   WHERE \u0441 \u0443\u0441\u043b\u043e\u0432\u0438\u0435 \u0434\u0432\u0443\u0445 \u0430\u0442\u0440\u0438\u0431\u0443\u0442 \u0447\u0442\u043e\u0431\u044b \u0431\u044b\u043b\u043e \u0442\u043e\u0447\u043d\u043e\u0435 \u0441\u043e\u0432\u043f\u0430\u0434\u0435\u043d\u0438\u0435 \u0441 DEPARTAMENT \u0438 MAX SALARY\n\nquery2 = \"\"\"\nSELECT\n       EMPLOYEE.NAME,\n       EMPLOYEE.DEPARTAMENT_ID,\n       EMPLOYEE.SALARY\nFROM EMPLOYEE\nWHERE\n    (DEPARTAMENT_ID, SALARY) IN (SELECT DEPARTAMENT_ID,MAX(SALARY)\n                                  FROM EMPLOYEE\n                                  GROUP BY DEPARTAMENT_ID)\n\"\"\"\n\nspark.sql(query2).show()\n</code></pre> <pre><code>+------+--------------+--------+\n|  NAME|DEPARTAMENT_ID|  SALARY|\n+------+--------------+--------+\n|\u0422\u0430\u043c\u0430\u0440\u0430|             1|100000.0|\n| \u041f\u0430\u0432\u0435\u043b|             2|200000.0|\n+------+--------------+--------+\n</code></pre> <pre><code># 3) \u0412\u044b\u0432\u0435\u0441\u0442\u0438 \u0441\u043f\u0438\u0441\u043e\u043a ID \u043e\u0442\u0434\u0435\u043b\u043e\u0432, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432 \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043d\u0435 \u043f\u0440\u0435\u0432\u044b\u0448\u0430\u0435\u0442 3 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430\n\n# \u0425\u043e\u0434 \u043c\u044b\u0448\u043b\u0435\u043d\u0438\u044f:\n# - \u0412 \u044d\u0442\u043e\u0442 \u0440\u0430\u0437 \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u043e\u0431\u0440\u0430\u0442\u0438\u0442\u0441\u044f \u043a DEPARTAMENT \u0438 \u043f\u0440\u0438\u0432\u044f\u0437\u0430\u0442\u044c \u0443\u0441\u043b\u043e\u0432\u0438\u0435 \u0434\u043b\u044f \u0432\u044b\u0431\u043e\u0440\u0430 ID \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f WHERE\n#   \u0434\u043b\u044f \u0443\u0441\u043b\u043e\u0432\u0438\u044f \u043c\u044b \u043e\u0431\u0440\u0430\u0442\u0438\u043c\u0441\u044f \u0441 EMPLOYEE \u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432 \u0441 SUM\n#   \u0434\u043e\u0431\u0430\u0432\u0438\u0432 \u0443\u0441\u043b\u043e\u0432\u0438\u0435 \u0434\u043b\u044f GROUP BY \u0441 HAVING \u043f\u0440\u0438 \u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u0430\u043d\u0430\u0438\u044f \u043f\u043e \u043e\u0442\u0434\u0435\u043b\u0430\u043c\n\nquery3 = \"\"\"\nSELECT NAME\nFROM DEPARTAMENT\nWHERE ID IN (SELECT DEPARTAMENT_ID\n             FROM EMPLOYEE\n             GROUP BY DEPARTAMENT_ID\n             HAVING COUNT(*) &gt; 2)\n\"\"\"\n\nspark.sql(query3).show()\n</code></pre> <pre><code>+---------+\n|     NAME|\n+---------+\n| Business|\n|Analytics|\n+---------+\n</code></pre> <pre><code># 4) \u0412\u044b\u0432\u0435\u0441\u0442\u0438 \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432, \u043d\u0435 \u0438\u043c\u0435\u044e\u0449\u0438\u0445 \u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d\u043d\u043e\u0433\u043e \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044f, \u0440\u0430\u0431\u043e\u0442\u0443\u044e\u0449\u0435\u0433\u043e \u0432 \u0442\u043e\u043c \u0436\u0435 \u043e\u0442\u0434\u0435\u043b\u0435\n\nquery4 = \"\"\"\nWITH COMBINED AS (\n                    SELECT\n                        e.NAME AS WORKER,\n                        e2.NAME AS CHIEF,\n                        d.NAME AS DEPARTAMENT\n                    FROM EMPLOYEE e\n                    JOIN EMPLOYEE e2 ON e.chief_id = e2.ID\n                    JOIN DEPARTAMENT d ON e.DEPARTAMENT_ID = d.ID\n                  )\n\nSELECT *\nFROM COMBINED\nWHERE CHIEF IS NULL\n\"\"\"\n\nspark.sql(query4).show()\n</code></pre> <pre><code>+------+-----+-----------+\n|WORKER|CHIEF|DEPARTAMENT|\n+------+-----+-----------+\n+------+-----+-----------+\n</code></pre> <pre><code># 5) \u041d\u0430\u0439\u0442\u0438 \u0441\u043f\u0438\u0441\u043e\u043a ID \u043e\u0442\u0434\u0435\u043b\u043e\u0432 \u0441 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0441\u0443\u043c\u043c\u0430\u0440\u043d\u043e\u0439 \u0437\u0430\u0440\u043f\u043b\u0430\u0442\u043e\u0439 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432\n\n# \u0425\u043e\u0434 \u043c\u044b\u0448\u043b\u0435\u043d\u0438\u044f:\n# - \u041d\u0430\u0439\u0442\u0438 \u0441\u043f\u0438\u0441\u043e\u043a ID \u043e\u0442\u0434\u0435\u043b\u043e\u0432 \u0441 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0441\u0443\u043c\u043c\u0430\u0440\u043d\u043e\u0439 \u0437\u0430\u0440\u043f\u043b\u0430\u0442\u043e\u0439 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432\n\ntquery = \"\"\"\n-- \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 salary \u0432 department_id\nWITH DEP_MAX AS (SELECT DEPARTAMENT_ID,\n                        SUM(SALARY) AS SALARY\n                    FROM EMPLOYEE\n                    GROUP BY DEPARTAMENT_ID)\n\n-- \u0412\u044b\u0432\u043e\u0434\u0438\u043c ID department \u0433\u0434\u0435 salary \u0440\u0430\u0432\u043d\u043e max(SALARY)\nSELECT DEPARTAMENT_ID\nFROM DEP_MAX\nWHERE DEP_MAX.SALARY = (SELECT max(SALARY)\n                        FROM DEP_MAX);\n\"\"\"\n\nspark.sql(tquery).show()\n</code></pre> <pre><code>+--------------+\n|DEPARTAMENT_ID|\n+--------------+\n|             2|\n+--------------+\n</code></pre>","tags":["sql","interview"]},{"location":"blog/2024/03/24/sql-analytics-problem.html#main-problem_1","title":"Main Problem","text":"<p>Now lets move onto the main problem of the interview questions. We need to have a posgres session started. I'll be using psycopg2 to connect to the database.gs The two main tables EMPLOYEE and DEPARTMENT which we'll need have been described above. We need to add new data to our database which describes the number of hours a an employee has worked.</p> <p>Our goal is to write a code that will allow employees to update the database with employee working hours data &amp; create a simple analysis based on the data that each employee provides us. </p> <pre><code>'''\n\n\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u044c \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0443 \u043d\u043e\u0432\u044b\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0432 \u0441\u0432\u044f\u0437\u0438 \u0441 \u0437\u0430\u0434\u0430\u0447\u0435\u0439 \u0444\u0438\u043a\u0441\u0430\u0446\u0438\u0438 \u043e\u0442\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438.\n\n\u0423\u0441\u043b\u043e\u0432\u0438\u044f:\n- \u041a\u0430\u0436\u0434\u044b\u0439 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0432 \u0441\u0432\u043e\u0435\u043c \u0447\u0430\u0441\u043e\u0432\u043e\u043c \u043f\u043e\u044f\u0441\u0435, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u044f\u0432\u043d\u043e \u043f\u0440\u043e\u043f\u0438\u0441\u0430\u043d \u0438 \u043d\u0435 \u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f.\n  \u0421\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0439 \u0433\u0440\u0430\u0444\u0438\u043a 8 \u0447\u0430\u0441\u043e\u0432 \u0441 9 \u0443\u0442\u0440\u0430 \u043f\u043e \u0435\u0433\u043e \u0447\u0430\u0441\u043e\u0432\u043e\u043c\u0443 \u043f\u043e\u044f\u0441\u0443 (\u041e\u0431\u0435\u0434 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u0432 \u0440\u0430\u043c\u043a\u0430\u0445 \u0434\u0430\u043d\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438)\n- \u0420\u0430\u0431\u043e\u0442\u043d\u0438\u043a \u043a \u043a\u043e\u043d\u0446\u0443 \u043d\u0435\u0434\u0435\u043b\u0438 \u0434\u043e\u043b\u0436\u0435\u043d \u0437\u0430\u043f\u043e\u043b\u043d\u0438\u0442\u044c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0442\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u044b\u0445 \u0447\u0430\u0441\u043e\u0432 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0434\u043d\u044f.\n  \u041e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u0447\u0430\u0441\u043e\u0432 \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u0430 \u043d\u0430 \u0440\u0430\u0431\u043e\u0447\u0435\u043c \u043c\u0435\u0441\u0442\u0435.\n\n\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441\u043a\u0440\u0438\u043f\u0442\u044b \u0434\u043b\u044f \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u044f \u0431\u0430\u0437\u044b \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043e\u0442\u0447\u0435\u0442,\n\u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u0442 \u043e\u0442\u0441\u043b\u0435\u0434\u0438\u0442\u044c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0447\u0430\u0441\u0430 \u0440\u0430\u0431\u043e\u0447\u0435\u0439 \u043d\u0435\u0434\u0435\u043b\u0438.\n\u0414\u043e\u043f\u0443\u0441\u043a\u0430\u0435\u0442\u0441\u044f \u0432\u044b\u0432\u043e\u0434\u0438\u0442\u044c 0 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432.\n\n'''\n\n# \u0425\u043e\u0434 \u043c\u044b\u0448\u043b\u0435\u043d\u0438\u044f:\n# - \u0414\u043b\u044f \u044d\u0442\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f postgres \u0438 \u0434\u043b\u044f \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u0438 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c python\n#   \u0432\u043c\u0435\u0441\u0442\u0435 \u0441 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u043e\u0439 psycopg2 \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0421\u0423\u0411\u0414\n# - \u0411\u0414 \u0441\u043e\u0437\u0434\u0430\u0435\u0442\u0441\u044f \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u043e \u043f\u043e\u044d\u0442\u043e\u043c \u043f\u0440\u0438 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 \u0432\u044b\u0433\u0440\u0443\u0437\u043a\u0438 \u0438\u0441\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u0432 colab\n\nimport psycopg2\nfrom datetime import date, datetime, timedelta\n</code></pre> <p>Create two new tables and add the data which has already been shown above into tables DEPARTMENT and EMPLOYEE</p> <pre><code># \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u043f\u0440\u0438\u043c\u0435\u0440 \u0434\u0430\u043d\u043d\u044b\u0445, \u043a\u0430\u043a \u0438 \u0434\u043b\u044f spark\n\nconn = psycopg2.connect(database = \"postgres\",\n                        user = \"postgres\",\n                        port = 5433)\n\n'''\n\nCREATE TABLE\n\n'''\n\n# Open a cursor to perform database operations\ncur = conn.cursor()\n# Execute a command: create datacamp_courses table\ncur.execute(\"\"\"\n                CREATE TABLE DEPARTMENT (\n                                            ID INT,\n                                            NAME VARCHAR(100)\n                                         );\n\n                CREATE TABLE EMPLOYEE (\n                                        ID INT PRIMARY KEY,\n                                        DEPARTMENT_ID INT,\n                                        CHIEF_ID INT,\n                                        NAME VARCHAR(100),\n                                        SALARY INT\n                                        );\n            \"\"\")\n\nconn.commit()\ncur.close()\n</code></pre> <pre><code>'''\n\nADD DATA TO TABLES\n\n'''\n\ncur = conn.cursor()\ncur.execute(\"\"\"\n    INSERT INTO EMPLOYEE (ID,DEPARTMENT_ID,CHIEF_ID,NAME,SALARY) VALUES (1,1,NULL,'\u0422\u0430\u043c\u0430\u0440\u0430',100000),\n                                                                        (2,2,3,'\u041d\u0430\u0442\u0430\u0448\u0430',150000),\n                                                                        (3,2,NULL,'\u0414\u0430\u0432\u0438\u0434',110000),\n                                                                        (4,1,1, '\u0410\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440',70000),\n                                                                        (5,2,3,'\u041f\u0430\u0432\u0435\u043b',200000),\n                                                                        (6,1,1,'\u041c\u0438\u0445\u0430\u0438\u043b',50000),\n                                                                        (7,1,1,'\u0415\u043a\u0430\u0442\u0435\u0440\u0438\u043d\u0430',90000)\n            \"\"\")\n\ncur.execute(\"\"\"\n                    INSERT INTO DEPARTMENT (ID,NAME) VALUES (1,'Business'),\n                                                             (2,'Analytics')\n                                                             \"\"\")\n\ncur.close()\nconn.commit()\n</code></pre> <p>Employees can be located in different timezones as stated in the problem, so lets add a new column to address this data in table EMPLOYEE and create a simple script to iteratively INSERT data from a python list</p> <pre><code># \u0442\u0435\u043f\u0435\u0440 \u0438\u043c\u0435\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043e \u0442\u043e\u043c \u0447\u0442\u043e \u043d\u0443\u0436\u043d\u043e \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0435\u0449\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\n# \u0434\u043e\u0431\u0430\u0432\u0438\u043c \u0435\u0433\u043e \u0447\u0435\u0440\u0435\u0437 ALTER TABLE\n\n'''\n\n[step 1] : ADD employee_id timezones\n\n'''\n\n# add new column to account for timezones\ncur = conn.cursor()\ncur.execute(\"\"\"\n              ALTER TABLE EMPLOYEE ADD TIMEZONE VARCHAR(50)\n             \"\"\")\ncur.close()\nconn.commit()\n</code></pre> <pre><code>'''\n\n[step 2] : ADD employee_id timezones\n\n'''\n\n# \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0441\u043e\u0431\u0440\u0430\u043d\u043d\u0443\u044e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u0432 \u0442\u0430\u0431\u043b\u0438\u0446\u0443 EMPLOYEE\n\nconn = psycopg2.connect(database = \"postgres\",\n                        user = \"postgres\",\n                        port = 5433)\n\n# say we store data in dictionaries (lists are just used for convenience)\ntimezone =  [\"UTC+3\",\"UTC-5\",\"UTC+3\",\"UTC+3\",\"UTC+3\",\"UTC-5\",\"UTC-5\"]\nids = [i for i in range(1,8)]\nnew_data = dict(zip(ids,timezone))\n\n# update each row in the\ncur = conn.cursor()\nfor ii,i in enumerate(new_data.items()):\n    cur.execute(f\"UPDATE EMPLOYEE SET TIMEZONE = '{timezone[ii]}' WHERE ID = {ids[ii]}\")\n\ncur.close()\nconn.commit()\n</code></pre> <p>Having added information about the timezones for each employee. Now lets create a table in which each employee from EMPLOYEE will be able to fill out their daily working hours information. One condition that needs to be mentioned is that our table will need to contain a constraint condition in order to prevent users from inputting working hours information multiple times for a particular date, hence the condition <code>CONSTRAINT pk_employee_date PRIMARY KEY (employee_id, date)</code></p> <pre><code>'''\n\nCREATE NEW TABLE FOR WORKING HOURS\n\n    add constraint to table to prevent user from adding duplicate entries\n    for a particular combination of (employee_id,date)\n\n'''\n\n# \u0442\u0435\u043f\u0435\u0440\u044c \u043d\u0430\u043c \u043d\u0443\u0434\u043d\u043e \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u043d\u043e\u0432\u0443\u044e \u0442\u0430\u0431\u043b\u0438\u0446\u0443 \u0432 \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u0438 \u0431\u0443\u0434\u0443\u0442\n# \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0442\u044c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0440\u043e\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u044b\u0445 \u0447\u0430\u0441\u043e\u0432\n# \u044d\u0434\u0438\u043d\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439 \u0432\u0430\u0436\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442 \u044d\u0442\u043e \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u044f CONTRAINT \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b\n# \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a \u0434\u0432\u0430\u0436\u0434\u044b \u043d\u0435 \u0432\u0432\u043e\u0434\u0438\u043b \u0442\u0443 \u0436\u0435 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e\n\nconn = psycopg2.connect(database = \"postgres\",\n                        user = \"postgres\",\n                        port = 5433)\n\ncur = conn.cursor()\ncur.execute(\"\"\"\n                    CREATE TABLE WORKED_HOURS (\n                        EMPLOYEE_ID INT,\n                        DATE DATE,\n                        HOURS_WORKED INT,\n                        START_TIME TIMESTAMP,\n                        END_TIME TIMESTAMP,\n                        CONSTRAINT pk_employee_date PRIMARY KEY (employee_id, date)\n                        )\n             \"\"\")\ncur.close()\nconn.commit()\n</code></pre> <p>Now lets go into the main functionality class which each employee is ought be be using, lets assume all of them know how to use python. Class store_working_hours contains the following methods:</p> <ul> <li>weekly_info gives the employee information about the current week's working days (mon-fri) returning a dictionary format which the user can fill out using simple dictionary notation for the 5 keys corresponding to each day of the current week.</li> <li>store_hours allows the user to add data to the database table WORKED_HOURS, taking as an argument a dictionary returned in weekly_info</li> <li>reset_weeklyhours allows us to reset the weekly working hours for the employee stored in WORKED_HOURS </li> <li> <p>extract_weekly_data is a simple method that will allow us to extract the number of working hours for all employees in all departments for a specified time period (start_date,end_date)</p> <ul> <li>To do this on a code level, we first need to create a datetime range, lets call it calendar which will define 120 hours of data</li> <li>Convert the local time for each user to a specific time zone (I decided to use UTC), an example of this <code>(wh.START_TIME AT TIME ZONE 'UTC') AT TIME ZONE e.TIMEZONE AS START_TIME</code></li> <li>Add the data available from WORKED_HOURS into our calendar and do a head count for the number of employees working at one time for each calendar hour using <code>count(distinct) with group by date</code></li> </ul> </li> </ul> <pre><code>'''\n\n\u0422\u0435\u043f\u0435\u0440\u044c \u0433\u043b\u0430\u0432\u043d\u044b\u0439 \u043a\u043b\u0430\u0441\u0441 \u0434\u043b\u044f \u0444\u0443\u043d\u0446\u0438\u043e\u043d\u0430\u043b\u0430 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 WORKED_HOURS\n\n[weekly_info] : \u0434\u0430\u0435\u0442 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044e dict \u0441 \u0434\u0430\u0442\u0430\u043c\u0438 \u044d\u0442\u043e\u0439 \u043d\u0435\u0434\u0435\u043b\u0438, \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043e\u043d \u0437\u0430\u043f\u043e\u043b\u043d\u044f\u0435\u0442\n                \u044d\u0442\u043e \u043d\u0430 \u0441\u043b\u0443\u0447\u0430\u0435 \u0435\u0441\u043b\u0438 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0434\u0438\u043d \u0440\u0430\u0437 \u0437\u0430\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u0442\u0430\u0431\u0435\u043b\u044c \u0432 \u043a\u043e\u043d\u0446\u0435 \u043d\u0435\u0434\u0435\u043b\u0438\n\n                \u041f\u0440\u0438\u043c\u0435\u0440 return:\n                {'2023-12-25': None, '2023-12-26': None, '2023-12-27': None, '2023-12-28': None, '2023-12-29': None}\n\n[store_hours] : \u0434\u0430\u0435\u0442 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044e \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0437\u0430\u0433\u0440\u0443\u0437\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 dict \u0432 WORKED_HOURS\n\n                \u041f\u0440\u0438\u043c\u0435\u0440:\n\n                        # \u041f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u0432\u0432\u043e\u0434\u0438\u0442 \u0432 \u0431\u0434 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0430\u0431\u043e\u0447\u0438\u0445 \u0447\u0430\u0441\u043e\u0432\n\n                        weekly_data['2023-12-25'] = 8\n                        weekly_data['2023-12-26'] = 8\n                        weekly_data['2023-12-27'] = 8\n                        weekly_data['2023-12-28'] = 8\n                        weekly_data['2023-12-29'] = 8\n\n                        store_data.store_hours(1,weekly_data)  # store weekly data\n\n                \u041f\u0440\u0438 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u043c\u043e\u0436\u0435\u0442 \u043a\u043e\u0440\u0435\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0432\u0432\u043e\u0434\u043d\u0443\u044e dict \u0438 \u043e\u043f\u044f\u0442\u044c\n                \u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0442\u044c \u0431\u0434\n\n[reset_weeklyhours] : \u0443\u0434\u0430\u043b\u044f\u0435\u0442 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u043d\u0438\u043a\u0430 (\u043d\u0430 \u044d\u0442\u043e\u0439 \u043d\u0435\u0434\u0435\u043b\u0438)!\n\n[store_today] : \u041f\u0440\u0438 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u044c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u043c\u043e\u0436\u0435\u0442 \u043e\u0442\u0447\u0438\u0442\u043e\u0432\u0430\u0442\u0441\u044f \u043a\u0430\u0436\u0434\u044b\u0439 \u0434\u0435\u043d\u044c\n\n[extract_weekly_data] : \u0414\u043b\u044f \u043e\u0442\u0447\u0435\u0442\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f\n\n    \"\"\n    \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043e\u0442\u0447\u0435\u0442\n    \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u0442 \u043e\u0442\u0441\u043b\u0435\u0434\u0438\u0442\u044c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0447\u0430\u0441\u0430 \u0440\u0430\u0431\u043e\u0447\u0435\u0439 \u043d\u0435\u0434\u0435\u043b\u0438.\n    \u0414\u043e\u043f\u0443\u0441\u043a\u0430\u0435\u0442\u0441\u044f \u0432\u044b\u0432\u043e\u0434\u0438\u0442\u044c 0 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432.\n    \"\"\n\n\u041f\u043e\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u0437\u0430\u0434\u0430\u0447\u0438 \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u043d\u0435 \u044f\u0441\u043d\u0430\u044f, \u043d\u043e \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u0442\u0430\u043a: \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0441\u043b\u043e\u0432\u043e \"\u043e\u0442\u0441\u043b\u0435\u0434\u0438\u0442\u044c\"\n\u0438 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0438\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043e \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u043c \u043f\u043e\u044f\u0441\u0435 \u043d\u0430\u043c\u0438\u043a\u0430\u0435\u0442 \u043d\u0430 \u0442\u043e \u0447\u0442\u043e \u043d\u0443\u0436\u043d\u043e \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u0438\u0442\u044c \u0432\u0441\u0435 \u043f\u043e\u044f\u0441\u0430\n\u0432 \u043e\u0434\u0438\u043d \u043f\u043e\u044f\u0441 \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u043e\u0434\u0438\u043d \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u043c\u043e\u0433 \u043f\u043e\u043d\u044f\u0442\u044c \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0431\u043e\u0442\u043d\u0438\u0443\u043e\u0432 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u043e\u0432\u0430\u043b\u0438\n\u043d\u0430 \u0440\u0430\u0431\u043e\u0442\u0435 \u0432 \u044d\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u043c \u043f\u043e\u044f\u0441\u0435 (\u043f\u0440\u0435\u0434\u043f\u043e\u043b\u043e\u0436\u0438\u043c \u0447\u0442\u043e \u044d\u0442\u043e UTC+0)\n\n        \u043f\u0440\u0438\u043c\u0435\u0440 \u0438\u0437 \u0432\u044b\u0433\u0440\u0443\u0437\u043a\u0438:\n\n        [\n        (datetime.datetime(2023, 12, 25, 0, 0), 0)\n        (datetime.datetime(2023, 12, 25, 1, 0), 0)\n        (datetime.datetime(2023, 12, 25, 2, 0), 0)\n        (datetime.datetime(2023, 12, 25, 3, 0), 0)\n        (datetime.datetime(2023, 12, 25, 4, 0), 0)\n        (datetime.datetime(2023, 12, 25, 5, 0), 0)\n        (datetime.datetime(2023, 12, 25, 6, 0), 4)\n        (datetime.datetime(2023, 12, 25, 7, 0), 4)\n        (datetime.datetime(2023, 12, 25, 8, 0), 4)\n        (datetime.datetime(2023, 12, 25, 9, 0), 3)\n        (datetime.datetime(2023, 12, 25, 10, 0), 3)\n        (datetime.datetime(2023, 12, 25, 11, 0), 1)\n        ...]\n\n\u041a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0438 \u0441\u043b\u0435\u0434\u0443\u044e\u0442 \u0432 \u0441\u0430\u043c\u043e\u043c \u043c\u0435\u0442\u043e\u0434\u0435\n\n'''\n\nconn = psycopg2.connect(database = \"postgres\",\n                        user = \"postgres\",\n                        port = 5433)\n\nclass store_working_hours:\n\n    def __init__(self):\n        pass\n\n    '''\n\n    Show weekly dictionary\n\n        generates and returns weekly dictionary, which users fill out and return\n        into store_hours\n\n    '''\n\n    def weekly_info(self):\n        current_date = datetime.today() # # Get current date\n        start_date = current_date - timedelta(days=current_date.weekday()) # monday\n        end_date = start_date + timedelta(days=4) # friday\n\n        # Print the dates from Monday to Friday\n        lst = ['monday','tuesday','wednesday','thursday','friday']; counter = 0\n        lst_dates = []; print('')\n        while start_date &lt;= end_date:\n            lst_dates.append(start_date.strftime(\"%Y-%m-%d\"))\n            start_date += timedelta(days=1)\n            counter += 1\n\n        return {key: None for key in lst_dates}\n\n    '''\n\n    Store Weekly Hours\n\n        requires employee_id : [int]\n           \"     weekly_hours : [dict]\n\n\n\n    '''\n\n    def store_hours(self,employee_id:int,weekly_hours:dict):\n\n        try:\n            ints_cond = all(isinstance(value, int) for value in weekly_hours.values())\n        except:\n            print('all values need to be integers')\n            ints_cond = False\n\n        # store the date into the database if all entries have been filled out &amp; are integers\n\n        if(ints_cond):\n\n            try:\n\n                conn = psycopg2.connect(database = \"postgres\",\n                                        user = \"postgres\",\n                                        port = 5433)\n                cur = conn.cursor()\n\n                # add new entry or modify current entry\n                for date,hours in weekly_hours.items():\n\n                    # define start and final time\n                    current_date = datetime.now().date(); start_time = '9:00 AM'\n                    start_datetime_str = str(date) + ' ' + start_time\n                    start_datetime = datetime.strptime(start_datetime_str, '%Y-%m-%d %I:%M %p')\n                    final_datetime = start_datetime + timedelta(hours=hours)\n\n                    query = f\"\"\"INSERT INTO WORKED_HOURS (EMPLOYEE_ID,DATE,HOURS_WORKED,START_TIME,END_TIME) VALUES ({employee_id}, '{date}', {hours}, '{start_datetime}', '{final_datetime}')\"\"\"\n                    cur.execute(query)\n                    cur.close()\n                    conn.commit()\n\n            except:\n\n                # reset user's weekly hours\n                self.reset_weeklyhours(employee_id)\n\n                conn = psycopg2.connect(database = \"postgres\",\n                                        user = \"postgres\",\n                                        port = 5433)\n                cur = conn.cursor()\n\n                # add new entry or modify current entry\n                for date,hours in weekly_hours.items():\n\n                    # define start and final time\n                    current_date = datetime.now().date(); start_time = '9:00 AM'\n                    start_datetime_str = str(date) + ' ' + start_time\n                    start_datetime = datetime.strptime(start_datetime_str, '%Y-%m-%d %I:%M %p')\n                    final_datetime = start_datetime + timedelta(hours=hours)\n\n                    query = f\"\"\"INSERT INTO WORKED_HOURS (EMPLOYEE_ID,DATE,HOURS_WORKED,START_TIME,END_TIME) VALUES ({employee_id}, '{date}', {hours}, '{start_datetime}', '{final_datetime}')\"\"\"\n                    cur.execute(query)\n\n                cur.close()\n                conn.commit()\n\n    '''\n\n    Delete all entries from Database for employee_id\n\n        requires employee_id : [int]\n\n    '''\n\n    def reset_weeklyhours(self,employee_id:int):\n\n        conn = psycopg2.connect(database = \"postgres\",\n                                user = \"postgres\",\n                                port = 5433)\n        cur = conn.cursor()\n\n        # generate weekly\n        current_date = datetime.today() # # Get current date\n        start_date = current_date - timedelta(days=current_date.weekday()) # monday\n        end_date = start_date + timedelta(days=4) # friday\n\n        # Print the dates from Monday to Friday for this week\n        lst = ['monday','tuesday','wednesday','thursday','friday']; counter = 0\n        lst_dates = [];\n        while start_date &lt;= end_date:\n            lst_dates.append(start_date.strftime(\"%Y-%m-%d\"))\n            start_date += timedelta(days=1)\n\n        # delete from database all the relevant entries\n        for date in lst_dates:\n            query = f\"DELETE FROM WORKED_HOURS WHERE EMPLOYEE_ID = {employee_id} AND DATE = '{str(date)}'\"\n            cur.execute(query)\n            conn.commit()\n\n    '''\n\n    Stores the number of hours a user has worked today\n\n        requires employee_id : [int]\n                  hours      : [int]\n\n    '''\n\n    def store_today(self,employee_id:int,hours:int):\n\n        conn = psycopg2.connect(database = \"postgres\",\n                                user = \"postgres\",\n                                port = 5433)\n        cur = conn.cursor()\n\n        # define start and final time\n        current_date = datetime.now().date()\n        start_time = '9:00 AM'\n\n        start_datetime_str = str(current_date) + ' ' + start_time\n        start_datetime = datetime.strptime(start_datetime_str, '%Y-%m-%d %I:%M %p')\n        final_datetime = start_datetime + timedelta(hours=hours)\n\n        query = f\"SELECT COUNT(*) FROM WORKED_HOURS WHERE EMPLOYEE_ID = {employee_id} AND DATE = '{str(current_date)}'\"\n        cur.execute(query)\n        data = cur.fetchall()[0][0]\n\n        if(data == 0):\n            # add new entry or modify current entry\n            query = f\"\"\"INSERT INTO WORKED_HOURS (EMPLOYEE_ID,DATE,HOURS_WORKED,START_TIME,END_TIME) VALUES ({employee_id}, '{str(current_date)}', {hours}, '{start_datetime}', '{final_datetime}')\"\"\"\n            cur.execute(query)\n            cur.close()\n            conn.commit()\n        else:\n            # modify existing entry\n            print('already in database, updating information!')\n            query = f\"UPDATE WORKED_HOURS SET END_TIME = '{final_datetime}', HOURS_WORKED = {hours} WHERE EMPLOYEE_ID = {employee_id} AND DATE = '{str(current_date)}'\"\n            cur.execute(query)\n            conn.commit()\n\n    '''\n\n    Extracts the employees present for a given period\n\n        requires start_date : [str]\n                  end_date  : [str]\n\n        returns list of rows data : [list]\n\n    '''\n\n    def extract_weekly_data(self,start_date:str,end_date:str):\n\n        conn = psycopg2.connect(database = \"postgres\",\n                                user = \"postgres\",\n                                port = 5433)\n        cur = conn.cursor()\n\n        query = f\"\"\"\n                    -- \u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043a\u0430\u043b\u0435\u043d\u0434\u0430\u0440\u044c \u0434\u043b\u044f start_date + 120 \u0447\u0430\u0441\u043e\u0432 (mon-fri)\n                    WITH calendar AS (\n                        SELECT '{start_date} 00:00:00'::timestamp + value * interval '1 hour' AS cal_date\n                        FROM generate_series(0, 120, 1) AS value\n                    ),\n\n                    -- \u0414\u043e\u0431\u0430\u0432\u0438\u043c TIMEZONE, \u0438 \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u043e\u0435 \u0432\u0440\u0435\u043c\u044f \u043d\u0430 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435 \u0432\u0440\u0435\u043c\u044f, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u0434\u043b\u044f\n                    -- \u0440\u0430\u0431\u043e\u0442\u043d\u0438\u043a\u0430 \u043d\u0430\u0445\u043e\u0434\u044f\u0449\u0438\u0439\u0441\u044f \u0432 \u043f\u043e\u044f\u0441\u0435 UTC\n                    combined AS (\n                    SELECT\n                      wh.EMPLOYEE_ID,\n                      e.NAME,\n                      wh.DATE,\n                      (wh.START_TIME AT TIME ZONE 'UTC') AT TIME ZONE e.TIMEZONE AS START_TIME,\n                      (wh.END_TIME AT TIME ZONE 'UTC') AT TIME ZONE e.TIMEZONE AS END_TIME\n                    FROM\n                      WORKED_HOURS wh\n                    JOIN EMPLOYEE e ON e.ID = wh.EMPLOYEE_ID\n                    WHERE DATE BETWEEN '{start_date}' AND '{end_date}'\n                    )\n\n                    -- \u043d\u0430\u0439\u0434\u0435\u043c \u043f\u0435\u0440\u0435\u0441\u0435\u0447\u0435\u043d\u0438\u0435 \u043a\u0430\u043b\u0435\u043d\u0434\u0430\u0440\u044f \u0441 \u0440\u0430\u0431\u043e\u0447\u0438\u043c \u0432\u0440\u0435\u043c\u0435\u043d\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f BETWEEN \u0438 AND \u0438 \u0441\u0433\u0440\u0443\u043f\u043f\u0438\u0440\u0443\u0435\u043c \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\n                    SELECT cal_date, COUNT(DISTINCT EMPLOYEE_ID) AS headcount\n                    FROM      calendar\n                    LEFT JOIN combined c\n                           ON calendar.cal_date BETWEEN c.start_time AND c.end_time\n                    GROUP BY cal_date\n                \"\"\"\n\n        cur.execute(query)\n        data = cur.fetchall()\n        return data   # can be converted straight into dataframe\n</code></pre> <p>Now lets assume our employees have created some entries for the working period ('2023-12-25','2023-12-29') using the method store_hours, which takes in the employee identifier and hours worked in a dictionary forat. I'll just load the data which is exported from extract_weekly_data from a presaved dataframe df</p> <p>From this data we can plot and visualise the number of working employees at any given time in a specific timezone.</p> <pre><code>'''\n\nFor Analysis for working hours\n\n'''\n\nimport plotly.express as px\nimport pandas as pd\n\n# instantiate class\n# store_data = store_working_hours()\n# lst_date = store_data.extract_weekly_data('2023-12-25','2023-12-29')\n# df = pd.DataFrame(lst_date,columns=['datetime','count'])\n# df['date'] = df['datetime'].dt.date\n# df['time'] = df['datetime'].dt.hour\ndf = pd.read_csv('temp.csv')\n# df.to_csv('temp.csv',index=False)\n\n# \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u043d\u0430 \u0440\u0430\u0431\u043e\u0442\u0435 \u043f\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 UTC+0\nfig = px.line(df,x='datetime',y='count',template='plotly_white',width=900,height=400,\n       title='\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u043d\u0430 \u0440\u0430\u0431\u043e\u0442\u0435 \u043f\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 UTC+0')\nfig.show(\"png\")\n\n# + \u0421\u0443\u043c\u043c\u0430 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u0437\u0430 \u043d\u0435\u0434\u0435\u043b\u044e\nfig = px.bar(df,x='time',y='count',template='plotly_white',width=900,height=400,color='date',\n       color_discrete_sequence=px.colors.qualitative.G10,\n       title='[\u0421\u0443\u043c\u043c\u0430] \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u043d\u0430 \u0440\u0430\u0431\u043e\u0442\u0435 \u043f\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 UTC+0')\nfig.show(\"png\")\n</code></pre> <p></p> <p></p>","tags":["sql","interview"]},{"location":"blog/2024/03/24/sql-analytics-problem.html#concluding-remarks","title":"Concluding Remarks","text":"<p>So there we go, we have made a simple script that will allow us to count the number of employees present at work at any given period in a company which operates in different timezone based on the data that the users working hour data reported by each employee. This would allow the company to monitor working load, and note periods at which there are too few employees working to keep the business operating effectively. There are probably ways to improve the code, but this is what I managed in a 24 hour timeframe.</p> <p>Thank you for reading!</p> <p>Any questions or comments about the above post can be addressed on the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning or simply below!</p>","tags":["sql","interview"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html","title":"Uplift Modeling Basics","text":"<p>Uplift modeling is a predictive modeling technique that aims to identify the individuals who are most likely to respond positively to a specific treatment or intervention. This technique is particularly useful in marketing and customer relationship management, where the goal is to target customers who are likely to be influenced by a marketing campaign or offer. By distinguishing between those who are positively influenced by the treatment and those who are not, uplift modeling helps organizations optimize their targeting strategies and maximize the return on investment of their marketing efforts.</p> <ul> <li> Open Kaggle Notebook</li> <li> GitHub Repository</li> </ul>","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#introduction","title":"Introduction","text":"","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#uplift-modeling","title":"Uplift Modeling","text":"<p>What is the main issue at hand:</p> <ul> <li>We cannot do an action &amp; not do an action at the same time to the same person</li> </ul> <p>So what it this modeling approach about:</p> <p>Uplift modeling is a technique that allows us to identify the subset of objects </p> <ul> <li>who upon being influenced by an event/action will do some action</li> <li>and if not influenced will not do the action</li> </ul>","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#uplift-modeling-example","title":"Uplift Modeling Example","text":"<p>Imagine we are selling a product and need to decide to whom we will be advertising, we have some constraints and cannot show it to all target audiences, </p> <ul> <li>we would like to find clients who will buy the product if they see our advertisement </li> <li>and will and not buy it if they don't see it</li> </ul>","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#uplift-modeling-components","title":"Uplift Modeling Components","text":"<p>In uplift modeling we need three components:</p> <p>Have two arrays we will be working with; Treatment Array, Target Array and standard customer related feature matrix</p> <ul> <li>The Treatment Array is a binary vector, where we have no influence (0) and influenced (1)</li> <li>The Target Array is also a binary vector, where we have no action (0) and action is made (1)</li> <li>The standard feature matrix (like other machine learning problems) is a matrix that contains features</li> </ul>","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#what-to-remember","title":"What to remember","text":"<p>So some important things to note in the context of uplift modeling:</p> <ul> <li>To model which users will do an action when influenced &amp; users which will not do an action when not influences we need to have aside from a standard feature matrix two additional vectors; Treatment Array (users were identified and interacted with) &amp; the result of our interaction with them, stored in the Target Array</li> <li>Having a trained model will enable us to identify on unseen data (without a treatment or target vector) the uplift value for a group for which we have a set of features used in training</li> <li>Our aim is to target influencible clients (those who upon being interacted with will commit a target action) and those who can be positively influenced (when not interacted with will not conduct a target action).</li> </ul>","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#loading-data","title":"Loading Data","text":"","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#kevin-hillstrom-dataset","title":"Kevin Hillstrom Dataset","text":"<p>Our dataset is available in the sklift library, called Kevin Hillstrom Dataset. You can also try other sample problems shown below:</p> <pre><code># Kevin Hillstrom Dataset\nfrom sklift.datasets import fetch_x5, fetch_lenta, clear_data_dir, fetch_megafon, fetch_hillstrom\n\ndata = fetch_hillstrom()\n</code></pre> <p>Lets introduce ourselves to the dataset we will be using in our notebook, by looking at the description provided with the dataset</p> <p>This dataset contains 64,000 customers who last purchased within twelve months. The customers were involved in an e-mail test.</p> <ul> <li>1/3 were randomly chosen to receive an e-mail campaign featuring Mens merchandise.</li> <li>1/3 were randomly chosen to receive an e-mail campaign featuring Womens merchandise.</li> <li>1/3 were randomly chosen to not receive an e-mail campaign.</li> </ul> <p>During a period of two weeks following the e-mail campaign, results were tracked. Your job is to tell the world if the Mens or Womens e-mail campaign was successful.</p> <p>Having read the above, lets summarise the important bits:</p> <ul> <li>We have 64000 customers who recently made a purchase, for these customers we have a matrix of features relevant to each of these customers</li> <li>We randomly send emails to these customers (treatment array); we have an array containing a marketing campaign defined subset groupings</li> <li>Finally we have a target containing post marketing campaign monitored results (confirmations of whether the email campaign worked or not)</li> </ul>","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#data-insight","title":"Data Insight","text":"","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#feature-matrix","title":"Feature Matrix","text":"<p>Let's also look at the feature matrix available to us:</p> <pre><code>+-------+---------------+-------+----+------+---------+------+-------+\n|recency|history_segment|history|mens|womens| zip_code|newbie|channel|\n+-------+---------------+-------+----+------+---------+------+-------+\n|     10| 2) $100 - $200| 142.44|   1|     0|Surburban|     0|  Phone|\n|      6| 3) $200 - $350| 329.08|   1|     1|    Rural|     1|    Web|\n|      7| 2) $100 - $200| 180.65|   0|     1|Surburban|     1|    Web|\n|      9| 5) $500 - $750| 675.83|   1|     0|    Rural|     1|    Web|\n|      2|   1) $0 - $100|  45.34|   1|     0|    Urban|     0|    Web|\n+-------+---------------+-------+----+------+---------+------+-------+\n</code></pre>","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#treatment-array","title":"Treatment Array","text":"<p>The treatment array contains text data which we will need to convert into numerical data, we have information about the marketing campaign, in which we roughtly speaking to do one of two things; send an email about the marketing campaign or don't send anything</p> <pre><code>t.sample(5)\n\n3947       Mens E-Mail\n48105      Mens E-Mail\n15614      Mens E-Mail\n58595        No E-Mail\n21571    Womens E-Mail\n</code></pre>","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#target-array","title":"Target Array","text":"<p>The target contains the result of the email marketing campaign influence and is already in numerical format, it reflects whether the campaign was successful or not</p> <pre><code>y.sample(5)\n\n19624    0\n38660    0\n49813    0\n23809    0\n45957    1\n</code></pre>","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#preprocessing","title":"Preprocessing","text":"","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#problem-simplification","title":"Problem Simplification","text":"<p>Lets do a little bit of preprocessing and problem simplification. As we saw in the above data, we have three categories in our treatment vector. Lets simplify it to just a binary case and not differentiate the male and female target cases, ie. marketing email has been sent or not sent. </p> <pre><code>t = t.map({'Womens E-Mail':1, 'Mens E-Mail':1, 'No E-Mail':0})\nt.head()\n</code></pre>","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#train-test-splitting","title":"Train Test Splitting","text":"<p>Lets also split the data into training &amp; test subsets, we will need some unseen data to validate our models.</p> <pre><code>X_train, X_test, y_train, y_test, t_train, t_test = train_test_split(X, y, t, test_size=0.3, random_state=42)\n</code></pre>","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#categorical-feature-treatment","title":"Categorical Feature Treatment","text":"<p>We have to also pay attention to categorical features which are present in our feature matrix, a common and most straightforward approach is to use One Hot Encoding, which will be applied to three columns. </p> <p>We will fit the one hot encoder on our training dataset, and only apply it to the test dataset</p> <pre><code>cat_columns = ['history_segment', 'zip_code', 'channel']\nenc = OneHotEncoder(sparse=False)\n\nX_train_cat = enc.fit_transform(X_train[cat_columns])\nX_train_cat = pd.DataFrame(X_train_cat, \n                           index=X_train.index,\n                           columns=enc.get_feature_names_out(cat_columns))\n\nX_test_cat = enc.transform(X_test[cat_columns])\nX_test_cat = pd.DataFrame(X_test_cat, \n                          index=X_test.index,\n                          columns=enc.get_feature_names_out(cat_columns))\n\nX_train = pd.concat([X_train_cat, X_train.drop(cat_columns, axis=1)], axis=1)\nX_test = pd.concat([X_test_cat, X_test.drop(cat_columns, axis=1)], axis=1)\n</code></pre>","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#modeling-approaches","title":"Modeling Approaches","text":"<p>Now that we have our data ready, lets talk libraries and approaches. There is a commonly used uplift modeling library called scikit-uplift, its based on scikit-learn machine learning models, but modified for uplift modeling. Lets remind ourselves of what the modeling actually wants to achieve:</p> <p>Abstract</p> <p>Uplift modeling focuses on predicting the impact of a treatment or intervention on an individual's behavior</p> <p>scikit-uplift has a number of different approaches for uplift modeling, you can find the models in the following link, we'll look at two of the three approaches used in the library</p>","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#one-model-approach","title":"One Model Approach","text":"<p>Starting with <code>s-learner</code> approach, we train two separate models</p> <ul> <li>We train a base model with all base features and the treatment vector (w), taking the target vector (y) as our independent variable</li> <li>Apply the model (predict) assuming we have interacted with all customers, ie. (t=1 for all customers), and ask to return the probability of a successful outcome (y=1) for this group</li> <li>Apply the model (predict) again but assuming that these has been no interaction with any customer (t=0 for all customers)</li> </ul> s-model approach <p>The difference between these two vectors will be taken as our uplift, to be more specific:</p> <p>model generates uplift scores that represent the estimated impact of a treatment on each individual's behavior</p> <p>The s-learner model can be used by importing SoloModel from <code>from sklift.models import SoloModel</code>, we just need to specify the base model we will be using in the two models and wrap it with SoloModel. Like other sklearn models, we use fit,predict methods, but with an additional input <code>t_train</code></p> <pre><code>name = 'slearner'\n\nbase_model = RandomForestClassifier(random_state=42)\nuplift_model = SoloModel(base_model)\nuplift_model = uplift_model.fit(X_train, y_train, t_train)\n\n# store the uplift values\nmodel_predictions[name] = uplift_model.predict(X_test)\n</code></pre> <p>We obtain our uplift values:</p> <pre><code>uplift_model.predict(X_test)\n# array([-0.03, -0.31, -0.01, ...,  0.03,  0.56,  0.14])\n</code></pre> <p>Interpretation of Results</p> <ul> <li>If the results are positive for a particular entry, it indicates that the treatment (email marketing campaign) has a positive effect on this individual and visa versa.</li> <li>In terms of magnitude, a larger positive (or negative) uplift score implies a more significant impact of the treatment on the individual's likelihood of a positive outcome.</li> </ul>","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#two-independent-model-approach","title":"Two Independent Model Approach","text":"<p>The two model approach, t-learner is similar to a one model approach, however instead of training a single model on all the data, we train two models, on two different subsets of data:</p> <ul> <li>One for the control group (no interaction, t=0)</li> <li>Another for the test group (there was interaction, t=1)</li> </ul> t-model approach (independent models) <p>In order to obtain the uplift, we apply the model on the test set like in the s-learner, with the exception that we dont add the additional treatment feature, instead we are using two independent models. The difference in predict_proba between these two models will be our uplift value.</p> <pre><code>from sklift.models import TwoModels\n\nname = 'tlearner'\n\n# control group\nbasic_model_control = RandomForestClassifier(random_state=42)\n\n# test group\nbasic_model_test = RandomForestClassifier(random_state=42)\n\nuplift_model = TwoModels(basic_model_test, basic_model_control, method='vanilla')\nuplift_model = uplift_model.fit(X_train, y_train, t_train)\n\n# store the uplift values\nmodel_predictions[name] = uplift_model.predict(X_test)\n</code></pre> <p>We obtain our uplift values:</p> <pre><code>uplift_model.predict(X_test)\n# array([ 0.        , -0.41      , -0.01      , ...,  0.08142857,\n#        0.59      ,  0.22      ])\n</code></pre> <p>We can also plot the uplift values predicted by both modeling approaches:</p> Comparing uplift values for s-learner &amp; t-learner <p>We can notice a very minor binomial tendency in the figure, a large portion of users having a low uplift values around 0-0.25 &amp; a secondary group around 0.3-0.75, in terms of model variation, we can clearly note that t-learner is less concentrated with values at 0 and instead has more values in the region 0-0.25, which would indicate that the model predicts on average that the user is more confident that the user is positively influenced than the s-model</p>","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#conclusion","title":"Conclusion","text":"","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#metric-evaluation","title":"Metric Evaluation","text":"<p>So now that we have obtained our uplift values, we ought to evaluate how well our modeling is. When it comes to uplift modeling problems, we can turn to a metric called uplift@k. </p> <p>The metric ranges from -1 to 1, where 1 is the best and -1 represents a model that doesnt work. A value of 0 is equivalent to a random model. Values in the range 0.05 to 1 can be considered as working models.</p> <p>The evaluation methodology is as follows:</p> <ul> <li>We take k objects with the highest uplift values</li> <li>Divide the subset into a contol (t=0) and test (t=1) group</li> <li>Evaluate the average target y for each individually</li> <li> <p>Find the difference:</p> \\[ uplift@k = \\bar{y}_{k \\space test} - \\bar{y}_{k \\space control} \\] \\[ \\bar{y}_k = \\frac{1}{n} \\sum_{i=1}^{k}{uplift_i,}  \\] </li> </ul> <p>There are some other metrics you can look into as well:</p> <ul> <li>Area Under Uplift Curve</li> <li>Area Under Qini Curve</li> <li>Weighted average uplift</li> </ul> <pre><code>results = dict()\nfor model_name, preds in model_predictions.items():\n\n    up_k_best = uplift_at_k(y_true=y_test, \n                            uplift=preds, \n                            treatment=t_test, \n                            strategy='overall', \n                            k=0.2)\n\n    results[model_name] = round(up_k_best,4)\n</code></pre> <pre><code>results\n{'s_learner': 0.0526, 't_learner': 0.0584}\n</code></pre> <p>What we can conclude is that the t-learner method has a slightly higher metric values compared to s-learer, which indicates that it is a slightly better modeling approach.</p>","tags":["machine learning"]},{"location":"blog/2024/04/07/uplift-modeling-basics.html#remarks","title":"Remarks","text":"<p>In this post we looked into a brief introduction into uplift modeling, which is a practical business task for which we can utilise machine learning &amp; use modeling for the prediction/identification of the subset of objects/dataset who upon being influenced by an event/action will do some action and if not influenced will not do the action</p> <p>We looked at two approaches s-learner &amp; t-learner modeling approaches and tried a sample problem Kevin Hillstrom Dataset</p> <p>The modeling approaches allow us to predict the lift value, and gives us to understanding how well the particular client is influenced based on historical data, the models can then be used on new unseen data.</p> <p>We can repeat the process similar to above using some other datasets such as: fetch_x5, fetch_lenta, fetch_megafon.</p> <p>Thank you for reading!</p> <p>Any questions or comments about the above post can be addressed on the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning or simply below!</p>","tags":["machine learning"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html","title":"Neural Collaborative Filtering","text":"<p>In this post we'll cover some of the basics of recommendation system approaches utilising neural networks. </p> <ul> <li> <p><code>Collaborative filtering</code> (CF) is a recommendation generation method that relies on user-item interactions to make predictions about what a user might like based on the historical data of user interaction with the item. </p> </li> <li> <p>We covered <code>collaborative filtration</code> using matrix factorisation is the notebook. </p> </li> <li> <p>Specifically we looked at methods such as <code>SVD</code> in order to generate the <code>user</code> and <code>item</code> matrices, these two matrices are then multiplied together to get the corresponding scores for each user, item pairing; a model-based approach. </p> </li> <li> <p><code>Neural Collaborative Filtering</code> (NCF) bears some similarity to CF whilst leveraging the benefits of deep learning techniques to enhance recommendation performance.</p> </li> </ul> <ul> <li> <p> ML-1M Dataset</p> </li> <li> <p> NeuMF Jupyter Notebook</p> </li> </ul>","tags":["recsys"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html#background","title":"Background","text":"","tags":["recsys"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html#aim","title":"Aim","text":"<ul> <li>Our aim is to create a neural network that will be able to generate recommendations for users, based on their previous interactions with a product. </li> <li>We will be using a dataset of movies with which users have explicitly interacted, having set ratings for movies</li> <li>Ultimately, we aim to replicate the network proposed in article</li> </ul>","tags":["recsys"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html#outline","title":"Outline","text":"<p>Here's what we will be doing in this post:</p> <ul> <li>We'll be using the dataset ML-1M, the dataset has been uploaded to Kaggle</li> <li>We'll be coding the Neural Collaborative Filtering Model from the article using PyTorch</li> <li>We'll also train the model(s) on the <code>training</code> dataset (positive samples) and we will need to create some negative samples as well (for the training set). To replicate the author results, we'll use the provided negative samples (for the test set)</li> <li>The problem will be formulated as a Binary Classification problem (similar to the DSSM notebook), positive samples (user has interacted with item) (label 1) and negative samples (user hasn't interacted) (label 0), so the model will learn to differentiate between interacted and not interacted, generating the embedding features, which will be used to generate scores.</li> <li>We'll be evaluating the models using common recommendation evaluation metrics HR and NDCG, on the <code>test</code> dataset </li> <li>Finally we'll compare the results to the article results</li> </ul>","tags":["recsys"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html#modeling-approaches","title":"Modeling Approaches","text":"","tags":["recsys"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html#problem-definition","title":"Problem definition","text":"<p>We will setup the the problem as a binary classification problem:</p> <ul> <li>We define positive samples for <code>user</code>, <code>item</code> combinations that exist, which implies that the user has rated the item (film in this case)</li> <li>Negative samples will need to be generated for <code>user</code>, <code>item</code> combinations which do not currently exist in the dataset </li> <li>Merging these two subset, we aim to create a model that will be able to predict whether a user has watched the movie or not</li> <li>The <code>test</code> set is a continuation of <code>train</code>, containing only the latest watched item by the user, and 99 items the user didn't watch (negative sample)</li> </ul>","tags":["recsys"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html#neumf-model-variations","title":"NeuMF model variations","text":"<p>The most important part is probably the neural network itself, lets examine what is contains. Based on the article, three different approaches are mentioned:</p> <ul> <li>MLP (Multilayer Perceptron)</li> <li>GMF (Generalised Matrix Factorisation)</li> <li>NeuMF (Combination of Both)</li> </ul> <p>We can visualise all three models in one figure, comments are added below the figure.</p>  ![](images/neumf.png)  <ul> <li>The left side of the figure show the <code>GMF</code> model. It only contains the embedding layers of both <code>user</code> &amp; <code>items</code>, the element wise product is then taken for the user and item vectors and output is fed into the linear layer</li> <li>The right side of the figure shows the <code>MLP</code> model, it starts with the embedding layers for both the <code>user</code> &amp; <code>items</code>, followed by the linear layers together with dropout/activation functions</li> </ul> <p>The NeuMF model contains both of these parts. They need to be concatenated together before been fed into the output <code>linear layer</code> </p>","tags":["recsys"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html#define-network-layers","title":"Define network layers","text":"<p>So we have a general overiew of how the neural network should look like, next using PyTorch, we'll put it into realisation. The model is essentially formed using two parts, <code>embedding</code> layers and <code>Sequential</code> segment.</p> Embedding Layers <p>First we need to define the embedding layers</p> <ul> <li>For layers which will connect with the Sequential MLP layers, we need to make sure the input size of the embedding layer matches the input of the Sequential layer</li> <li>For all layers we need to define <code>user_num</code> (number of users), <code>item_num</code> (number of items) and <code>factor_num</code> (number of factors)</li> <li>If we are using the <code>MLP</code> model, then we need to define how many layers will be used in the <code>Sequential</code> part of the model.</li> </ul> <pre><code>self.embed_user_GMF = nn.Embedding(user_num, factor_num) # LHS embedding matrix of user\nself.embed_item_GMF = nn.Embedding(item_num, factor_num) # LHS embedding matrix of items\nself.embed_user_MLP = nn.Embedding(\n        user_num, factor_num * (2 ** (num_layers - 1)))  # RHS embedding matrix of users to be connected to Sequential\nself.embed_item_MLP = nn.Embedding(\n        item_num, factor_num * (2 ** (num_layers - 1)))  # RHS embedding matrix of items to be connected to Sequential\n</code></pre> Sequential Layers <p>We also need the Sequential part of the model:</p> <ul> <li><code>input_size</code> for each layer is defined in a way that will allow us to setup multiple segments  of the MLP section. </li> <li>The MLP sequential section contains a <code>dropout layer</code>, a <code>linear layer</code> and a <code>ReLU</code> activation function.</li> </ul> <pre><code>MLP_modules = []\nfor i in range(num_layers):\n    input_size = factor_num * (2 ** (num_layers - i))\n    MLP_modules.append(nn.Dropout(p=self.dropout))\n    MLP_modules.append(nn.Linear(input_size, input_size//2))\n    MLP_modules.append(nn.ReLU())\nself.MLP_layers = nn.Sequential(*MLP_modules)\n</code></pre> Output layer <p>Finally we need to define the output linear layer. The </p> <pre><code>self.predict_layer = nn.Linear(predict_size, 1)\n</code></pre>","tags":["recsys"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html#initialised-layers","title":"Initialised layers","text":"<p>Having defined the layers of the neural network. Lets see how the layers will look after they have been initialised using a <code>factor_num</code> of 16.</p> MLPGMFNeuMF <pre><code>(embed_user_MLP): Embedding(6040, 64)\n(embed_item_MLP): Embedding(3706, 64)\n(MLP_layers): Sequential(\n    (0): Dropout(p=0.0, inplace=False)\n    (1): Linear(in_features=128, out_features=64, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.0, inplace=False)\n    (4): Linear(in_features=64, out_features=32, bias=True)\n    (5): ReLU()\n    (6): Dropout(p=0.0, inplace=False)\n    (7): Linear(in_features=32, out_features=16, bias=True)\n    (8): ReLU()\n)\n(predict_layer): Linear(in_features=16, out_features=1, bias=True)\n</code></pre> <pre><code>NCF(\n(embed_user_GMF): Embedding(6040, 16)\n(embed_item_GMF): Embedding(3706, 16)\n(predict_layer): Linear(in_features=16, out_features=1, bias=True)\n)\n</code></pre> <pre><code>NCF(\n(embed_user_GMF): Embedding(6040, 16)\n(embed_item_GMF): Embedding(3706, 16)\n(embed_user_MLP): Embedding(6040, 64)\n(embed_item_MLP): Embedding(3706, 64)\n(MLP_layers): Sequential(\n    (0): Dropout(p=0.0, inplace=False)\n    (1): Linear(in_features=128, out_features=64, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.0, inplace=False)\n    (4): Linear(in_features=64, out_features=32, bias=True)\n    (5): ReLU()\n    (6): Dropout(p=0.0, inplace=False)\n    (7): Linear(in_features=32, out_features=16, bias=True)\n    (8): ReLU()\n)\n(predict_layer): Linear(in_features=16, out_features=1, bias=True)\n)\n</code></pre>","tags":["recsys"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html#forward-method","title":"Forward method","text":"<p>Now lets define what will be part of the <code>forward pass operation</code></p> <ul> <li>The forward pass operation contains a logical selection condition based on the model that is chosen</li> <li>The only part not yet shown is the concatenation of the two segments <code>GMF</code> and <code>MLP</code>, it is defined by the following line</li> </ul> <pre><code>concat = torch.cat((output_GMF, output_MLP), -1)\n</code></pre> <p>And the <code>forward method</code> itself:</p> <pre><code>def forward(self, user, item):\n\n    # if not MLP (GMF/NeuMF)\n    if not self.model == 'MLP':\n        embed_user_GMF = self.embed_user_GMF(user)\n        embed_item_GMF = self.embed_item_GMF(item)\n        output_GMF = embed_user_GMF * embed_item_GMF\n\n    # if not GMF (MLP/NeuMF)\n    if not self.model == 'GMF':\n        embed_user_MLP = self.embed_user_MLP(user)\n        embed_item_MLP = self.embed_item_MLP(item)\n        interaction = torch.cat((embed_user_MLP, embed_item_MLP), -1)\n        output_MLP = self.MLP_layers(interaction)\n\n    # concatinate matrices\n    if self.model == 'GMF':\n        concat = output_GMF\n    elif self.model == 'MLP':\n        concat = output_MLP\n    else:\n        concat = torch.cat((output_GMF, output_MLP), -1)\n\n    prediction = self.predict_layer(concat)\n    return prediction.view(-1)\n</code></pre>","tags":["recsys"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html#the-dataset","title":"The Dataset","text":"","tags":["recsys"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html#load-dataset","title":"Load dataset","text":"<p>As mentioned we will be using the ML-1M dataset, the dataset contains thee main columns, user, item and its set rating.</p> <ul> <li>The data has been normalised, meaning the user and item values correspond to a mapped value, which differs from the original dataset.</li> <li>In the snippet below we load the <code>training</code> data, which is in the form of <code>user_id</code>, <code>item_id</code>, these are our <code>positive samples</code> (we know the user has interacted with the item)</li> <li>The data is then converted into a sparse matrix of size (user_num, item_num). For each user/item combination that exists we set a 1 and where it doesn't 0, effectively just creating a matrix which specifies whether the user has interacted with the item or not. This matrix will be used in the torch dataset when creating <code>negative samples</code></li> <li>The <code>test</code> dataset has some extra steps; the dataset we are reading contains one positive sample and 99 negative samples, giving us 100 samples per user</li> <li>As mentioned before, the test set will be used for metric evaluation (<code>hr</code> &amp; <code>ndcg</code>)</li> </ul> <pre><code>def load_all():\n\n    # load training dataset\n    train_data = pd.read_csv(\n        config.train_rating, \n        sep='\\t', header=None, names=['user', 'item'], \n        usecols=[0, 1], dtype={0: np.int32, 1: np.int32})\n\n    user_num = train_data['user'].max() + 1\n    item_num = train_data['item'].max() + 1\n\n    train_data = train_data.values.tolist()\n\n    # load ratings as a sparse matrix\n    train_mat = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n    for x in train_data:\n        train_mat[x[0], x[1]] = 1.0\n\n    # load test dataset\n    test_data = []\n    with open(config.test_negative, 'r') as fd:\n        line = fd.readline()\n        while line != None and line != '':\n            arr = line.split('\\t')\n            u = eval(arr[0])[0]\n            test_data.append([u, eval(arr[0])[1]])\n            for i in arr[1:]:\n                test_data.append([u, int(i)])\n            line = fd.readline()\n</code></pre>","tags":["recsys"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html#create-torch-dataset","title":"Create torch dataset","text":"<p>Since we are using <code>PyTorch</code>, we need to create a torch compatible <code>dataset</code> &amp; <code>data loader</code></p> <ul> <li>The torch dataset is where the features to be fed into the model are actually generated</li> <li>The class <code>NFCData</code> inputs the user, item pairs data, from <code>load_all</code>. These pairs are set positive labels, <code>1</code> </li> <li>In addition to these positive pairs we generate a specific number of negative samples <code>num_ng</code> for each user. </li> <li> <p>These negative samples make it into the data frame on the condition:</p> <ul> <li>A number is generated between 1 and the maximum item number</li> <li>If the combination (<code>user</code>,<code>item</code>) exists in the positive samples, we generate another number as above. These negative samples will have labels <code>0</code>. </li> </ul> </li> <li> <p>So we have a dataset of <code>positive samples</code> (user has watched) and <code>negative samples</code> (user hasn't watched) </p> </li> <li>Finally we add the special method <code>__getitem__</code>, which is used to get the data, when needed tracing the model.</li> </ul> <pre><code>class NCFData(data.Dataset):\n\n    def __init__(self, features, \n                num_item, train_mat=None, num_ng=0, is_training=None):\n        super(NCFData, self).__init__()\n\n        self.features_ps = features\n        self.num_item = num_item\n        self.train_mat = train_mat\n        self.num_ng = num_ng\n        self.is_training = is_training\n\n        # default label for both train/test\n        # replaced if train\n        self.labels = [0 for _ in range(len(features))]\n\n\n    def ng_sample(self):\n\n        '''\n\n        Adjust input data (features &amp; label)\n\n        '''\n\n        assert self.is_training, 'no need to sampling when testing'\n\n        self.features_ng = []\n        for x in self.features_ps:\n            u = x[0]\n            for t in range(self.num_ng):\n                j = np.random.randint(self.num_item)\n                while (u, j) in self.train_mat:\n                    j = np.random.randint(self.num_item)\n                self.features_ng.append([u, j])\n\n        labels_ps = [1 for _ in range(len(self.features_ps))]\n        labels_ng = [0 for _ in range(len(self.features_ng))]\n\n        self.features_fill = self.features_ps + self.features_ng\n        self.labels_fill = labels_ps + labels_ng\n\n    def __len__(self):\n        return (self.num_ng + 1) * len(self.labels)\n\n    def __getitem__(self, idx):\n\n        '''\n\n        Get item (user,item,label)\n\n        '''\n\n        features = self.features_fill if self.is_training \\\n                    else self.features_ps\n        labels = self.labels_fill if self.is_training \\\n                    else self.labels\n\n        user = features[idx][0]\n        item = features[idx][1]\n        label = labels[idx]\n        return user, item ,label\n</code></pre>","tags":["recsys"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html#model-preparation","title":"Model Preparation","text":"","tags":["recsys"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html#global-parameters","title":"Global parameters","text":"<ul> <li>We will be using a factor number of 32 dimensions</li> <li>And 4 negative samples <code>num_ng</code> are added per positive sample (maximum) for the training dataset</li> <li>The test dataset contains 99 negative samples already generated for us, so we define <code>test_num_ng</code> to 99</li> <li>20 iterations should be enough to get a picture of how well the models perform </li> <li>The selected model needs to be explicitly defined in <code>Config</code> as an input parameter</li> </ul> <p>The other parameters are defined as following:</p> <pre><code>@dataclass\nclass Config:\n\n    model : str\n    batch_size : int = 256  # training group size \n    factor_num : int = 32   # number of dimensions for user,item matrix\n    num_layers : int = 3    # number of MLP layer groups\n    test_num_ng : int = 99\n    num_ng : int = 4         # number of negative samples for each user \n    dropout : float = 0.0    # dropout percentage in MLP\n    lr : float = 0.001       # model learning rate\n    epochs : int = 20        # number of iterations \n    top_k = 10               # top k recommendations \n\nconfig = Config(model='GMF')\n</code></pre>","tags":["recsys"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html#evaluation-model-parameters","title":"Evaluation model parameters","text":"<p>During training, the model outputs <code>logits</code> (prediction). The loss function inputs these logits and binary <code>labels</code> (1/0) into the loss evaluation.</p> <pre><code>prediction = model(user, item)\nloss = loss_function(prediction, label)\n</code></pre> <p>We also need to define how we will interpret the model performance. Two metrics will be monitored <code>hr</code> and <code>ndcg</code>. The function <code>metrics</code> calculates these metrics; once the predictions are made for the batch (100 items) for each user, the positive sample is selected, and we also have our list of predictions recommendations from both of these we can determine these two metrics. </p> <pre><code># hitrate \ndef hit(gt_item, pred_items):\n    if gt_item in pred_items:\n        return 1\n    return 0\n\n# ndcg \ndef ndcg(gt_item, pred_items):    \n    if gt_item in pred_items:\n        index = pred_items.index(gt_item)\n        return np.reciprocal(np.log2(index+2))\n    return 0\n\ndef metrics(model, test_loader, top_k):\n\n    HR, NDCG = [], []\n    for user, item, label in test_loader:\n\n        user = user.cuda()\n        item = item.cuda()\n\n        predictions = model(user, item)\n        _, indices = torch.topk(predictions, top_k)\n        recommends = torch.take(\n                item, indices).cpu().numpy().tolist()\n        gt_item = item[0].item() # get positive sample only\n\n        hitval = hit(gt_item, recommends)\n        ngcgval = ndcg(gt_item, recommends)\n\n        HR.append(hitval)\n        NDCG.append(ngcgval)\n\n    return np.mean(HR), np.mean(NDCG)\n</code></pre>","tags":["recsys"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html#model-settings","title":"Model settings","text":"<p>Our labels contain two classes, so we'll use <code>BCE</code>. Effectively, we are doing binary classification. As for the optimiser, <code>Adam</code> will be used. A learning rate of 0.001 is used in all models.</p> <pre><code>model = NCF(user_num, \n            item_num, \n            config.factor_num, \n            config.num_layers, \n            config.dropout, \n            config.model)\n\nmodel.cuda()\nloss_function = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=config.lr)\n</code></pre>","tags":["recsys"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html#training-loop","title":"Training Loop","text":"<p>The training and evaluation loop is quite straightforward; we loop through all batches, for a predetermined number of iterations, evaluating the loss function and updating the model hyper parameters. Only then we evaluate the desired user metrics on the test set. One thing to note is that each iteration negative samples are generated for training. </p> <pre><code>count, best_hr = 0, 0\nfor epoch in range(config.epochs):\n    model.train() \n    start_time = time.time()\n    train_loader.dataset.ng_sample() # generate negative samples\n\n    for user, item, label in train_loader:\n        user = user.cuda()\n        item = item.cuda()\n        label = label.float().cuda()\n\n        model.zero_grad()\n        prediction = model(user, item)\n        loss = loss_function(prediction, label)\n        loss.backward()\n        optimizer.step()\n        count += 1\n\n    model.eval()\n    HR, NDCG = metrics(model, test_loader, config.top_k)\n\n    elapsed_time = time.time() - start_time\n    print(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + \n            time.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n    print(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n\n    if HR &gt; best_hr:\n        best_hr, best_ndcg, best_epoch = HR, NDCG, epoch\n        if config.out:\n            if not os.path.exists(config.model_path):\n                os.mkdir(config.model_path)\n            torch.save(model, \n                '{}{}.pth'.format(config.model_path, config.model))\n\nprint(\"End. Best epoch {:03d}: HR = {:.3f}, NDCG = {:.3f}\".format(\n                                    best_epoch, best_hr, best_ndcg))\n</code></pre>","tags":["recsys"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html#results","title":"Results","text":"<p>Finally let's compare the results of the three approaches. You can run your own code using the provided notebook and dataset</p>","tags":["recsys"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html#training-results","title":"Training results","text":"<p>MLP</p> <pre><code>...\nThe time elapse of epoch 018 is: 00: 01: 55 HR: 0.678   NDCG: 0.414\nThe time elapse of epoch 019 is: 00: 01: 53 HR: 0.669   NDCG: 0.408\nEnd. Best epoch 013: HR = 0.696, NDCG = 0.420\n</code></pre> <p>GMF</p> <pre><code>...\nThe time elapse of epoch 018 is: 00: 01: 42 HR: 0.703   NDCG: 0.424\nThe time elapse of epoch 019 is: 00: 01: 41 HR: 0.703   NDCG: 0.426\nEnd. Best epoch 016: HR = 0.709, NDCG = 0.427\n</code></pre> <p>NeuMF</p> <pre><code>...\nThe time elapse of epoch 018 is: 00: 01: 59 HR: 0.681   NDCG: 0.414\nThe time elapse of epoch 019 is: 00: 01: 59 HR: 0.676   NDCG: 0.407\nEnd. Best epoch 005: HR = 0.702, NDCG = 0.425\n</code></pre>","tags":["recsys"]},{"location":"blog/2024/10/14/neural-collaborative-filtering.html#compare-results","title":"Compare results","text":"<p>Both <code>GMF</code> and <code>NeuMF</code> have quite similar results, showing better metric performance than <code>MLP</code>. This probably implies that <code>GMF</code> alone is a good model, utilising the sequential layers in our model doesn't really change the result too much. Nevertheless a slight metric improvement was achieved using <code>NeuMF</code>. </p> <p>Original author metric results</p> Models MovieLens HR@10 MovieLens NDCG@10 Pinterest HR@10 Pinterest NDCG@10 MLP 0.696 0.420 - - GMF 0.709 0.427 - - NeuMF (without pre-training) 0.702 0.425 - - NeuMF (with pre-training) - - - - <p>The authors of the paper show that one of the largest benefits of using a neural approach is the reutilisation of embedding matrices. Utilising the original authors code a substantial improvement was possible as shown below:</p> <p>Our model metric results</p> Models MovieLens HR@10 MovieLens NDCG@10 Pinterest HR@10 Pinterest NDCG@10 MLP 0.692 0.425 0.868 0.542 GMF - - - - NeuMF (without pre-training) 0.701 0.425 0.870 0.549 NeuMF (with pre-training) 0.726 0.445 0.879 0.555 <p>If we look at the metrics, we can observe that the model has both good relevant item prediction capability (<code>hitrate</code>) and its order of prediction/ranking (<code>ndcg</code>) on the entire dataset of unseen data. </p>","tags":["recsys"]},{"location":"blog/2025/02/01/neural-recsys.html","title":"Neural Networks for Recommendation Systems","text":"<p>In this notebook we will look at how to use a neural network approach to making recommendations</p> <ul> <li>The user/item pairings are the main source of data used to create recommendations</li> <li>Scalar product of both the <code>user_id</code> and <code>item_id</code> embeddings will be our relevancy scores</li> <li>User film interactions will be <code>positive</code> feedback &amp; negative samples which will be created randomly are our <code>negative</code> samples</li> <li>The dataset is split into two, <code>train</code> will be used to train a model on historical user data, <code>test</code> will be used to provide user recommendations</li> <li>What we will be telling the model is to learn and differentiate between the films they actually watched apart from those they haven\u2019t (ideally)</li> <li>We have already looked at <code>DSSM</code> in a previous notebook  , well be simplifying things a little here, not including user and item features and will keep things more simple.</li> </ul> <ul> <li> Jupyter Notebook</li> <li> Replay Library</li> </ul>","tags":["recsys","neural","replay"]},{"location":"blog/2025/02/01/neural-recsys.html#setup","title":"Setup","text":"<p>We have summarised all preprocessing steps before the definition of <code>datasets</code> and <code>dataloaders</code>, so the sections associated with preprocessing take up less space </p> <ul> <li><code>replay</code> will help us keep things more compact, by utilising existing methods for preprocessing</li> <li><code>MovieLensPrepare</code> : Initialising this class will read the dataset</li> <li><code>preprocess</code> : Calling this method will define filtration of low item count envents for each user, reset the user and item identifiers and convert the time based feature into something we can work with</li> <li><code>split_data</code> : Calling this method will create two subsets based on the last 20% of datetime split</li> <li><code>filter_test</code> : Calling this method will remove all the poorly rated events for each user</li> </ul> <pre><code>class MovieLensPrepare:\n\n    def __init__(self):\n        rs = MovieLens('100k')\n        self.data = rs.ratings\n        self.u_features = rs.users\n        self.i_features = rs.items\n\n    def preprocess(self):\n\n        data = self.data\n        u_features = self.u_features\n        i_features = self.i_features\n\n        data = MinCountFilter(num_entries=20).transform(data)\n\n        # interactions and user &amp; item features must be synchronised\n        data = data[data[config.USER_COL].isin(u_features[config.USER_COL].unique())]\n        data = data[data[config.ITEM_COL].isin(i_features[config.ITEM_COL].unique())]\n\n        print(f\"Number of unique users {data['user_id'].nunique()}\")\n        print(f\"Number of unique items {data['item_id'].nunique()}\")\n\n        # interactions and user &amp; item features must be synchronised\n        data = data[data[config.USER_COL].isin(u_features[config.USER_COL].unique())]\n        data = data[data[config.ITEM_COL].isin(i_features[config.ITEM_COL].unique())]\n\n        data[config.TIMESTAMP] = pd.to_datetime(data['timestamp'],unit='s')\n\n        self.data = data\n\n    def split_data(self):\n\n        data = self.data\n        u_features = self.u_features\n        i_features = self.i_features\n\n        splitter = TimeSplitter(time_threshold=0.2,  # 20% into test subset\n                            drop_cold_users=True,\n                            drop_cold_items=True,\n                            query_column=config.USER_COL)\n\n        train,test = splitter.split(data)\n        print('train size',train.shape[0])\n        print('test size', test.shape[0])\n\n        # user features and item features must be present in interactions dataset and only\n        u_features = u_features[u_features[config.USER_COL].isin(train[config.USER_COL].unique())]\n        i_features = i_features[i_features[config.ITEM_COL].isin(train[config.ITEM_COL].unique())]\n\n        # encoders for users\n        encoder_user = LabelEncoder()\n        encoder_user.fit(train[config.USER_COL])\n\n        # encoders for items\n        encoder_item = LabelEncoder()\n        encoder_item.fit(train[config.ITEM_COL])\n\n        train[config.USER_COL] = encoder_user.transform(train[config.USER_COL])\n        train[config.ITEM_COL] = encoder_item.transform(train[config.ITEM_COL])\n\n        test[config.USER_COL] = encoder_user.transform(test[config.USER_COL])\n        test[config.ITEM_COL] = encoder_item.transform(test[config.ITEM_COL])\n\n        u_features[config.USER_COL] = encoder_user.transform(u_features[config.USER_COL])\n        i_features[config.ITEM_COL] = encoder_item.transform(i_features[config.ITEM_COL])\n\n        self.train = train \n        self.test = test\n\n        self.u_features = u_features\n        self.i_features = i_features\n\n    def filter_test(self):\n        filter_rating = LowRatingFilter(value=4)        \n        self.test = filter_rating.transform(self.test)\n</code></pre> <p>The parameters which we will be using are as follows, mainly noting that we are using <code>rating</code> as our feedback column</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass config:\n\n    USER_COL : str = 'user_id'\n    ITEM_COL : str = 'item_id'\n    RATING_COL : str = 'rating'\n    TIMESTAMP : str = 'timestamp'\n    NUM_EPOCHS : int = 30\n\n    K = 10\n    SEED = 123\n\nconfig = config()\nrandom.seed(config.SEED)\ntorch.manual_seed(config.SEED)\nnp.random.seed(config.SEED)\n</code></pre>","tags":["recsys","neural","replay"]},{"location":"blog/2025/02/01/neural-recsys.html#1-load-dataset","title":"1 | Load Dataset","text":"<p>We will be using a simplified dataset <code>MovieLens</code> with 100,000 interactions of <code>user_id</code> with films <code>item_id</code></p> <p>Today, we will be focusing on recommendations using only interactions</p> <pre><code>study = MovieLensPrepare()\n</code></pre>","tags":["recsys","neural","replay"]},{"location":"blog/2025/02/01/neural-recsys.html#2-preprocessing","title":"2 | Preprocessing","text":"<ul> <li><code>Replay</code> contains a handy &amp; quick way for preprocessing interactions</li> <li><code>MinCountFilter</code> can be used for filtering our interactions that have less than num_entries</li> <li>Lets use this method for removing user interactions with less than 20 items </li> </ul> <pre><code>study.preprocess()\n</code></pre>","tags":["recsys","neural","replay"]},{"location":"blog/2025/02/01/neural-recsys.html#3-splitting-dataset-in-time","title":"3 | Splitting Dataset in time","text":"<ul> <li>The next step after preprocessing the dataset to our liking is to split it into subsets, so we can train the model on one subset and use another for model validation (20%)</li> <li>replay has a function named <code>TimeSplitter</code>, which we will use to create our subsets</li> </ul> <p>class TimeSplitter(replay.splitters.base_splitter.Splitter)  |  TimeSplitter(time_threshold: Union[datetime.datetime, str, float], query_column: str = 'query_id', drop_cold_users: bool = False, drop_cold_items: bool = False, item_column: str = 'item_id', timestamp_column: str = 'timestamp', session_id_column: Optional[str] = None, session_id_processing_strategy: str = 'test', time_column_format: str = '%Y-%m-%d %H:%M:%S')</p> <pre><code>study.split_data()\n</code></pre>","tags":["recsys","neural","replay"]},{"location":"blog/2025/02/01/neural-recsys.html#4-rating-filter","title":"4 | Rating Filter","text":"<p>We want to recommend only items that have been rated highly, so for the <code>test</code> subset, we will be using <code>LowRatingFilter</code> to remove iteractions with low ratings</p> <pre><code>study.filter_test()\n</code></pre> <p>So what we have going into the next part</p> <ul> <li><code>study.train</code> (training subsett</li> <li><code>study.test</code> (test subset)</li> </ul> <p>Let's take a look at a sample from the training set</p> user_id item_id rating timestamp 1000138 5399 789 4 2000-04-25 23:05:32 1000153 5399 2162 4 2000-04-25 23:05:54 999873 5399 573 5 2000-04-25 23:05:54 1000007 5399 1756 4 2000-04-25 23:06:17 1000192 5399 1814 5 2000-04-25 23:06:17","tags":["recsys","neural","replay"]},{"location":"blog/2025/02/01/neural-recsys.html#5-create-torch-dataset","title":"5 | Create Torch Dataset","text":"<p>We need to create a torch dataset from our matrix of interactions <code>data</code>, which will be passing data to our model</p> <ul> <li>The dataset <code>TowerTrain</code> <code>get_item</code> for each index inputs the <code>user_id</code> and <code>item_id</code> (which will be our positive feedback) from the interaction dataset : (positive_item_id)</li> <li>Additionally for this user <code>user_id</code>, we generate an additional number of random <code>item_id</code> which will be the negative samples, which the user hasn't watched, we\u2019ll be adding 10 to the 1 positive</li> <li>Both of these are concatenated into a single array vector (<code>items</code>)</li> <li>Lastly we also return the labels, corresponding to either the <code>positive (1)</code> or <code>negative (0)</code> sample id </li> </ul> <pre><code>from torch.utils.data import Dataset, DataLoader\n\nclass TowerTrain(Dataset):\n\n    def __init__(self, \n                 data, \n                 num_negatives=10, \n                 i_features=None, \n                 u_features=None):\n\n        # user, item\n        self.data = data[[config.USER_COL,config.ITEM_COL]].to_numpy()\n        self.num_negatives = num_negatives\n        self.num_items = len(np.unique(self.data[:, 1]))\n        self.i_features = i_features\n        self.u_features = u_features\n\n    def __len__(self):\n        return len(self.data)\n\n    # get item of row in data\n    def __getitem__(self, idx):\n\n        # index to -&gt; user_id, item_id\n        user_id, pos_item_id = self.data[idx, 0], self.data[idx, 1]\n\n        # create positive, negative samples\n        # torch tensor for each item_id (pos sample) create 10 neg samples\n        items = torch.tensor(np.hstack([pos_item_id,\n                                       np.random.randint(\n                                           low=0,\n                                           high=self.num_items,\n                                           size=self.num_negatives)]),\n                             dtype=torch.int32)\n\n        # set all labels to 0\n        labels = torch.zeros(self.num_negatives + 1, dtype=torch.float32)\n        labels[0] = 1. # positive label\n\n        return {'user_ids': torch.tensor([user_id], dtype=torch.int32),\n                'item_ids': items,\n                'labels': labels}\n</code></pre> <p>To demonstrate the output of the data class, let\u2019s create the <code>dataset</code> and subsequent <code>dataloader</code>, setting a batch size of 2</p> <pre><code># create dataset\nds_train = TowerTrain(study.train)\n\n# create data loader\ndl_train = DataLoader(ds_train,\n                          batch_size=2,\n                          shuffle=True,\n                          num_workers=0)\n\nbatch = next(iter(dl_train))\nbatch\n</code></pre> <p>As we can see we get a batch of user identifiers, their array of items and corresponding labels to specify which item is a positive or negative sample</p> <pre><code>{'user_ids': tensor([[ 91],\n         [320]], dtype=torch.int32),\n 'item_ids': tensor([[ 565, 1534, 1389, 1406, 1346, 1122, 1041,  106, 1147, 1593, 1238],\n         [ 317,   96,  113,  638,   47,   73, 1568,  942,  224,  111, 1433]],\n        dtype=torch.int32),\n 'labels': tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])}\n</code></pre>","tags":["recsys","neural","replay"]},{"location":"blog/2025/02/01/neural-recsys.html#6-model-definition","title":"6 | Model Definition","text":"<p>We will be creating a subclass <code>SimpleTower</code>, which only includes the embeddings of both <code>user_id</code> and <code>item_id</code> when we\u2019ll define them in the main class</p> <ul> <li>We can recall that for matrix factorisation approaches, we get the score matrix by using the scalar product of user and item embedding, similarly we will take the same approach to calculate the score for each user/item combination in the row</li> <li>The <code>forward</code> method, when called simply returns the user/item row of the corresponding embedding matrix</li> <li>And calculates the dot product between the <code>user_id</code> &amp; <code>item_id</code> matrices returning the array for all user/item combinations (batch,11)</li> </ul> <pre><code># subclass contains only embedding layer but we can \n# expand on this by importing user, item features\nclass SimpleTower(nn.Module):\n    def __init__(self, num_embeddings, emb_dim):\n        super().__init__()\n        self.emb = nn.Embedding(num_embeddings, emb_dim)\n\n    def forward(self, ids, features=None):\n        return self.emb(ids)\n\n\nclass BaseTwoHead(nn.Module):\n\n    def __init__(self, \n                 emb_dim, \n                 user_config=None,\n                 item_config=None):\n\n        super().__init__()\n        self.emb_dim = emb_dim\n        self.user_tower = SimpleTower(emb_dim=emb_dim, **user_config) # (emb_dim,n_users)\n        self.item_tower = SimpleTower(emb_dim=emb_dim, **item_config) # (emb_dim,n_items)\n\n    # forward method defines two 'towers'\n    # and the scalar product of the two\n    # which will gives us the scores\n    def forward(self, batch):\n\n        item_emb = self.item_tower(batch[\"item_ids\"]) # (batch,1,16) \n        user_emb = self.user_tower(batch[\"user_ids\"]) # (batch,11,16)\n        dot_product = (user_emb * item_emb).sum(dim=-1) # (batch,11)\n        return dot_product\n\n    # methods for extracting embeddings\n    def infer_users(self, batch):\n        return self.user_tower(batch[\"user_ids\"])\n\n    def infer_items(self, batch):\n        return self.item_tower(batch[\"item_ids\"])\n</code></pre> <p>We\u2019ll be defining several dictionaries, which will store the common settings, setting for users and items</p> <ul> <li><code>embed_config</code> : stores the common embedding dimension size emb_dim</li> <li><code>user_config</code> : stores data about the user</li> <li><code>item_config</code> : stores data about the item</li> </ul> <pre><code># model parameters\nembed_config = {'emb_dim' : 16}  # embedding dimension\nuser_config = {'num_embeddings' : study.train[config.USER_COL].max() + 1,} # number of users\nitem_config = {'num_embeddings' : study.train[config.ITEM_COL].max() + 1,} # number of items\n\n# import the embedding dimension \nmodel = BaseTwoHead(**embed_config, \n                    user_config=user_config, \n                    item_config=item_config)\nmodel\n</code></pre> <pre><code>BaseTwoHead(\n  (user_tower): SimpleTower(\n    (emb): Embedding(751, 16)\n  )\n  (item_tower): SimpleTower(\n    (emb): Embedding(1616, 16)\n  )\n)\n</code></pre> <p>Model <code>forward</code> pass</p> <ul> <li>The output of the model will give us the logits for each of the 11 items, for each user row</li> </ul> <pre><code># output for a single batch\noutput = model(batch)\noutput\n</code></pre> <pre><code>tensor([[  1.6632,   5.8888,   0.0997,   7.6885,   8.2156,   4.0495,   3.0272,\n           1.9775,  -1.8750,   4.3952,   0.2714],\n        [  5.3873, -10.4797,  -4.2230,  -0.4488,   0.9215,  -5.0823,  -0.5018,\n           4.9579,   0.8251,  -6.3608,  -4.5723]], grad_fn=&lt;SumBackward1&gt;)\n</code></pre>","tags":["recsys","neural","replay"]},{"location":"blog/2025/02/01/neural-recsys.html#7-preparing-loaders","title":"7 | Preparing Loaders","text":"<p>We have already previously created a sample dataloder, now let\u2019s create both for the two subsets</p> <pre><code># create train dataset\nds_train = TowerTrain(study.test)\n\n# create test data loader\ndl_train = DataLoader(ds_train,\n                      batch_size=1024,\n                      shuffle=True,\n                      num_workers=0)\n\n# create test dataset\nds_test = TowerTrain(study.test)\n\n# create test data loader\ndl_test = DataLoader(ds_test,\n                      batch_size=1024,\n                      shuffle=True,\n                      num_workers=0)\n</code></pre>","tags":["recsys","neural","replay"]},{"location":"blog/2025/02/01/neural-recsys.html#8-modeling-iteration","title":"8 | Modeling Iteration","text":"<p>Let\u2019s define the optimiser and loss function which are pretty much standard across other binary classification problems</p> <pre><code>optimizer = torch.optim.Adam(model.parameters(),\n                             lr=0.001)\nloss_fn = nn.BCEWithLogitsLoss()\n</code></pre> <p>And also the training loop is standard, we\u2019ll be looping through a fixed number of <code>epoch</code> and passing the batches and predicting, calculating the loss, do a step of <code>backpropagation</code>, calculating the gradients and updating the model weights via the <code>optimiser</code></p> <pre><code>train_loss_per_epoch = []\ntest_loss_per_epoch = []\n\n# loop through all epochs\nfor epoch in tqdm(range(config.NUM_EPOCHS)):\n\n    # training loop for all batches\n    model.train()\n    train_loss = 0.0\n    for iteration, batch in enumerate(dl_train):\n        optimizer.zero_grad()\n        preds = model(batch)\n        loss = loss_fn(preds, batch['labels'])\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    train_loss /= len(dl_train)\n\n    # evaluation loop for all batches\n    model.eval()\n    test_loss = 0\n    for iteration, batch in enumerate(dl_test):\n        preds = model(batch)\n        loss = loss_fn(preds, batch['labels'])\n        test_loss += loss.item()\n\n    # evaluation of loss\n    test_loss /= len(dl_test)\n    test_loss_per_epoch.append(test_loss)\n    train_loss_per_epoch.append(train_loss)\n</code></pre> <ul> <li>So in turn, our model is learning to classify between <code>positive</code> and <code>negative</code> samples for each row of data</li> <li>Once the model is finished learning, we can utilise the model methods and extract the embeddings from the two towers.</li> <li>And save the model as well for future use!</li> </ul> <pre><code># save our model state\ntorch.save(model.state_dict(), f\"/content/model_{config.NUM_EPOCHS}\"\n</code></pre>","tags":["recsys","neural","replay"]},{"location":"blog/2025/02/01/neural-recsys.html#9-generating-user-recommendations","title":"9 | Generating user recommendations","text":"<p>Time has come to use our trained model</p> <ul> <li>We will be making recommendations by using the model that we trained on the train dataset and using the test users to make predictions</li> <li>To make predictions, we will extract the embedding matrix weights for user and items, calculate the scores, get the top k results for each user based on the largest score values</li> </ul>","tags":["recsys","neural","replay"]},{"location":"blog/2025/02/01/neural-recsys.html#91-load-weights","title":"9.1. Load Weights","text":"<p>First things first, we need to load the model weights, and put it in inference mode</p> <pre><code>model = BaseTwoHead(**config, user_config=user_config, item_config=item_config)\nmodel.load_state_dict(torch.load(f\"/content/model_{config.NUM_EPOCHS}\"))\nmodel.eval()\n</code></pre>","tags":["recsys","neural","replay"]},{"location":"blog/2025/02/01/neural-recsys.html#92-get-test-users","title":"9.2. Get test users","text":"<p>Get the user identifiers that are in the test test, the test set was saved in <code>study.test</code></p> <pre><code>test_users = study.test[[config.USER_COL]].drop_duplicates().reset_index(drop=True)\n</code></pre> user_id 0 2 1 233 2 736 3 49 4 600","tags":["recsys","neural","replay"]},{"location":"blog/2025/02/01/neural-recsys.html#93-extract-weights","title":"9.3. Extract Weights","text":"<p>Extract the embedding weights for all users and items which is located in the model</p> <pre><code># extract the user / item embedding weights\nuser_embed = model.user_tower.emb.weight.detach().cpu().numpy()\nitem_embed = model.item_tower.emb.weight.detach().cpu().numpy()\nuser_embed.shape, item_embed.shape\n</code></pre>","tags":["recsys","neural","replay"]},{"location":"blog/2025/02/01/neural-recsys.html#94-scalar-product","title":"9.4. Scalar product","text":"<p>Calculate the scores for each user &amp; item combination by calculating the scalar product of them</p> <pre><code># calcualate the scores (751,1616)\nscores = user_embed[test_users[config.USER_COL].values] @ item_embed.T\n</code></pre> <pre><code>[[-2.219962   -2.8183699  -1.2701275  ... -1.7878596  -2.3029149\n  -5.1351438 ]\n [-0.2002018  -3.269224   -3.5974343  ... -5.4825845  -4.0557184\n  -4.9202886 ]\n [-0.24603942 -1.9250925  -1.2330636  ... -4.066546   -3.6852539\n  -6.3292623 ]\n ...\n [ 1.3434778  -2.2150192  -1.8992031  ... -4.7611713  -4.1526904\n  -5.917045  ]\n [ 0.067677   -2.6156569  -2.6362207  ... -3.8871505  -3.1315584\n  -3.5736673 ]\n [-1.3127992  -1.5567051  -1.1855109  ... -2.6913378  -3.2935755\n  -5.5215263 ]]\n</code></pre>","tags":["recsys","neural","replay"]},{"location":"blog/2025/02/01/neural-recsys.html#95-get-highest-scores","title":"9.5. Get highest scores","text":"<p>Get the highest value indicies (idx) &amp; their corresponding values (scores). The scores correspond to the index of the item in the encoder <code>encoder_item</code>, which we stored in class instance <code>study</code></p> <pre><code># get top 10 idx by value &amp; get its value\nids = np.argpartition(scores, -config.K)[:, -config.K:]\nscores = np.take_along_axis(scores, ids, axis=1)\nscores[:5]\n</code></pre> <pre><code>array([[ 1.3017656 ,  1.4262905 ,  1.4305891 ,  1.5401053 ,  1.5945268 ,\n         1.9945638 ,  1.9178314 ,  2.8111196 ,  1.5959901 ,  2.221249  ],\n       [-0.02534078,  1.0504715 ,  0.6823742 ,  0.6663627 , -0.00748574,\n         0.5298525 ,  0.49601346,  0.32487705,  0.04160966,  0.02862556],\n       [ 1.7142106 ,  1.8349895 ,  2.43454   ,  2.896079  ,  3.0631516 ,\n         2.1554096 ,  1.8832399 ,  2.087269  ,  3.876807  ,  2.2215443 ],\n       [ 0.2731401 ,  0.30537376,  0.3488819 ,  0.53589934,  1.0000901 ,\n         0.77159363,  0.6785181 ,  0.7471067 ,  0.55528575,  1.0426229 ],\n       [ 0.89288795,  0.92402935,  0.97583646,  0.98947227,  1.0060023 ,\n         1.1556187 ,  1.4170016 ,  1.4296795 ,  1.7379148 ,  1.2944818 ]],\n      dtype=float32)\n</code></pre>","tags":["recsys","neural","replay"]},{"location":"blog/2025/02/01/neural-recsys.html#96-recommendations-matrix","title":"9.6. Recommendations Matrix","text":"<p>Prepare the usual format, <code>user_id</code>, <code>item_id</code> and rating <code>rating</code>, which will enable us to quickly evaluate the metrics using <code>experiment</code> function from replay. We need to add both lists to each user &amp; expand them together</p> <pre><code># prepare recommendations matrix\ndef prepare_recs(test_users, \n                 rec_item_ids, \n                 rec_relevances):\n\n    predict = test_users.copy()\n    predict[config.ITEM_COL] = rec_item_ids.tolist()  # add list of indicies for each user\n    predict['rating'] = rec_relevances.tolist() # add rating list of scores for each user\n    predict = predict.explode(column=[config.ITEM_COL, 'rating']).reset_index(drop=True) # expand both lists\n    predict[config.ITEM_COL] = predict[config.ITEM_COL].astype(int)\n    predict['rating'] = predict['rating'].astype(\"double\")\n    return predict\n\n\nmodel_recommendations = prepare_recs(test_users,      # user columns \n                                     rec_item_ids=ids,  # indicies of top 10 in scores\n                                     rec_relevances=scores) # scores of top 10\n</code></pre> user_id item_id rating 0 2 302 1.30177 1 2 218 1.42629 2 2 233 1.43059 3 2 139 1.54011 4 2 6 1.59453 <p>We'll evaluate the prediction &amp; test overlapping items using hitrate, to measure how well the model predicts at least one relevant recommendation for users. NDCG, for the evaluation of how well the model can correcly order the relevant items &amp; coverage to measure how well the model predicts a range of items from all available items</p> <pre><code>metrics = Experiment(\n    [NDCG(config.K), HitRate(config.K), Coverage(config.K)],\n    study.test,\n    study.train,\n    query_column=config.USER_COL, \n    item_column=config.ITEM_COL,\n)\nmetrics.add_result(\"dssm_model\", model_recommendations)\nmetrics.results\n</code></pre> NDCG@10 HitRate@10 Coverage@10 dssm_model 0.0345152 0.221053 0.191213 <p>Thank you for reading!</p> <p>Any questions or comments about the above post can be addressed on the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning or simply below!</p>","tags":["recsys","neural","replay"]},{"location":"blog/2025/07/16/pyspark-pipelines.html","title":"PySpark Pipelines","text":"<p>Todays post covers the following:</p> <ul> <li>Missing data treatment classification pipeline</li> <li>Feature scaling using ScandardScaler classification pipeline</li> <li>TF-IDF corpus classification pipeline</li> <li>PCA dimensionality reduction classification pipeline</li> </ul>","tags":["pyspark","pipeline"]},{"location":"blog/2025/07/16/pyspark-pipelines.html#missing-data-treatment-pipeline","title":"Missing Data Treatment Pipeline","text":"<p>Simple data imputation treatment pipeline in pyspark</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\nfrom pyspark.ml.classification import LogisticRegression\n\n# Sample data with missing values (None or null)\ndata = [\n    (1, \"red\", 10.0, None, 0),\n    (2, \"blue\", None, 20.0, 1),\n    (3, \"green\", 30.0, 30.0, 0),\n    (4, None, 40.0, 40.0, 1),\n    (5, \"blue\", 50.0, 50.0, 0)\n]\n\ndf = spark.createDataFrame(data, [\"id\", \"color\", \"feature1\", \"feature2\", \"label\"])\n\n# Step 1: Impute missing numeric values with mean\nimputer = Imputer(\n                inputCols=[\"feature1\", \"feature2\"],\n                outputCols=[\"feature1_imputed\", \"feature2_imputed\"]\n).setStrategy(\"mean\")\n\n# Step 2: Handle missing categorical values by filling with a placeholder\ndf = df.fillna({\"color\": \"missing\"})\n\n# Step 3: StringIndexer for categorical column\nindexer = StringIndexer(inputCol=\"color\", outputCol=\"color_index\")\n\n# Step 4: OneHotEncoder for categorical feature\nencoder = OneHotEncoder(inputCols=[\"color_index\"], outputCols=[\"color_ohe\"])\n\n# Step 5: Assemble all features into a single vector\nassembler = VectorAssembler(\n    inputCols=[\"color_ohe\", \"feature1_imputed\", \"feature2_imputed\"],\n    outputCol=\"features\"\n)\n\n# Step 6: Scale features\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n\n# Step 7: Logistic Regression model\nlr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"label\")\n\n# Build pipeline with all stages\npipeline = Pipeline(stages=[imputer, indexer, encoder, assembler, scaler, lr])\n\n# Fit pipeline\nmodel = pipeline.fit(df)\npredictions = model.transform(df)\n\n# Show results\npredictions.select(\"id\", \"color\", \"feature1\", \"feature2\", \"label\", \"prediction\", \"probability\").show(truncate=True)\n</code></pre> <pre><code>+---+-------+--------+--------+-----+----------+--------------------+\n| id|  color|feature1|feature2|label|prediction|         probability|\n+---+-------+--------+--------+-----+----------+--------------------+\n|  1|    red|    10.0|    NULL|    0|       0.0|[0.99999999167350...|\n|  2|   blue|    NULL|    20.0|    1|       1.0|[1.11267794732560...|\n|  3|  green|    30.0|    30.0|    0|       0.0|[0.99999999391649...|\n|  4|missing|    40.0|    40.0|    1|       1.0|[8.02598294965153...|\n|  5|   blue|    50.0|    50.0|    0|       0.0|[0.99999999102672...|\n+---+-------+--------+--------+-----+----------+--------------------+\n</code></pre>","tags":["pyspark","pipeline"]},{"location":"blog/2025/07/16/pyspark-pipelines.html#feature-scaling-pipline","title":"Feature Scaling Pipline","text":"<p>Once we have our assembler, we can scale our features</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\nfrom pyspark.ml.classification import LogisticRegression\n\n\n# Sample data with features and label\ndata = [\n    (1, 25.0, 50000.0, 0),\n    (2, 35.0, 60000.0, 1),\n    (3, 45.0, 70000.0, 0),\n    (4, 20.0, 40000.0, 1)\n]\n\ndf = spark.createDataFrame(data, [\"id\", \"age\", \"income\", \"label\"])\n\n# Step 1: Assemble features into a vector column\nassembler = VectorAssembler(inputCols=[\"age\", \"income\"], \n                            outputCol=\"features\")\n\n# Step 2: StandardScaler to standardize features\nscaler = StandardScaler(inputCol=\"features\", \n                        outputCol=\"scaled_features\", \n                        withMean=True, withStd=True)\n\n# Step 3: Logistic Regression using scaled features\nlr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"label\")\n\n# Create pipeline with stages\npipeline = Pipeline(stages=[assembler, \n                            scaler, \n                            lr])\n\n# Fit pipeline on training data &amp; predict\nmodel = pipeline.fit(df)\npredictions = model.transform(df)\n\n# Show predictions\npredictions.select(\"id\", \"label\", \"prediction\", \"scaled_features\").show()\n</code></pre> <pre><code>+---+-----+----------+--------------------+\n| id|label|prediction|     scaled_features|\n+---+-----+----------+--------------------+\n|  1|    0|       0.0|[-0.5637345210021...|\n|  2|    1|       0.0|[0.33824071260127...|\n|  3|    0|       0.0|[1.24021594620466...|\n|  4|    1|       1.0|[-1.0147221378038...|\n+---+-----+----------+--------------------+\n</code></pre>","tags":["pyspark","pipeline"]},{"location":"blog/2025/07/16/pyspark-pipelines.html#tf-idf-pipline","title":"TF-IDF Pipline","text":"<p>TF-IDF machine learning pipeline using pyspark</p> <pre><code>from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.pipeline import Pipeline\n\n# Sample data: (id, text, label)\ndata = [\n    (0, \"spark is great for big data processing\", \"positive\"),\n    (1, \"hadoop is an old technology\", \"negative\"),\n    (2, \"spark and hadoop are big data tools\", \"positive\"),\n    (3, \"I dislike slow processing\", \"negative\")\n]\n\ndf = spark.createDataFrame(data, [\"id\", \"text\", \"label\"])\n\n# Step 1: Tokenize text\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n\n# Step 2: Remove stopwords (optional but recommended)\nremover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n\n# Step 3: TF feature extraction\nhashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=20)\n\n# Step 4: IDF to get TF-IDF features\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n\n# Step 5: Convert string labels to numeric indices\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"labelIndex\")\n\n# Step 6: Define classifier (e.g., Logistic Regression)\nlr = LogisticRegression(featuresCol=\"features\", labelCol=\"labelIndex\")\n\n# Build pipeline\npipeline = Pipeline(stages=[tokenizer, \n                            remover, \n                            hashingTF, \n                            idf, \n                            labelIndexer, \n                            lr])\n\n# Train model\nmodel = pipeline.fit(df)\n\n# Predict on training data (or new data)\npredictions = model.transform(df)\npredictions.select(\"text\", \"label\", \"prediction\").show()\n</code></pre> <pre><code>+--------------------+--------+----------+\n|                text|   label|prediction|\n+--------------------+--------+----------+\n|spark is great fo...|positive|       1.0|\n|hadoop is an old ...|negative|       0.0|\n|spark and hadoop ...|positive|       1.0|\n|I dislike slow pr...|negative|       0.0|\n+--------------------+--------+----------+\n</code></pre>","tags":["pyspark","pipeline"]},{"location":"blog/2025/07/16/pyspark-pipelines.html#pca-pipline","title":"PCA Pipline","text":"<p>Dimensionality reduction machine learning classification pipeline example using pyspark</p> <pre><code>from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler, PCA, StandardScaler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n\n# Sample data loading (replace with actual data loading)\ndata = [\n    (7, 1.1, 0.3, 1.5, 0),\n    (8, 2.4, 1.7, 0.3, 1),\n    (9, 0.3, 2.5, 2.3, 0),\n    (10, 1.3, 0.4, 1.7, 1),\n    (11, 2.5, 1.8, 0.4, 0),\n    (12, 0.4, 2.6, 2.4, 1),\n    (13, 1.4, 0.5, 1.6, 0),\n    (14, 2.6, 1.6, 0.5, 1),\n    (15, 0.5, 2.7, 2.5, 0),\n    (16, 1.6, 0.6, 1.4, 1),\n    (17, 2.7, 1.5, 0.6, 0),\n    (18, 0.6, 2.8, 2.6, 1),\n    (19, 1.7, 0.7, 1.3, 0),\n    (20, 2.8, 1.4, 0.7, 1)\n]\n\ndf = spark.createDataFrame(data, [\"id\", \"feature1\", \"feature2\", \"feature3\", \"label\"])\n\n# Assemble features into a vector\nassembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"feature3\"], outputCol=\"raw_features\")\n\n# Optional: Scale features\nscaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\")\n\n# PCA to reduce dimensionality\npca = PCA(k=2, \n            inputCol=\"features\", \n            outputCol=\"pca_features\")\n\n# Use PCA features for classification\nlr = LogisticRegression(featuresCol=\"pca_features\", labelCol=\"label\")\n\n# Create pipeline\npipeline = Pipeline(stages=[assembler, \n                            scaler, \n                            pca, \n                            lr])\n\n# Train-test split\ntrain_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n\n# Fit pipeline\nmodel = pipeline.fit(train_df)\n\n# Predict\npredictions = model.transform(test_df)\n\n# Evaluate\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(f\"Test Accuracy: {accuracy:.2f}\")\n</code></pre> <pre><code>Test Accuracy: 0.50\n</code></pre> <p>Thank you for reading!</p> <p>Any questions or comments about the posts below can be addressed to the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>","tags":["pyspark","pipeline"]},{"location":"blog/2025/07/14/pyspark-select-drop-rename-columns.html","title":"PySpark Select, Drop, Rename Columns","text":"<p>Todays post covers the following:</p> <ul> <li>Reading CSV &amp; Parquet formats</li> <li>Column selection</li> <li>Renaming columns</li> <li>Adding columns</li> <li>Dropping columns</li> </ul>","tags":["pyspark"]},{"location":"blog/2025/07/14/pyspark-select-drop-rename-columns.html#reading-csv","title":"Reading CSV","text":"<p>Simple data imputation treatment pipeline in pyspark.</p> <pre><code>#Basic CSV files\ndf = spark.read.format(\"csv\").load(\"/path/to/sample.csv\")\n\n#csv with header\ndf = spark.read.option(\"header\",True).csv(\"/path/to/sample.csv\")\n\n# multiple options\ndf = spark.read.option(\"inferSchema\",True).option(\"delimiter\",\",\").csv(\"/path/to/sample.csv\")\n\n# with defined schema\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\nschema = StructType([\n    StructField(\"name\", StringType(), True),\n    StructField(\"age\", IntegerType(), True)\n])\ndf = spark.read.format(\"csv\").schema(schema).load(\"/path/to/sample.csv\")\n</code></pre>","tags":["pyspark"]},{"location":"blog/2025/07/14/pyspark-select-drop-rename-columns.html#selecting-columns","title":"Selecting Columns","text":"<pre><code># Select single column\ndf = df.select(\"name\")\n\n# Select multiple columns\ndf = df.select(\"name\", \"age\")\n\n# Select columns dynamically\ncolumns_to_select = [\"name\", \"department\"]\ndf = df.select(*columns_to_select)\n</code></pre>","tags":["pyspark"]},{"location":"blog/2025/07/14/pyspark-select-drop-rename-columns.html#renaming-columns","title":"Renaming Columns","text":"<pre><code># Rename a column\ndf = df.withColumnRenamed(\"name\", \"full_name\")\n\n# Rename multiple columns with chained calls\ndf = df.withColumnRenamed(\"old_col1\", \"new_col1\")\\\n       .withColumnRenamed(\"old_col2\", \"new_col2\")\n\n# Rename columns using select and alias\nfrom pyspark.sql.functions import col\ndf = df.select(\n    col(\"old_column_name1\").alias(\"new_column_name1\"),\n    col(\"old_column_name2\").alias(\"new_column_name2\"),\n    # Add more columns as needed\n)\n</code></pre>","tags":["pyspark"]},{"location":"blog/2025/07/14/pyspark-select-drop-rename-columns.html#adding-columns","title":"Adding Columns","text":"<pre><code>from pyspark.sql.functions import col, lit, expr, when\n\n# Add a new column with a constant value\ndf = df.withColumn(\"country\", lit(\"USA\"))\n\n# Add a new column with a calculated value\ndf = df.withColumn(\"salary_after_bonus\", col(\"salary\") * 1.1)\n\n# Add a column using an SQL expression\ndf = df.withColumn(\"tax\", expr(\"salary * 0.2\"))\n\n# Add a column with conditional logic\ndf = df.withColumn(\"high_earner\", when(col(\"salary\") &gt; 55000, \"Yes\").otherwise(\"No\"))\n\n# Case When with multiple conditions\ndf = df.withColumn(\n    \"salary_category\",\n    when(col(\"salary\") &lt; 60000, \"Low\")\n    .when((col(\"salary\") &gt;= 60000) &amp; (col(\"salary\") &lt; 90000), \"Medium\")\n    .otherwise(\"High\")\n)\n\n# Add multiple columns at once\ndf = df.withColumns({\n    \"bonus\": col(\"salary\") * 0.1,\n    \"net_salary\": col(\"salary\") - (col(\"salary\") * 0.2)\n})\n</code></pre>","tags":["pyspark"]},{"location":"blog/2025/07/14/pyspark-select-drop-rename-columns.html#dropping-columns","title":"Dropping Columns","text":"<pre><code># Drop a column\ndf = df.drop(\"department\")\n\n# Drop multiple columns\ndf = df.drop('column1', 'column2', 'column3')\n</code></pre> <p>Thank you for reading!</p> <p>Any questions or comments about the posts below can be addressed to the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>","tags":["pyspark"]},{"location":"blog/2025/07/18/pyspark-data-filtration.html","title":"PySpark Data Filtration","text":"<p>Todays post covers the following:</p> <ul> <li>Filtration by column value (one or multiple conditions)</li> <li>String related filtration using like / contains</li> <li>Missing data filtration </li> <li>List based filtration using isin</li> <li>General data clearning operations</li> </ul>","tags":["pyspark"]},{"location":"blog/2025/07/18/pyspark-data-filtration.html#basic-filtering","title":"Basic filtering","text":"<p>You can refer to columns using any of these notations: df.age , df['age'], col('age') Basic Filtering</p> <pre><code># Filter on &gt;, &lt;, &gt;=, &lt;=, == condition\ndf_filtered = df.where(df.age &gt; 30)\ndf_filtered = df.where(df['age'] &gt; 30)\n\n# Using col() function\nfrom pyspark.sql.functions import col\ndf_filtered = df.where(f.col(\"age\") &gt; 30)\n</code></pre>","tags":["pyspark"]},{"location":"blog/2025/07/18/pyspark-data-filtration.html#filter-with-multiple-conditions","title":"Filter with multiple conditions","text":"<p>Multiple conditions require parentheses around each condition</p> <pre><code># AND condition ( &amp; )\ndf_filtered = df.filter((df.age &gt; 25) &amp; (df.department == \"Engineering\"))\n# OR condition ( | )\ndf_filtered = df.filter((df.age &lt; 30) | (df.department == \"Finance\"))\n</code></pre>","tags":["pyspark"]},{"location":"blog/2025/07/18/pyspark-data-filtration.html#string-filters","title":"String filters","text":"<p>String column related filtration using soft and hard matches as well as regex</p> <pre><code># Filter rows where department equals 'Marketing'\ndf_filtered = df.where(df.department == \"Marketing\")\n\n# Case-insensitive filter\ndf_filtered = df.where(f.col(\"department\").like(\"MARKETING\"))\n\n# Contains a substring\ndf_filtered = df.where(f.col(\"department\").contains(\"Engineer\"))\n\n# Filter rows where the name starts with 'A'\ndf.where(f.col(\"name\").startswith(\"A\")).show()\n\n# Filter rows where the name ends with 'e'\ndf.where(f.col(\"name\").endswith(\"e\")).show()\n\n# Filter rows where the name matches a regex\ndf.where(f.col(\"name\").rlike(\"^A.*\")).show()\n</code></pre>","tags":["pyspark"]},{"location":"blog/2025/07/18/pyspark-data-filtration.html#null-filters","title":"Null filters","text":"<p>Show rows with column value is null for a single column filtering rows with missing data in that column and visa versa</p> <pre><code># Filter rows where a column is null\ndf_filtered = df.where(df.department.isNull())\n\n# Filter rows where a column is not null\ndf_filtered = df.where(df.department.isNotNull())\n</code></pre>","tags":["pyspark"]},{"location":"blog/2025/07/18/pyspark-data-filtration.html#filter-from-list","title":"Filter from list","text":"<pre><code># Filter rows where department is in a list\ndepartments = [\"Engineering\", \"Finance\"]\ndf_filtered = df.filter(f.col(\"department\").isin(departments))\n# Negate the filter (not in list)\ndf_filtered = df.where(~f.col(\"department\").isin(departments))\n</code></pre>","tags":["pyspark"]},{"location":"blog/2025/07/18/pyspark-data-filtration.html#data-cleaning","title":"Data cleaning","text":"<pre><code># 1. Drop all fully duplicate rows\n# Removes rows where all columns match exactly\ndf = df.dropDuplicates()\n\n# 2. Drop duplicates based on specific columns\n# Keeps the first row for each unique email\ndf = df.dropDuplicates([\"email\"])\n\n# 3. Get only distinct rows (same as SELECT DISTINCT)\n# Removes duplicates across all columns\ndf = df.distinct()\n\n# 4. Drop rows with any null values\n# Removes rows with even a single null field\ndf = df.dropna()\n\n# 5. Drop rows with nulls in specific columns\n# Only keeps rows where 'email' and 'age' are not null\ndf = df.dropna(subset=[\"email\", \"age\"])\n\n# 6. Fill missing values for all columns\n# Replaces all nulls with a default value\ndf = df.fillna(\"N/A\")\n\n# 7. Fill missing values for specific columns\n# Sets default age as 0 and country as \"Unknown\" if missing\ndf = df.fillna({\"age\": 0, \"country\": \"Unknown\"})\n</code></pre> <p>Thank you for reading!</p> <p>Any questions or comments about the posts below can be addressed to the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>","tags":["pyspark"]},{"location":"blog/2025/07/19/pyspark-grouping.html","title":"PySpark Aggregations","text":"<p>Todays post covers the following:</p> <ul> <li>Aggregations without grouping</li> <li>Aggregations with grouping </li> <li>Filtering after grouping</li> </ul>","tags":["pyspark"]},{"location":"blog/2025/07/19/pyspark-grouping.html#aggregations-without-grouping","title":"Aggregations without grouping**","text":"<p>You can refer to columns using any of these notations: df.age , df['age'], col('age') Basic Filtering</p> <pre><code>#Count rows\ndf.count()\n\n#Count Distinct Values in a column\ndf.select(countDistinct(\"Department\")).show()\n\n#Sum\ndf.select(sum(\"Salary\")).show()\n\n#Multiple Aggregations\ndf.select(min(\"Salary\"), max(\"Salary\")).show()\n</code></pre>","tags":["pyspark"]},{"location":"blog/2025/07/19/pyspark-grouping.html#aggregations-with-grouping","title":"Aggregations with grouping","text":"<pre><code>#Group by a single column\ndf.groupBy(\"Department\").sum(\"Salary\").show()\n\n#GroupBy with Multiple Columns\ndf.groupBy(\"Department\", \"Employee\").sum(\"Salary\").show()\n\n#Group by with multiple aggregations\ndf.groupBy(\"Department\").agg(\n                              count(\"Employee\").alias(\"Employee_Count\"),\n                              avg(\"Salary\").alias(\"Average_Salary\"),\n                              max(\"Salary\").alias(\"Max_Salary\")\n)\n</code></pre>","tags":["pyspark"]},{"location":"blog/2025/07/19/pyspark-grouping.html#filtration-after-aggregation","title":"Filtration after aggregation","text":"<p>Like in SQL filtration after grouping data (having)</p> <pre><code>#Filter after aggregation\ndf.groupBy(\"Department\").agg(sum(\"Salary\").alias(\"Total_Salary\")).filter(\"Total_Salary &gt; 8000\").show()\n</code></pre>","tags":["pyspark"]},{"location":"blog/2025/07/19/pyspark-grouping.html#filtration-after-aggregation_1","title":"Filtration after aggregation","text":"<p>Commonly used aggregation</p> Function Description Example <code>count()</code> Counts rows in a group. <code>groupBy(\"Department\").count()</code> <code>sum()</code> Sums values in a group. <code>groupBy(\"Department\").sum(\"Salary\")</code> <code>avg()</code> / <code>mean()</code> Calculates average values. <code>groupBy(\"Department\").avg(\"Salary\")</code> <code>min()</code> Finds the minimum value. <code>groupBy(\"Department\").min(\"Salary\")</code> <code>max()</code> Finds the maximum value. <code>groupBy(\"Department\").max(\"Salary\")</code> <code>countDistinct()</code> Counts distinct values in a group. <code>countDistinct(\"Employee\")</code> <code>collect_list()</code> Collects all values into a list. <code>collect_list(\"Employee\")</code> <code>collect_set()</code> Collects unique values into a set. <code>collect_set(\"Employee\")</code> <p>Thank you for reading!</p> <p>Any questions or comments about the posts below can be addressed to the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>","tags":["pyspark"]},{"location":"blog/2025/07/21/pyspark-pivoting.html","title":"PySpark Pivoting","text":"<p>Today's post covers the following:</p> <ul> <li>Basic pivot operation</li> <li>Pivot with multiple aggregations</li> <li>Conditional pivoting</li> <li>Pivoting with specified column values</li> </ul>","tags":["pyspark"]},{"location":"blog/2025/07/21/pyspark-pivoting.html#basic-pivot-operation","title":"Basic pivot operation","text":"<pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import sum, avg, when\n\nspark = SparkSession.builder.getOrCreate()\n\ndata = [(\"Alice\", \"HR\", 50000), \n        (\"Bob\", \"IT\", 60000), \n        (\"Cathy\", \"HR\", 55000)]\n\ndf = spark.createDataFrame(data, [\"name\", \"dept\", \"salary\"])\ndf.show()\n</code></pre> <pre><code>+-----+----+------+\n| name|dept|salary|\n+-----+----+------+\n|Alice|  HR| 50000|\n|  Bob|  IT| 60000|\n|Cathy|  HR| 55000|\n+-----+----+------+\n</code></pre> <pre><code># Pivot department to columns with salary summed per name\npivot_df = df.groupBy(\"name\").pivot(\"dept\").sum(\"salary\")\npivot_df.show()\n</code></pre> <pre><code>+-----+-----+-----+\n| name|   HR|   IT|\n+-----+-----+-----+\n|  Bob| NULL|60000|\n|Alice|50000| NULL|\n|Cathy|55000| NULL|\n+-----+-----+-----+\n</code></pre>","tags":["pyspark"]},{"location":"blog/2025/07/21/pyspark-pivoting.html#pivot-with-multiple-aggregations","title":"Pivot with multiple aggregations","text":"<p>You can apply multiple aggregations to each pivoted column using agg()</p> <pre><code># Transform rows into columns using the pivot() function, typically after grouping by one or more columns. \nmulti_agg_df = df.groupBy(\"name\").pivot(\"dept\").agg(\n    sum(\"salary\").alias(\"total\"),\n    avg(\"salary\").alias(\"avg\")\n)\nmulti_agg_df.show()\n</code></pre> <pre><code>+-----+--------+-------+--------+-------+\n| name|HR_total| HR_avg|IT_total| IT_avg|\n+-----+--------+-------+--------+-------+\n|  Bob|    NULL|   NULL|   60000|60000.0|\n|Alice|   50000|50000.0|    NULL|   NULL|\n|Cathy|   55000|55000.0|    NULL|   NULL|\n+-----+--------+-------+--------+-------+\n</code></pre>","tags":["pyspark"]},{"location":"blog/2025/07/21/pyspark-pivoting.html#conditional-pivoting","title":"Conditional pivoting","text":"<pre><code># Use expressions with when inside your aggregation to pivot with conditions.\nconditional_df = df.groupBy(\"name\")\\\n                    .pivot(\"dept\")\\\n                    .agg(\n    sum(when(df.salary &gt; 52000, df.salary)).alias(\"high_salary\")\n)\nconditional_df.show()\n</code></pre> <pre><code>+-----+-----+-----+\n| name|   HR|   IT|\n+-----+-----+-----+\n|  Bob| NULL|60000|\n|Alice| NULL| NULL|\n|Cathy|55000| NULL|\n+-----+-----+-----+\n</code></pre>","tags":["pyspark"]},{"location":"blog/2025/07/21/pyspark-pivoting.html#pivoting-with-explicit-values","title":"Pivoting with explicit values","text":"<pre><code># To control which values are used as output columns, explicitly provide them to pivot()\ndf.groupBy(\"name\")\\\n  .pivot(\"dept\", [\"HR\", \"IT\",\"Eng\"]).sum(\"salary\").show()\n</code></pre> <pre><code>+-----+-----+-----+----+\n| name|   HR|   IT| Eng|\n+-----+-----+-----+----+\n|  Bob| NULL|60000|NULL|\n|Alice|50000| NULL|NULL|\n|Cathy|55000| NULL|NULL|\n+-----+-----+-----+----+\n</code></pre> <p>Thank you for reading!</p> <p>Any questions or comments about the posts below can be addressed to the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>","tags":["pyspark"]},{"location":"blog/2025/07/23/pyspark-time-series-pipelines.html","title":"PySpark Time Series Pipelines","text":"<p>Today's post covers the following:</p> <ul> <li>Basic pipeline conversion of timestamp to unix time</li> <li>Lag feature combination pipelines </li> <li>Aggregation based statistics pipelines</li> </ul>","tags":["pyspark"]},{"location":"blog/2025/07/23/pyspark-time-series-pipelines.html#basic-pipeline","title":"Basic pipeline","text":"<p>Convert timestamp data to unix time format and use it as the only feature in the VectorAssembler</p> <pre><code>from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nimport pyspark.sql.functions as f\n\nspark = SparkSession\\\n        .builder\\\n        .getOrCreate()\n\n# Example DataFrame with timestamp and target value\ndata = spark.createDataFrame([\n    (\"2025-07-21 10:00:00\", 100.0),\n    (\"2025-07-21 11:00:00\", 110.0),\n    (\"2025-07-21 12:00:00\", 115.0),\n    (\"2025-07-21 13:00:00\", 120.0)\n], [\"timestamp\", \"value\"])\ndata.show(5)\n</code></pre> <pre><code>+-------------------+-----+\n|          timestamp|value|\n+-------------------+-----+\n|2025-07-21 10:00:00|100.0|\n|2025-07-21 11:00:00|110.0|\n|2025-07-21 12:00:00|115.0|\n|2025-07-21 13:00:00|120.0|\n+-------------------+-----+\n</code></pre> <pre><code># convert timestamp to unix time &amp; convert integer to double format\ndata = data.withColumn('timestamp_num',f.unix_timestamp('timestamp').cast('double'))\ndata.show()\n</code></pre> <pre><code>+-------------------+-----+-------------+\n|          timestamp|value|timestamp_num|\n+-------------------+-----+-------------+\n|2025-07-21 10:00:00|100.0|  1.7530812E9|\n|2025-07-21 11:00:00|110.0|  1.7530848E9|\n|2025-07-21 12:00:00|115.0|  1.7530884E9|\n|2025-07-21 13:00:00|120.0|   1.753092E9|\n+-------------------+-----+-------------+\n</code></pre> <pre><code># assemble features\nassembler = VectorAssembler(inputCols=['timestamp_num'],outputCol='features')\n\n# Define Linear Regression model\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"value\")\n\n# Build pipeline\npipeline = Pipeline(stages=[assembler, lr])\n\n# Train model\nmodel = pipeline.fit(data)\n\n# Make predictions\npredictions = model.transform(data)\npredictions.select(\"timestamp\", \"value\", \"prediction\").show()\n</code></pre> <pre><code>+-------------------+-----+------------------+\n|          timestamp|value|        prediction|\n+-------------------+-----+------------------+\n|2025-07-21 10:00:00|100.0| 101.4994629053399|\n|2025-07-21 11:00:00|110.0|107.99982096813619|\n|2025-07-21 12:00:00|115.0|114.50017903093249|\n|2025-07-21 13:00:00|120.0|121.00053709419444|\n+-------------------+-----+------------------+\n</code></pre>","tags":["pyspark"]},{"location":"blog/2025/07/23/pyspark-time-series-pipelines.html#lagrolling-pipeline","title":"Lag/Rolling pipeline","text":"<p>Incorporating lag features into the pipeline before defining the vector assembler, such as rolling mean and maximum, as well as taking the differences between different shifts in time series using <code>Window</code></p> <p><code>windowSpec = Window.partitionBy(\"user\").orderBy(\"event_time\").rowsBetween(Window.unboundedPreceding, Window.currentRow)</code></p> <pre><code>from pyspark.sql.functions import col, lag, avg, max as smax\nfrom pyspark.sql.window import Window\n\n# Sample data with timestamp and target value\ndata = [\n    (\"2025-07-21 10:00:00\", 100.0),\n    (\"2025-07-21 11:00:00\", 110.0),\n    (\"2025-07-21 12:00:00\", 115.0),\n    (\"2025-07-21 13:00:00\", 120.0),\n    (\"2025-07-21 14:00:00\", 130.0),\n    (\"2025-07-21 15:00:00\", 125.0),\n    (\"2025-07-21 16:00:00\", 135.0),\n    (\"2025-07-21 17:00:00\", 140.0),\n]\n\n# create &amp; convert string to timestamp\ndf = spark.createDataFrame(data, [\"timestamp\", \"value\"]) \\\n    .withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\ndf.show(5)\n</code></pre> <pre><code>+-------------------+-----+\n|          timestamp|value|\n+-------------------+-----+\n|2025-07-21 10:00:00|100.0|\n|2025-07-21 11:00:00|110.0|\n|2025-07-21 12:00:00|115.0|\n|2025-07-21 13:00:00|120.0|\n|2025-07-21 14:00:00|130.0|\n+-------------------+-----+\n</code></pre> <pre><code># define a window function with orderBy  ... over ( ... order by timestamp asc)\nfwindow = Window.orderBy(\"timestamp\")\n\n# define a rolling window \nrwindow = Window.orderBy(\"timestamp\").rowsBetween(-2, 0)\n</code></pre> <p><pre><code>df = df.withColumn('lag_1',f.lag('value',1).over(fwindow))\\\n        .withColumn('lag_2',f.lag('value',2).over(fwindow))\\\n        .withColumn('lag_3',f.lag('value',3).over(fwindow))\ndf.show()\n</code></pre> <pre><code>+-------------------+-----+------------------+-------------+-----+-----+-----+\n|          timestamp|value|    rolling_mean_3|rolling_max_3|lag_1|lag_2|lag_3|\n+-------------------+-----+------------------+-------------+-----+-----+-----+\n|2025-07-21 10:00:00|100.0|             100.0|        100.0| NULL| NULL| NULL|\n|2025-07-21 11:00:00|110.0|             105.0|        110.0|100.0| NULL| NULL|\n|2025-07-21 12:00:00|115.0|108.33333333333333|        115.0|110.0|100.0| NULL|\n|2025-07-21 13:00:00|120.0|             115.0|        120.0|115.0|110.0|100.0|\n|2025-07-21 14:00:00|130.0|121.66666666666667|        130.0|120.0|115.0|110.0|\n|2025-07-21 15:00:00|125.0|             125.0|        130.0|130.0|120.0|115.0|\n|2025-07-21 16:00:00|135.0|             130.0|        135.0|125.0|130.0|120.0|\n|2025-07-21 17:00:00|140.0|133.33333333333334|        140.0|135.0|125.0|130.0|\n+-------------------+-----+------------------+-------------+-----+-----+-----+\n</code></pre></p> <pre><code># Delta features: current value minus lag_1\ndf = df.withColumn(\"delta_1\", col(\"value\") - col(\"lag_1\"))\ndf.show(5)\n</code></pre> <pre><code>+-------------------+-----+------------------+-------------+-----+-----+-----+-------+\n|          timestamp|value|    rolling_mean_3|rolling_max_3|lag_1|lag_2|lag_3|delta_1|\n+-------------------+-----+------------------+-------------+-----+-----+-----+-------+\n|2025-07-21 10:00:00|100.0|             100.0|        100.0| NULL| NULL| NULL|   NULL|\n|2025-07-21 11:00:00|110.0|             105.0|        110.0|100.0| NULL| NULL|   10.0|\n|2025-07-21 12:00:00|115.0|108.33333333333333|        115.0|110.0|100.0| NULL|    5.0|\n|2025-07-21 13:00:00|120.0|             115.0|        120.0|115.0|110.0|100.0|    5.0|\n|2025-07-21 14:00:00|130.0|121.66666666666667|        130.0|120.0|115.0|110.0|   10.0|\n+-------------------+-----+------------------+-------------+-----+-----+-----+-------+\n</code></pre> <pre><code># rolling mean and max\ndf = df.withColumn(\"rolling_mean_3\", avg(\"value\").over(rwindow)) \\\n       .withColumn(\"rolling_max_3\", smax(\"value\").over(rwindow))\ndf.show(5)\n</code></pre> <pre><code>+-------------------+-----+------------------+-------------+-----+-----+-----+-------+\n|          timestamp|value|    rolling_mean_3|rolling_max_3|lag_1|lag_2|lag_3|delta_1|\n+-------------------+-----+------------------+-------------+-----+-----+-----+-------+\n|2025-07-21 10:00:00|100.0|             100.0|        100.0| NULL| NULL| NULL|   NULL|\n|2025-07-21 11:00:00|110.0|             105.0|        110.0|100.0| NULL| NULL|   10.0|\n|2025-07-21 12:00:00|115.0|108.33333333333333|        115.0|110.0|100.0| NULL|    5.0|\n|2025-07-21 13:00:00|120.0|             115.0|        120.0|115.0|110.0|100.0|    5.0|\n|2025-07-21 14:00:00|130.0|121.66666666666667|        130.0|120.0|115.0|110.0|   10.0|\n+-------------------+-----+------------------+-------------+-----+-----+-----+-------+\n</code></pre> <pre><code># Drop any rows with nulls introduced by lagging\ndf = df.na.drop()\n\n# Assemble features into a vector for ML regression\nassembler = VectorAssembler(\n    inputCols=[\"lag_1\", \"lag_2\", \"lag_3\", \"delta_1\", \"rolling_mean_3\", \"rolling_max_3\"],\n    outputCol=\"features\"\n)\n\n# Assemble features into a vector for ML regression\nassembler = VectorAssembler(\n    inputCols=[\"lag_1\", \"lag_2\", \"lag_3\", \"delta_1\", \"rolling_mean_3\", \"rolling_max_3\"],\n    outputCol=\"features\"\n)\n\n# Use linear regression model\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"value\")\n\n# Build pipeline with assembler and model\npipeline = Pipeline(stages=[assembler, lr])\n\n# Fit the pipeline model\nmodel = pipeline.fit(df)\n\n# Show predictions on training data\npredictions = model.transform(df)\npredictions.select(\"timestamp\", \"value\", \"prediction\").show(truncate=False)\n</code></pre> <pre><code>+-------------------+-----+------------------+\n|timestamp          |value|prediction        |\n+-------------------+-----+------------------+\n|2025-07-21 13:00:00|120.0|120.0000001508051 |\n|2025-07-21 14:00:00|130.0|130.00000023988125|\n|2025-07-21 15:00:00|125.0|124.9999995468485 |\n|2025-07-21 16:00:00|135.0|134.99999985059745|\n|2025-07-21 17:00:00|140.0|140.00000021186773|\n+-------------------+-----+------------------+\n</code></pre>","tags":["pyspark"]},{"location":"blog/2025/07/23/pyspark-time-series-pipelines.html#window-aggregation-group-statistics","title":"Window aggregation group statistics","text":"<p>Another approach is to instead ot <code>pyspark.sql.Window</code> utilise <code>pyspark.sql.functions</code>'s f.window,  which allows us to use time interval aggregation statistics instead of just time shifting stats</p> <p><code>f.window(\"column\", \"interval\")</code></p> <pre><code>from pyspark.sql.functions import window, avg\n\n# Sample data with timestamp and target value\ndata = [\n    (\"2025-07-21 10:00:00\", 100.0),\n    (\"2025-07-21 11:00:00\", 110.0),\n    (\"2025-07-21 12:00:00\", 115.0),\n    (\"2025-07-21 13:00:00\", 120.0),\n    (\"2025-07-21 14:00:00\", 130.0),\n    (\"2025-07-21 15:00:00\", 125.0),\n    (\"2025-07-21 16:00:00\", 135.0),\n    (\"2025-07-21 17:00:00\", 140.0),\n]\n\n# create &amp; convert string to timestamp\ndf = spark.createDataFrame(data, [\"timestamp\", \"value\"]) \\\n    .withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n\n# Windowed aggregation (e.g., 30-minute average) [interval! ]\nagg_df = df.groupBy(window(\"timestamp\", \"30 minutes\")).agg(avg(\"value\").alias(\"avg_value\"))\n\n# Feature assembler for ML\nassembler = VectorAssembler(inputCols=[\"avg_value\"], outputCol=\"features\")\n\n# Linear regression model for forecasting\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"avg_value\")\n\n# Pipeline assembling\npipeline = Pipeline(stages=[assembler, lr])\n\n# Fit the model\nmodel = pipeline.fit(agg_df)\n\n# Predict and show results\npredictions = model.transform(agg_df)\npredictions.show(truncate=False)\n</code></pre> <pre><code>+------------------------------------------+---------+--------+------------------+\n|window                                    |avg_value|features|prediction        |\n+------------------------------------------+---------+--------+------------------+\n|{2025-07-21 10:00:00, 2025-07-21 10:30:00}|100.0    |[100.0] |99.99999999999962 |\n|{2025-07-21 11:00:00, 2025-07-21 11:30:00}|110.0    |[110.0] |109.9999999999998 |\n|{2025-07-21 12:00:00, 2025-07-21 12:30:00}|115.0    |[115.0] |114.99999999999989|\n|{2025-07-21 13:00:00, 2025-07-21 13:30:00}|120.0    |[120.0] |119.99999999999997|\n|{2025-07-21 14:00:00, 2025-07-21 14:30:00}|130.0    |[130.0] |130.00000000000014|\n|{2025-07-21 15:00:00, 2025-07-21 15:30:00}|125.0    |[125.0] |125.00000000000006|\n|{2025-07-21 16:00:00, 2025-07-21 16:30:00}|135.0    |[135.0] |135.00000000000023|\n|{2025-07-21 17:00:00, 2025-07-21 17:30:00}|140.0    |[140.0] |140.0000000000003 |\n+------------------------------------------+---------+--------+------------------+\n</code></pre> <p>Thank you for reading!</p> <p>Any questions or comments about the posts below can be addressed to the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>","tags":["pyspark"]},{"location":"portfolio/index.html","title":"Portfolio","text":""},{"location":"portfolio/index.html#data-science-projects","title":"Data Science Projects","text":"<p>Data Science is indeed an exciting and rapidly growing field that combines various disciplines such as statistics, computer science, and domain knowledge to extract insights and knowledge from data. The field of Data Science has gained significant attention in recent years due to the explosion of data generated by digital technologies and the increasing importance of data-driven decision-making in various industries.</p> <p>A collection of data science projects which utilise machine and/or deep learning, I've grouped them based on the most relevant topic for your conveniene.</p> <ul> <li> Natural Language Processing</li> <li> Business</li> <li> Finance</li> <li> Physics</li> <li> Health</li> <li> Geospatial</li> </ul>"},{"location":"portfolio/index.html#learning-material","title":"Learning Material","text":"<p>A collection of material aimed more for learning</p> <ul> <li> Apache PySpark Basics </li> <li> Recommendation Systems</li> <li> Natural Language Processing</li> <li> Machine Learning</li> </ul>"},{"location":"portfolio/index.html#internship-reports","title":"Internship Reports","text":"<ul> <li> Lloyds Banking Group Data Science</li> </ul> <p>Other ...</p> <p>Any questions or comments about the above post can be addressed on the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>"},{"location":"portfolio/biology.html","title":"Biology","text":"<p>Machine learning plays a big part in biology due to its ability to analyze and make sense of large and complex biological datasets. Here are a few reasons why machine learning is important in biology:</p> <ol> <li> <p>Data analysis: Biology generates vast amounts of data, such as genomic sequences, protein structures, and gene expression profiles. Machine learning algorithms can process and analyze these datasets to identify patterns, relationships, and insights that may not be apparent to human researchers.</p> </li> <li> <p>Predictive modeling: Machine learning can be used to build predictive models in biology. For example, it can predict protein structures, drug-target interactions, or disease outcomes based on genetic or clinical data. These models help researchers understand biological processes, develop new drugs, and diagnose diseases more accurately.</p> </li> <li> <p>Image analysis: Machine learning techniques, such as deep learning, have revolutionized image analysis in biology. They can automatically detect and classify features in images, such as cells, tissues, or organisms. This is particularly useful in fields like microscopy, pathology, and genetics.</p> </li> </ol> <p>and of course many more...</p>"},{"location":"portfolio/biology.html#identifying-antibiotic-resistant-bacteria","title":"Identifying Antibiotic Resistant Bacteria","text":"<p>In this study, we investigate data associated with antibiotic resistance for different <code>bacteria</code>, conducting an explotatory data analysis (looking at geospatial relations &amp; sankey diagrams) &amp; train antibiotic resistance models for different types antibiotics, based on unitig (part of DNA) data (which convey the presence or absence of a particular nucleotide sequence) in the Bacteria's DNA. We train a model(s) that is able to distinguish whether the bacteria is resistant to a particular antibiotic or not resistant and determine which unitigs are the most influential in the model's prediction.</p>  ![img](images/kfold_antibiotics.png)"},{"location":"portfolio/biology.html#hummingbird-classification-keras-cnn-models","title":"Hummingbird Classification | Keras CNN Models","text":"<p>In this project, we aimed to create an automated hummindbird recognition deep learning model. In our quest to create an automated approach, we can be left with a collection or under or over exposed images that will create difficulties for the model to distinguish between different classes correctly. For this reason we tried different image augmentation techniques &amp; and different combinations of them and found combinations that would generalise well in a variety of ambient lighting conditions. We also trained a basic convolution type model &amp; fine tuned different SOA (such as ResNet) models in order to utilise already preexisting model features. </p>  ![](images/finetune1.png) ![](images/finetune2.png)"},{"location":"portfolio/biology.html#transcription-factor-binding-location-prediction","title":"Transcription Factor Binding Location Prediction","text":"<p>The process of converting a nucleotide sequence into an amino acid chain containing proteins is not a very straightforward process, the complex process is not straightforward to not easy to understand. What we can do is attempt to utilise deep learning in order to model a relation for our biological phenomenon associated with the above biological process. Our model model will attempt to predict segments in the DNA at which a so called transcription factor will attach itself, the problem is treated as a binary classification problem. The model itself contains 1D convolution blocks &amp; is very simple in its structure. To improve the model accuracy, we try a couple of things: sample weighting, dropout effects, all of which have a prositive effect on the generalisation performance.</p> <p></p>"},{"location":"portfolio/biology.html#histopathic-cancer-detection","title":"Histopathic Cancer Detection","text":"<p>In this project, we dive into the world of computer vision. Microscopic evaluation of histopathalogic stained tissue &amp; its subsequent digitalisation is now a more feasible due to the advances in slide scanning technology, as well a reduction in digital storage cost in recent years. There are certain advantages that come with such digitalised pathology; including remote diagnosis, instant archival access &amp; simplified procedure of consultations with expert pathologists. Digitalised Analysis based on Deep Learning has shown potential benefits as a potential diagnosis tool &amp; strategy. Assessment of the extent of cancer spread by histopathological analysis of sentinel axillary lymph nodes (SLNs) is an essential part of breast cancer staging process. The aim of this study was to investigate the potential of using PyTorch Deep Learning module for the detection of metastases in SLN slides and compare them with the predefined pathologist diagnosis.</p> <p></p> <p>Any questions or comments about the above post can be addressed on the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>"},{"location":"portfolio/business.html","title":"Business","text":"<p>Machine learning offers businesses a range of benefits, including predictive analytics to forecast future trends and outcomes, customer segmentation to tailor marketing strategies, and personalized recommendations to enhance customer satisfaction. Additionally, machine learning can help detect fraudulent activities, automate repetitive tasks, and streamline business processes, leading to increased efficiency and cost savings. Natural language processing can also be used to automate customer service interactions and analyze customer feedback. Overall, machine learning can improve decision-making, increase efficiency, and give businesses a competitive advantage in the market.</p> <p></p> <p>Thank you for reading!</p> <p>Any questions or comments about the posts below can be addressed to the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>"},{"location":"portfolio/business.html#modeling-of-product-stock-levels","title":"Modeling of product stock levels","text":"<p> Run on Colab</p> <p>Part of the Cognizant Artificial Intelligence Internship program </p> <p>Groceries are highly perishable items. If you overstock, you are wasting money on excessive storage and waste, but if you understock, then you risk losing customers. </p> <ul> <li>In this project, we work with a client Gala Groceries, who has contacted Cognizant for logistics advice about product storage</li> <li>Specifically, they are interested in wanting to know how better stock the items that they sell.</li> <li>Our role is to take on this project as a data scientist and understand what the client actually needs. This will result in the formulation/confirmation of a new project statement, in which we will be focusing on predicting stock levels of products.</li> <li>Such a model would enable the client to estimate their product stock levels at a given time &amp; make subsequent business decisions in a more effective manner reducing understocking and overstocking losses.</li> <li>We also explore how well different models perform and give feedback to the client about what most significantly affects the stock levels.                                                             </li> </ul>"},{"location":"portfolio/business.html#retaining-customer-clients-vulnerable-to-churn","title":"Retaining customer clients vulnerable to churn","text":"<p> GitHub Repository</p> <p>Part of the BCG Data Science Internship program </p> <p>In this project, we work with another client who is a major gas and electricity utility that supplies to small and medium sized enterprises</p> <ul> <li>The energy market has had a lot of change in recent years and there are more options than ever for customers to choose from</li> <li>They are concerned about their customers leaving for better offers from other energy providers</li> <li>We investigate whether price sensitivity is the most influential factor for a customer churning</li> <li>Conduct feature engineering that is used to test hypotheses related to customer churn</li> <li>And finally we utilise predictive modelling so that it can be used to highlight customers at risk of churn</li> </ul>"},{"location":"portfolio/financial.html","title":"Financial","text":"<p>Machine learning plays a significant role in finance by helping to analyse large volumes of financial data, identify patterns and trends, make predictions, and automate decision-making processes. It can be used for tasks such as risk management, fraud detection, algorithmic trading, credit scoring, customer segmentation, and personalized investment recommendations. Machine learning algorithms can process and analyze data much faster and more accurately than humans, enabling financial institutions to make better-informed decisions and improve their overall performance.</p> <p></p> <p>Thank you for reading!</p> <p>Any questions or comments about the posts below can be addressed to the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>"},{"location":"portfolio/financial.html#customer-transaction-predictive-analytics","title":"Customer Transaction Predictive Analytics","text":"<p> Run on Colab  |  Download dataset</p> <p>Part of the Data@ANZ Internship program </p> <ul> <li>The aim of this study was to analyse ANZ customer banking transactions, visualise trends that exist in the data, investigate spending habits of customers &amp; subsequently determine the annual income of each customer, based on debit/credit transactions. </li> <li>From all available customer &amp; transaction data, the next challenge was to create a machine learning model that would estimate this target variable (annual income), which would be used on new customers. </li> <li>Based on the deduced data, we created several regression models that were able to predict annual income with relatively high accuracy. </li> <li>Due to the limitation of available data, two approaches were investigates, transaction based (all transactions) &amp; customer aggregative (customer's transaction) &amp; subsequently their differences were studied</li> </ul>"},{"location":"portfolio/financial.html#building-an-asset-trading-strategy","title":"Building an Asset Trading Strategy","text":"<p>A major drawback of crypocurrency trading is the volatility of the market. The currency trades can occur 24/7 &amp; tracking crypto position can be an impossible task to manage without automation. Automated Machine Learning trading algorithms can assist in managing this task, in order to predict the market's movement </p> <ul> <li>The problem of predicting a buy (value=1) or sell (value=0) signal for a trading strategy is defined in the binary classification framework. </li> <li>The buy or sell signal are decided on the basis of a comparison of short term vs. long term price. Data harvesting (just data collection here) &amp; feature engineering are relevant factors in time series model improvement.</li> <li>It's interesting to investigate whether traditionally stock orientated feature engineering modifications are relevant to digital assets, and if so which ones. Last but not least, model generation efficiency becomes much more significant when dealing with High Frequency Tick Data as each added feature can have a substatial impact on the turnaround time of a model, due to the amount of data &amp; balancing model accuracy &amp; model output turnaround time is definitely worth managing.</li> </ul>"},{"location":"portfolio/financial.html#prediction-of-stable-customer-funds","title":"Prediction of Stable Customer Funds","text":"<p> Run on Colab  |  Download dataset</p> <p>In this project, we aim to create machine learning models that will be able to predict future customer funds, based on historical trends</p> <ul> <li>The total customer assets can vary significantly in time, and since banks are in the business of lending money, this is needed for more accurate fund allocation (optimise the allocation for lending) so they can be utilised for credit applications. </li> <li>We utilise gradient boosting models (CatBoost) &amp; do some feature engineering in order to improve the models for short term predictions (3 month) and longer term predictions (6 months). </li> <li>Having created baseline models, we also optimise the model hyperparameters using Optuna for different prediction periods.</li> </ul>"},{"location":"portfolio/geospatial.html","title":"Geospatial","text":"<p>Any questions or comments about the above post can be addressed on the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>"},{"location":"portfolio/geospatial.html#australian-geospatial-analysis","title":"Australian Geospatial Analysis","text":"<p> Github Repository</p> <p>An exploratory data analysis of geospatial data</p> <ul> <li>In this study, we provide a brief overview on what type of geospatial library tools we can use to visualise &amp; analyse map geospatial data, such as Choropleth, Hexbin, Scatter and Heatmaps. </li> <li>In particular, we explore Australian based geospatial maps &amp; visualisation data. </li> <li>We look at problems such as unemployment rates for different states and demographic. </li> <li>Analyse housing median values, house sale locations for different suburbs as well as use kriging interpolation model to estimate temperatures at locations for which we don't have data</li> </ul>"},{"location":"portfolio/health.html","title":"Health","text":"<p>Health is an important topic in machine learning because it has the potential to significantly improve healthcare outcomes. Machine learning algorithms can be used to analyse large amounts of medical data and identify patterns that may not be immediately apparent to human analysts. This can help doctors and researchers make more accurate diagnoses, develop more effective treatments, and even predict and prevent certain diseases.</p> <p></p> <p>Thank you for reading!</p> <p>Any questions or comments about the posts below can be addressed to the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>"},{"location":"portfolio/health.html#lower-back-pain-symptoms-modeling","title":"Lower Back Pain Symptoms Modeling","text":"<p>In this study we investigate patient back pain biomedical data obtained from a medical resident in Lyon. </p> <ul> <li>We create a classification model which is able to determine the difference between normal patients and patients who have either disk hernia or spondylolisthesis, which is a binary classification problem. </li> <li>We utilise PyTorch and created a custom dataset class to load the tabular CSV data &amp; load the data into batches using data loaders. </li> <li>A rather simple neural network structure that utilises standard generalisation strategies such as dropout and batch normalisation was assembled &amp; the model was trained and tested in the validation dataset.                                                         </li> </ul>"},{"location":"portfolio/health.html#ovarian-phase-classification-in-felids","title":"Ovarian Phase Classification in Felids","text":"<p>In this study, we investigate feline reproductology data, conducting an exploratory data analysis of experimental measurements of estradiol and progesterone levels and attempt to find the relation between different hormone levels during different phases of pregnancy.  - We  then use the available data to create machine learning models that are able to predict at which stage of an estrous cycle a feline is at the time of testing for different measurement methods, which is a multiclass classification problem.</p>"},{"location":"portfolio/health.html#heart-disease-classification","title":"Heart Disease Classification","text":"<p>In this study, we explore different feature engineering approaches, we group features into different combinations based on their subgroup types and attempt to find the best combination for classifying patients with heart disease. </p> <ul> <li>Having found the best feature combinations, we utilise brute force grid searches for hyperparameter optimisation in order to find the best performing model</li> <li>We utilise an sklearn compatible custom Regressor model (model found here) based on Kriging, which we turned in a classifier by simply setting the threshold to 0.5 (basically the prediction method in sklearn models). </li> <li>We also tried different ensembles of different models in order to improve the model accuracy even further.</li> </ul>"},{"location":"portfolio/nlp.html","title":"Nlp","text":"<p>Natural language processing (NLP) is a branch of artificial intelligence (AI) that deals with the interaction between computers and humans using natural language. It involves the development of algorithms and computational models that can understand, analyze, and generate human language. </p> <p></p> <p>Thank you for reading!</p> <p>Any questions or comments about the above post can be addressed on the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>"},{"location":"portfolio/nlp.html#banking-consumer-complaint-analysis","title":"Banking Consumer Complaint Analysis","text":"<p>In this study, we aim to create an automated ticket classification model for incoming text based complaints, which is a multiclass classification problem. </p> <ul> <li>Such a model is useful for a company in order to automate the process of sorting financial product reviews &amp; subsequently pass the review to an experient in the relevant field. </li> <li>We explore traditional ML methods, which utilise hidden-state BERT embedding for features, as well as fine-tune DistilBert for our classification problem &amp; compare the two approaches, comparing the model accuracies of both approaches</li> </ul>"},{"location":"portfolio/nlp.html#news-sentiment-based-trading-strategy","title":"News sentiment based trading strategy","text":"<p>In this project, we'll apply NLP to financial stock movement prediction. Using NLP, we can ask ourselves questions such as, how positive or negative a news article (related to financial markets is). It provides a way to monitor financial market sentiments by utilising any text based source so we can determine whether the text based source posted on specific day has a positive or negative sentiment score. By combining historic market data with news sources related to financial markets, we can create a trading strategy that utilises NLP. The whole project revolved around generating accurate sentiment labels that would correlate to event returns</p> <p>Based on historic data, we can calculate the return of a specific event, however one of challenges to utilise NLP for such application are the target labels and the ground truths would be set as the even return direction. We first need to create a model that is able to accurately define the sentiment of the news source, to do this we try a couple of different approaches: </p> <ul> <li>The first method, we manually define labels and evaluate the performance of the model. The manual approach utilised three strategies combined into one (percentage value extraction, TextBlob &amp; Beat/Misses). For encoding, we utilised static spacy word embeddings &amp; investigated how the dimensionality of the vectors affected the model accuracy.</li> <li>We also utilised an expertly labeled dataset &amp; tested the resulting model on the dataset, however there wasn't a too significant increase in accuracy.</li> </ul> <p>The best performance boost came from the utilisation of Deep Learning LSTM with a trainable embedding laber architecture, which showed much better generalisation performance than classical machine learning models, including ensemble methods</p> <p>The last approach we tried as VADER, which allows us to utilise a custom lexicon, which we can change to something more related: financial markets. It was interesting to note that the VADER approach resulted in a high postive correlation to event return</p>"},{"location":"portfolio/nlp.html#twitter-emotion-classification","title":"Twitter Emotion Classification","text":"<p>In this study, we fine-tune a transformer model so it can classify the sentiment of user tweets for 6 different emotions (multiclass classification). </p> <ul> <li>We first create a baseline by utilising traditional ML methods, which for features use extracted BERT embeddings for each sentence. </li> <li>Once we have our baseline model, we then turn to more complex transformer models, DistilBert &amp; fine-tune its model weights for our classification problem</li> </ul>"},{"location":"portfolio/nlp.html#edx-course-recommendations","title":"edX Course Recommendations","text":"<p>In this study, we create an NLP based recommendation system which informs a user about possible courses they make like, based on a couse they have jusy added</p> <ul> <li>We will utilise scrapped edX course description data, clean the text data and then convert document into vector form using two different approaches BoW based TF-IDF and word2vec,</li> <li>Then utilising consine similarity we calculate the most relevant courses which we can recommendand</li> </ul>"},{"location":"portfolio/nlp.html#creating-a-transformer-attention-encoder","title":"Creating a Transformer Attention Encoder","text":"<p>In this study, we look at some of the basics of a transformer architecture model, the encoder, by writing and utilising custom pytorch classes. Encoder simply put: Converts a series tokens into a series of embedding vectors (hidden state) &amp; consists of multiple layers (blocks) constructed together </p> <p>The encoder structure:</p> <ul> <li>Composed of multiple encoder layers (blocks) stacked next to each other (similar to CNN layer stacks)</li> <li>Each encoder block contains multi-head self attention &amp; fully connected feed forward layer (for each input embedding)</li> </ul> <p>Purpose of the Encoder:</p> <ul> <li>Input tokens are encoded &amp; modified into a form that stores some contextual information in the sequence</li> </ul> <p>The basis of the encoder can be utilised for a number of different applications, as is common in HuggingFace, we'll create a simple tail end classification class, so the model can be utilised for classification</p>"},{"location":"portfolio/nlp.html#banking-user-review-analysis-modeling","title":"Banking User Review Analysis &amp; Modeling","text":"<p> 1 | Parsing Dataset |  Github Notebook</p> <p>In this study we look at the parsing/scraping side of data, extracting the customer review data </p> <ul> <li>Its no secret that a lot text important information is stored on websites, as a result, for us to utilise this data in our of analyses and modeling, we need a way to extract this information, this process is referred to website parsing. </li> <li>For our study we need to extract customer user reviews from irecommend. </li> <li>We'll be parsing a common banking coorporation that offers a variety of services so the reviews aren't too specific to a particular product. </li> <li>Having parsed our dataset, we'll follow up this with a rather basic exploratory data analysis based on ngram word combinations, so we can very quickly understand the content of the entire corpus.</li> </ul> <p> 2 | Banking Product Review Sentiment Modeling |  Github Notebook</p> <p>Once we have parsed and created our dataset, we look at creating a sentiment model based on traditional NLP machine learning approaches. </p> <ul> <li>We will be using the parsed dataset about bank service reviews, which consists of ratings as well as recommend/don't recommend type labels.</li> <li>We'll be using TF-IDF &amp; Word2Vec methods to encode text data &amp; use typical shallow and deep tree based enseble models. </li> <li>Once we have found the best performing approaches, we'll be doing a brute force based GridSearchCV hyperparameter optimisation in order to tune our model. After selecting the best model, we'll make some conclusions about our predicts &amp; make some future work comments.</li> </ul>"},{"location":"portfolio/nlp.html#mllibs","title":"mllibs","text":"<p>mllibs is a project aimed to automate various processes using text commands</p> <ul> <li>Development of such helper modules are motivated by the fact that everyones understanding of coding &amp; subject matter (ML in this case) may be different</li> <li>Often we see people create functions and classes to simplify the process of code automation (which is good practice)</li> <li>Likewise, NLP based interpreters follow this trend as well, except, in this case our only inputs for activating certain code is natural language</li> <li>Using python, we can interpret natural language in the form of string type data, using natural langauge interpreters mllibs aims to provide an automated way to do machine learning using natural language</li> </ul>"},{"location":"portfolio/nlp.html#otus-nlp-course-work","title":"OTUS NLP Course Work","text":"<p>Natural language course related work on a variety of Natural Language Processing topics</p>"},{"location":"portfolio/nlp.html#ner-with-preset-tools-renatasha","title":"NER with preset tools (re,natasha)","text":"<p>In this notebook, we look at how to utilise the natasha &amp; re libraries to do predefined NER  tagging </p> <ul> <li>natasha comes with already predefined set of classification labels, whilst re can be used to identify capitalised words using regular expressions. </li> <li>These tools, together with lemmatisers from <code>pymorphy2</code> allow us to very easily utilise ready instruments for named entity recognition in documents without any model training.</li> </ul>"},{"location":"portfolio/nlp.html#training-a-ner-model-with-gru","title":"Training a NER model with GRU","text":"<p>In this project, we train a neural network NER model based on GRU architecture, which can recognise named entities using BIO tags based on car user review data. </p> <ul> <li>Unlike the previous notebook, the concept of NER is used a little more abstractly, we are interested in any markups for word(s) that we create in the text, not just names.</li> <li>For markups we use tags that describe the quality of the car (eg. appearance, comfort, costs, etc.). </li> <li>The model learns to classify tokens in the text that belong to one of the tag classes. </li> <li>Recognition of such labels is convenient for quick understanding of the content of the review.</li> </ul>"},{"location":"portfolio/nlp.html#sentiment-analysis-of-kazakh-news","title":"Sentiment Analysis of Kazakh News","text":"<p>In this project we create create a model that is used for sentiment analysis using classical NLP + machine learning approaches </p> <ul> <li>We utilise standard baseline approaches such as <code>TF-IDF</code> and <code>BoW</code> together with <code>RandomForestClassifier</code> and standard <code>train_test_split</code> to train and evaluate the generalisation performance of the model using <code>f1_score</code> metric since we end up having slightly disbalanced sentiment classes</li> </ul>"},{"location":"portfolio/nlp.html#fine-tuning-bert-for-multilabel-toxic-comment-classification","title":"Fine Tuning BERT for Multilabel Toxic Comment Classification","text":"<p>In this project we will be creating a multilabel model for toxic comment classification using transfomer architecture BERT. </p> <ul> <li>This main difference between multilabel and multiclass classification is that we are treating this as a binary classification problem, but checking for multiple labels for whether the text belongs to the class or not.</li> </ul>"},{"location":"portfolio/nlp.html#fine-tuning-bert-for-linguistic-acceptability","title":"Fine Tuning BERT for Linguistic Acceptability","text":"<p>In this project we will be fine-tuning a transformer model for the language acceptability problem (CoLa) problem. </p> <ul> <li>The CoLa dataset itself is a benchmark dataset for evaluating natural language understanding models. </li> <li>CoLa stands for \"Corpus of Linguistic Acceptability\" and consists of sentences from various sources, such as news articles and fiction, that have been labeled as either grammatically correct or incorrect. </li> <li>The dataset is commonly used to evaluate models' ability to understand and interpret the grammatical structure of sentences. </li> <li>For this task we'll be utilising the bert-base-uncased model and utilise huggingface's convenient downstream task task adaptation variation for binary classification using BertForSequenceClassification </li> </ul>"},{"location":"portfolio/nlp.html#customer-service-dialogue-system-for-grabrfi","title":"Customer Service Dialogue System for GrabrFi","text":"<p>As part of the final project of the nlp course, the aim of the project was to create a dialogue system for a banking service business GrabrFi, focusing on combining various NLP methods that can be utilised in chatbots. </p> <ul> <li>Combining a Telegram structure that utilises TF-IDF with cosine_similarity, multiclass classification based approach, Question Answering (BERT), generative (DialoGPT). </li> <li>The task of answering user questions and queries was split up into different subgroups found in the help section so that each model would be in charge of its own section, as a result of experimenting with different method activation thresholds, a dialogue system that utilised all of the above methods was created, and all methods were able to work together. </li> <li>This allowed for an understanding of the different approaches that can be utilised in the creation of a dialogue system. </li> </ul>"},{"location":"portfolio/physics.html","title":"Physics","text":"<p>Machine learning plays a significant role in physics-related applications by enabling the analysis, prediction, and modeling of complex physical phenomena. Here is an example of how machine learning is used in physics:</p> <p>Data analysis and pattern recognition: Machine learning algorithms can be used to analyze large datasets generated from experiments or simulations. They can identify patterns, correlations, and anomalies that may not be apparent to human researchers, helping to gain insights into physical processes. </p> <p></p> <p>Thank you for reading!</p> <p>Any questions or comments about the above post can be addressed on the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>"},{"location":"portfolio/physics.html#cfd-trade-off-study-visualisation-response-model","title":"CFD Trade-Off Study Visualisation | Response Model","text":"<p> Github Repository</p> <p>In this study, we do an exploratory data analysis of a CFD optimisation study, having extracted table data for different variables in a simulation, we aim to find the most optimal design using different visualisation techniques. </p> <ul> <li>The data is then utilised to create a response model for L/D (predict L/D based on other parameters), we investigate which machine learning models work the best for this problem</li> </ul>"},{"location":"portfolio/physics.html#gaussian-processes-airfoil-noise-modeling","title":"Gaussian Processes | Airfoil Noise Modeling","text":"<p>In this study, we do an exploratory data analysis of experimental measurement data associated with NACA0012 airfoil noise measurements. </p> <ul> <li>We outline the dependencies of parameter and setting variation and its influence on SPL noise level. </li> <li>The data is then used to create a machine learning model, which is able to predict the sound pressure level (SPL) for different combinations of airfoil design parameters</li> </ul>"},{"location":"portfolio/physics.html#spectogram-broadband-model-peak-identifier","title":"Spectogram Broadband Model &amp; Peak Identifier","text":"<p>Noise generally can be divided into broadband noise (general noise level) &amp; tonal noises (peaks at specific frequency bins). </p> <ul> <li>They don't have precise definitions, but broadband noises can be abstractly defined as the general noise level in an environement coming from various locations, creating a broad frequency range noise relation to output noise level. </li> <li>Tonal noise sources tend be associated to very clearly distinguishible noise peaks at specific frequencies ( or over a small frequency range ). </li> <li>When we look at a spectogram, each bird specie tends to create quite a repetitive collection of freq vs time structures, usually across a specific frequency range, usually it's a combination of tonal peaks that make up an entire bird call. In this approach, the two terms are used even looser, since there is a time element to this model from the STFT, which can be useful in a variety of scenarios. </li> <li>The tonal peak frequency identification approach relies on the assumption that the more data is fed into the system, the more precise the result should get, as occasional secondary birds &amp; other noises should eventually start to show more dissipative distribution in the entire subset that is analysed.</li> </ul> <p>Looping over all desired audio files of a subset of interest to us (a particular primary label subset):</p> <ul> <li>First, we load an audio recording that we wish to convert to desired to be used as inputs for CNN models.</li> <li>The audio is then split into segments that will define the spectogram time domain limits. Usually we would start with the entire frequency range [0,12.5kHz] and split the recording into a 5 second chunks, creating a time &amp; frequency domain relation.</li> <li>For reference, we find the maximum dB value in the entire frequency range, this model will define the peaks of the tonal noises and will always be the maximum.</li> <li>The spectogram is then divided into time bins, cfg.model_bins &amp; for each time bin, the maximum value for each frequency is determined.</li> <li>A model (kriging) for each time bin is created and a simple enemble of all time segments is constructed, this should always create a model that is lower in dB level than the global peak model mentioned earlier. There are certain cases where this is not the case, usually an indicator that there exist an anomaly in the structure of the curve (as shown in the example below).</li> <li>The peaks of the model are then found using scipy's find_peaks module, stored into a global list &amp; the Counter module counts all list entries.</li> <li>The results are subsequently plotted for each pixel value. The corresponding frequency values can be extracted using the function pxtohz.</li> </ul>"},{"location":"portfolio/pyspark_basics.html","title":"PySpark Basics","text":""},{"location":"portfolio/pyspark_basics.html#pyspark-basics","title":"PySpark Basics","text":"<p>Some simple and brief collection of pyspark operations </p> <ul> <li> <p> Select, Drop, Rename Columns</p> <ul> <li>Column selection</li> <li>Dropping columns</li> <li>Renaming of columns</li> </ul> </li> <li> <p> Pipelines</p> <ul> <li>Missing data treatment classification pipeline</li> <li>Feature scaling using ScandardScaler classification pipeline</li> <li>TF-IDF corpus classification pipeline</li> <li>PCA dimensionality reduction classification pipeline</li> </ul> </li> <li> <p> Data Filtration</p> <ul> <li>Filtration by column value</li> <li>String related filtration using like / contains</li> <li>Missing data filtration</li> <li>List based filtration using isin</li> <li>General data clearning operations</li> </ul> </li> <li> <p> Aggregations</p> <ul> <li>Aggregations without grouping</li> <li>Aggregations with grouping </li> <li>Filtering after grouping</li> </ul> </li> <li> <p> Pivoting</p> <ul> <li>Basic pivot operation</li> <li>Pivot with multiple aggregations</li> <li>Conditional pivoting</li> <li>Pivoting with specified column values</li> </ul> </li> <li> <p> Time Series Pipelines</p> <ul> <li>Basic pipeline conversion of timestamp to unix time</li> <li>Lag feature combination pipelines </li> <li>Aggregation based statistics pipelines</li> </ul> </li> </ul>"},{"location":"portfolio/pyspark_basics.html#basic-classification-project","title":"Basic Classification Project","text":"<p>A classification project for beginners, shows how one can utilise pyspark in a machine learning project</p> <ul> <li> <p> Data Preprocessing with PySpark</p> <ul> <li>We'll look at how to start a spark_session</li> <li>Setting up data types for the dataset using StructType</li> <li>Focuses on data preparation in the preprocessing state</li> </ul> </li> <li> <p> Training ML Models with PySpark</p> <ul> <li>Using spark.ml.classification to train binary classification models</li> <li>Introduction to StringIndexer, VectorAssembler</li> <li>Splitting dataset into subsets using .randomSplit</li> <li>Saving &amp; loading models</li> </ul> </li> <li> <p> Hyperparameter Tuning with Pipelines</p> <ul> <li>Using spark.ml.pipeline introduce a compact training approach</li> <li>Saving &amp; loading pipelines</li> <li>Model evaluation using MulticlassClassificationEvaluator</li> <li>pyspark.ml.tuning for hyperparameter optimisation</li> </ul> </li> </ul>"},{"location":"portfolio/course_nlp/index.html","title":"NLP","text":"<p>In this section we focus on Natural Language Processing </p>"},{"location":"portfolio/course_nlp/index.html#course-material","title":"Course Material","text":"<ul> <li>\u0412\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u043b\u043e\u0432 (BoW)</li> <li>\u0412\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u043b\u043e\u0432 (Embedding)</li> <li>\u0420\u0430\u0441\u043f\u043e\u0437\u043d\u043e\u0432\u0430\u043d\u0438\u044f \u0418\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0421\u0443\u0449\u043d\u043e\u0441\u0442\u0435\u0439 w/ Natasha/Re</li> <li>\u0420\u0430\u0441\u043f\u043e\u0437\u043d\u043e\u0432\u0430\u043d\u0438\u044f \u0418\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0421\u0443\u0449\u043d\u043e\u0441\u0442\u0435\u0439 w/ GRU</li> </ul>"},{"location":"portfolio/course_nlp/index.html#projects","title":"Projects","text":"<p>Different projects undertaken</p> <ul> <li>\u041f\u0430\u0440\u0441\u0438\u043d\u0433 \u0421\u0430\u0439\u0442\u0430 \u041e\u0442\u0437\u044b\u0432\u043e\u0432</li> <li>\u041d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u044c \u043a\u0430\u043a \u0423\u043d\u0438\u0432\u0435\u0440\u0441\u0430\u043b\u044c\u043d\u044b\u0439 \u0410\u043f\u043f\u0440\u043e\u043a\u0441\u0438\u043c\u0430\u0442\u043e\u0440</li> <li>\u0410\u043d\u0430\u043b\u0438\u0437 \u0422\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u041e\u0442\u0437\u044b\u0432\u043e\u0432</li> </ul>"},{"location":"portfolio/course_nlp/1_parsing.html","title":"1 parsing","text":"In\u00a0[\u00a0]: Copied! <pre># all page review sources\n\n# sources = ['https://irecommend.ru/content/sberbank?new=50']\n# sources.extend([f'https://irecommend.ru/content/sberbank?page={i}&amp;new=50'for i in range(1,2)])\n# sources\n\nsources = [f'https://irecommend.ru/content/sberbank?page={i}&amp;new=50'for i in range(22,24)]\nsources\n</pre> # all page review sources  # sources = ['https://irecommend.ru/content/sberbank?new=50'] # sources.extend([f'https://irecommend.ru/content/sberbank?page={i}&amp;new=50'for i in range(1,2)]) # sources  sources = [f'https://irecommend.ru/content/sberbank?page={i}&amp;new=50'for i in range(22,24)] sources Out[\u00a0]: <pre>['https://irecommend.ru/content/sberbank?page=22&amp;new=50',\n 'https://irecommend.ru/content/sberbank?page=23&amp;new=50']</pre> <ul> <li>\u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438</li> <li>\u041f\u0440\u0438\u0433\u043e\u0434\u044f\u0442\u0441\u044f \u0432\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0434\u043b\u044f \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0441\u043f\u0438\u0441\u043a\u043e\u0432 (\u0434\u043b\u044f \u0432\u044b\u0433\u0440\u0443\u0437\u043a\u0438 \u0438\u043d\u0434\u0438\u0432\u0438\u0434\u0443\u0430\u043b\u044c\u043d\u044b\u0445 \u043e\u0442\u0437\u044b\u0432\u043e\u0432)</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import requests\nfrom bs4 import BeautifulSoup\nimport time as t\nfrom tqdm.notebook import tqdm\nfrom random import randint\nimport numpy as np\nimport json\n\ndef write_list(a_list):\n    print(\"Started writing list data into a json file\")\n    with open(\"names.json\", \"w\") as fp:\n        json.dump(a_list, fp)\n        print(\"Done writing JSON data into .json file\")\n    fp.close()\n\n# Read list to memory\ndef read_list():\n    # for reading also binary mode is important\n    with open('names.json', 'rb') as fp:\n        n_list = json.load(fp)\n        return n_list\n</pre> import requests from bs4 import BeautifulSoup import time as t from tqdm.notebook import tqdm from random import randint import numpy as np import json  def write_list(a_list):     print(\"Started writing list data into a json file\")     with open(\"names.json\", \"w\") as fp:         json.dump(a_list, fp)         print(\"Done writing JSON data into .json file\")     fp.close()  # Read list to memory def read_list():     # for reading also binary mode is important     with open('names.json', 'rb') as fp:         n_list = json.load(fp)         return n_list <ul> <li>\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u0441\u0435\u0441\u0441\u0438\u044e \u0432 <code>requests</code>, \u0438\u0437 \u0431\u0440\u0430\u0443\u0437\u0435\u0440\u0430 \u043c\u044b \u043a\u043e\u043f\u0438\u0440\u0443\u0435\u043c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u0438 \u0432\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0435\u0435 \u0432 \u043d\u0430\u0448\u0443 \u0441\u0435\u0441\u0441\u0438\u044e</li> <li>\u042d\u0442\u043e \u043f\u043e\u043c\u043e\u0433\u0430\u0435\u0442 \u043e\u0431\u0445\u043e\u0434\u0438\u0442\u044c \u043f\u0435\u0440\u0438\u043e\u0434\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u0431\u043b\u043e\u043a\u0438\u0440\u043e\u0432\u043a\u0443 \u0432\u044b\u0433\u0440\u0443\u0437\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445</li> <li>\u0422\u0430\u043a \u0436\u0435 \u043c\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c proxy \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u043d\u0430\u0448 IP \u043d\u0435 \u0432\u043d\u0435\u0441\u043b\u0438 \u0432 \u0447\u0435\u0440\u043d\u044b\u0439 \u0441\u043f\u0438\u0441\u043e\u043a \u043f\u0440\u0438 \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u0438\u0438 \u043f\u0430\u0440\u0441\u0435\u0440\u0430 (\u043d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u043d\u0438\u043a\u0430\u043a\u0438\u0445 \u043b\u0438\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445)</li> </ul> In\u00a0[\u00a0]: Copied! <pre>s = requests.Session()\ns.headers.update({\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/112.0',\n    \n})\n\ns.cookies.update({\n    'sid': 'ZmIxNDk1MGViZDdmOWQ3NTkxYjBkY2VhOTUxMzhiYTE=|1683738281|4b50f094379eebb1ef9971cce80bd5a5a2aa4510',\n    'srv_menu_gender': 'women',\n    'lid': 'ZEACnGRb07W1ZAzPoeVGAgA=',\n    '_gcl_au': '1.1.178690257.1683739575',\n    'wt_cdbeid': '1',\n    'wt3_eid': '%3B717012440280310%7C2168373957543385423%232168374008056837262',\n    'wt3_sid': '%3B717012440280310',\n    'wt_rla': '717012440280310%2C18%2C1683739575310',\n    'sessionId': '1683739575312935624',\n    'is_synced': 'true',\n    'qrator_ssid': '1683739576.090.Zk4lm6ShpXdKDJ1p-858bkd8q7lopa6e0s03okmu6qa3kp1fa',\n    '_gid': 'GA1.2.1392953172.1683739578'\n})\n\n# proxies = {'http': ''}\n# s.proxies.update(proxies)\n</pre> s = requests.Session() s.headers.update({     'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',     'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/112.0',      })  s.cookies.update({     'sid': 'ZmIxNDk1MGViZDdmOWQ3NTkxYjBkY2VhOTUxMzhiYTE=|1683738281|4b50f094379eebb1ef9971cce80bd5a5a2aa4510',     'srv_menu_gender': 'women',     'lid': 'ZEACnGRb07W1ZAzPoeVGAgA=',     '_gcl_au': '1.1.178690257.1683739575',     'wt_cdbeid': '1',     'wt3_eid': '%3B717012440280310%7C2168373957543385423%232168374008056837262',     'wt3_sid': '%3B717012440280310',     'wt_rla': '717012440280310%2C18%2C1683739575310',     'sessionId': '1683739575312935624',     'is_synced': 'true',     'qrator_ssid': '1683739576.090.Zk4lm6ShpXdKDJ1p-858bkd8q7lopa6e0s03okmu6qa3kp1fa',     '_gid': 'GA1.2.1392953172.1683739578' })  # proxies = {'http': ''} # s.proxies.update(proxies) <ul> <li>\u041f\u0430\u0440\u0441\u0438\u043c \u043e\u0431\u0449\u044e\u044e \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u0443 \u0441 \u043e\u0442\u0437\u044b\u0432\u0430\u043c\u0438, \u043d\u0430 \u043d\u0435\u0439 \u043c\u044b \u043d\u0430\u0439\u0434\u0435\u043c URL, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u0432 \u0441\u043f\u0438\u0441\u043e\u043a <code>reviews</code> (\u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 url)</li> <li>\u041f\u043e\u043b\u043d\u044b\u0439 URL \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u0432 <code>review_link</code> \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0432 <code>JSON</code>, \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u043d\u0435 \u043f\u043e\u0432\u0442\u043e\u0440\u044f\u0442\u044c \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u044e</li> </ul> In\u00a0[\u00a0]: Copied! <pre>def parse_page(url):\n\n    r = s.get(url)\n    page = r.text\n    print(r.status_code)\n    soup = BeautifulSoup(page, 'html.parser')\n\n    # review summary\n    content = soup.find_all(class_='reviewTextSnippet')\n\n    # extract review links\n    title = soup.find_all(class_='reviewTitle')\n\n    reviews = []\n    for div in title:\n        reviews.append(div.find('a')['href'])\n\n    review_link = []\n    for review in reviews:\n        review_link.append('https://irecommend.ru' + review)\n\n    return review_link\n\nlst_all_review_sources = []\nfor link in tqdm(sources):\n    lst_all_review_sources.extend(parse_page(link))\n    t.sleep(randint(10,15))\n\nwrite_list(lst_all_review_sources)\n\n#lst_all_review_sources = read_list()\n# lst_all_review_sources\n</pre> def parse_page(url):      r = s.get(url)     page = r.text     print(r.status_code)     soup = BeautifulSoup(page, 'html.parser')      # review summary     content = soup.find_all(class_='reviewTextSnippet')      # extract review links     title = soup.find_all(class_='reviewTitle')      reviews = []     for div in title:         reviews.append(div.find('a')['href'])      review_link = []     for review in reviews:         review_link.append('https://irecommend.ru' + review)      return review_link  lst_all_review_sources = [] for link in tqdm(sources):     lst_all_review_sources.extend(parse_page(link))     t.sleep(randint(10,15))  write_list(lst_all_review_sources)  #lst_all_review_sources = read_list() # lst_all_review_sources <pre>  0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>200\n200\nStarted writing list data into a json file\nDone writing JSON data into .json file\n</pre> In\u00a0[\u00a0]: Copied! <pre>read_list()\n</pre> read_list() Out[\u00a0]: <pre>['https://irecommend.ru/content/ochen-plokho-57',\n 'https://irecommend.ru/content/budte-vnimatelny-pri-pomoshchi-mobilnogo-banka-s-karty-kradut-dengi',\n 'https://irecommend.ru/content/obsluzhivanie-klientov-vse-khuzhe-i-khuzhe',\n 'https://irecommend.ru/content/vozvrat-deneg',\n 'https://irecommend.ru/content/ogromnye-ocheredi',\n 'https://irecommend.ru/content/otkrovenno-lzhivyi-piar-obernetsya-sberbanku-povalnym-nedoveriem',\n 'https://irecommend.ru/content/kak-my-vlipli-s-ssudoi-v-etom-sderbanke',\n 'https://irecommend.ru/content/produmannyi-bank',\n 'https://irecommend.ru/content/vse-luchshe-i-luchshe-0',\n 'https://irecommend.ru/content/kak-podlechit-nervy-ili-kak-my-brali-ipoteku-ot-sberbanka',\n 'https://irecommend.ru/content/bank-kotoryi-rabotaet-ne-dlya-lyudei-protiv-lyudei',\n 'https://irecommend.ru/content/u-menya-propadayut-dengi-s-kartochki',\n 'https://irecommend.ru/content/lzhivyi-biznes',\n 'https://irecommend.ru/content/pri-vkhode-odna-mysl-tolko-ne-ochered',\n 'https://irecommend.ru/content/koshmar-265',\n 'https://irecommend.ru/content/kazhetsya-rabota-banka-uluchshaetsya',\n 'https://irecommend.ru/content/karta-kredit-momentum',\n 'https://irecommend.ru/content/bolshoi-i-poetomu-neudobnyi',\n 'https://irecommend.ru/content/izdevateltvo-kakoe',\n 'https://irecommend.ru/content/zhal-nelzya-postavit-otsenku-10',\n 'https://irecommend.ru/content/dobryi-den-ya-spetsialist-sberbanka-rosii-zvonyu-vam-iz-kieva',\n 'https://irecommend.ru/content/bank-konechno-ne-vinovat-chto-rukovoditel-otdeleniya-idiot-no-vse-zhe',\n 'https://irecommend.ru/content/kak-poteryat-5-tysyach-i-isportit-sebe-nastroenie',\n 'https://irecommend.ru/content/idite-v-nogu-so-vremenem-podarochnaya-karta-nemnogo-foto',\n 'https://irecommend.ru/content/poluchila-kartu-za-10-minut-ochen-udobnaya',\n 'https://irecommend.ru/content/kak-obchistit-klientov-drugikh-bankov-dlya-sberbanka-ne-vopros',\n 'https://irecommend.ru/content/brali-tam-kredit',\n 'https://irecommend.ru/content/bonusnaya-programma-spasibo-ne-ozhidala-takogo-ot-sbera',\n 'https://irecommend.ru/content/sberbank-slov-net-odni-emotsii',\n 'https://irecommend.ru/content/mobilnyi-bank-ot-sberbanka-chto-eto-takoe-i-stoit-li-podklyuchat',\n 'https://irecommend.ru/content/back-ussr-0',\n 'https://irecommend.ru/content/dengi-derzhu-tolko-v-sberbanke',\n 'https://irecommend.ru/content/khotite-poluchit-sms-v-3-chasa-nochi-vam-v-sberbank',\n 'https://irecommend.ru/content/bez-preduprezhdeniya-podklyuchili-platnuyu-uslugu',\n 'https://irecommend.ru/content/spasibo-sotrudnikam-sbera-chto-komu-pomogli-smyt-s-karty-pensionerki-nevelikie-dengi-nu-pust',\n 'https://irecommend.ru/content/sobralsya-zakryt-debetovuyu-kartu-sberatebya-zhdut-syurprizy',\n 'https://irecommend.ru/content/potrebitelskii-kredit-spb-100-kreditosposobnost-ne-garantiya-polucheniya-kredita-v-sberbanke',\n 'https://irecommend.ru/content/sberbank-obnaglel-v-konets',\n 'https://irecommend.ru/content/ostorozhno-moshenniki-11',\n 'https://irecommend.ru/content/navyazchivye-uslugi',\n 'https://irecommend.ru/content/bank-ne-bez-izyanov',\n 'https://irecommend.ru/content/vechnye-ocheredisotrudniki-mnogoe-ne-dogovarivayut',\n 'https://irecommend.ru/content/kak-ya-zakryvala-schet-v-sberbanke-vykup-sebya-iz-rabstva',\n 'https://irecommend.ru/content/pro-kreditnuyu-i-debetovuyu-kartu-ot-sberbanka-v-rossii-konkurentov-tochno-net',\n 'https://irecommend.ru/content/protivorechivyi-opyt-moi-priklyucheniya',\n 'https://irecommend.ru/content/lichno-dlya-menya-minusov-i-nedochetov-v-sberbanke-polno',\n 'https://irecommend.ru/content/menya-raduet-1',\n 'https://irecommend.ru/content/sberbank-rasshchedrilsya-na-bonusy',\n 'https://irecommend.ru/content/povyazana-ya-so-sberom',\n 'https://irecommend.ru/content/ob-optsii-sberbank-line-onlain',\n 'https://irecommend.ru/content/moi-bank',\n 'https://irecommend.ru/content/otnoshenie-kak-k-skotam',\n 'https://irecommend.ru/content/neuvazhenie',\n 'https://irecommend.ru/content/prosto-uzhasnaya-istoriya',\n 'https://irecommend.ru/content/ranshe-schitala-chto-eto-samyi-luchshii-bank-no-kak-ya-oshibalas',\n 'https://irecommend.ru/content/uzhasnyi-servis-9',\n 'https://irecommend.ru/content/vysokaya-komissiya-za-uslugi',\n 'https://irecommend.ru/content/o-tom-kak-ne-nado-oplachivat-scheta-zarubezhnykh-internet-magazinov-perevodit-dengi-inostran',\n 'https://irecommend.ru/content/moe-khoroshee-vpechatlenie-o-sbere-och-isportilos-posle-etogo-sluchaya',\n 'https://irecommend.ru/content/mobilnyi-bank-osteregaites-moshennikov',\n 'https://irecommend.ru/content/pochemu-zhe-takoe-uzhasnoe-otnoshenie-k-lyudyam-kotorykh-zarabatyvayut',\n 'https://irecommend.ru/content/meloch-priyatno-15',\n 'https://irecommend.ru/content/ya-ne-dovolna-3',\n 'https://irecommend.ru/content/kak-ni-stranno-u-nas-s-sb-osobykh-problem-net',\n 'https://irecommend.ru/content/lenivye-i-ne-kompetentnye',\n 'https://irecommend.ru/content/zhdite-zhdite-zhdite',\n 'https://irecommend.ru/content/zastuplyus-i-pust-v-menya-letyat-yaitsa-pomidory-i-tapki',\n 'https://irecommend.ru/content/sberbank-0',\n 'https://irecommend.ru/content/ipoteka-sberbank-bez-pervonachalnogo-vznosa-opisanie-skhemy-za-kotoruyu-nasha-semya-zaplatil',\n 'https://irecommend.ru/content/kvalifikatsiya-rabotnikov-ostavlyaet-zhelat-luchshego',\n 'https://irecommend.ru/content/moya-istoriya-otklyucheniya-uslugi-ili-srazu-idite-v-ofis',\n 'https://irecommend.ru/content/udivlena-naskolko-bystro-sberbank-mne-vydal-kartu-foto-udobnaya-karta-momentum-teper-so-mnoi',\n 'https://irecommend.ru/content/operatsii-po-karte-s-mobilnogo-telefona',\n 'https://irecommend.ru/content/moi-opyt-obshcheniya-so-sberbankom',\n 'https://irecommend.ru/content/vor-v-zakone',\n 'https://irecommend.ru/content/samyi-nadezhnyi-bank-ili-kak-u-menya-ukrali-dengi',\n 'https://irecommend.ru/content/nevygodno-i-neinteresno',\n 'https://irecommend.ru/content/ne-kompitentnost-sotrudnikov-i-uzhasy-oplaty-cherez-terminal',\n 'https://irecommend.ru/content/poluchit-legko-potratit-slozhno',\n 'https://irecommend.ru/content/uzhasnyi-bank-no-vybora-net',\n 'https://irecommend.ru/content/ochen-silno-podvel-chetvero-sutok-khodila-k-nim-kak-na-rabotu',\n 'https://irecommend.ru/content/khamstvo-i-obman-klient-vsegda-ne-prav',\n 'https://irecommend.ru/content/moi-priklyucheniya-v-sberbanke',\n 'https://irecommend.ru/content/apofeoz-marazma',\n 'https://irecommend.ru/content/karta-momentum-ochen-udobnaya-nuzhnaya-i-prostaya-v-ispolzovanii',\n 'https://irecommend.ru/content/5050-60',\n 'https://irecommend.ru/content/moi-sekrety-pochemu-ya-polzuyus-imenno-im',\n 'https://irecommend.ru/content/zagadochnoe-kodovoe-slovo',\n 'https://irecommend.ru/content/normalnyi-bank-8',\n 'https://irecommend.ru/content/razocharovalas-120',\n 'https://irecommend.ru/content/novogodnii-syurpriz-ot-sberbanka',\n 'https://irecommend.ru/content/dotoshnye-sotrudniki-pytayutsya-vse-vtyukhat',\n 'https://irecommend.ru/content/sperbank',\n 'https://irecommend.ru/content/nachinat-nado-s-sebya',\n 'https://irecommend.ru/content/telefonnye-strasti-sberbanka',\n 'https://irecommend.ru/content/lokhotron-v-terminale',\n 'https://irecommend.ru/content/delayut-dengi-iz-vozdukha-0',\n 'https://irecommend.ru/content/potrebitelskii-kredit-legko-bystro-dostupno',\n 'https://irecommend.ru/content/pomenyal-bank-i-zazhil-spokoino',\n 'https://irecommend.ru/content/esli-ne-nekotorye-sotrudniki-debily-ya-nazvala-ego-khoroshim-ili-kak-mozhno-ostatsya-bez-den']</pre> In\u00a0[\u00a0]: Copied! <pre>flatten = lambda l: [item for sublist in l for item in sublist]\n\n'''\n\nHelper function #1\nReturn review \n\n'''\n\ndef parse_review(r,review_url):\n\n    page = r.text\n    soup = BeautifulSoup(page, 'html.parser')\n\n    # extract review links\n    review = soup.find_all(class_='description hasinlineimage')\n\n    # review text stored in &lt;p&gt;, can have multiple parts\n    page_review = []\n    for i in review:\n        page_review.append(i.find_all('p'))\n    \n    # flatten &amp; store only strings\n    review_text = []\n    for i in flatten(page_review):\n        review_text.append(i.get_text())\n        \n    return review_text\n\n'''\n\nHelper function #2 \nReturn positive points, negative points &amp; conclusion\n\n'''\n\ndef parse_additional(r,review_url):\n\n    page = r.text\n    soup = BeautifulSoup(page, 'html.parser')\n    \n    # extract user \n    user_id = soup.find(itemprop='url')\n    if(user_id is not None):\n        user = user_id.get_text()\n    else:\n        user = None\n    \n    # publish time\n    \n    date_id = soup.find(itemprop='datePublished')\n    \n    if(date_id is not None):\n        date = date_id.get('content')\n    else:\n        date = None\n\n    # extract user verdict (positive/negative)\n    conclusion_id = soup.find(class_='verdict')\n    if(conclusion_id is not None):\n        conclusion = conclusion_id.get_text()\n    else:\n        conclusion = None\n    \n    # extract user rating (1-5 rating)\n    rating_id = soup.find(itemprop=\"ratingValue\")\n    if(rating_id is not None):\n        rating = rating_id.get(\"content\")\n    else:\n        rating = None\n    \n    return user,date,conclusion,rating\n</pre> flatten = lambda l: [item for sublist in l for item in sublist]  '''  Helper function #1 Return review   '''  def parse_review(r,review_url):      page = r.text     soup = BeautifulSoup(page, 'html.parser')      # extract review links     review = soup.find_all(class_='description hasinlineimage')      # review text stored in <p>, can have multiple parts     page_review = []     for i in review:         page_review.append(i.find_all('p'))          # flatten &amp; store only strings     review_text = []     for i in flatten(page_review):         review_text.append(i.get_text())              return review_text  '''  Helper function #2  Return positive points, negative points &amp; conclusion  '''  def parse_additional(r,review_url):      page = r.text     soup = BeautifulSoup(page, 'html.parser')          # extract user      user_id = soup.find(itemprop='url')     if(user_id is not None):         user = user_id.get_text()     else:         user = None          # publish time          date_id = soup.find(itemprop='datePublished')          if(date_id is not None):         date = date_id.get('content')     else:         date = None      # extract user verdict (positive/negative)     conclusion_id = soup.find(class_='verdict')     if(conclusion_id is not None):         conclusion = conclusion_id.get_text()     else:         conclusion = None          # extract user rating (1-5 rating)     rating_id = soup.find(itemprop=\"ratingValue\")     if(rating_id is not None):         rating = rating_id.get(\"content\")     else:         rating = None          return user,date,conclusion,rating</p> <ul> <li>\u041f\u0430\u0440\u0441\u0438\u043c \u0447\u0430\u0441\u0442\u044f\u043c\u0438 (\u043f\u043e 100 \u0434\u0430\u043d\u043d\u044b\u0445), \u0432\u0441\u0435\u0433\u043e 1000 \u043e\u0442\u0437\u044b\u0432\u043e\u0432 + \u0447\u0442\u043e \u0432\u043f\u043e\u043b\u043d\u0435 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0434\u043b\u044f \u043f\u0440\u0438\u043c\u0435\u0440\u0430</li> <li>\u0414\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 \u043f\u0430\u0440\u0441\u0435\u0440\u0430 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u0438\u0442\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u043e \u0432 \u0441\u043f\u0438\u0441\u043a\u0438 <code>user</code>,<code>time</code>,<code>review</code>,<code>conclusion</code>,<code>rating</code></li> <li>\u0418\u0437 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438, \u0441\u043e\u0432\u0435\u0442\u043e\u0432\u0430\u043b\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0440\u0430\u0437\u043d\u0443\u044e \u0434\u0435\u043b\u044c\u0442\u0443 \u0434\u043b\u044f \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 sleep, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0438\u043d\u0442\u0435\u0440\u0432\u0430\u043b (15-22\u0441)</li> </ul> In\u00a0[\u00a0]: Copied! <pre>'''\n\nParse Data\n\n'''\n\n# get_data - stores parsed data \n\ndef get_data(urls):\n\n    # Store parsed data for each reviewer\n    lst_user = []\n    lst_time = []\n    lst_review = []\n    lst_conclusion = []\n    lst_rating = []\n\n    # cycle through all review links on a single page\n    for ii,review_link in enumerate(urls):\n\n        # send request \n        r = s.get(review_link)\n        # r = requests.get(review_link)\n        \n        if(r.status_code != 200):\n            print(r.status_code)\n            break\n\n        # store review\n        lst_review.append(parse_review(r,review_link))\n\n        # store additional information\n        user,time,conclusion,rating = parse_additional(r,review_link)\n\n        lst_user.append(user)\n        lst_time.append(time)\n        lst_conclusion.append(conclusion)\n        lst_rating.append(rating)\n\n        if(ii%10 == 0):\n            print(user,conclusion,rating)\n            print(f'review extracted: {ii}')\n            \n        # random sleep\n        t.sleep(randint(15,22))\n        \n    return lst_user,lst_time,lst_review,lst_conclusion,lst_rating\n\nuser,time,review,conclusion,rating = get_data(lst_all_review_sources)\n</pre> '''  Parse Data  '''  # get_data - stores parsed data   def get_data(urls):      # Store parsed data for each reviewer     lst_user = []     lst_time = []     lst_review = []     lst_conclusion = []     lst_rating = []      # cycle through all review links on a single page     for ii,review_link in enumerate(urls):          # send request          r = s.get(review_link)         # r = requests.get(review_link)                  if(r.status_code != 200):             print(r.status_code)             break          # store review         lst_review.append(parse_review(r,review_link))          # store additional information         user,time,conclusion,rating = parse_additional(r,review_link)          lst_user.append(user)         lst_time.append(time)         lst_conclusion.append(conclusion)         lst_rating.append(rating)          if(ii%10 == 0):             print(user,conclusion,rating)             print(f'review extracted: {ii}')                      # random sleep         t.sleep(randint(15,22))              return lst_user,lst_time,lst_review,lst_conclusion,lst_rating  user,time,review,conclusion,rating = get_data(lst_all_review_sources) <pre>irbisirbis \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 1\nreview extracted: 0\n\u0412\u043b\u0430\u0434\u0438\u043c\u0438\u0440\u043e\u0432\u0430 \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 1\nreview extracted: 10\nMaritaimi \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 4\nreview extracted: 20\nFrau J \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 1\nreview extracted: 30\n\u0415\u043b\u0435\u043d\u04309924 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 3\nreview extracted: 40\nkareglazoe_4udo \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 4\nreview extracted: 50\nLubovZ \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 1\nreview extracted: 60\n\u0434\u0430\u0440\u044c\u044f2512 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 4\nreview extracted: 70\nJeN\u04354\u0435k \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 1\nreview extracted: 80\n\u043f\u043b\u044e\u0448\u043a\u0430 \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 1\nreview extracted: 90\n</pre> In\u00a0[\u00a0]: Copied! <pre># Post process &amp; store data in dataframe\n\nreview_concat = []\nfor lst_review in review:\n    review_concat.append(\" \".join(lst_review))\n    \nimport pandas as pd\n\ndf = pd.DataFrame({'user':user,'time':time,'review':review_concat,'conclusion':conclusion,'rating':rating})\ndf.to_csv('df_2223.csv')\ndisplay(df)\n</pre> # Post process &amp; store data in dataframe  review_concat = [] for lst_review in review:     review_concat.append(\" \".join(lst_review))      import pandas as pd  df = pd.DataFrame({'user':user,'time':time,'review':review_concat,'conclusion':conclusion,'rating':rating}) df.to_csv('df_2223.csv') display(df) user time review conclusion rating 0 irbisirbis 2013-08-19T21:42:32+02:00 \u043f\u043e\u0445\u043e\u0436\u0435 \u043d\u0435 \u043e\u0434\u0438\u043d \u0438 \u043d\u0438 \u0434\u0432\u0430 \u043a\u043b\u0438\u0435\u043d\u0442\u0430 \u0421\u0411 \u0441\u0442\u043e\u043b\u043a\u043d\u0443\u043b\u0438\u0441\u044c... \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 1 1 hellknows 2013-08-14T10:46:43+02:00 \u0423\u0441\u043b\u0443\u0433\u0430\u043c\u0438 \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430 \u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0441\u044c \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0434\u043e\u043b\u0433\u043e,... \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 3 2 mescaline_ 2013-08-13T10:03:15+02:00 \u041e\u0442\u0432\u0440\u0430\u0442\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435 \u043e\u0431\u0441\u043b\u0443\u0436\u0438\u0432\u0430\u043d\u0438\u0435 \u043a\u043b\u0438\u0435\u043d\u0442\u0430! \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a ... \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 1 3 DenisDDDDD 2013-08-13T09:51:49+02:00 \u0433.\u041a\u0430\u0437\u0430\u043d\u044c \u041e\u0447\u0435\u043d\u044c \u043e\u0442\u0432\u0440\u0430\u0442\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043f\u0440\u043e\u0446\u0435\u0434\u0443\u0440\u0430 \u0441\u043e\u0437\u0434\u0430\u043d... \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 1 4 Natalia23 2013-08-12T17:09:38+02:00 \u0415\u0449\u0435 \u043a\u043e\u0433\u0434\u0430 \u0440\u0430\u0431\u043e\u0442\u0430\u043b\u0430 \u0441\u0442\u0430\u043b \u0432\u043e\u043f\u0440\u043e\u0441 \u043e \u0442\u043e\u043c, \u0433\u0434\u0435 \u0436\u0435 \u0445... \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 4 ... ... ... ... ... ... 95 \u0412\u0438\u043a\u0442\u043e\u0440\u0438\u044f \u042e\u0440\u044c\u0435\u0432\u043d\u0430 2012-12-12T18:05:00+01:00 \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 1 96 stasya555 2012-12-06T09:56:14+01:00 \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 1 97 \u041c\u043e\u0442\u044b\u043b\u0435\u043a 2012-11-29T16:46:44+01:00 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 5 98 osvald197934310 2012-11-21T04:06:59+01:00 \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 3 99 vishnya 2012-11-21T03:35:34+01:00 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 3 <p>100 rows \u00d7 5 columns</p> <ul> <li>\u0418\u0442\u0430\u043a \u043c\u044b \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043b\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 <code>csv</code> df_01,df_23,df_45,df_67,df_89,df_1011,df_1213,df_1415,df_1617,df_1819,df_2021,df_2223</li> <li>\u0418\u0442\u043e\u0433\u043e \u0443 \u043d\u0430\u0441 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u043e\u0441\u044c \u043f\u0430\u0440\u0441\u0438\u0442\u044c 1200 \u043e\u0442\u0437\u044b\u0432\u043e\u0432</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import os\nos.getcwd()\n</pre> import os os.getcwd() Out[\u00a0]: <pre>'c:\\\\otus_nlp-main\\\\4_parsing'</pre> In\u00a0[\u00a0]: Copied! <pre>import os\nimport pandas as pd\n\npd_lst = []\nfor dirname, _, filenames in os.walk('c:\\\\otus_nlp-main\\\\4_parsing'):\n    for filename in filenames:\n        if('csv' in filename):\n            pd_lst.append(pd.read_csv(os.path.join(dirname, filename)))\n\nparsed_data = pd.concat(pd_lst,axis=0)\nparsed_data = parsed_data.drop(['Unnamed: 0'],axis=1) # \u0437\u0430\u0431\u044b\u043b\u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c CSV \u0431\u0435\u0437 \u0438\u043d\u0434\u0435\u043a\u0441\u0430\n</pre> import os import pandas as pd  pd_lst = [] for dirname, _, filenames in os.walk('c:\\\\otus_nlp-main\\\\4_parsing'):     for filename in filenames:         if('csv' in filename):             pd_lst.append(pd.read_csv(os.path.join(dirname, filename)))  parsed_data = pd.concat(pd_lst,axis=0) parsed_data = parsed_data.drop(['Unnamed: 0'],axis=1) # \u0437\u0430\u0431\u044b\u043b\u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c CSV \u0431\u0435\u0437 \u0438\u043d\u0434\u0435\u043a\u0441\u0430 In\u00a0[\u00a0]: Copied! <pre>parsed_data.head()\n</pre> parsed_data.head() Out[\u00a0]: user time review conclusion rating 0 dncmail 2023-06-21T08:34:25+02:00 \u041f\u043e\u0434\u0435\u043b\u044e\u0441\u044c \u0441 \u0432\u0430\u043c\u0438 \u0438\u0441\u0442\u043e\u0440\u0438\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u0430 \u0441\u043e... \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 2 1 fomicevaa851 2023-06-21T07:39:25+02:00 \u0421\u0430\u043c\u0430 \u043d\u0435\u0434\u0430\u0432\u043d\u043e \u0443\u0437\u043d\u0430\u043b\u0430, \u0447\u0442\u043e \u0432 \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0435 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u043b... \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 5 2 AlexStulov 2023-06-14T13:52:43+02:00 \u0421\u0431\u0435\u0440 \u043f\u043e\u0442\u0435\u0440\u044f\u043b \u043c\u043e\u0439 \u043c\u0438\u043b\u043b\u0438\u043e\u043d. \u0412 \u0430\u043f\u0440\u0435\u043b\u0435 \u0431\u0440\u0430\u043b \u0438\u043f\u043e\u0442\u0435\u043a... \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 1 3 Zakharkot 2023-06-13T08:04:53+02:00 \u0414\u043e\u0431\u0440\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0441\u0443\u0442\u043e\u043a \u0432\u0441\u0435\u043c, \u044f \u043e\u0442\u043a\u0440\u044b\u043b \u0432 \u0421\u0431\u0435\u0440\u0435 \u0432... \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 5 4 sanaan 2023-06-11T23:40:00+02:00 \u0416\u0438\u0432\u0443 \u0441 \u043c\u0430\u043c\u043e\u0439, \u043e\u043f\u043b\u0430\u0442\u043e\u0439 \u043a\u043e\u043c\u043c\u0443\u043d\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u043b\u0430\u0442\u0435\u0436\u0435\u0439 \u0434\u043e... \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 4 In\u00a0[\u00a0]: Copied! <pre>parsed_data.to_csv('sberbank_reviews.csv',index=False)\n</pre> parsed_data.to_csv('sberbank_reviews.csv',index=False) In\u00a0[\u00a0]: Copied! <pre>import re\nimport pandas as pd\n\nsber_reviews = pd.read_csv('sberbank_reviews.csv')\n</pre> import re import pandas as pd  sber_reviews = pd.read_csv('sberbank_reviews.csv') In\u00a0[\u00a0]: Copied! <pre>display(sber_reviews.isna().sum())\nsber_reviews_edit = sber_reviews.dropna()\n</pre> display(sber_reviews.isna().sum()) sber_reviews_edit = sber_reviews.dropna() <pre>user           0\ntime           0\nreview        78\nconclusion     0\nrating         0\ndtype: int64</pre> In\u00a0[\u00a0]: Copied! <pre>sber_reviews_edit.iloc[4]['review']\n</pre> sber_reviews_edit.iloc[4]['review'] Out[\u00a0]: <pre>'\u0416\u0438\u0432\u0443 \u0441 \u043c\u0430\u043c\u043e\u0439, \u043e\u043f\u043b\u0430\u0442\u043e\u0439 \u043a\u043e\u043c\u043c\u0443\u043d\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u043b\u0430\u0442\u0435\u0436\u0435\u0439 \u0434\u043e\u043b\u0433\u043e \u0437\u0430\u043d\u0438\u043c\u0430\u043b\u0430\u0441\u044c \u0438\u043c\u0435\u043d\u043d\u043e \u043e\u043d\u0430. \u041f\u043b\u0430\u0442\u0438\u043b\u0430 \u043e\u043d\u0430 \u0432\u0441\u0435\u0433\u0434\u0430 \u043d\u0430\u043b\u0438\u0447\u043a\u043e\u0439 (\u043a\u0430\u0440\u0442\u044b \u043d\u0435 \u043e\u0442\u043a\u0440\u044b\u0432\u0430\u0435\u0442 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u0438\u0430\u043b\u044c\u043d\u043e) \u0438 \u0447\u0435\u0440\u0435\u0437 \u0441\u0431\u0435\u0440\u043a\u0430\u0441\u0441\u0443 \u2013 \u043f\u043e \u0441\u0442\u0430\u0440\u0438\u043d\u043a\u0435. \u041f\u043e\u043a\u0430 \u0435\u0439 \u0442\u0430\u043c \u043d\u0435 \u043d\u0430\u0447\u0430\u043b\u0438 \u0433\u043e\u0432\u043e\u0440\u0438\u0442\u044c, \u0447\u0442\u043e \u0447\u0435\u0440\u0435\u0437 \u0442\u0435\u0440\u043c\u0438\u043d\u0430\u043b\u044b \u043c\u043e\u0436\u043d\u043e \u043f\u043b\u0430\u0442\u0438\u0442\u044c \u0431\u0435\u0437 \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u0438. \u0412\u043e\u0442 \u0442\u0443\u0442 \u043e\u043d\u0430 \u043d\u0430\u0441\u0435\u043b\u0430 \u043d\u0430 \u043c\u0435\u043d\u044f. \u0414\u043e \u0442\u0435\u0440\u043c\u0438\u043d\u0430\u043b\u043e\u0432 \u043c\u044b \u0442\u0430\u043a \u0438 \u043d\u0435 \u0434\u043e\u0448\u043b\u0438, \u043d\u043e \u044f \u043d\u0430\u0447\u0430\u043b\u0430 \u0441\u0430\u043c\u0430 \u043e\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u0442\u044c \u0441\u0447\u0435\u0442\u0430 \u0447\u0435\u0440\u0435\u0437 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u043e\u0435 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430, \u0431\u043b\u0430\u0433\u043e QR-\u043a\u043e\u0434\u044b \u0432 \u044d\u0442\u043e\u043c \u0441\u0438\u043b\u044c\u043d\u043e \u043f\u043e\u043c\u043e\u0433\u0430\u044e\u0442. \u041a\u043e\u043c\u0438\u0441\u0441\u0438\u044e \u0437\u0430 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043b\u0430\u0442\u0435\u0436\u0438 \u0441\u043d\u0438\u043c\u0430\u043b\u0438, \u043d\u043e \u044f \u043e\u0441\u043e\u0431\u043e \u044d\u0442\u0438\u043c \u043d\u0435 \u043f\u0430\u0440\u0438\u043b\u0430\u0441\u044c, \u043b\u0438\u0448\u044c \u0431\u044b \u043c\u0430\u0442\u0443\u0448\u043a\u0430 \u0431\u044b\u043b\u0430 \u0434\u043e\u0432\u043e\u043b\u044c\u043d\u0430. \u041a\u0430\u043a\u043e\u0435-\u0442\u043e \u0432\u0440\u0435\u043c\u044f \u043c\u044b \u0442\u0430\u043a \u0438 \u0436\u0438\u043b\u0438, \u043f\u043e\u043a\u0430 \u044f \u043d\u0435 \u043d\u0430\u0447\u0430\u043b\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u043a\u0430\u0440\u0442\u0430\u043c\u0438 \u0434\u0440\u0443\u0433\u0438\u0445 \u0431\u0430\u043d\u043a\u043e\u0432. \u0418 \u0443\u0441\u043b\u044b\u0445\u0430\u043b\u0430, \u0447\u0442\u043e \u0432 \u0434\u0440\u0443\u0433\u0438\u0445 \u0431\u0430\u043d\u043a\u0430\u0445 \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u044e \u0437\u0430 \u043a\u043e\u043c\u043c\u0443\u043d\u0430\u043b\u043a\u0443 \u043d\u0435 \u0432\u0437\u0438\u043c\u0430\u044e\u0442. \u042f \u043e\u0447\u0435\u043d\u044c \u0443\u0434\u0438\u0432\u0438\u043b\u0430\u0441\u044c, \u043d\u043e \u0440\u0435\u0448\u0438\u043b\u0430 \u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c. \u0421\u043d\u0430\u0447\u0430\u043b\u0430 \u0447\u0435\u0440\u0435\u0437 \u043e\u0434\u0438\u043d \u0431\u0430\u043d\u043a. \u041d\u043e \u0442\u043e \u043b\u0438 \u0443 \u043c\u0435\u043d\u044f QR-\u043a\u043e\u0434 \u0442\u043e\u0433\u0434\u0430 \u043d\u0435 \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b, \u0438 \u0438\u0441\u043a\u0430\u0442\u044c \u043f\u043e \u0440\u0435\u043a\u0432\u0438\u0437\u0438\u0442\u0430\u043c \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u044e \u044f \u043d\u0435 \u0437\u0430\u0445\u043e\u0442\u0435\u043b\u0430, \u0442\u0430\u043a \u0447\u0442\u043e \u0431\u0440\u043e\u0441\u0438\u043b\u0430 \u044d\u0442\u043e \u0434\u0435\u043b\u043e, \u0438 \u0441\u043d\u043e\u0432\u0430 \u043f\u0435\u0440\u0435\u0448\u043b\u0430 \u043d\u0430 \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u0441 \u0435\u0433\u043e \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u0435\u0439. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\u041f\u043e\u0442\u043e\u043c \u043f\u0440\u0438\u0448\u0435\u043b \u0447\u0435\u0440\u0435\u0434 \u043e\u0442\u043a\u0440\u044b\u0442\u044c \u043a\u0430\u0440\u0442\u0443 \u0442\u0440\u0435\u0442\u044c\u0435\u0433\u043e \u0431\u0430\u043d\u043a\u0430, \u0433\u0434\u0435 \u043c\u043d\u0435 \u043e\u043f\u044f\u0442\u044c \u043d\u0430\u043f\u043e\u043c\u043d\u0438\u043b\u0438 \u043f\u0440\u043e \u043f\u043b\u0430\u0442\u0435\u0436\u0438 \u0431\u0435\u0437 \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u0438. \u041f\u043b\u044e\u0441 \u043f\u043e\u044f\u0432\u0438\u043b\u0430\u0441\u044c \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u043b\u043e\u044f\u043b\u044c\u043d\u043e\u0441\u0442\u0438, \u0433\u0434\u0435 \u043f\u043e\u0441\u0443\u043b\u0438\u043b\u0438 5% \u043a\u0435\u0448\u0431\u044d\u043a \u0438\u043c\u0435\u043d\u043d\u043e \u0437\u0430 \u043a\u043e\u043c\u043c\u0443\u043d\u0430\u043b\u043a\u0443. \u0412\u043e\u0442 \u044d\u0442\u043e \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u043c\u0435\u043d\u044f \u043d\u0435 \u043d\u0430 \u0448\u0443\u0442\u043a\u0443 \u0437\u0430\u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043e\u0432\u0430\u043b\u043e. \u041d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0442\u043c\u0435\u043d\u0430 \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u0438, \u0434\u0430\u043a \u0435\u0449\u0435 \u0438 \u043a\u0435\u0448\u0431\u044d\u043a! \u0421\u043f\u043b\u043e\u0448\u043d\u0430\u044f \u0432\u044b\u0433\u043e\u0434\u0430! \u0418 \u044f \u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u043b\u0430. \u0418 \u0447\u0442\u043e \u043c\u043e\u0433\u0443 \u0441\u043a\u0430\u0437\u0430\u0442\u044c. \u0423 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0431\u0430\u043d\u043a\u0430 \u0441\u0432\u043e\u0438 \u0437\u0430\u043c\u043e\u0440\u043e\u0447\u043a\u0438, \u0441\u0432\u043e\u044e \u043f\u043b\u044e\u0441\u044b \u0438 \u043c\u0438\u043d\u0443\u0441\u044b. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0435\u0441\u043b\u0438 \u0431\u0440\u0430\u0442\u044c \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a, \u0442\u043e \u043f\u043b\u0430\u0442\u0438\u0442\u044c \u0447\u0435\u0440\u0435\u0437 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u043e\u0435 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0431\u044b\u043b\u043e \u043b\u0435\u0433\u043a\u043e, \u0431\u044b\u0441\u0442\u0440\u043e \u0438 \u0443\u0434\u043e\u0431\u043d\u043e. \u0412\u0441\u0435 \u043a\u0432\u0438\u0442\u0430\u043d\u0446\u0438\u0438 \u0441\u043a\u0430\u043d\u0438\u0440\u043e\u0432\u0430\u043b\u0438\u0441\u044c, \u0432\u0441\u0451 \u0431\u044b\u043b\u043e \u043f\u043e\u043d\u044f\u0442\u043d\u043e, \u043d\u043e \u0434\u0430 \u0432\u0437\u0438\u043c\u0430\u043b\u0438 \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u044e. \u0410 \u0432\u043e\u0442 \u0447\u0435\u0440\u0435\u0437 \u0434\u0440\u0443\u0433\u0438\u0435 \u0431\u0430\u043d\u043a\u0438 \u043c\u043e\u0436\u043d\u043e \u043e\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u0442\u044c \u043a\u043e\u043c\u043c\u0443\u043d\u0430\u043b\u043a\u0443 \u0431\u0435\u0437 \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u0438, \u043d\u043e \u0442\u0430\u043c \u044f \u0441\u0442\u043e\u043b\u043a\u043d\u0443\u043b\u0430\u0441\u044c \u0441\u043e \u0441\u0432\u043e\u0438\u043c\u0438 \u0442\u0440\u0443\u0434\u043d\u043e\u0441\u0442\u044f\u043c\u0438. \u0412 \u0431\u0430\u043d\u043a\u0435, \u0433\u0434\u0435 \u043d\u0430\u0447\u0438\u0441\u043b\u044f\u044e\u0442 \u043a\u0435\u0448\u0431\u044d\u043a \u0437\u0430 \u043a\u043e\u043c\u043c\u0443\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043f\u043b\u0430\u0442\u0435\u0436\u0438, \u044f \u0437\u0430\u043c\u0443\u0447\u0438\u043b\u0430\u0441\u044c \u0441\u043a\u0430\u043d\u0438\u0440\u043e\u0432\u0430\u0442\u044c QR-\u043a\u043e\u0434\u044b. \u041e\u043d\u0438 \u0442\u0443\u043f\u043e \u0447\u0438\u0442\u0430\u043b\u0438\u0441\u044c \u0447\u0435\u0440\u0435\u0437 \u0440\u0430\u0437. \u041f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0432\u044b\u0431\u0438\u0432\u0430\u043b\u043e \u0438 \u0441\u0431\u043e\u0438\u043b\u043e, \u043a\u043e\u0433\u0434\u0430 \u044f \u043f\u044b\u0442\u0430\u043b\u0430\u0441\u044c \u043d\u0430\u0432\u0435\u0441\u0442\u0438 \u043a\u0430\u043c\u0435\u0440\u0443 \u043d\u0430 QR-\u043a\u043e\u0434. \u041c\u043d\u0435 \u043f\u0440\u0438\u0448\u043b\u043e\u0441\u044c \u0438\u0441\u043a\u0430\u0442\u044c \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u043e \u0440\u0435\u043a\u0432\u0438\u0437\u0438\u0442\u0430\u043c. \u0418 \u043f\u043e\u043a\u0430 \u044f \u0441 \u044d\u0442\u0438\u043c \u0434\u0435\u043b\u043e\u043c \u043d\u0430\u0441\u0442\u043e\u0439\u0447\u0438\u0432\u043e \u0440\u0430\u0437\u0431\u0438\u0440\u0430\u043b\u0430\u0441\u044c, \u0443 \u043c\u0435\u043d\u044f \u0432\u043e\u0437\u043d\u0438\u043a\u043b\u043e \u043e\u0449\u0443\u0449\u0435\u043d\u0438\u0435, \u0447\u0442\u043e \u043f\u0430\u0440\u0443 \u043f\u043b\u0430\u0442\u0435\u0436\u0435\u0439 \u044f \u043f\u0435\u0440\u0435\u0432\u0435\u043b\u0430 \u043f\u0440\u043e\u0441\u0442\u043e \u043d\u0435 \u0442\u0443\u0434\u0430! \u0412\u044b\u044f\u0441\u043d\u0438\u0442\u044c \u044d\u0442\u043e \u044f \u0441\u043c\u043e\u0433\u043b\u0430 \u0442\u043e\u043b\u044c\u043a\u043e \u0447\u0435\u0440\u0435\u0437 \u043c\u0435\u0441\u044f\u0446, \u043a\u043e\u0433\u0434\u0430 \u043f\u0440\u0438\u0448\u043b\u0438 \u043d\u043e\u0432\u044b\u0435 \u043a\u0432\u0438\u0442\u0430\u043d\u0446\u0438\u0438. \u041d\u0430 \u0443\u0434\u0438\u0432\u043b\u0435\u043d\u0438\u0435 \u043e\u043a\u0430\u0437\u0430\u043b\u043e\u0441\u044c, \u0447\u0442\u043e \u0432\u0441\u0435 \u043f\u043b\u0430\u0442\u0435\u0436\u0438 \u043f\u0440\u043e\u0448\u043b\u0438, \u044d\u0442\u043e \u0445\u043e\u0440\u043e\u0448\u043e. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u0431\u0443\u0434\u044c\u0442\u0435 \u0433\u043e\u0442\u043e\u0432\u044b \u0438\u0441\u043a\u0430\u0442\u044c \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u043e \u0440\u0435\u043a\u0432\u0438\u0437\u0438\u0442\u0430\u043c. \u041d\u043e \u0432 \u044d\u0442\u043e\u043c \u043f\u043b\u0430\u043d\u0435 \u0432\u0430\u043c \u043f\u043e\u043c\u043e\u0433\u0443\u0442 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u043d\u044b\u0435 \u0448\u0430\u0431\u043b\u043e\u043d\u044b. \u041f\u043e\u0442\u043e\u043c \u044f \u043f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u043b\u0430 \u043f\u043b\u0430\u0442\u0438\u0442\u044c \u0447\u0435\u0440\u0435\u0437 \u0442\u0440\u0435\u0442\u0438\u0439 \u0431\u0430\u043d\u043a, \u0438 \u0442\u0430\u043c \u043f\u0440\u043e\u0431\u043b\u0435\u043c \u0441 \u0447\u0442\u0435\u043d\u0438\u0435\u043c QR-\u043a\u043e\u0434\u043e\u0432 \u0443\u0436\u0435 \u043d\u0435 \u0432\u043e\u0437\u043d\u0438\u043a\u043b\u043e, \u0442\u0430\u043a \u0447\u0442\u043e \u043e\u043a\u0430\u0437\u0430\u043b\u043e\u0441\u044c, \u0447\u0442\u043e \u0435\u0441\u0442\u044c \u0438\u0437 \u0447\u0435\u0433\u043e \u0432\u044b\u0431\u0438\u0440\u0430\u0442\u044c. \u0412 \u0446\u0435\u043b\u043e\u043c \u0434\u043b\u044f \u043c\u0435\u043d\u044f \u0441\u0442\u0430\u043b\u043e \u043e\u0442\u043a\u0440\u044b\u0442\u0438\u0435\u043c, \u0447\u0442\u043e \u043f\u043b\u0430\u0442\u0438\u0442\u044c \u043a\u043e\u043c\u043c\u0443\u043d\u0430\u043b\u043a\u0443 \u0447\u0435\u0440\u0435\u0437 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u043e\u0435 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0431\u0430\u043d\u043a\u043e\u0432 \u043c\u043e\u0436\u043d\u043e \u0431\u0435\u0437 \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u0438. \u041d\u0443, \u0438\u043b\u0438 \u043f\u043e \u0441\u0442\u0430\u0440\u0438\u043d\u043a\u0435, \u0441 \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u0435\u0439 \u0447\u0435\u0440\u0435\u0437 \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u2013 \u0432\u044b\u0431\u0438\u0440\u0430\u0439\u0442\u0435 \u0441\u0430\u043c\u0438.'</pre> In\u00a0[\u00a0]: Copied! <pre>sber_reviews_edit.iloc[7]['review']\n</pre> sber_reviews_edit.iloc[7]['review'] Out[\u00a0]: <pre>'\u041e\u0444\u043e\u0440\u043c\u0438\u043b\u0438 \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u044f\u043c \u043a\u0430\u0440\u0442\u044b \u041c\u0418\u0420. \u041f\u0430\u043f\u0435 71, \u043c\u0430\u043c\u0435 75. \u041f\u0430\u043f\u0430 \u0445\u043e\u0434\u0438\u0442 \u0431\u043e\u043b\u0435\u0435-\u043c\u0435\u043d\u0435\u0435, \u043c\u0430\u043c\u0430 \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043d\u0435 \u0445\u043e\u0434\u0438\u0442. \u0421 \u0442\u0440\u0443\u0434\u043e\u043c \u0438 \u043d\u0430 2 \u043a\u043e\u0441\u0442\u044b\u043b\u044f\u0445. \u0414\u043e\u0435\u0445\u0430\u043b\u0438 \u043d\u0430 \u0442\u0430\u043a\u0441\u0438, \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438, \u0432\u0435\u0440\u043d\u0443\u043b\u0438\u0441\u044c \u0434\u043e\u043c\u043e\u0439 \u043d\u0430 \u0442\u0430\u043a\u0441\u0438. \u041d\u0430 \u043f\u043b\u0430\u043d\u0448\u0435\u0442\u0435 \u0432\u043e\u0448\u043b\u0438 \u0432 \u041b\u041a \u043f\u0430\u043f\u044b, \u0432\u0441\u0435 \u043e\u0442\u043b\u0438\u0447\u043d\u043e. \u0412\u044b\u0448\u043b\u0438, \u043f\u043e\u043f\u044b\u0442\u0430\u043b\u0438\u0441\u044c \u0432\u043e\u0439\u0442\u0438 \u0432 \u041b\u041a \u043c\u0430\u043c\u044b - \u043f\u0440\u0438\u0448\u043b\u0430 \u0421\u041c\u0421, \u0447\u0442\u043e \u041b\u041a \u0437\u0430\u0431\u043b\u043e\u043a\u0438\u0440\u043e\u0432\u0430\u043d. \u041f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u043b\u0438 \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u0432 \u043f\u0430\u043f\u0438\u043d \u041b\u041a \u0437\u0430\u0439\u0442\u0438 - \u0438 \u0442\u043e\u0442 \u0443\u0436\u0435 \u0437\u0430\u0431\u043b\u043e\u043a\u0438\u0440\u043e\u0432\u0430\u043d. \u041f\u043e\u0437\u0432\u043e\u043d\u0438\u043b\u0438 900 - \u043d\u0430\u043c \u0441\u043a\u0430\u0437\u0430\u043b\u0438, \u0447\u0442\u043e \u0434\u043b\u044f \u0440\u0430\u0437\u0431\u043b\u043e\u043a\u0438\u0440\u043e\u0432\u043a\u0438 \u043d\u0430\u0434\u043e \u041f\u0420\u0418\u0419\u0422\u0418 \u0412 \u041e\u0422\u0414\u0415\u041b\u0415\u041d\u0418\u0415 \u0418 \u041d\u0410\u041f\u0418\u0421\u0410\u0422\u042c \u0417\u0410\u042f\u0412\u041b\u0415\u041d\u0418\u0415!!! \u0412\u043e\u0442 \u044d\u0442\u043e \u0447\u0442\u043e \u0432\u043e\u043e\u0431\u0449\u0435??? \u041a\u043b\u0438\u0435\u043d\u0442\u043e\u043e\u0440\u0438\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0441\u0442\u044c??? \u0421\u043a\u043e\u043b\u044c\u043a\u043e \u0442\u0430\u043a\u0441\u0438 \u043d\u0430\u0434\u043e \u0435\u0449\u0451 \u0432\u044b\u0437\u0432\u0430\u0442\u044c, \u0447\u0442\u043e\u0431\u044b \u0432\u0441\u0451 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e \u0437\u0430\u0440\u0430\u0431\u043e\u0442\u0430\u043b\u043e??? \u0421\u0430\u043c\u0438 \u043a\u0430\u0440\u0442\u044b \u0440\u0430\u0431\u043e\u0442\u0430\u044e\u0442, \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u0432 \u043d\u0435\u0442. \u0412 \u0441\u043f\u0438\u0441\u043a\u0435 \u0434\u043e\u0441\u0442\u043e\u0438\u043d\u0441\u0442\u0432 \u0433\u0430\u043b\u043a\u0438 \"\u041c\u043e\u0436\u043d\u043e \u043d\u0435 \u0445\u043e\u0434\u0438\u0442\u044c \u0432 \u0431\u0430\u043d\u043a\" \u0438 \"\u041d\u0435 \u043d\u0443\u0436\u043d\u043e \u0432\u044b\u0445\u043e\u0434\u0438\u0442\u044c \u0438\u0437 \u0434\u043e\u043c\u0430\" \u0432\u044b\u0433\u043b\u044f\u0434\u044f\u0442 \u043a\u0430\u043a \u043e\u0442\u043a\u0440\u043e\u0432\u0435\u043d\u043d\u043e\u0435 \u0438\u0437\u0434\u0435\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u043e. \u041a\u043e\u0440\u043e\u0447\u0435 - \u043e\u0442\u0441\u0442\u043e\u0439. \u0423 \u043c\u0435\u043d\u044f \u0441\u0430\u043c\u043e\u0433\u043e \u0422\u0438\u043d\u044c\u043a\u043e\u0432, \u0410\u043b\u044c\u0444\u0430 \u0438 \u0421\u0431\u0435\u0440. \u0421\u0431\u0435\u0440\u043e\u043c \u043c\u043d\u043e\u0433\u043e \u043b\u0435\u0442 \u043d\u0435 \u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0441\u044c. \u0412\u0438\u0434\u0438\u043c\u043e, \u043a \u0441\u0447\u0430\u0441\u0442\u044c\u044e. \u0420\u0435\u0437\u044e\u043c\u0435: \u043d\u0430\u043c \u043f\u0440\u043e\u0449\u0435 \u043f\u043e\u0440\u0435\u0437\u0430\u0442\u044c \u0438 \u0432\u044b\u043a\u0438\u043d\u0443\u0442\u044c \u044d\u0442\u0438 \u043a\u0430\u0440\u0442\u044b \u0438 \u043e\u0444\u043e\u0440\u043c\u0438\u0442\u044c \u0442\u043e\u0442 \u0436\u0435 \u0422\u0438\u043d\u044c\u043a\u043e\u0432. \u0422\u0430\u043c \u043c\u0435\u043d\u0435\u0434\u0436\u0435\u0440\u044b \u0441\u0430\u043c\u0438 \u043f\u0440\u0438\u0435\u0437\u0436\u0430\u044e\u0442 \u0438 \u043d\u0430 \u0434\u043e\u043c\u0443 \u043e\u0444\u043e\u0440\u043c\u043b\u044f\u044e\u0442, \u0430 \u043d\u0435 \u043f\u0440\u043e\u0441\u044f\u0442 \u0445\u0440\u043e\u043d\u0438\u0447\u0435\u0441\u043a\u0438 \u0431\u043e\u043b\u044c\u043d\u044b\u0445 \u043b\u044e\u0434\u0435\u0439 \u0441\u043e \u0441\u043b\u0435\u0437\u0430\u043c\u0438 \u043f\u043e\u043b\u0437\u0442\u0438 \u0432 \u043e\u0442\u0434\u0435\u043b\u0435\u043d\u0438\u0435!!! \\xa0 P.S.: 1 \u043f\u043e\u0441\u0442\u0430\u0432\u0438\u043b \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e 0 \u043d\u0435 \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u0442\u0441\u044f. \u041d\u0435 \u0442\u044f\u043d\u0435\u0442 \u044d\u0442\u043e\u0442 \u0441\u0435\u0440\u0432\u0438\u0441 \u043d\u0430 1.'</pre> In\u00a0[\u00a0]: Copied! <pre>def clean(ltext):\n    ltext = re.sub(r':.+?:', '', ltext)\n    ltext = re.sub(r'\\xa0','',ltext)\n    ltext = re.sub(r'[\\n]','',ltext)\n    return ltext\n</pre> def clean(ltext):     ltext = re.sub(r':.+?:', '', ltext)     ltext = re.sub(r'\\xa0','',ltext)     ltext = re.sub(r'[\\n]','',ltext)     return ltext In\u00a0[\u00a0]: Copied! <pre>sber_reviews_edit['review'].apply(clean)[4]\n</pre> sber_reviews_edit['review'].apply(clean)[4] Out[\u00a0]: <pre>'\u0416\u0438\u0432\u0443 \u0441 \u043c\u0430\u043c\u043e\u0439, \u043e\u043f\u043b\u0430\u0442\u043e\u0439 \u043a\u043e\u043c\u043c\u0443\u043d\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u043b\u0430\u0442\u0435\u0436\u0435\u0439 \u0434\u043e\u043b\u0433\u043e \u0437\u0430\u043d\u0438\u043c\u0430\u043b\u0430\u0441\u044c \u0438\u043c\u0435\u043d\u043d\u043e \u043e\u043d\u0430. \u041f\u043b\u0430\u0442\u0438\u043b\u0430 \u043e\u043d\u0430 \u0432\u0441\u0435\u0433\u0434\u0430 \u043d\u0430\u043b\u0438\u0447\u043a\u043e\u0439 (\u043a\u0430\u0440\u0442\u044b \u043d\u0435 \u043e\u0442\u043a\u0440\u044b\u0432\u0430\u0435\u0442 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u0438\u0430\u043b\u044c\u043d\u043e) \u0438 \u0447\u0435\u0440\u0435\u0437 \u0441\u0431\u0435\u0440\u043a\u0430\u0441\u0441\u0443 \u2013 \u043f\u043e \u0441\u0442\u0430\u0440\u0438\u043d\u043a\u0435. \u041f\u043e\u043a\u0430 \u0435\u0439 \u0442\u0430\u043c \u043d\u0435 \u043d\u0430\u0447\u0430\u043b\u0438 \u0433\u043e\u0432\u043e\u0440\u0438\u0442\u044c, \u0447\u0442\u043e \u0447\u0435\u0440\u0435\u0437 \u0442\u0435\u0440\u043c\u0438\u043d\u0430\u043b\u044b \u043c\u043e\u0436\u043d\u043e \u043f\u043b\u0430\u0442\u0438\u0442\u044c \u0431\u0435\u0437 \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u0438. \u0412\u043e\u0442 \u0442\u0443\u0442 \u043e\u043d\u0430 \u043d\u0430\u0441\u0435\u043b\u0430 \u043d\u0430 \u043c\u0435\u043d\u044f. \u0414\u043e \u0442\u0435\u0440\u043c\u0438\u043d\u0430\u043b\u043e\u0432 \u043c\u044b \u0442\u0430\u043a \u0438 \u043d\u0435 \u0434\u043e\u0448\u043b\u0438, \u043d\u043e \u044f \u043d\u0430\u0447\u0430\u043b\u0430 \u0441\u0430\u043c\u0430 \u043e\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u0442\u044c \u0441\u0447\u0435\u0442\u0430 \u0447\u0435\u0440\u0435\u0437 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u043e\u0435 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430, \u0431\u043b\u0430\u0433\u043e QR-\u043a\u043e\u0434\u044b \u0432 \u044d\u0442\u043e\u043c \u0441\u0438\u043b\u044c\u043d\u043e \u043f\u043e\u043c\u043e\u0433\u0430\u044e\u0442. \u041a\u043e\u043c\u0438\u0441\u0441\u0438\u044e \u0437\u0430 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043b\u0430\u0442\u0435\u0436\u0438 \u0441\u043d\u0438\u043c\u0430\u043b\u0438, \u043d\u043e \u044f \u043e\u0441\u043e\u0431\u043e \u044d\u0442\u0438\u043c \u043d\u0435 \u043f\u0430\u0440\u0438\u043b\u0430\u0441\u044c, \u043b\u0438\u0448\u044c \u0431\u044b \u043c\u0430\u0442\u0443\u0448\u043a\u0430 \u0431\u044b\u043b\u0430 \u0434\u043e\u0432\u043e\u043b\u044c\u043d\u0430. \u041a\u0430\u043a\u043e\u0435-\u0442\u043e \u0432\u0440\u0435\u043c\u044f \u043c\u044b \u0442\u0430\u043a \u0438 \u0436\u0438\u043b\u0438, \u043f\u043e\u043a\u0430 \u044f \u043d\u0435 \u043d\u0430\u0447\u0430\u043b\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u043a\u0430\u0440\u0442\u0430\u043c\u0438 \u0434\u0440\u0443\u0433\u0438\u0445 \u0431\u0430\u043d\u043a\u043e\u0432. \u0418 \u0443\u0441\u043b\u044b\u0445\u0430\u043b\u0430, \u0447\u0442\u043e \u0432 \u0434\u0440\u0443\u0433\u0438\u0445 \u0431\u0430\u043d\u043a\u0430\u0445 \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u044e \u0437\u0430 \u043a\u043e\u043c\u043c\u0443\u043d\u0430\u043b\u043a\u0443 \u043d\u0435 \u0432\u0437\u0438\u043c\u0430\u044e\u0442. \u042f \u043e\u0447\u0435\u043d\u044c \u0443\u0434\u0438\u0432\u0438\u043b\u0430\u0441\u044c, \u043d\u043e \u0440\u0435\u0448\u0438\u043b\u0430 \u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c. \u0421\u043d\u0430\u0447\u0430\u043b\u0430 \u0447\u0435\u0440\u0435\u0437 \u043e\u0434\u0438\u043d \u0431\u0430\u043d\u043a. \u041d\u043e \u0442\u043e \u043b\u0438 \u0443 \u043c\u0435\u043d\u044f QR-\u043a\u043e\u0434 \u0442\u043e\u0433\u0434\u0430 \u043d\u0435 \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b, \u0438 \u0438\u0441\u043a\u0430\u0442\u044c \u043f\u043e \u0440\u0435\u043a\u0432\u0438\u0437\u0438\u0442\u0430\u043c \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u044e \u044f \u043d\u0435 \u0437\u0430\u0445\u043e\u0442\u0435\u043b\u0430, \u0442\u0430\u043a \u0447\u0442\u043e \u0431\u0440\u043e\u0441\u0438\u043b\u0430 \u044d\u0442\u043e \u0434\u0435\u043b\u043e, \u0438 \u0441\u043d\u043e\u0432\u0430 \u043f\u0435\u0440\u0435\u0448\u043b\u0430 \u043d\u0430 \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u0441 \u0435\u0433\u043e \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u0435\u0439. \u041f\u043e\u0442\u043e\u043c \u043f\u0440\u0438\u0448\u0435\u043b \u0447\u0435\u0440\u0435\u0434 \u043e\u0442\u043a\u0440\u044b\u0442\u044c \u043a\u0430\u0440\u0442\u0443 \u0442\u0440\u0435\u0442\u044c\u0435\u0433\u043e \u0431\u0430\u043d\u043a\u0430, \u0433\u0434\u0435 \u043c\u043d\u0435 \u043e\u043f\u044f\u0442\u044c \u043d\u0430\u043f\u043e\u043c\u043d\u0438\u043b\u0438 \u043f\u0440\u043e \u043f\u043b\u0430\u0442\u0435\u0436\u0438 \u0431\u0435\u0437 \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u0438. \u041f\u043b\u044e\u0441 \u043f\u043e\u044f\u0432\u0438\u043b\u0430\u0441\u044c \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u043b\u043e\u044f\u043b\u044c\u043d\u043e\u0441\u0442\u0438, \u0433\u0434\u0435 \u043f\u043e\u0441\u0443\u043b\u0438\u043b\u0438 5% \u043a\u0435\u0448\u0431\u044d\u043a \u0438\u043c\u0435\u043d\u043d\u043e \u0437\u0430 \u043a\u043e\u043c\u043c\u0443\u043d\u0430\u043b\u043a\u0443. \u0412\u043e\u0442 \u044d\u0442\u043e \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u043c\u0435\u043d\u044f \u043d\u0435 \u043d\u0430 \u0448\u0443\u0442\u043a\u0443 \u0437\u0430\u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043e\u0432\u0430\u043b\u043e. \u041d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0442\u043c\u0435\u043d\u0430 \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u0438, \u0434\u0430\u043a \u0435\u0449\u0435 \u0438 \u043a\u0435\u0448\u0431\u044d\u043a! \u0421\u043f\u043b\u043e\u0448\u043d\u0430\u044f \u0432\u044b\u0433\u043e\u0434\u0430! \u0418 \u044f \u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u043b\u0430. \u0418 \u0447\u0442\u043e \u043c\u043e\u0433\u0443 \u0441\u043a\u0430\u0437\u0430\u0442\u044c. \u0423 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0431\u0430\u043d\u043a\u0430 \u0441\u0432\u043e\u0438 \u0437\u0430\u043c\u043e\u0440\u043e\u0447\u043a\u0438, \u0441\u0432\u043e\u044e \u043f\u043b\u044e\u0441\u044b \u0438 \u043c\u0438\u043d\u0443\u0441\u044b. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0435\u0441\u043b\u0438 \u0431\u0440\u0430\u0442\u044c \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a, \u0442\u043e \u043f\u043b\u0430\u0442\u0438\u0442\u044c \u0447\u0435\u0440\u0435\u0437 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u043e\u0435 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0431\u044b\u043b\u043e \u043b\u0435\u0433\u043a\u043e, \u0431\u044b\u0441\u0442\u0440\u043e \u0438 \u0443\u0434\u043e\u0431\u043d\u043e. \u0412\u0441\u0435 \u043a\u0432\u0438\u0442\u0430\u043d\u0446\u0438\u0438 \u0441\u043a\u0430\u043d\u0438\u0440\u043e\u0432\u0430\u043b\u0438\u0441\u044c, \u0432\u0441\u0451 \u0431\u044b\u043b\u043e \u043f\u043e\u043d\u044f\u0442\u043d\u043e, \u043d\u043e \u0434\u0430 \u0432\u0437\u0438\u043c\u0430\u043b\u0438 \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u044e. \u0410 \u0432\u043e\u0442 \u0447\u0435\u0440\u0435\u0437 \u0434\u0440\u0443\u0433\u0438\u0435 \u0431\u0430\u043d\u043a\u0438 \u043c\u043e\u0436\u043d\u043e \u043e\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u0442\u044c \u043a\u043e\u043c\u043c\u0443\u043d\u0430\u043b\u043a\u0443 \u0431\u0435\u0437 \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u0438, \u043d\u043e \u0442\u0430\u043c \u044f \u0441\u0442\u043e\u043b\u043a\u043d\u0443\u043b\u0430\u0441\u044c \u0441\u043e \u0441\u0432\u043e\u0438\u043c\u0438 \u0442\u0440\u0443\u0434\u043d\u043e\u0441\u0442\u044f\u043c\u0438. \u0412 \u0431\u0430\u043d\u043a\u0435, \u0433\u0434\u0435 \u043d\u0430\u0447\u0438\u0441\u043b\u044f\u044e\u0442 \u043a\u0435\u0448\u0431\u044d\u043a \u0437\u0430 \u043a\u043e\u043c\u043c\u0443\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043f\u043b\u0430\u0442\u0435\u0436\u0438, \u044f \u0437\u0430\u043c\u0443\u0447\u0438\u043b\u0430\u0441\u044c \u0441\u043a\u0430\u043d\u0438\u0440\u043e\u0432\u0430\u0442\u044c QR-\u043a\u043e\u0434\u044b. \u041e\u043d\u0438 \u0442\u0443\u043f\u043e \u0447\u0438\u0442\u0430\u043b\u0438\u0441\u044c \u0447\u0435\u0440\u0435\u0437 \u0440\u0430\u0437. \u041f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0432\u044b\u0431\u0438\u0432\u0430\u043b\u043e \u0438 \u0441\u0431\u043e\u0438\u043b\u043e, \u043a\u043e\u0433\u0434\u0430 \u044f \u043f\u044b\u0442\u0430\u043b\u0430\u0441\u044c \u043d\u0430\u0432\u0435\u0441\u0442\u0438 \u043a\u0430\u043c\u0435\u0440\u0443 \u043d\u0430 QR-\u043a\u043e\u0434. \u041c\u043d\u0435 \u043f\u0440\u0438\u0448\u043b\u043e\u0441\u044c \u0438\u0441\u043a\u0430\u0442\u044c \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u043e \u0440\u0435\u043a\u0432\u0438\u0437\u0438\u0442\u0430\u043c. \u0418 \u043f\u043e\u043a\u0430 \u044f \u0441 \u044d\u0442\u0438\u043c \u0434\u0435\u043b\u043e\u043c \u043d\u0430\u0441\u0442\u043e\u0439\u0447\u0438\u0432\u043e \u0440\u0430\u0437\u0431\u0438\u0440\u0430\u043b\u0430\u0441\u044c, \u0443 \u043c\u0435\u043d\u044f \u0432\u043e\u0437\u043d\u0438\u043a\u043b\u043e \u043e\u0449\u0443\u0449\u0435\u043d\u0438\u0435, \u0447\u0442\u043e \u043f\u0430\u0440\u0443 \u043f\u043b\u0430\u0442\u0435\u0436\u0435\u0439 \u044f \u043f\u0435\u0440\u0435\u0432\u0435\u043b\u0430 \u043f\u0440\u043e\u0441\u0442\u043e \u043d\u0435 \u0442\u0443\u0434\u0430! \u0412\u044b\u044f\u0441\u043d\u0438\u0442\u044c \u044d\u0442\u043e \u044f \u0441\u043c\u043e\u0433\u043b\u0430 \u0442\u043e\u043b\u044c\u043a\u043e \u0447\u0435\u0440\u0435\u0437 \u043c\u0435\u0441\u044f\u0446, \u043a\u043e\u0433\u0434\u0430 \u043f\u0440\u0438\u0448\u043b\u0438 \u043d\u043e\u0432\u044b\u0435 \u043a\u0432\u0438\u0442\u0430\u043d\u0446\u0438\u0438. \u041d\u0430 \u0443\u0434\u0438\u0432\u043b\u0435\u043d\u0438\u0435 \u043e\u043a\u0430\u0437\u0430\u043b\u043e\u0441\u044c, \u0447\u0442\u043e \u0432\u0441\u0435 \u043f\u043b\u0430\u0442\u0435\u0436\u0438 \u043f\u0440\u043e\u0448\u043b\u0438, \u044d\u0442\u043e \u0445\u043e\u0440\u043e\u0448\u043e. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u0431\u0443\u0434\u044c\u0442\u0435 \u0433\u043e\u0442\u043e\u0432\u044b \u0438\u0441\u043a\u0430\u0442\u044c \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u043e \u0440\u0435\u043a\u0432\u0438\u0437\u0438\u0442\u0430\u043c. \u041d\u043e \u0432 \u044d\u0442\u043e\u043c \u043f\u043b\u0430\u043d\u0435 \u0432\u0430\u043c \u043f\u043e\u043c\u043e\u0433\u0443\u0442 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u043d\u044b\u0435 \u0448\u0430\u0431\u043b\u043e\u043d\u044b. \u041f\u043e\u0442\u043e\u043c \u044f \u043f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u043b\u0430 \u043f\u043b\u0430\u0442\u0438\u0442\u044c \u0447\u0435\u0440\u0435\u0437 \u0442\u0440\u0435\u0442\u0438\u0439 \u0431\u0430\u043d\u043a, \u0438 \u0442\u0430\u043c \u043f\u0440\u043e\u0431\u043b\u0435\u043c \u0441 \u0447\u0442\u0435\u043d\u0438\u0435\u043c QR-\u043a\u043e\u0434\u043e\u0432 \u0443\u0436\u0435 \u043d\u0435 \u0432\u043e\u0437\u043d\u0438\u043a\u043b\u043e, \u0442\u0430\u043a \u0447\u0442\u043e \u043e\u043a\u0430\u0437\u0430\u043b\u043e\u0441\u044c, \u0447\u0442\u043e \u0435\u0441\u0442\u044c \u0438\u0437 \u0447\u0435\u0433\u043e \u0432\u044b\u0431\u0438\u0440\u0430\u0442\u044c. \u0412 \u0446\u0435\u043b\u043e\u043c \u0434\u043b\u044f \u043c\u0435\u043d\u044f \u0441\u0442\u0430\u043b\u043e \u043e\u0442\u043a\u0440\u044b\u0442\u0438\u0435\u043c, \u0447\u0442\u043e \u043f\u043b\u0430\u0442\u0438\u0442\u044c \u043a\u043e\u043c\u043c\u0443\u043d\u0430\u043b\u043a\u0443 \u0447\u0435\u0440\u0435\u0437 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u043e\u0435 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0431\u0430\u043d\u043a\u043e\u0432 \u043c\u043e\u0436\u043d\u043e \u0431\u0435\u0437 \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u0438. \u041d\u0443, \u0438\u043b\u0438 \u043f\u043e \u0441\u0442\u0430\u0440\u0438\u043d\u043a\u0435, \u0441 \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u0435\u0439 \u0447\u0435\u0440\u0435\u0437 \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u2013 \u0432\u044b\u0431\u0438\u0440\u0430\u0439\u0442\u0435 \u0441\u0430\u043c\u0438.'</pre> In\u00a0[\u00a0]: Copied! <pre>sber_reviews_edit['review'].apply(clean)[7]\n</pre> sber_reviews_edit['review'].apply(clean)[7] Out[\u00a0]: <pre>'\u041e\u0444\u043e\u0440\u043c\u0438\u043b\u0438 \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u044f\u043c \u043a\u0430\u0440\u0442\u044b \u041c\u0418\u0420. \u041f\u0430\u043f\u0435 71, \u043c\u0430\u043c\u0435 75. \u041f\u0430\u043f\u0430 \u0445\u043e\u0434\u0438\u0442 \u0431\u043e\u043b\u0435\u0435-\u043c\u0435\u043d\u0435\u0435, \u043c\u0430\u043c\u0430 \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043d\u0435 \u0445\u043e\u0434\u0438\u0442. \u0421 \u0442\u0440\u0443\u0434\u043e\u043c \u0438 \u043d\u0430 2 \u043a\u043e\u0441\u0442\u044b\u043b\u044f\u0445. \u0414\u043e\u0435\u0445\u0430\u043b\u0438 \u043d\u0430 \u0442\u0430\u043a\u0441\u0438, \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438, \u0432\u0435\u0440\u043d\u0443\u043b\u0438\u0441\u044c \u0434\u043e\u043c\u043e\u0439 \u043d\u0430 \u0442\u0430\u043a\u0441\u0438. \u041d\u0430 \u043f\u043b\u0430\u043d\u0448\u0435\u0442\u0435 \u0432\u043e\u0448\u043b\u0438 \u0432 \u041b\u041a \u043f\u0430\u043f\u044b, \u0432\u0441\u0435 \u043e\u0442\u043b\u0438\u0447\u043d\u043e. \u0412\u044b\u0448\u043b\u0438, \u043f\u043e\u043f\u044b\u0442\u0430\u043b\u0438\u0441\u044c \u0432\u043e\u0439\u0442\u0438 \u0432 \u041b\u041a \u043c\u0430\u043c\u044b - \u043f\u0440\u0438\u0448\u043b\u0430 \u0421\u041c\u0421, \u0447\u0442\u043e \u041b\u041a \u0437\u0430\u0431\u043b\u043e\u043a\u0438\u0440\u043e\u0432\u0430\u043d. \u041f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u043b\u0438 \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u0432 \u043f\u0430\u043f\u0438\u043d \u041b\u041a \u0437\u0430\u0439\u0442\u0438 - \u0438 \u0442\u043e\u0442 \u0443\u0436\u0435 \u0437\u0430\u0431\u043b\u043e\u043a\u0438\u0440\u043e\u0432\u0430\u043d. \u041f\u043e\u0437\u0432\u043e\u043d\u0438\u043b\u0438 900 - \u043d\u0430\u043c \u0441\u043a\u0430\u0437\u0430\u043b\u0438, \u0447\u0442\u043e \u0434\u043b\u044f \u0440\u0430\u0437\u0431\u043b\u043e\u043a\u0438\u0440\u043e\u0432\u043a\u0438 \u043d\u0430\u0434\u043e \u041f\u0420\u0418\u0419\u0422\u0418 \u0412 \u041e\u0422\u0414\u0415\u041b\u0415\u041d\u0418\u0415 \u0418 \u041d\u0410\u041f\u0418\u0421\u0410\u0422\u042c \u0417\u0410\u042f\u0412\u041b\u0415\u041d\u0418\u0415!!! \u0412\u043e\u0442 \u044d\u0442\u043e \u0447\u0442\u043e \u0432\u043e\u043e\u0431\u0449\u0435??? \u041a\u043b\u0438\u0435\u043d\u0442\u043e\u043e\u0440\u0438\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0441\u0442\u044c??? \u0421\u043a\u043e\u043b\u044c\u043a\u043e \u0442\u0430\u043a\u0441\u0438 \u043d\u0430\u0434\u043e \u0435\u0449\u0451 \u0432\u044b\u0437\u0432\u0430\u0442\u044c, \u0447\u0442\u043e\u0431\u044b \u0432\u0441\u0451 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e \u0437\u0430\u0440\u0430\u0431\u043e\u0442\u0430\u043b\u043e??? \u0421\u0430\u043c\u0438 \u043a\u0430\u0440\u0442\u044b \u0440\u0430\u0431\u043e\u0442\u0430\u044e\u0442, \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u0432 \u043d\u0435\u0442. \u0412 \u0441\u043f\u0438\u0441\u043a\u0435 \u0434\u043e\u0441\u0442\u043e\u0438\u043d\u0441\u0442\u0432 \u0433\u0430\u043b\u043a\u0438 \"\u041c\u043e\u0436\u043d\u043e \u043d\u0435 \u0445\u043e\u0434\u0438\u0442\u044c \u0432 \u0431\u0430\u043d\u043a\" \u0438 \"\u041d\u0435 \u043d\u0443\u0436\u043d\u043e \u0432\u044b\u0445\u043e\u0434\u0438\u0442\u044c \u0438\u0437 \u0434\u043e\u043c\u0430\" \u0432\u044b\u0433\u043b\u044f\u0434\u044f\u0442 \u043a\u0430\u043a \u043e\u0442\u043a\u0440\u043e\u0432\u0435\u043d\u043d\u043e\u0435 \u0438\u0437\u0434\u0435\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u043e. \u041a\u043e\u0440\u043e\u0447\u0435 - \u043e\u0442\u0441\u0442\u043e\u0439. \u0423 \u043c\u0435\u043d\u044f \u0441\u0430\u043c\u043e\u0433\u043e \u0422\u0438\u043d\u044c\u043a\u043e\u0432, \u0410\u043b\u044c\u0444\u0430 \u0438 \u0421\u0431\u0435\u0440. \u0421\u0431\u0435\u0440\u043e\u043c \u043c\u043d\u043e\u0433\u043e \u043b\u0435\u0442 \u043d\u0435 \u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0441\u044c. \u0412\u0438\u0434\u0438\u043c\u043e, \u043a \u0441\u0447\u0430\u0441\u0442\u044c\u044e. \u0420\u0435\u0437\u044e\u043c\u0435 1 \u043f\u043e\u0441\u0442\u0430\u0432\u0438\u043b \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e 0 \u043d\u0435 \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u0442\u0441\u044f. \u041d\u0435 \u0442\u044f\u043d\u0435\u0442 \u044d\u0442\u043e\u0442 \u0441\u0435\u0440\u0432\u0438\u0441 \u043d\u0430 1.'</pre> In\u00a0[\u00a0]: Copied! <pre>sber_reviews_edit['review_cleaned'] = sber_reviews_edit['review'].apply(clean)\n</pre> sber_reviews_edit['review_cleaned'] = sber_reviews_edit['review'].apply(clean) <pre>C:\\Users\\abduv\\AppData\\Local\\Temp\\ipykernel_28680\\2578540058.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  sber_reviews_edit['review_cleaned'] = sber_reviews_edit['review'].apply(clean)\n</pre> In\u00a0[\u00a0]: Copied! <pre>sber_reviews_edit.to_csv('review_cleaned.csv',index=False)\n</pre> sber_reviews_edit.to_csv('review_cleaned.csv',index=False) In\u00a0[\u00a0]: Copied! <pre>from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nstopwords = stopwords.words(\"russian\") # stop words \n\ndef tokenise_documents(corpus:pd.Series):\n\n    # define a function for preprocessing\n    def text_cleaning(ltext):\n\n        ltext = ltext.lower() #changes to lower case\n        tokens = word_tokenize(ltext) #tokenize the text\n        punctuation_double = ['``',\"''\"]\n\n        clean_list = [] \n        for token in tokens:\n            if (token not in stopwords):\n                if(token not in punctuation and token not in punctuation_double):\n                    clean_list.append(token) \n                \n        return \" \".join(clean_list)# joins the tokens\n        \n    return corpus.apply(text_cleaning)\n\ntokenised_corpus = tokenise_documents(sber_reviews_edit['review'])\n</pre> from nltk.tokenize import word_tokenize from nltk.corpus import stopwords from string import punctuation stopwords = stopwords.words(\"russian\") # stop words   def tokenise_documents(corpus:pd.Series):      # define a function for preprocessing     def text_cleaning(ltext):          ltext = ltext.lower() #changes to lower case         tokens = word_tokenize(ltext) #tokenize the text         punctuation_double = ['``',\"''\"]          clean_list = []          for token in tokens:             if (token not in stopwords):                 if(token not in punctuation and token not in punctuation_double):                     clean_list.append(token)                           return \" \".join(clean_list)# joins the tokens              return corpus.apply(text_cleaning)  tokenised_corpus = tokenise_documents(sber_reviews_edit['review']) In\u00a0[\u00a0]: Copied! <pre>from collections import Counter\n\ndef n_grams(tokens,n):\n    lst_bigrams = [' '.join(i) for i in [tokens[i:i+n] for i in range(len(tokens)-n+1)]]\n    return lst_bigrams\n\nrecommend = sber_reviews_edit[sber_reviews_edit['conclusion'] == '\u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442']\ntokenised_corpus = tokenise_documents(recommend['review'])\nprint(recommend.shape)\n\n# x-gram lists\ntokenised_documents = [i.split(' ') for i in tokenised_corpus]\ntokenised_documents_bigram = [n_grams(i.split(' '),2) for i in tokenised_corpus]\ntokenised_documents_trigram = [n_grams(i.split(' '),3) for i in tokenised_corpus]\ntokenised_documents_quadragram = [n_grams(i.split(' '),4) for i in tokenised_corpus]\n</pre> from collections import Counter  def n_grams(tokens,n):     lst_bigrams = [' '.join(i) for i in [tokens[i:i+n] for i in range(len(tokens)-n+1)]]     return lst_bigrams  recommend = sber_reviews_edit[sber_reviews_edit['conclusion'] == '\u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442'] tokenised_corpus = tokenise_documents(recommend['review']) print(recommend.shape)  # x-gram lists tokenised_documents = [i.split(' ') for i in tokenised_corpus] tokenised_documents_bigram = [n_grams(i.split(' '),2) for i in tokenised_corpus] tokenised_documents_trigram = [n_grams(i.split(' '),3) for i in tokenised_corpus] tokenised_documents_quadragram = [n_grams(i.split(' '),4) for i in tokenised_corpus] <pre>(548, 5)\n</pre> In\u00a0[\u00a0]: Copied! <pre>Counter(x for xs in tokenised_documents for x in set(xs)).most_common(20)\n</pre> Counter(x for xs in tokenised_documents for x in set(xs)).most_common(20) Out[\u00a0]: <pre>[('\u044d\u0442\u043e', 413),\n ('\u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 330),\n ('\u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a', 318),\n ('\u0434\u0435\u043d\u044c\u0433\u0438', 318),\n ('\u043e\u0447\u0435\u043d\u044c', 305),\n ('\u0431\u0430\u043d\u043a', 304),\n ('\u043a\u0430\u0440\u0442\u0443', 267),\n ('\u0431\u0430\u043d\u043a\u0430', 244),\n ('\u043a\u0430\u0440\u0442\u044b', 242),\n ('\u043f\u0440\u043e\u0441\u0442\u043e', 229),\n ('\u0441\u043f\u0430\u0441\u0438\u0431\u043e', 203),\n ('\u043d\u0443\u0436\u043d\u043e', 202),\n ('\u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0435', 181),\n ('\u0432\u0440\u0435\u043c\u044f', 180),\n ('\u0441\u0440\u0430\u0437\u0443', 176),\n ('\u043a\u0430\u0440\u0442\u0430', 172),\n ('\u043e\u043d\u043b\u0430\u0439\u043d', 171),\n ('\u0440\u0443\u0431\u043b\u0435\u0439', 169),\n ('\u0434\u0435\u043d\u0435\u0433', 165),\n ('\u043b\u0435\u0442', 163)]</pre> In\u00a0[\u00a0]: Copied! <pre>Counter(x for xs in tokenised_documents_bigram for x in set(xs)).most_common(20)\n</pre> Counter(x for xs in tokenised_documents_bigram for x in set(xs)).most_common(20) Out[\u00a0]: <pre>[('\u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u043e\u043d\u043b\u0430\u0439\u043d', 121),\n ('\u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u0439 \u0431\u0430\u043d\u043a', 77),\n ('\u043e\u0447\u0435\u043d\u044c \u0443\u0434\u043e\u0431\u043d\u043e', 71),\n ('\u0441\u043f\u0430\u0441\u0438\u0431\u043e \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 59),\n ('\u043a\u0430\u0440\u0442\u0443 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 48),\n ('\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0434\u0435\u043d\u044c', 44),\n ('\u044d\u0442\u043e \u043e\u0447\u0435\u043d\u044c', 42),\n ('\u043a\u0440\u0435\u0434\u0438\u0442\u043d\u0443\u044e \u043a\u0430\u0440\u0442\u0443', 41),\n ('\u0434\u0435\u0431\u0435\u0442\u043e\u0432\u0443\u044e \u043a\u0430\u0440\u0442\u0443', 36),\n ('3 \u0433\u043e\u0434\u0430', 35),\n ('\u043a\u0440\u0435\u0434\u0438\u0442\u043d\u0430\u044f \u043a\u0430\u0440\u0442\u0430', 34),\n ('\u0432\u0441\u0435\u043c \u043f\u0440\u0438\u0432\u0435\u0442', 34),\n ('\u043b\u044c\u0433\u043e\u0442\u043d\u044b\u0439 \u043f\u0435\u0440\u0438\u043e\u0434', 33),\n ('\u043a\u0430\u0440\u0442\u0430 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 33),\n ('\u043e\u0442\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 33),\n ('\u0434\u0440\u0443\u0433\u0438\u0445 \u0431\u0430\u043d\u043a\u0430\u0445', 32),\n ('\u044d\u0442\u043e \u0432\u0440\u0435\u043c\u044f', 32),\n ('\u0431\u043e\u043d\u0443\u0441\u044b \u0441\u043f\u0430\u0441\u0438\u0431\u043e', 32),\n ('\u043a\u0430\u0436\u0434\u044b\u0439 \u043c\u0435\u0441\u044f\u0446', 32),\n ('\u043f\u0440\u043e\u0446\u0435\u043d\u0442\u043d\u0430\u044f \u0441\u0442\u0430\u0432\u043a\u0430', 32)]</pre> In\u00a0[\u00a0]: Copied! <pre>Counter(x for xs in tokenised_documents_trigram for x in set(xs)).most_common(20)\n</pre> Counter(x for xs in tokenised_documents_trigram for x in set(xs)).most_common(20) Out[\u00a0]: <pre>[('\u0434\u043e\u0431\u0440\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0441\u0443\u0442\u043e\u043a', 28),\n ('\u044d\u0442\u043e \u043e\u0447\u0435\u043d\u044c \u0443\u0434\u043e\u0431\u043d\u043e', 19),\n ('\u044f\u0432\u043b\u044f\u044e\u0441\u044c \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u043c \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 12),\n ('\u0432\u0441\u0435\u043c \u0434\u043e\u0431\u0440\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438', 11),\n ('\u0431\u043e\u043d\u0443\u0441\u043d\u0430\u044f \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u0441\u043f\u0430\u0441\u0438\u0431\u043e', 11),\n ('\u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u043e\u043d\u043b\u0430\u0439\u043d', 10),\n ('\u0432\u0441\u0435\u043c \u043f\u0440\u0438\u0432\u0435\u0442 \u0441\u0435\u0433\u043e\u0434\u043d\u044f', 10),\n ('\u0443\u0441\u043b\u0443\u0433\u0430\u043c\u0438 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430 \u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0441\u044c', 9),\n ('\u043f\u0435\u0440\u0438\u043e\u0434 50 \u0434\u043d\u0435\u0439', 9),\n ('\u0431\u043e\u043d\u0443\u0441\u044b \u0441\u043f\u0430\u0441\u0438\u0431\u043e \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 9),\n ('\u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u043e\u043d\u043b\u0430\u0439\u043d \u043e\u0447\u0435\u043d\u044c', 9),\n ('60 \u0440\u0443\u0431\u043b\u0435\u0439 \u043c\u0435\u0441\u044f\u0446', 8),\n ('\u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u0439 \u0431\u0430\u043d\u043a', 8),\n ('\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0441\u044c \u0443\u0441\u043b\u0443\u0433\u0430\u043c\u0438 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 7),\n ('\u00ab \u0441\u043f\u0430\u0441\u0438\u0431\u043e \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 7),\n ('\u0441\u043f\u0430\u0441\u0438\u0431\u043e \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430 \u00bb', 7),\n ('\u00ab \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u00bb', 7),\n ('\u043a\u0430\u0440\u0442\u0443 \u0434\u0440\u0443\u0433\u043e\u0433\u043e \u0431\u0430\u043d\u043a\u0430', 7),\n ('\u043b\u044c\u0433\u043e\u0442\u043d\u044b\u0439 \u043f\u0435\u0440\u0438\u043e\u0434 50', 7),\n ('\u0434\u0435\u0431\u0435\u0442\u043e\u0432\u0443\u044e \u043a\u0430\u0440\u0442\u0443 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 7)]</pre> In\u00a0[\u00a0]: Copied! <pre>Counter(x for xs in tokenised_documents_quadragram for x in set(xs)).most_common(20)\n</pre> Counter(x for xs in tokenised_documents_quadragram for x in set(xs)).most_common(20) Out[\u00a0]: <pre>[('\u0432\u0441\u0435\u043c \u0434\u043e\u0431\u0440\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0441\u0443\u0442\u043e\u043a', 11),\n ('\u00ab \u0441\u043f\u0430\u0441\u0438\u0431\u043e \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430 \u00bb', 7),\n ('\u043b\u044c\u0433\u043e\u0442\u043d\u044b\u0439 \u043f\u0435\u0440\u0438\u043e\u0434 50 \u0434\u043d\u0435\u0439', 7),\n ('\u00ab \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u0439 \u0431\u0430\u043d\u043a \u00bb', 5),\n ('\u0434\u043e\u0431\u0440\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0441\u0443\u0442\u043e\u043a \u0441\u0435\u0433\u043e\u0434\u043d\u044f', 5),\n ('-- -- -- --', 4),\n ('\u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0441\u0443\u0442\u043e\u043a \u0441\u0435\u0433\u043e\u0434\u043d\u044f \u0445\u043e\u0447\u0443', 4),\n ('\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0430\u043b\u044c\u043d\u044b\u0439 \u0434\u043e\u0433\u043e\u0432\u043e\u0440 \u0431\u0430\u043d\u043a\u043e\u0432\u0441\u043a\u043e\u0433\u043e \u043e\u0431\u0441\u043b\u0443\u0436\u0438\u0432\u0430\u043d\u0438\u044f', 4),\n ('\u0437\u0430\u0440\u0430\u0431\u043e\u0442\u043d\u0443\u044e \u043f\u043b\u0430\u0442\u0443 \u043a\u0430\u0440\u0442\u0443 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 3),\n ('\u043f\u043e\u043b\u0443\u0447\u0430\u043b\u0430 \u0441\u0442\u0438\u043f\u0435\u043d\u0434\u0438\u044e \u043a\u0430\u0440\u0442\u0443 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 3),\n ('\u0445\u043e\u0447\u0443 \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u0441\u0432\u043e\u0435\u043c \u043e\u043f\u044b\u0442\u0435', 3),\n ('\u0441\u043f\u0430\u0441\u0438\u0431\u043e \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043d\u0430\u0434\u0435\u044e\u0441\u044c \u043e\u0442\u0437\u044b\u0432', 3),\n ('\u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043d\u0430\u0434\u0435\u044e\u0441\u044c \u043e\u0442\u0437\u044b\u0432 \u043f\u043e\u043b\u0435\u0437\u0435\u043d', 3),\n ('\u00ab \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u0440\u043e\u0441\u0441\u0438\u0438 \u00bb', 3),\n ('\u043f\u043e\u043b\u043d\u044b\u0439 \u043f\u0430\u043a\u0435\u0442 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u043e\u0433\u043e \u0431\u0430\u043d\u043a\u0430', 3),\n ('\u043f\u043e\u043b\u0443\u0447\u0430\u044e \u0437\u0430\u0440\u043f\u043b\u0430\u0442\u0443 \u043a\u0430\u0440\u0442\u0443 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 3),\n ('\u0443\u0441\u043b\u0443\u0433\u0430\u043c\u0438 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430 \u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0441\u044c \u0434\u0430\u0432\u043d\u043e', 3),\n ('\u043c\u043e\u0431\u0438\u043b\u044c\u043d\u043e\u043c \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0438 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u043e\u043d\u043b\u0430\u0439\u043d', 3),\n ('\u00ab \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u043e\u043d\u043b\u0430\u0439\u043d \u00bb', 3),\n ('\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0441\u044c \u0443\u0441\u043b\u0443\u0433\u043e\u0439 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u0439 \u0431\u0430\u043d\u043a', 3)]</pre> In\u00a0[\u00a0]: Copied! <pre>not_recommend = sber_reviews_edit[sber_reviews_edit['conclusion'] == '\u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442']\nprint(not_recommend.shape)\n</pre> not_recommend = sber_reviews_edit[sber_reviews_edit['conclusion'] == '\u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442'] print(not_recommend.shape) <pre>(574, 5)\n</pre> In\u00a0[\u00a0]: Copied! <pre>tokenised_corpus = tokenise_documents(not_recommend['review'])\n</pre> tokenised_corpus = tokenise_documents(not_recommend['review']) In\u00a0[\u00a0]: Copied! <pre>tokenised_documents = [i.split(' ') for i in tokenised_corpus]\ntokenised_documents_bigram = [n_grams(i.split(' '),2) for i in tokenised_corpus]\ntokenised_documents_trigram = [n_grams(i.split(' '),3) for i in tokenised_corpus]\ntokenised_documents_quadragram = [n_grams(i.split(' '),4) for i in tokenised_corpus]\n</pre> tokenised_documents = [i.split(' ') for i in tokenised_corpus] tokenised_documents_bigram = [n_grams(i.split(' '),2) for i in tokenised_corpus] tokenised_documents_trigram = [n_grams(i.split(' '),3) for i in tokenised_corpus] tokenised_documents_quadragram = [n_grams(i.split(' '),4) for i in tokenised_corpus] In\u00a0[\u00a0]: Copied! <pre>Counter(x for xs in tokenised_documents for x in set(xs)).most_common(20)\n</pre> Counter(x for xs in tokenised_documents for x in set(xs)).most_common(20) Out[\u00a0]: <pre>[('\u044d\u0442\u043e', 448),\n ('\u0431\u0430\u043d\u043a', 384),\n ('\u0434\u0435\u043d\u044c\u0433\u0438', 354),\n ('\u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a', 346),\n ('\u0431\u0430\u043d\u043a\u0430', 345),\n ('\u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 331),\n ('\u043a\u0430\u0440\u0442\u0443', 316),\n ('\u043a\u0430\u0440\u0442\u044b', 295),\n ('\u043f\u0440\u043e\u0441\u0442\u043e', 242),\n ('\u043e\u0447\u0435\u043d\u044c', 231),\n ('\u0432\u0440\u0435\u043c\u044f', 225),\n ('\u043a\u0430\u0440\u0442\u0430', 200),\n ('\u0434\u0435\u043d\u044c', 195),\n ('...', 192),\n ('\u0434\u0435\u043d\u0435\u0433', 189),\n ('\u043d\u0443\u0436\u043d\u043e', 185),\n ('\u0440\u0443\u0431\u043b\u0435\u0439', 182),\n ('\u043e\u0442\u0434\u0435\u043b\u0435\u043d\u0438\u0435', 181),\n ('\u0431\u0430\u043d\u043a\u0435', 172),\n ('\u0441\u0440\u0430\u0437\u0443', 164)]</pre> In\u00a0[\u00a0]: Copied! <pre>Counter(x for xs in tokenised_documents_bigram for x in set(xs)).most_common(30)\n</pre> Counter(x for xs in tokenised_documents_bigram for x in set(xs)).most_common(30) Out[\u00a0]: <pre>[('\u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u043e\u043d\u043b\u0430\u0439\u043d', 91),\n ('\u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u0439 \u0431\u0430\u043d\u043a', 76),\n ('\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0434\u0435\u043d\u044c', 57),\n ('\u0433\u043e\u0440\u044f\u0447\u0443\u044e \u043b\u0438\u043d\u0438\u044e', 56),\n ('\u043e\u0442\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 56),\n ('\u043e\u0442\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0431\u0430\u043d\u043a\u0430', 45),\n ('\u044d\u0442\u043e \u0432\u0440\u0435\u043c\u044f', 45),\n ('\u043a\u0430\u0440\u0442\u0443 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 45),\n ('\u0433\u043e\u0440\u044f\u0447\u0435\u0439 \u043b\u0438\u043d\u0438\u0438', 43),\n ('\u043a\u0440\u0435\u0434\u0438\u0442\u043d\u0443\u044e \u043a\u0430\u0440\u0442\u0443', 41),\n ('\u0441\u0438\u0445 \u043f\u043e\u0440', 40),\n ('\u0434\u0440\u0443\u0433\u043e\u043c \u0431\u0430\u043d\u043a\u0435', 39),\n ('\u0441\u0432\u043e\u0438 \u0434\u0435\u043d\u044c\u0433\u0438', 35),\n ('\u0434\u0435\u043d\u0435\u0436\u043d\u044b\u0445 \u0441\u0440\u0435\u0434\u0441\u0442\u0432', 35),\n ('\u044d\u0442\u0438\u043c \u0431\u0430\u043d\u043a\u043e\u043c', 34),\n ('\u043c\u043e\u0438 \u0434\u0435\u043d\u044c\u0433\u0438', 33),\n ('\u0434\u0440\u0443\u0433\u043e\u0433\u043e \u0431\u0430\u043d\u043a\u0430', 33),\n ('\u043a\u0430\u0440\u0442\u044b \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 30),\n ('2 \u043d\u0435\u0434\u0435\u043b\u0438', 30),\n ('\u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u0438 \u0431\u0430\u043d\u043a\u0430', 29),\n ('\u0442\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c', 29),\n ('\u043a\u043b\u0438\u0435\u043d\u0442\u043e\u043c \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 29),\n ('\u0441\u043d\u044f\u0442\u044c \u0434\u0435\u043d\u044c\u0433\u0438', 29),\n ('\u043d\u043e\u043c\u0435\u0440 \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u0430', 29),\n ('\u0441\u0430\u043c\u043e\u0435 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u043e\u0435', 29),\n ('\u043b\u0438\u0447\u043d\u044b\u0439 \u043a\u0430\u0431\u0438\u043d\u0435\u0442', 28),\n ('\u044d\u0442\u043e \u043f\u0440\u043e\u0441\u0442\u043e', 28),\n ('\u0434\u0435\u043d\u044c\u0433\u0438 \u043a\u0430\u0440\u0442\u044b', 28),\n ('\u0434\u0440\u0443\u0433\u0438\u0445 \u0431\u0430\u043d\u043a\u043e\u0432', 28),\n ('\u043b\u0438\u0447\u043d\u043e\u043c \u043a\u0430\u0431\u0438\u043d\u0435\u0442\u0435', 28)]</pre> In\u00a0[\u00a0]: Copied! <pre>Counter(x for xs in tokenised_documents_trigram for x in set(xs)).most_common(30)\n</pre> Counter(x for xs in tokenised_documents_trigram for x in set(xs)).most_common(30) Out[\u00a0]: <pre>[('\u043f\u043e\u0437\u0432\u043e\u043d\u0438\u043b\u0430 \u0433\u043e\u0440\u044f\u0447\u0443\u044e \u043b\u0438\u043d\u0438\u044e', 18),\n ('\u0434\u043e\u0431\u0440\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0441\u0443\u0442\u043e\u043a', 15),\n ('\u043a\u0430\u043a\u043e\u0432\u043e \u043c\u043e\u0435 \u0443\u0434\u0438\u0432\u043b\u0435\u043d\u0438\u0435', 10),\n ('\u044f\u0432\u043b\u044f\u044e\u0441\u044c \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u043c \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 10),\n ('\u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043b\u0435\u0442 \u043d\u0430\u0437\u0430\u0434', 10),\n ('\u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0435\u0435 \u043e\u0442\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 9),\n ('\u0433\u043e\u0440\u044f\u0447\u0443\u044e \u043b\u0438\u043d\u0438\u044e \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 9),\n ('\u0437\u0430\u0445\u043e\u0436\u0443 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u043e\u043d\u043b\u0430\u0439\u043d', 8),\n ('\u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0438\u0442\u044c \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u0439 \u0431\u0430\u043d\u043a', 8),\n ('\u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0435\u043d \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u0439 \u0431\u0430\u043d\u043a', 8),\n ('\u0437\u0432\u043e\u043d\u0438\u0442\u044c \u0433\u043e\u0440\u044f\u0447\u0443\u044e \u043b\u0438\u043d\u0438\u044e', 8),\n ('\u0443\u0441\u043b\u0443\u0433\u0430 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u0439 \u0431\u0430\u043d\u043a', 7),\n ('\u043e\u043a\u043e\u043d\u0447\u0430\u043d\u0438\u044f \u0441\u0440\u043e\u043a\u0430 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f', 7),\n ('\u043d\u0438\u0437\u043a\u0438\u0435 \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u044b \u0432\u043a\u043b\u0430\u0434\u0430\u043c', 7),\n ('\u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u043e\u043d\u043b\u0430\u0439\u043d', 7),\n ('\u0437\u0432\u043e\u043d\u044e \u0433\u043e\u0440\u044f\u0447\u0443\u044e \u043b\u0438\u043d\u0438\u044e', 7),\n ('150 \u0440\u0443\u0431\u043b\u0435\u0439 \u0433\u043e\u0434', 6),\n ('\u0432\u0441\u0435\u043c \u0434\u043e\u0431\u0440\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438', 6),\n ('\u0442\u0435\u0445 \u043f\u043e\u0440 \u043f\u043e\u043a\u0430', 6),\n ('\u0441\u0440\u043e\u043a \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043c\u043e\u0435\u0439', 6),\n ('\u043a\u0430\u0440\u0442\u0443 \u0434\u0440\u0443\u0433\u043e\u0433\u043e \u0431\u0430\u043d\u043a\u0430', 6),\n ('\u044d\u0442\u043e \u043e\u0447\u0435\u043d\u044c \u0443\u0434\u043e\u0431\u043d\u043e', 6),\n ('60 \u0440\u0443\u0431\u043b\u0435\u0439 \u043c\u0435\u0441\u044f\u0446', 6),\n ('\u0443\u0441\u043b\u0443\u0433\u0443 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u0439 \u0431\u0430\u043d\u043a', 6),\n ('\u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0438 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u043e\u043d\u043b\u0430\u0439\u043d', 5),\n ('\u0441\u0435\u0433\u043e\u0434\u043d\u044f \u0445\u043e\u0447\u0443 \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u0430\u0442\u044c', 5),\n ('\u0441\u0440\u043e\u043a \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043a\u0430\u0440\u0442\u044b', 5),\n ('\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0436\u0435\u043b\u0430\u0442\u044c \u043b\u0443\u0447\u0448\u0435\u0433\u043e', 5),\n ('\u0437\u0430\u043a\u043e\u043d\u0447\u0438\u043b\u0441\u044f \u0441\u0440\u043e\u043a \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f', 5),\n ('\u0437\u0430\u044f\u0432\u043b\u0435\u043d\u0438\u0435 \u0437\u0430\u043a\u0440\u044b\u0442\u0438\u0435 \u0441\u0447\u0435\u0442\u0430', 5)]</pre> In\u00a0[\u00a0]: Copied! <pre>Counter(x for xs in tokenised_documents_quadragram for x in set(xs)).most_common(30)\n</pre> Counter(x for xs in tokenised_documents_quadragram for x in set(xs)).most_common(30) Out[\u00a0]: <pre>[('\u0432\u0441\u0435\u043c \u0434\u043e\u0431\u0440\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0441\u0443\u0442\u043e\u043a', 6),\n ('\u043f\u043e\u0437\u0432\u043e\u043d\u0438\u043b\u0430 \u0433\u043e\u0440\u044f\u0447\u0443\u044e \u043b\u0438\u043d\u0438\u044e \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 5),\n ('\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0443\u0441\u043b\u0443\u0433\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0431\u0430\u043d\u043a\u0430', 4),\n ('\u043e\u043a\u043e\u043d\u0447\u0430\u043d\u0438\u044f \u0441\u0440\u043e\u043a\u0430 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043a\u0430\u0440\u0442\u044b', 4),\n ('\u0432\u0441\u0435\u043c \u043f\u0440\u0438\u0432\u0435\u0442 \u0441\u0435\u0433\u043e\u0434\u043d\u044f \u0445\u043e\u0447\u0443', 3),\n ('\u043b\u0438\u0447\u043d\u044b\u0439 \u043a\u0430\u0431\u0438\u043d\u0435\u0442 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u043e\u043d\u043b\u0430\u0439\u043d', 3),\n ('\u044d\u0442\u043e \u0441\u0430\u043c\u044b\u0439 \u043d\u0430\u0434\u0435\u0436\u043d\u044b\u0439 \u0431\u0430\u043d\u043a', 3),\n ('\u043d\u0430\u0447\u0430\u043b\u0430 \u0437\u0432\u043e\u043d\u0438\u0442\u044c \u0433\u043e\u0440\u044f\u0447\u0443\u044e \u043b\u0438\u043d\u0438\u044e', 3),\n ('\u043e\u0430\u043e \u00ab \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u0440\u043e\u0441\u0441\u0438\u0438', 3),\n ('\u00ab \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u0440\u043e\u0441\u0441\u0438\u0438 \u00bb', 3),\n ('\u043f\u0430\u043e \u00ab \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u00bb', 3),\n ('\u0441\u0440\u043e\u043a \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043c\u043e\u0435\u0439 \u043a\u0430\u0440\u0442\u044b', 3),\n ('\u043b\u0435\u0442 \u044f\u0432\u043b\u044f\u044e\u0441\u044c \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u043c \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 3),\n ('\u0438\u0441\u0442\u043e\u0440\u0438\u0435\u0439 \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u0430 \u043c\u043d\u043e\u0439', 2),\n ('\u043b\u0438\u0447\u043d\u043e\u0433\u043e \u043a\u0430\u0431\u0438\u043d\u0435\u0442\u0430 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u043e\u043d\u043b\u0430\u0439\u043d', 2),\n ('\u043f\u043e\u043b\u0443\u0447\u0430\u044e \u0437\u0430\u0440\u043f\u043b\u0430\u0442\u0443 \u043a\u0430\u0440\u0442\u0443 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430', 2),\n ('\u043f\u0440\u0438\u0432\u0435\u0442 \u0441\u0435\u0433\u043e\u0434\u043d\u044f \u0445\u043e\u0447\u0443 \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u0430\u0442\u044c', 2),\n ('\u0437\u0430\u043a\u043e\u043d\u0447\u0438\u043b\u0441\u044f \u0441\u0440\u043e\u043a \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043c\u043e\u0435\u0439', 2),\n ('\u0441\u0430\u043c\u044b\u0445 \u043d\u0438\u0437\u043a\u0438\u0445 \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u043e\u0432 \u0432\u043a\u043b\u0430\u0434\u0430\u043c', 2),\n ('\u043e\u0434\u043d\u0438 \u0441\u0430\u043c\u044b\u0445 \u043d\u0438\u0437\u043a\u0438\u0445 \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u043e\u0432', 2),\n ('\u0438\u0441\u0442\u0435\u043a \u0441\u0440\u043e\u043a \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043c\u043e\u0435\u0439', 2),\n ('\u0434\u043e\u0431\u0440\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0441\u0443\u0442\u043e\u043a \u0434\u0440\u0443\u0437\u044c\u044f', 2),\n ('\u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043b\u0435\u0442 \u043d\u0430\u0437\u0430\u0434', 2),\n ('\u0442\u0430\u0440\u0438\u0444 \u043f\u043e\u043b\u043d\u044b\u0439 \u0441\u043d\u0438\u043c\u0430\u0435\u0442\u0441\u044f 60', 2),\n ('\u043b\u043e\u0433\u0438\u043d \u043f\u0430\u0440\u043e\u043b\u044c \u043b\u0438\u0447\u043d\u043e\u0433\u043e \u043a\u0430\u0431\u0438\u043d\u0435\u0442\u0430', 2),\n ('\u043f\u043e\u043b\u043d\u044b\u0439 \u0441\u043d\u0438\u043c\u0430\u0435\u0442\u0441\u044f 60 \u0440\u0443\u0431\u043b\u0435\u0439', 2),\n ('\u0441\u0443\u043c\u043c\u0430 50 \u0442\u044b\u0441\u044f\u0447 \u0440\u0443\u0431\u043b\u0435\u0439', 2),\n ('\u0431\u0443\u0434\u044c\u0442\u0435 \u0431\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u044b \u0441\u043f\u0430\u0441\u0438\u0431\u043e \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435', 2),\n ('\u0441\u0430\u043c\u044b\u0435 \u043d\u0438\u0437\u043a\u0438\u0435 \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u044b \u0432\u043a\u043b\u0430\u0434\u0430\u043c', 2),\n ('\u043d\u0438\u0437\u043a\u0438\u0435 \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u044b \u0432\u043a\u043b\u0430\u0434\u0430\u043c \u0432\u044b\u0441\u043e\u043a\u0438\u0435', 2)]</pre> <p>\u0412\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0435 \u043f\u0440\u0438\u0447\u0438\u043d\u044b \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u043e\u0442\u0437\u044b\u0432\u043e\u0432:</p> <ul> <li>\u044d\u0442\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0435 \u0441 \u0433\u043e\u0440\u044f\u0447\u0435\u0439 \u043b\u0438\u043d\u0438\u0438</li> <li>\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0435 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b \u043f\u0440\u0438 \u0438\u0441\u0442\u0435\u043a\u0438 \u0441\u0440\u043e\u043a\u0430 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u043a\u0430\u0440\u0442\u044b</li> <li>\u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b \u0441 \u043b\u0438\u0447\u043d\u044b\u043c \u043a\u0430\u0431\u0438\u043d\u0435\u0442\u043e\u043c \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430 \u043e\u043d\u043b\u0430\u0439\u043d \u0438 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u043c \u0431\u0430\u043d\u043a\u043e\u043c</li> <li>\u043d\u0438\u0437\u043a\u0438\u0439 \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u044b (\u0432\u043a\u043b\u0430\u0434\u043e\u0432)</li> <li>\u0442\u0430\u0440\u0438\u0444 \u043d\u0430 \u043a\u0430\u043a\u043e\u0439 \u0442\u043e \u0442\u043e\u0432\u0430\u0440 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u043a\u0440\u0435\u0434\u0438\u0442\u043d\u0430\u044f \u043a\u0430\u0440\u0442\u0430)</li> </ul>"},{"location":"portfolio/course_nlp/1_parsing.html#1","title":"1 | \u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/course_nlp/1_parsing.html","title":"\u0426\u0435\u043b\u044c\u00b6","text":"<p>\u0412 \u044d\u0442\u043e\u043c \u0414\u0417 \u0432\u044b \u043d\u0430\u043f\u0438\u0448\u0438\u0442\u0435 \u0441\u0432\u043e\u0439 \u043f\u0430\u0440\u0441\u0435\u0440, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u0431\u0435\u0433\u0430\u0442\u044c \u043f\u043e \u0441\u0442\u0440\u0430\u043d\u0438\u0447\u043a\u0430\u043c \u0438 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0447\u0442\u043e-\u0442\u043e \u0441\u043e\u0431\u0438\u0440\u0430\u0442\u044c.</p>"},{"location":"portfolio/course_nlp/1_parsing.html","title":"\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435\u00b6","text":"<p>\u041f\u043e \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0438 \u0441 \u0437\u0430\u043d\u044f\u0442\u0438\u0435\u043c \u043f\u043e \u043f\u0430\u0440\u0441\u0438\u043d\u0433\u0443 \u0434\u0430\u043d\u043d\u044b\u0445, \u0432\u043e\u0437\u044c\u043c\u0438\u0442\u0435 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u0443\u044e\u0449\u0438\u0439 \u0432\u0430\u0441 \u0441\u0430\u0439\u0442, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0441\u043e\u0431\u0438\u0440\u0430\u0442\u044c \u043a\u0430\u043a\u0438\u0435-\u0442\u043e \u0434\u0430\u043d\u043d\u044b\u0435 (\u0438 \u043f\u0440\u0438 \u044d\u0442\u043e\u043c API \u043d\u0435 \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f). \u0418\u0434\u0435\u0430\u043b\u044c\u043d\u044b\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0434\u043e\u043b\u0436\u0435\u043d \u0438\u043c\u0435\u0442\u044c \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u0435 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u0438 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u0443\u044e \u0446\u0435\u043b\u0435\u0432\u0443\u044e \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0443\u044e, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0443\u044e \u044d\u0442\u043e\u043c\u0443 \u043e\u0431\u044a\u0435\u043a\u0442\u0443. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440:</p> <ul> <li>\u0421\u0430\u0439\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0435\u0439: \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u0435 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 - \u0441\u0430\u043c\u0430 \u043d\u043e\u0432\u043e\u0441\u0442\u044c, \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f - \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432 \u043d\u043e\u0432\u043e\u0441\u0442\u0438 (\u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0434\u0435\u043b\u0438\u0442\u044c \u043d\u0430 \u0447\u0438\u0441\u043b\u043e \u0434\u043d\u0435\u0439 \u0441 \u043c\u043e\u043c\u0435\u043d\u0442\u0430 \u0434\u0430\u0442\u044b \u043f\u0443\u0431\u043b\u0438\u043a\u0430\u0446\u0438\u0438, \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u201c\u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0447\u0438\u0441\u043b\u043e \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432 \u0432 \u0434\u0435\u043d\u044c\u201d).</li> <li>\u0421\u0430\u0439\u0442 \u0441 \u0442\u043e\u0432\u0430\u0440\u0430\u043c\u0438/\u043a\u043d\u0438\u0433\u0430\u043c\u0438/\u0444\u0438\u043b\u044c\u043c\u0430\u043c\u0438: \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u0435 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0442\u043e\u0432\u0430\u0440\u0430/\u043a\u043d\u0438\u0433\u0438/\u0444\u0438\u043b\u044c\u043c\u0430 + \u0441\u0440\u0435\u0434\u043d\u0438\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439.</li> <li>\u0411\u043b\u043e\u0433\u0438 - \u0442\u0435\u043a\u0441\u0442\u044b \u0437\u0430\u043c\u0435\u0442\u043e\u043a + \u0447\u0438\u0441\u043b\u043e \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432.</li> <li>\u0418 \u043b\u044e\u0431\u044b\u0435 \u0434\u0440\u0443\u0433\u0438\u0435 \u0432\u0430\u0448\u0438 \u0438\u0434\u0435\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0442 \u043f\u043e\u0434 \u0442\u0430\u043a\u043e\u0439 \u0444\u043e\u0440\u043c\u0430\u0442.</li> </ul> <p>\u041d\u0430\u043f\u0438\u0448\u0438\u0442\u0435 \u0441\u0432\u043e\u0439 \u043f\u0430\u0440\u0441\u0435\u0440, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u0431\u0435\u0433\u0430\u0442\u044c \u043f\u043e \u0441\u0442\u0440\u0430\u043d\u0438\u0447\u043a\u0430\u043c \u0438 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0447\u0442\u043e-\u0442\u043e \u0441\u043e\u0431\u0438\u0440\u0430\u0442\u044c.</p> <ul> <li>\u041d\u0435 \u0437\u0430\u0431\u044b\u0432\u0430\u0439\u0442\u0435, \u0447\u0442\u043e \u043f\u0430\u0440\u0441\u0438\u043d\u0433 - \u044d\u0442\u043e \u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0435 \u043c\u0435\u0440\u043e\u043f\u0440\u0438\u044f\u0442\u0438\u0435, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043d\u0435 \u0431\u043e\u043c\u0431\u0430\u0440\u0434\u0438\u0440\u0443\u0439\u0442\u0435 \u043d\u0435\u0441\u0447\u0430\u0441\u0442\u043d\u044b\u0435 \u0441\u0430\u0439\u0442\u044b \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0447\u0430\u0441\u0442\u044b\u043c\u0438 \u0437\u0430\u043f\u0440\u043e\u0441\u0430\u043c\u0438 (\u043c\u043e\u0436\u043d\u043e \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0438\u0442\u044c \u0447\u0438\u0441\u043b\u043e \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u0432 \u0441\u0435\u043a\u0443\u043d\u0434\u0443 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 time.sleep(0.3), \u0432\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u043e\u0433\u043e \u0432 \u0442\u0435\u043b\u0435 \u0446\u0438\u043a\u043b\u0430)</li> <li>\u041f\u0440\u0438 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0447\u0438\u0441\u0442\u0438\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043e\u0442 \u043c\u0443\u0441\u043e\u0440\u0430 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u044b\u0445 \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u0439).</li> <li>\u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u043f\u043e \u0441\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u043c \u0434\u0430\u043d\u043d\u044b\u043c \u0438 \u043f\u0440\u043e\u0432\u0435\u0441\u0442\u0438 EDA \u0441\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 (\u0432 \u0441\u043b\u0443\u0447\u0430\u0435, \u0435\u0441\u043b\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u0441\u043e\u0431\u043e\u0439 \u0442\u0435\u043a\u0441\u0442 - \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0447\u0430\u0441\u0442\u043e\u0442\u043d\u043e\u0441\u0442\u0438 \u0441\u043b\u043e\u0432, \u0432\u044b\u044f\u0432\u0438\u0442\u044c \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u043e\u0442\u043d\u044b\u0435 \u0441\u043b\u043e\u0432\u0430 \u0438 \u0442. \u043f)</li> <li>\u041d\u0435 \u0437\u0430\u0431\u0443\u0434\u044c\u0442\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442, \u043e\u043d \u0432\u0430\u043c \u0435\u0449\u0435 \u043f\u0440\u0438\u0433\u043e\u0434\u0438\u0442\u044c\u0441\u044f \u0432 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0438\u0445 \u0434\u043e\u043c\u0430\u0448\u043d\u0438\u0445 \u0437\u0430\u0434\u0430\u043d\u0438\u044f\u0445.</li> </ul>"},{"location":"portfolio/course_nlp/1_parsing.html","title":"\u041a\u0440\u0438\u0442\u0435\u0440\u0438\u0438 \u043e\u0446\u0435\u043d\u043a\u0438\u00b6","text":"<ul> <li>\u041d\u0430\u043f\u0438\u0441\u0430\u043d \u043f\u0430\u0440\u0441\u0435\u0440, \u043d\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043f\u043e \u043a\u0430\u043a\u0438\u043c-\u0442\u043e \u043f\u0440\u0438\u0447\u0438\u043d\u0430\u043c \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043d\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0438\u0437-\u0437\u0430 \u0431\u043b\u043e\u043a\u0438\u0440\u043e\u0432\u043e\u043a) - 4 \u0431\u0430\u043b\u043b\u0430</li> <li>\u041d\u0430\u043f\u0438\u0441\u0430\u043d \u043f\u0430\u0440\u0441\u0435\u0440 \u0438 \u0441\u043e\u0431\u0440\u0430\u043d \u0434\u0430\u0442\u0430\u0441\u0435\u0442 - 8 \u0431\u0430\u043b\u043b\u043e\u0432</li> <li>\u041f\u0440\u043e\u0432\u0435\u0434\u0435\u043d EDA \u0441\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 - 2 \u0431\u0430\u043b\u043b\u0430</li> </ul>"},{"location":"portfolio/course_nlp/1_parsing.html#2","title":"2 | \u041f\u0430\u0440\u0441\u0435\u0440\u00b6","text":""},{"location":"portfolio/course_nlp/1_parsing.html","title":"\u041e\u0442\u0437\u044b\u0432\u044b \u043e \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0435\u00b6","text":"<ul> <li>\u0414\u043b\u044f \u044d\u0442\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 \u0432\u044b\u0431\u0435\u0440\u0435\u043c \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u0443 <code>irecommend.ru</code>. \u041d\u0430 \u044d\u0442\u043e\u0439 \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u0435 \u043c\u043e\u0436\u043d\u043e \u043d\u0430\u0439\u0442\u0438 \u043e\u0442\u0437\u044b\u0432\u044b \u043e \u0440\u0430\u0437\u043d\u044b\u0445 \u0442\u043e\u0432\u0430\u0440\u0430\u0445 \u0438 \u0443\u0441\u043b\u0443\u0433\u0430\u0445</li> <li>\u0423\u0437\u043d\u0430\u0435\u043c \u0447\u0442\u043e \u043d\u0440\u0430\u0432\u0438\u0442\u0441\u044f \u043d\u0430\u0448\u0438\u043c \u043a\u043b\u0438\u0435\u043d\u0442\u0430\u043c, \u0438 \u043f\u043e\u043f\u044b\u0442\u0430\u0435\u043c\u0441\u044f \u043d\u0430\u0439\u0442\u0438 \u043f\u0440\u0438\u0447\u0438\u043d\u044b \u043f\u043b\u043e\u0445\u0438\u0445 \u043e\u0442\u0437\u044b\u0432\u043e\u0432 c \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0433\u043e EDA, \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u044d\u0442\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c, \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u043f\u0430\u0440\u0441\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u0438 \u043f\u043e\u0442\u043e\u043c \u0438\u0445 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c</li> <li>\u0422\u0430\u043a \u043a\u0430\u043a \u043e\u0442\u0437\u044b\u0432\u044b \u0435\u0441\u0442\u044c \u0434\u043b\u044f \u0440\u0430\u0437\u043d\u044b\u0445 \u0443\u0441\u043b\u0443\u0433 \u0438 \u0438\u0445 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043c\u043d\u043e\u0433\u043e, \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0438\u043c\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0431\u0449\u0438\u043c\u0438 \u043e\u0442\u0437\u044b\u0432\u0430\u043c\u0438 \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430</li> </ul>"},{"location":"portfolio/course_nlp/1_parsing.html","title":"\u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u0441\u0441\u044b\u043b\u043a\u0438 \u043e\u0442\u0437\u044b\u0432\u043e\u0432\u00b6","text":"<ul> <li>\u041e\u0442 \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u044b \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u043f\u043e\u043b\u0443\u0447\u0430\u0442\u044c \u043f\u043e 100 \u043e\u0442\u0437\u044b\u0432\u043e\u0432</li> <li>\u041f\u0430\u0440\u0441\u0438\u043d\u0433 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0434\u0435\u043b\u0430\u0442\u044c \u0447\u0430\u0441\u0442\u0438\u0447\u043d\u043e (\u043f\u043e 2 \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u044b), \u0442\u0430\u043a \u043a\u0430\u043a \u043f\u0430\u0440\u0441\u0438\u043d\u0433 \u0432\u0441\u0435\u0445 \u0441\u0442\u0440\u0430\u043d\u0438\u0446 (\u0434\u0430\u0436\u0435 \u0441 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u043e\u0439 time) \u043f\u0440\u0438\u0432\u0430\u0434\u0438\u043b\u043e \u043a \u0431\u043b\u043e\u043a\u0438\u0440\u043e\u0432\u043a\u0430\u043c, \u043b\u0438\u0431\u043e \u043a\u0430\u043a\u0430\u044f \u0442\u043e \u0447\u0430\u0441\u0442\u044c \u043a\u043e\u043d\u0442\u0435\u0442\u043d\u0430 \u043f\u0440\u043e\u0441\u0442\u043e \u043d\u0435 \u0432\u044b\u0433\u0440\u0443\u0436\u0430\u043b\u0430\u0441\u044c</li> <li>\u041a\u0430\u0436\u0434\u044b\u0439 \u043e\u0442\u0437\u044b\u0432 \u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d \u043d\u0430 \u0441\u0432\u043e\u0435\u043c \u043b\u0438\u0447\u043d\u043e\u043c URL, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u043f\u0430\u0440\u0441\u0438\u043c \u0441\u0431\u043e\u0440\u043a\u0443 \u043e\u0442\u0437\u044b\u0432\u043e\u0432 (<code>/sberbank?</code>) (\u043e\u0431\u043e\u0431\u0449\u0435\u043d\u043d\u044b\u0439 \u0432\u0438\u0434 \u043e\u0442\u0437\u044b\u0432\u043e\u0432) \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c URL \u0432\u0441\u0435\u0445 \u043e\u0442\u0437\u044b\u0432\u043e\u0432 \u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u044b\u0445 \u043d\u0430 \u044d\u0442\u043e\u0439 \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u0435</li> <li>\u041f\u043e\u0442\u043e\u043c \u0443\u0436\u0435 \u043f\u0430\u0440\u0441\u0438\u043c URL \u043e\u0442\u0437\u044b\u0432\u043e\u0432 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432 \u0438 \u0432\u044b\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u0443\u044e\u0449\u0435\u0435 \u043d\u0430\u0441 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043e\u0431 \u043e\u0442\u0437\u044b\u0432\u0435</li> </ul>"},{"location":"portfolio/course_nlp/1_parsing.html","title":"\u041f\u0430\u0440\u0441\u0438\u043c \u043e\u0442\u0437\u044b\u0432\u044b \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432\u00b6","text":"<ul> <li><p>\u0418\u043c\u0435\u044f URL \u043e\u0442\u0437\u044b\u0432\u043e\u0432, \u0431\u0443\u0434\u0435\u043c \u043f\u0430\u0440\u0441\u0438\u0442\u044c \u0441\u0430\u043c\u0438 \u043e\u0442\u0437\u044b\u0432\u044b</p> </li> <li><p>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0434\u0432\u0435 \u0432\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438</p> <ul> <li><code>parse_review</code> \u0434\u044f\u043b \u0432\u044b\u0433\u0440\u0443\u0437\u043a\u0438 \u0442\u0435\u043a\u0441\u0442\u0430 \u043e\u0442\u0437\u044b\u0432\u0430</li> <li><code>parse_additional</code> \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0432\u044b\u0433\u0440\u0443\u0436\u0430\u0435\u0442 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u0443\u044e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043e\u0431 \u043e\u0442\u0437\u044b\u0432\u0435 (\u044e\u0437\u0435\u0440,\u0434\u0430\u0442\u0430 \u043e\u0442\u0437\u044b\u0432\u0430,\u0438\u0442\u043e\u0433,\u0440\u0435\u0439\u0442\u0438\u043d\u0433)</li> </ul> </li> </ul>"},{"location":"portfolio/course_nlp/1_parsing.html","title":"\u0421\u0431\u043e\u0440 \u0447\u0430\u0441\u0442\u0438\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<ul> <li>\u0422\u0430\u043a \u043a\u0430\u043a \u043c\u044b \u0441\u043e\u0431\u0438\u0440\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0447\u0430\u0441\u0442\u044f\u043c\u0438, \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u0438\u0445 \u0432 <code>csv</code> \u0444\u043e\u0440\u043c\u0430\u0442\u0435, \u043a\u043e\u0433\u0434\u0430 \u0432\u0441\u0435 \u0437\u0430\u043a\u043e\u043d\u0447\u0438\u043c, \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u0432\u0441\u0435 \u0432 \u0435\u0434\u0438\u043d\u044b\u0439 <code>csv</code></li> <li>\u041a\u0430\u043a \u0432\u0438\u0434\u0438\u043c, <code>review</code> \u0443\u0436\u0435 \u043f\u043e\u0434 \u043a\u043e\u043d\u0435\u0446 \u0441\u0442\u0430\u043b \u0432\u044b\u0434\u0430\u0432\u0430\u0442\u044c \u043f\u0443\u0441\u0442\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0447\u0442\u043e \u043c\u043e\u0436\u0435\u0442 \u043e\u0431\u043e\u0437\u043d\u0430\u0447\u0430\u0442\u044c \u0447\u0442\u043e \u043b\u0438\u0431\u043e \u043d\u0430\u0441 \u0443\u0436\u0435 \u0447\u0430\u0441\u0442\u0438\u0447\u043d\u043e \u0431\u043b\u043e\u043a\u0438\u0440\u0443\u044e\u0442, \u043b\u0438\u0431\u043e \u0432 \u0432 2012 \u0433\u043e\u0434\u0443 \u0435\u0449\u0435 \u043d\u0435 \u0431\u044b\u043b\u043e \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0432\u0432\u043e\u0434\u0438\u0442\u044c \u0441\u0432\u043e\u0438 \u043e\u0442\u0437\u044b\u0432</li> </ul>"},{"location":"portfolio/course_nlp/1_parsing.html#3","title":"3 | \u0420\u0430\u0437\u0432\u0435\u0434\u044b\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0410\u043d\u0430\u043b\u0438\u0437\u00b6","text":""},{"location":"portfolio/course_nlp/1_parsing.html#eda","title":"\u0426\u0435\u043b\u044c EDA\u00b6","text":"<p>\u0412\u0435\u0440\u043d\u0435\u043c\u0441\u044f \u043a \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u0435\u043c:</p> <ul> <li>\u041f\u0440\u0438 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0447\u0438\u0441\u0442\u0438\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043e\u0442 \u043c\u0443\u0441\u043e\u0440\u0430 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u044b\u0445 \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u0439.</li> <li>\u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u043f\u043e \u0441\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u043c \u0434\u0430\u043d\u043d\u044b\u043c \u0438 \u043f\u0440\u043e\u0432\u0435\u0441\u0442\u0438 EDA \u0441\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 (\u0432 \u0441\u043b\u0443\u0447\u0430\u0435, \u0435\u0441\u043b\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u0441\u043e\u0431\u043e\u0439 \u0442\u0435\u043a\u0441\u0442 - \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0447\u0430\u0441\u0442\u043e\u0442\u043d\u043e\u0441\u0442\u0438 \u0441\u043b\u043e\u0432, \u0432\u044b\u044f\u0432\u0438\u0442\u044c \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u043e\u0442\u043d\u044b\u0435 \u0441\u043b\u043e\u0432\u0430 \u0438 \u0442. \u043f)</li> <li>\u041d\u0435 \u0437\u0430\u0431\u0443\u0434\u044c\u0442\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442, \u043e\u043d \u0432\u0430\u043c \u0435\u0449\u0435 \u043f\u0440\u0438\u0433\u043e\u0434\u0438\u0442\u044c\u0441\u044f \u0432 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0438\u0445 \u0434\u043e\u043c\u0430\u0448\u043d\u0438\u0445 \u0437\u0430\u0434\u0430\u043d\u0438\u044f\u0445.</li> </ul> <p>\u0414\u0430\u043d\u043d\u044b\u0435 \u043c\u044b \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043b\u0438, \u0442\u0435\u043f\u0435\u0440\u044c \u0431\u0443\u0434\u0435\u043c \u0434\u0435\u043b\u0430\u0442\u044c \u0440\u0430\u0437\u0432\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437; \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e \u043d\u0430\u0441 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u0443\u0435\u0442 \u0447\u0442\u043e \u043a\u043b\u0438\u0435\u043d\u0442\u0430\u043c \u043d\u0440\u0430\u0432\u0438\u0442\u0441\u044f \u0438 \u043d\u0435 \u043d\u0440\u0430\u0432\u0438\u0442\u0441\u044f \u0432 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0435</p>"},{"location":"portfolio/course_nlp/1_parsing.html","title":"\u0427\u0438\u0441\u0442\u043a\u0430 \u0422\u0435\u043a\u0441\u0442\u0430\u00b6","text":"<p>\u0421\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0447\u0438c\u0442\u044b\u0435 \u0438 \u043d\u0435 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 \u043b\u0438\u0448\u043d\u0438\u0439 \u043c\u0443\u0441\u043e\u0440, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443 <code>re</code></p>"},{"location":"portfolio/course_nlp/1_parsing.html","title":"\u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u0422\u0435\u043a\u0441\u0442\u0430\u00b6","text":"<ul> <li>\u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043d\u0430\u0448\u0438 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u044b \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043d\u044f\u0442\u044c \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u0435 \u0442\u0435\u043a\u0441\u0442\u0430</li> <li>\u041c\u044b \u043c\u043e\u0436\u0435\u043c \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e unigram,bigram,trigram \u0438 \u0442\u0434, \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e-\u0442\u043e \u043f\u043e\u043d\u044f\u0442\u044c \u043e \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u0435 \u043e\u0442\u0437\u044b\u0432\u0430</li> </ul>"},{"location":"portfolio/course_nlp/1_parsing.html","title":"\u0427\u0430\u0441\u0442\u043e\u0442\u044b \u0441\u043b\u043e\u0432 \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u043e\u0442\u0437\u044b\u0432\u043e\u0432\u00b6","text":"<ul> <li>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0432\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u043d-\u0433\u0440\u0430\u043c\u043c \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430 \u0432 \u043a\u043e\u0440\u043f\u0443\u0441\u0435</li> <li>\u041f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u043e\u0442\u0437\u044b\u0432\u043e\u0432 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043c\u043d\u043e\u0433\u043e</li> </ul>"},{"location":"portfolio/course_nlp/1_parsing.html","title":"\u0412\u044b\u0432\u043e\u0434\u044b\u00b6","text":"<p>\u041f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u044b\u0445 \u0442\u0435\u043c \u0432 \u043e\u0442\u0437\u044b\u0432\u0430\u0445 \u043c\u043d\u043e\u0433\u043e:</p> <p><code>\u0431\u043e\u043d\u0443\u0441\u043d\u0430\u044f \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u0441\u043f\u0430\u0441\u0438\u0431\u043e</code>, <code>\u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u043e\u043d\u043b\u0430\u0439</code>, <code>\u043b\u044c\u0433\u043e\u0442\u043d\u044b\u0439 \u043f\u0435\u0440\u0438\u043e\u0434 50 \u0434\u043d\u0435\u0439</code>, <code>\u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u0439 \u0431\u0430\u043d\u043a</code></p> <p>\u0412\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0435 \u043f\u0440\u0438\u0447\u0438\u043d\u044b \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u043e\u0442\u0437\u044b\u0432\u043e\u0432:</p> <ul> <li>\u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u0441\u043f\u0430\u0441\u0438\u0431\u043e \u0431\u043e\u043b\u044c\u0448\u0435 \u0430\u0441\u0441\u043e\u0446\u0438\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u0441 \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u043c\u0438 \u043e\u0442\u0437\u044b\u0432\u0430\u043c\u0438</li> <li>\u0443\u0434\u043e\u0431\u0441\u0442\u0432\u043e \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430 \u043e\u043d\u043b\u0430\u0439\u043d \u0438 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u043e\u0433\u043e \u0431\u0430\u043d\u043a\u0430</li> <li>\u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u043b\u044c\u0433\u043e\u0442\u043d\u044b\u0439 \u043f\u0435\u0440\u0438\u043e\u0434 \u0434\u043b\u044f \u043a\u0440\u0435\u0434\u0438\u0442\u043d\u043e \u043a\u0430\u0440\u0442\u044b</li> </ul>"},{"location":"portfolio/course_nlp/1_parsing.html","title":"\u0427\u0430\u0441\u0442\u043e\u0442\u044b \u0441\u043b\u043e\u0432 \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u043e\u0442\u0437\u044b\u0432\u043e\u0432\u00b6","text":"<ul> <li>\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043e\u0442\u0437\u044b\u0432\u044b \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u043b\u0438 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a, \u0438 \u043d\u0430\u0439\u0434\u0435\u043c \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u0435 \u0442\u0435\u043c\u044b</li> <li>\u042d\u0442\u043e \u043f\u043e\u043c\u043e\u0436\u0435\u0442 \u043d\u0430\u043c \u0431\u044b\u0441\u0442\u0440\u043e \u043f\u043e\u043d\u044f\u0442\u044c \u0447\u0442\u043e \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e \u043d\u0435 \u0443\u0441\u0442\u0440\u0430\u0438\u0432\u0430\u0435\u0442 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432</li> </ul>"},{"location":"portfolio/course_nlp/1_parsing.html","title":"\u0412\u044b\u0432\u043e\u0434\u044b\u00b6","text":"<p>\u041d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u0442\u0435\u043c \u0432 \u043e\u0442\u0437\u044b\u0432\u0430\u0445 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043c\u043d\u043e\u0433\u043e</p> <p><code>\u0433\u043e\u0440\u044f\u0447\u0443\u044e \u043b\u0438\u043d\u0438\u044e</code>, <code>\u043e\u043a\u043e\u043d\u0447\u0430\u043d\u0438\u044f \u0441\u0440\u043e\u043a\u0430 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043a\u0430\u0440\u0442\u044b</code>, <code>\u043b\u0438\u0447\u043d\u044b\u0439 \u043a\u0430\u0431\u0438\u043d\u0435\u0442 \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u043e\u043d\u043b\u0430\u0439\u043d</code>, <code>\u043e\u0434\u043d\u0438 \u0441\u0430\u043c\u044b\u0445 \u043d\u0438\u0437\u043a\u0438\u0445 \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u043e\u0432</code>, <code>\u0441\u0440\u043e\u043a \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f</code>, <code>\u0442\u0430\u0440\u0438\u0444 \u043f\u043e\u043b\u043d\u044b\u0439 \u0441\u043d\u0438\u043c\u0430\u0435\u0442\u0441\u044f 60 \u0440\u0443\u0431\u043b\u0435\u0439</code>, <code>\u0443\u0441\u043b\u0443\u0433\u0443 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u0439 \u0431\u0430\u043d\u043a</code>, <code>\u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a \u043e\u043d\u043b\u0430\u0439\u043d</code></p>"},{"location":"portfolio/course_nlp/2_pytorch.html","title":"2 pytorch","text":"In\u00a0[1]: Copied! <pre>import torch\nimport pandas as pd\nimport numpy as np\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tqdm.notebook as tqdm\n</pre> import torch import pandas as pd import numpy as np import torch.nn as nn import matplotlib.pyplot as plt import seaborn as sns import tqdm.notebook as tqdm In\u00a0[6]: Copied! <pre># random point [0,1]\nx = torch.rand(20000)\ny = torch.rand(20000)\n\n# change [-10,10]\nx = x * 20 - 10\ny = y * 20 - 10\nx = x[None,:]; y = y[None,:]\nprint(x.min(),x.max())\nprint(y.min(),y.max())\n\n# function\ndef funct(x,y):\n    return torch.sin(x+2*y)*torch.exp(-(2*x+y)**2)\n\n# def funct(x,y):\n#     return x**2 + y**2\n</pre> # random point [0,1] x = torch.rand(20000) y = torch.rand(20000)  # change [-10,10] x = x * 20 - 10 y = y * 20 - 10 x = x[None,:]; y = y[None,:] print(x.min(),x.max()) print(y.min(),y.max())  # function def funct(x,y):     return torch.sin(x+2*y)*torch.exp(-(2*x+y)**2)  # def funct(x,y): #     return x**2 + y**2 <pre>tensor(-10.0000) tensor(9.9998)\ntensor(-9.9984) tensor(9.9999)\n</pre> In\u00a0[7]: Copied! <pre>z = funct(x,y)\n\ndata = torch.concat([x,y,z],dim=0).t()\nnumpy_data = data.numpy()\ndf = pd.DataFrame(numpy_data,columns=['x','y','z'])\ndf\n</pre> z = funct(x,y)  data = torch.concat([x,y,z],dim=0).t() numpy_data = data.numpy() df = pd.DataFrame(numpy_data,columns=['x','y','z']) df Out[7]: x y z 0 5.907999 -7.999525 2.919641e-07 1 -4.965662 7.948799 -1.959597e-02 2 6.798136 6.879669 0.000000e+00 3 4.882330 -4.780624 1.627852e-11 4 8.118679 -8.338394 -6.093208e-28 ... ... ... ... 19995 6.186462 -0.920638 -0.000000e+00 19996 -6.567829 -5.229598 0.000000e+00 19997 8.102180 -8.343935 -1.091805e-27 19998 7.542414 -9.079653 2.025379e-16 19999 -6.011496 -8.549576 0.000000e+00 <p>20000 rows \u00d7 3 columns</p> In\u00a0[18]: Copied! <pre>''' split data into training,validation &amp; test subsets '''\n# \u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 \u0442\u0440\u0438 \u0447\u0430\u0441\u0442\u0438 \n\ntrain, validate, test = np.split(df.sample(frac=1),[int(0.7*len(df)),int(0.85*len(df))])\nprint(f'train {train.shape[0]/df.shape[0]}')\nprint(f'validate {validate.shape[0]/df.shape[0]}')\nprint(f'test {test.shape[0]/df.shape[0]}')\n</pre> ''' split data into training,validation &amp; test subsets ''' # \u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 \u0442\u0440\u0438 \u0447\u0430\u0441\u0442\u0438   train, validate, test = np.split(df.sample(frac=1),[int(0.7*len(df)),int(0.85*len(df))]) print(f'train {train.shape[0]/df.shape[0]}') print(f'validate {validate.shape[0]/df.shape[0]}') print(f'test {test.shape[0]/df.shape[0]}') <pre>train 0.7\nvalidate 0.15\ntest 0.15\n</pre> In\u00a0[19]: Copied! <pre>''' Convert data back into tensor '''\n# \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u0432 tensor\n\ntrain = torch.FloatTensor(train.values)\nvalidate = torch.FloatTensor(validate.values)\ntest = torch.FloatTensor(test.values)\n\nprint(train.size(),validate.size(),test.size())\n</pre> ''' Convert data back into tensor ''' # \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u0432 tensor  train = torch.FloatTensor(train.values) validate = torch.FloatTensor(validate.values) test = torch.FloatTensor(test.values)  print(train.size(),validate.size(),test.size()) <pre>torch.Size([14000, 3]) torch.Size([3000, 3]) torch.Size([3000, 3])\n</pre> In\u00a0[20]: Copied! <pre>''' Define x_train, x_val '''\n# \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u0444\u0438\u0447\u0435\u0440 z \u0438 \u043d\u0435\u0446\u0435\u043b\u0435\u0432\u044b\u0435 \u0444\u0438\u0447\u0435\u0440\u044b x,y\n\nx_train = train[:,:2]; y_train = train[:,2].squeeze()\nx_val = validate[:,:2]; y_val = validate[:,2].squeeze()\nx_test = test[:,:2]; y_test = test[:,2].squeeze()\n</pre> ''' Define x_train, x_val ''' # \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u0444\u0438\u0447\u0435\u0440 z \u0438 \u043d\u0435\u0446\u0435\u043b\u0435\u0432\u044b\u0435 \u0444\u0438\u0447\u0435\u0440\u044b x,y  x_train = train[:,:2]; y_train = train[:,2].squeeze() x_val = validate[:,:2]; y_val = validate[:,2].squeeze() x_test = test[:,:2]; y_test = test[:,2].squeeze() In\u00a0[21]: Copied! <pre>''' Define Model '''\n# model definition\n\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.linear_1 = nn.Linear(in_features=2, out_features=100)\n        self.act_1 = nn.ReLU()\n        self.linear_2 = nn.Linear(in_features=100,out_features=200)\n        self.act_2 = nn.ReLU()\n        self.linear_3 = nn.Linear(in_features=200,out_features=100)\n        self.act_3 = nn.ReLU()\n        self.linear_4 = nn.Linear(in_features=100,out_features=50)\n        self.act_4 = nn.ReLU()\n        self.linear_5 = nn.Linear(in_features=50,out_features=1)\n\n    def forward(self,x):\n        x = self.linear_1(x)\n        x = self.act_1(x)\n        x = self.linear_2(x)\n        x= self.act_2(x)\n        x = self.linear_3(x)\n        x = self.act_3(x)\n        x = self.linear_4(x)\n        x = self.act_4(x)\n        x = self.linear_5(x)\n        return x\n\n''' Define device and model '''\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\nmodel = Net().to(device)\n</pre> ''' Define Model ''' # model definition  class Net(torch.nn.Module):     def __init__(self):         super(Net, self).__init__()         self.linear_1 = nn.Linear(in_features=2, out_features=100)         self.act_1 = nn.ReLU()         self.linear_2 = nn.Linear(in_features=100,out_features=200)         self.act_2 = nn.ReLU()         self.linear_3 = nn.Linear(in_features=200,out_features=100)         self.act_3 = nn.ReLU()         self.linear_4 = nn.Linear(in_features=100,out_features=50)         self.act_4 = nn.ReLU()         self.linear_5 = nn.Linear(in_features=50,out_features=1)      def forward(self,x):         x = self.linear_1(x)         x = self.act_1(x)         x = self.linear_2(x)         x= self.act_2(x)         x = self.linear_3(x)         x = self.act_3(x)         x = self.linear_4(x)         x = self.act_4(x)         x = self.linear_5(x)         return x  ''' Define device and model '''  device = \"cuda\" if torch.cuda.is_available() else \"cpu\" print(f\"Using {device} device\")  model = Net().to(device) <pre>Using cuda device\n</pre> In\u00a0[24]: Copied! <pre>''' Optimiser &amp; Loss Function '''\n# \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440 \u0438 \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0438 \u043e\u0446\u0435\u043d\u043a\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\n\noptimizer = torch.optim.Adam(model.parameters(),lr=0.0001)\ncriterion = nn.MSELoss()\n</pre> ''' Optimiser &amp; Loss Function ''' # \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440 \u0438 \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0438 \u043e\u0446\u0435\u043d\u043a\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430  optimizer = torch.optim.Adam(model.parameters(),lr=0.0001) criterion = nn.MSELoss() In\u00a0[25]: Copied! <pre>''' Train model '''\n# \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0439 \u0446\u0438\u043a\u043b, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u043c\u0438\u043d\u0438 \u0431\u0430\u0442\u0447\u0438\n\nlst_loss = []; lst_iter = []\nfor epoch_index in tqdm.tqdm(range(50000)):\n    \n    x_train = x_train.to(device)\n    y_train = y_train.to(device)\n     \n    # use randomised mini-batch\n    ix = torch.randint(0, x_train.shape[0], size=(32,))\n    \n    optimizer.zero_grad()\n    y_pred = model(x_train[ix])\n    \n    # MSE loss\n    loss = criterion(y_pred.squeeze(), y_train[ix])\n    \n    if(epoch_index % 1000 == 0):\n        lst_loss.append(loss.item())\n        lst_iter.append(epoch_index)\n\n    optimizer.zero_grad()    \n    loss.backward()  \n    optimizer.step()\n</pre> ''' Train model ''' # \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0439 \u0446\u0438\u043a\u043b, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u043c\u0438\u043d\u0438 \u0431\u0430\u0442\u0447\u0438  lst_loss = []; lst_iter = [] for epoch_index in tqdm.tqdm(range(50000)):          x_train = x_train.to(device)     y_train = y_train.to(device)           # use randomised mini-batch     ix = torch.randint(0, x_train.shape[0], size=(32,))          optimizer.zero_grad()     y_pred = model(x_train[ix])          # MSE loss     loss = criterion(y_pred.squeeze(), y_train[ix])          if(epoch_index % 1000 == 0):         lst_loss.append(loss.item())         lst_iter.append(epoch_index)      optimizer.zero_grad()         loss.backward()       optimizer.step() <pre>  0%|          | 0/50000 [00:00&lt;?, ?it/s]</pre> In\u00a0[\u00a0]: Copied! <pre>sns.set_style(\"whitegrid\", {\n            \"ytick.major.size\": 0.1,\n            \"ytick.minor.size\": 0.05,\n            'grid.linestyle': '--'\n         })\n\nplt.plot(lst_iter,lst_loss,)\nplt.legend(loc='upper left')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.title('\u0438\u0441\u0442\u043e\u0440\u0438\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c')\n</pre> sns.set_style(\"whitegrid\", {             \"ytick.major.size\": 0.1,             \"ytick.minor.size\": 0.05,             'grid.linestyle': '--'          })  plt.plot(lst_iter,lst_loss,) plt.legend(loc='upper left') plt.xlabel('epoch') plt.ylabel('loss') plt.title('\u0438\u0441\u0442\u043e\u0440\u0438\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c') In\u00a0[\u00a0]: Copied! <pre>''' Show Prediction vs Truth '''\n# \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f \u0438\u0441\u0442\u0435\u043d\u043d\u044b\u0445 \u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0438\n\ndef compare_y(net,x,y,title:str):\n    \n    sns.set_style(\"whitegrid\", {\n            \"ytick.major.size\": 0.1,\n            \"ytick.minor.size\": 0.05,\n            'grid.linestyle': '--'\n         })\n    \n    x = x.to(device)\n    y = y.to(device)\n    \n    # make prediction \n    y_pred = net.forward(x)\n    y2 = y_pred.cpu().detach().numpy()[:,0]\n    mse_loss = criterion(y_pred.squeeze(), y)\n    print(f'mse: {mse_loss}')\n       \n    plt.plot(y_pred.cpu().detach().numpy(),\n             y.cpu().detach().numpy(), 'o', mec='k',ms=5)\n    \n    plt.legend(loc='upper left')\n    plt.xlabel('y_model')\n    plt.ylabel('y_true')\n    plt.title(title)\n</pre> ''' Show Prediction vs Truth ''' # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f \u0438\u0441\u0442\u0435\u043d\u043d\u044b\u0445 \u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0438  def compare_y(net,x,y,title:str):          sns.set_style(\"whitegrid\", {             \"ytick.major.size\": 0.1,             \"ytick.minor.size\": 0.05,             'grid.linestyle': '--'          })          x = x.to(device)     y = y.to(device)          # make prediction      y_pred = net.forward(x)     y2 = y_pred.cpu().detach().numpy()[:,0]     mse_loss = criterion(y_pred.squeeze(), y)     print(f'mse: {mse_loss}')             plt.plot(y_pred.cpu().detach().numpy(),              y.cpu().detach().numpy(), 'o', mec='k',ms=5)          plt.legend(loc='upper left')     plt.xlabel('y_model')     plt.ylabel('y_true')     plt.title(title) In\u00a0[39]: Copied! <pre>compare_y(model,x_train,y_train,'\u0422\u0440\u0435\u043d\u0435\u0438\u0440\u043e\u0432\u0430\u0447\u043d\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430'); plt.show()\ncompare_y(model,x_val,y_val,'\u0412\u0430\u043b\u0435\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430'); plt.show()\ncompare_y(model,x_test,y_test,'\u0422\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430'); plt.show()\n</pre> compare_y(model,x_train,y_train,'\u0422\u0440\u0435\u043d\u0435\u0438\u0440\u043e\u0432\u0430\u0447\u043d\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430'); plt.show() compare_y(model,x_val,y_val,'\u0412\u0430\u043b\u0435\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430'); plt.show() compare_y(model,x_test,y_test,'\u0422\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430'); plt.show() <pre>mse: 0.0004913838347420096\n</pre> <pre>mse: 0.0005845963023602962\n</pre> <pre>mse: 0.0005324790254235268\n</pre> In\u00a0[\u00a0]: Copied! <pre>''' Show Prediction vs Truth '''\n# \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f \u0438\u0441\u0442\u0435\u043d\u043d\u044b\u0445 \u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0438\n\nfrom mpl_toolkits.mplot3d import Axes3D  \n\ndef compare_pred(net,x,y,title:str):\n    \n    sns.set_style(\"whitegrid\", {\n            \"ytick.major.size\": 0.1,\n            \"ytick.minor.size\": 0.05,\n            'grid.linestyle': '--'\n         })\n    \n    x = x.to(device)\n    y = y.to(device)\n    \n    # create meshgrid\n    xg, yg = torch.meshgrid(x[:,0], x[:,0], indexing='xy')\n    \n    x1 = x[:,0].cpu()\n    x2 = x[:,1].cpu()\n    y1 = y.cpu()\n    \n    fig = plt.figure(figsize=(15,7))\n    ax = fig.add_subplot(1, 2, 1, projection='3d')\n    ax.plot_trisurf(x1,x2,y1, linewidth=0.0,edgecolors=None,alpha=0.3)\n    ax.scatter(x1,x2,y1,edgecolor='black', linewidth=0.5, facecolor=None,s=8)\n    ax.view_init(20, 35)\n    ax.set_title('\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u0417\u043d\u0430\u0447\u0435\u043d\u0438\u044f')\n    \n    # make prediction \n    y_pred = net.forward(x)\n    y2 = y_pred.cpu().detach().numpy()[:,0]\n    mse_loss = criterion(y_pred.squeeze(), y)\n    \n    ax = fig.add_subplot(1, 2, 2, projection='3d')\n    ax.plot_trisurf(x1,x2,y2, linewidth=0.0,edgecolors=None,alpha=0.3)\n    ax.scatter(x1,x2,y2,edgecolor='black', linewidth=0.5, facecolor=None,s=8)\n    ax.view_init(20, 35)\n    ax.set_title(f'\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u041c\u043e\u0434\u0435\u043b\u0438')\n    plt.suptitle(f'{title} : loss {mse_loss:.4f}')\n    plt.show()\n    \n</pre> ''' Show Prediction vs Truth ''' # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f \u0438\u0441\u0442\u0435\u043d\u043d\u044b\u0445 \u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0438  from mpl_toolkits.mplot3d import Axes3D    def compare_pred(net,x,y,title:str):          sns.set_style(\"whitegrid\", {             \"ytick.major.size\": 0.1,             \"ytick.minor.size\": 0.05,             'grid.linestyle': '--'          })          x = x.to(device)     y = y.to(device)          # create meshgrid     xg, yg = torch.meshgrid(x[:,0], x[:,0], indexing='xy')          x1 = x[:,0].cpu()     x2 = x[:,1].cpu()     y1 = y.cpu()          fig = plt.figure(figsize=(15,7))     ax = fig.add_subplot(1, 2, 1, projection='3d')     ax.plot_trisurf(x1,x2,y1, linewidth=0.0,edgecolors=None,alpha=0.3)     ax.scatter(x1,x2,y1,edgecolor='black', linewidth=0.5, facecolor=None,s=8)     ax.view_init(20, 35)     ax.set_title('\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u0417\u043d\u0430\u0447\u0435\u043d\u0438\u044f')          # make prediction      y_pred = net.forward(x)     y2 = y_pred.cpu().detach().numpy()[:,0]     mse_loss = criterion(y_pred.squeeze(), y)          ax = fig.add_subplot(1, 2, 2, projection='3d')     ax.plot_trisurf(x1,x2,y2, linewidth=0.0,edgecolors=None,alpha=0.3)     ax.scatter(x1,x2,y2,edgecolor='black', linewidth=0.5, facecolor=None,s=8)     ax.view_init(20, 35)     ax.set_title(f'\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u041c\u043e\u0434\u0435\u043b\u0438')     plt.suptitle(f'{title} : loss {mse_loss:.4f}')     plt.show()      In\u00a0[29]: Copied! <pre>compare_pred(model,x_train,y_train,'\u0422\u0440\u0435\u043d\u0435\u0438\u0440\u043e\u0432\u0430\u0447\u043d\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430'); plt.show()\ncompare_pred(model,x_val,y_val,'\u0412\u0430\u043b\u0435\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430'); plt.show()\ncompare_pred(model,x_test,y_test,'\u0422\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430'); plt.show()\n</pre> compare_pred(model,x_train,y_train,'\u0422\u0440\u0435\u043d\u0435\u0438\u0440\u043e\u0432\u0430\u0447\u043d\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430'); plt.show() compare_pred(model,x_val,y_val,'\u0412\u0430\u043b\u0435\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430'); plt.show() compare_pred(model,x_test,y_test,'\u0422\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430'); plt.show()"},{"location":"portfolio/course_nlp/2_pytorch.html","title":"\u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/course_nlp/2_pytorch.html","title":"\u0426\u0435\u043b\u044c\u00b6","text":"<p>\u0412 \u044d\u0442\u043e\u043c \u0414\u0417 \u0432\u044b \u043f\u043e\u043f\u0440\u0430\u043a\u0442\u0438\u043a\u0443\u0435\u0442\u0435\u0441\u044c \u0441 PyTorch.</p>"},{"location":"portfolio/course_nlp/2_pytorch.html","title":"\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435\u00b6","text":"<ul> <li>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f PyTorch, \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u0438 \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u043e\u043d\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0430\u043f\u043f\u0440\u043e\u043a\u0441\u0438\u043c\u0438\u0440\u0443\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 sin(x + 2*y)exp(-(2x + y)^2) \u043d\u0430 \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 [-10;10] \u043f\u043e \u0445 \u0438 \u0443</li> <li>\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0441\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c 20000 \u0442\u043e\u0447\u0435\u043a \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c</li> <li>\u0420\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u044c \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0439\u0441\u044f \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043d\u0430 train / test / val \u0432 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044f\u0445 70% / 15% / 15%, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e</li> </ul>"},{"location":"portfolio/course_nlp/2_pytorch.html","title":"\u041a\u0440\u0438\u0442\u0435\u0440\u0438\u0438 \u043e\u0446\u0435\u043d\u043a\u0438\u00b6","text":"<ul> <li>(a) \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043c\u0435\u0442\u0440\u0438\u043a\u0443 Mean Square Error(MSE) \u043d\u0430 test (+7 \u0431\u0430\u043b\u043b\u0430 \u0437\u0430 (a))</li> <li>(b) \u041d\u0430\u0440\u0438\u0441\u043e\u0432\u0430\u0442\u044c \u0433\u0440\u0430\u0444\u0438\u043a, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0441\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u0438\u0441\u0442\u0438\u043d\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0438 \u0435\u0435 \u0430\u043f\u043f\u0440\u043e\u043a\u0441\u0438\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u0432\u0430\u0440\u0438\u0430\u043d\u0442 (\u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c\u044b\u0439 \u043c\u043e\u0434\u0435\u043b\u044c\u044e). +3 \u0431\u0430\u043b\u043b \u0437\u0430 (b)</li> </ul>"},{"location":"portfolio/course_nlp/2_pytorch.html","title":"\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0438\u0441\u0442\u0438\u043d\u043d\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438\u00b6","text":"<ul> <li>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f PyTorch, \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u0438 \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u043e\u043d\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0430\u043f\u043f\u0440\u043e\u043a\u0441\u0438\u043c\u0438\u0440\u0443\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 sin(x + 2*y)exp(-(2x + y)^2) \u043d\u0430 \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 [-10;10] \u043f\u043e \u0445 \u0438 \u0443</li> <li>\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0441\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c 20000 \u0442\u043e\u0447\u0435\u043a \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c</li> </ul>"},{"location":"portfolio/course_nlp/2_pytorch.html","title":"\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438\u00b6","text":"<ul> <li>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f PyTorch, \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u0438 \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u043e\u043d\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0430\u043f\u043f\u0440\u043e\u043a\u0441\u0438\u043c\u0438\u0440\u0443\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 sin(x + 2*y)exp(-(2x + y)^2) \u043d\u0430 \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 [-10;10] \u043f\u043e \u0445 \u0438 \u0443</li> <li>\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0441\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c 20000 \u0442\u043e\u0447\u0435\u043a \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c \u0420\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u044c \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0439\u0441\u044f \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043d\u0430 train / test / val \u0432 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044f\u0445 70% / 15% / 15%, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e</li> </ul> <p>\u0421\u043d\u0430\u0447\u0430\u043b\u0430 \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u043d\u0430\u0448 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c:</p> <ul> <li>\u0412 \u0437\u0430\u0434\u0430\u0447\u0435 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0447\u0442\u043e \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0443 \u043d\u0430\u0441 20000 \u0442\u043e\u0447\u0435\u043a, \u043f\u043e\u0441\u043b\u0435 \u0447\u0435\u0433\u043e \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435; \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u044d\u0442\u043e \u043c\u043e\u0436\u043d\u043e \u0432\u043e\u0441\u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0442\u044c \u043f\u043e \u0440\u0430\u0437\u043d\u043e\u043c\u0443; \u0435\u0441\u043b\u0438 train \u044d\u0442\u043e \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 \u0442\u043e \u0443 \u043d\u0430\u0441 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f (20000,3000,3000)</li> <li>\u041e\u0431\u044b\u0447\u043d\u043e \u043c\u044b \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442, \u043f\u043e\u0441\u043b\u0435 \u0447\u0435\u0433\u043e \u0435\u0433\u043e \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u043d\u0430 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u044d\u0442\u043e \u043c\u043e\u0436\u043d\u043e \u0438 \u0432\u043e\u0441\u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0442\u044c \u043a\u0430\u043a (20000) -&gt; (14000,3000,3000)</li> </ul>"},{"location":"portfolio/course_nlp/2_pytorch.html","title":"\u0413\u043e\u0442\u043e\u0432\u0438\u043c\u0441\u044f \u043a \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044e\u00b6","text":""},{"location":"portfolio/course_nlp/2_pytorch.html","title":"\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<ul> <li>\u0420\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u043b\u0438\u0441\u044c \u0440\u0430\u0437\u043d\u044b\u0435 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u044b \u043c\u043d\u043e\u0433\u043e\u0441\u043b\u043e\u0439\u043d\u043e\u0433\u043e \u043f\u0435\u0440\u0441\u0435\u043f\u0442\u0440\u043e\u043d\u0430, \u0432 \u043a\u043e\u043d\u0435\u0447\u043d\u043e\u043c \u0438\u0442\u043e\u0433\u0435 \u043c\u043e\u0434\u0435\u043b\u044c <code>Net</code> \u043f\u043e\u043a\u0430\u0437\u0430\u043b\u0430 \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442.</li> <li>\u0410\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430 \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0438\u0437 5 \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u0441\u043b\u043e\u0435\u0432 \u0438 4 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0441\u0442\u0438</li> </ul>"},{"location":"portfolio/course_nlp/2_pytorch.html","title":"\u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440/\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c\u00b6","text":"<p>\u0414\u0430\u043b\u0435\u0435 \u0432 \u043d\u0430\u0448\u0435\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0432\u0435\u0441\u043e\u0432, \u043d\u0430\u043c \u043d\u0443\u0436\u0435\u043d \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440 (Adam) \u0441 learning rate 0.0001 \u0438 \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c (\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c MSELoss)</p>"},{"location":"portfolio/course_nlp/2_pytorch.html","title":"\u0422\u0440\u0435\u043d\u0438\u0440\u0443\u0435\u043c \u041c\u043e\u0434\u0435\u043b\u044c\u00b6","text":"<p>\u0412 \u0437\u0430\u0434\u0430\u0447\u0438 \u043d\u0435 \u0443\u0442\u043e\u0447\u043d\u0430\u0435\u0442\u0441\u044f \u0447\u0442\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043b\u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u0446\u0438\u043a\u043b \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438\u043b\u0438 \u043d\u0435\u0442, \u0434\u043b\u044f \u0443\u0441\u043a\u0430\u0440\u0435\u043d\u0438\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438, \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0435 \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u043d\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 val</p>"},{"location":"portfolio/course_nlp/2_pytorch.html","title":"\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u00b6","text":""},{"location":"portfolio/course_nlp/2_pytorch.html","title":"\u041e\u0446\u0435\u043d\u043a\u0430 \u041c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<p>\u0427\u0430\u0441\u0442\u043e \u043c\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0433\u0440\u0430\u0444\u0438\u043a (y_true vs y_model) \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u043e\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438, \u043e\u0446\u0435\u043d\u0438\u043c \u043d\u0430 \u0432\u0441\u0435\u0445 \u0432\u044b\u0431\u043e\u0440\u043a\u0430\u0445</p>"},{"location":"portfolio/course_nlp/2_pytorch.html","title":"\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\u00b6","text":"<p>\u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043c\u0435\u0442\u0440\u0438\u043a\u0443 Mean Square Error(MSE) \u043d\u0430 test  \u041d\u0430\u0440\u0438\u0441\u043e\u0432\u0430\u0442\u044c \u0433\u0440\u0430\u0444\u0438\u043a, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0441\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u0438\u0441\u0442\u0438\u043d\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0438 \u0435\u0435 \u0430\u043f\u043f\u0440\u043e\u043a\u0441\u0438\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u0432\u0430\u0440\u0438\u0430\u043d\u0442 (\u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c\u044b\u0439 \u043c\u043e\u0434\u0435\u043b\u044c\u044e)</p> <p>\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043d\u0430 \u0432\u0441\u0435\u0445 \u0432\u044b\u0431\u043e\u0440\u043a\u0430\u0445; \u043d\u0430 \u043b\u0435\u0432\u043e\u0439 \u0441\u0442\u043e\u0440\u043e\u043d\u0435 \u0443 \u043d\u0430\u0441 \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0430 \u043d\u0430 \u043f\u0440\u0430\u0432\u043e\u043c \u0433\u0440\u0430\u0444\u0438\u043a\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438, \u043c\u044b \u0432\u0438\u0434\u0438\u043c \u0447\u0442\u043e \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u044c \u043c\u043e\u0436\u0435\u0442 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0445\u043e\u0440\u043e\u0448\u043e \u0430\u043f\u043f\u0440\u043e\u043a\u0441\u0438\u043c\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u044e, MSE \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 0.0005324</p>"},{"location":"portfolio/course_nlp/3_product-reviews.html","title":"3 product reviews","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\n\nreviews = pd.read_csv('/kaggle/input/product-review/review_cleaned.csv')\nreviews.drop(['review'],axis=1,inplace=True)\nreviews\n</pre> import pandas as pd  reviews = pd.read_csv('/kaggle/input/product-review/review_cleaned.csv') reviews.drop(['review'],axis=1,inplace=True) reviews Out[1]: user time conclusion rating review_cleaned 0 dncmail 2023-06-21T08:34:25+02:00 \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 2 \u041f\u043e\u0434\u0435\u043b\u044e\u0441\u044c \u0441 \u0432\u0430\u043c\u0438 \u0438\u0441\u0442\u043e\u0440\u0438\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u0430 \u0441\u043e... 1 fomicevaa851 2023-06-21T07:39:25+02:00 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 5 \u0421\u0430\u043c\u0430 \u043d\u0435\u0434\u0430\u0432\u043d\u043e \u0443\u0437\u043d\u0430\u043b\u0430, \u0447\u0442\u043e \u0432 \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0435 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u043b... 2 AlexStulov 2023-06-14T13:52:43+02:00 \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 1 \u0421\u0431\u0435\u0440 \u043f\u043e\u0442\u0435\u0440\u044f\u043b \u043c\u043e\u0439 \u043c\u0438\u043b\u043b\u0438\u043e\u043d. \u0412 \u0430\u043f\u0440\u0435\u043b\u0435 \u0431\u0440\u0430\u043b \u0438\u043f\u043e\u0442\u0435\u043a... 3 Zakharkot 2023-06-13T08:04:53+02:00 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 5 \u0414\u043e\u0431\u0440\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0441\u0443\u0442\u043e\u043a \u0432\u0441\u0435\u043c, \u044f \u043e\u0442\u043a\u0440\u044b\u043b \u0432 \u0421\u0431\u0435\u0440\u0435 \u0432... 4 sanaan 2023-06-11T23:40:00+02:00 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 4 \u0416\u0438\u0432\u0443 \u0441 \u043c\u0430\u043c\u043e\u0439, \u043e\u043f\u043b\u0430\u0442\u043e\u0439 \u043a\u043e\u043c\u043c\u0443\u043d\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u043b\u0430\u0442\u0435\u0436\u0435\u0439 \u0434\u043e... ... ... ... ... ... ... 1117 Mila_Krom 2018-10-14T19:41:26+02:00 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 3 \u041f\u043e\u0441\u043b\u0435 \u0442\u043e\u0433\u043e, \u043a\u0430\u043a \u043d\u0430\u043c \u0432 \u0412\u0422\u0411 \u043f\u043e\u043d\u0438\u0437\u0438\u043b\u0438 \u0441\u0442\u0430\u0432\u043a\u0443 \u0432\u0441\u0435... 1118 Inrak 2018-10-10T07:39:12+02:00 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 5 \u0412 2013 \u0433\u043e\u0434\u0443 \u043f\u0440\u0438 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0438 \u0437\u0430\u0440\u043f\u043b\u0430\u0442\u043d\u043e\u0439 \u043a\u0430\u0440\u0442\u044b, \u0441\u043e... 1119 nastyamostya 2018-10-03T12:19:22+02:00 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 5 \u0414\u0430\u0432\u043d\u043e \u0432\u044b\u0431\u0440\u0430\u043b\u0430 \u0434\u043b\u044f \u0441\u0435\u0431\u044f \u043b\u0443\u0447\u0448\u0435\u0435 \u043e\u0442\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0421\u0431\u0435\u0440\u0430.... 1120 \u0422\u0430\u0442\u044c\u044f\u043d\u0430 \u041e\u043a\u0440\u0430\u0439\u0447\u0438\u043a 2018-10-02T15:59:41+02:00 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 5 \u0412\u0441\u0435\u043c \u0434\u043e\u0431\u0440\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0441\u0443\u0442\u043e\u043a! \u0421\u0435\u0433\u043e\u0434\u043d\u044f \u0445\u043e\u0447\u0443 \u0440\u0430\u0441\u0441\u043a... 1121 \u0413\u0430\u043b\u0438\u043d\u0430_8_8_8 2018-09-26T13:35:02+02:00 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 5 \u0410 \u043c\u044b \u0432\u043e\u0442 \u0431\u0440\u0430\u043b\u0438 \u0438\u043f\u043e\u0442\u0435\u043a\u0443 \u0432 \u0421\u0431\u0435\u0440\u0435. \u041f\u043e \u043d\u0430\u0448\u0438\u043c \u0441 \u043c\u0443\u0436... <p>1122 rows \u00d7 5 columns</p> In\u00a0[2]: Copied! <pre>reviews['conclusion'].value_counts(dropna=False)\n</pre> reviews['conclusion'].value_counts(dropna=False) Out[2]: <pre>\u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442    574\n\u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442       548\nName: conclusion, dtype: int64</pre> In\u00a0[3]: Copied! <pre>from sklearn.model_selection import train_test_split as tts\n\nX_train,X_test = tts(reviews,test_size=0.2,random_state=32)\n</pre> from sklearn.model_selection import train_test_split as tts  X_train,X_test = tts(reviews,test_size=0.2,random_state=32) In\u00a0[4]: Copied! <pre># \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u0430\u0447\u043d\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430\n\ncorpus_tr = list(X_train['review_cleaned'])  # \u043a\u043e\u0440\u043f\u0443\u0441 \ntarget_tr = list(X_train['conclusion'])   # \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f\n</pre> # \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u0430\u0447\u043d\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430  corpus_tr = list(X_train['review_cleaned'])  # \u043a\u043e\u0440\u043f\u0443\u0441  target_tr = list(X_train['conclusion'])   # \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f In\u00a0[5]: Copied! <pre># \u0422\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430\n\ncorpus_te = list(X_test['review_cleaned'])  # \u043a\u043e\u0440\u043f\u0443\u0441 \ntarget_te = list(X_test['conclusion'])   # \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f\n</pre> # \u0422\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430  corpus_te = list(X_test['review_cleaned'])  # \u043a\u043e\u0440\u043f\u0443\u0441  target_te = list(X_test['conclusion'])   # \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f In\u00a0[6]: Copied! <pre>from sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords \nrussian_stopwords = stopwords.words(\"russian\")\n\n# \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0447\u0430\u0441\u0442\u043e\u0442\u0430 \u0441\u043b\u043e\u0432\nv1_vectoriser = TfidfVectorizer(min_df=3,norm=None)\nv1_vectoriser.fit(corpus_tr)\nX1 = v1_vectoriser.transform(corpus_tr)\n\nv2_vectoriser = TfidfVectorizer(min_df=2,norm=None)\nv2_vectoriser.fit(corpus_tr)\nX2 = v2_vectoriser.transform(corpus_tr)\n\nv3_vectoriser = TfidfVectorizer(min_df=1,norm=None)\nv3_vectoriser.fit(corpus_tr)\nX3 = v3_vectoriser.transform(corpus_tr)\n</pre> from sklearn.feature_extraction.text import TfidfVectorizer from nltk.corpus import stopwords  russian_stopwords = stopwords.words(\"russian\")  # \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0447\u0430\u0441\u0442\u043e\u0442\u0430 \u0441\u043b\u043e\u0432 v1_vectoriser = TfidfVectorizer(min_df=3,norm=None) v1_vectoriser.fit(corpus_tr) X1 = v1_vectoriser.transform(corpus_tr)  v2_vectoriser = TfidfVectorizer(min_df=2,norm=None) v2_vectoriser.fit(corpus_tr) X2 = v2_vectoriser.transform(corpus_tr)  v3_vectoriser = TfidfVectorizer(min_df=1,norm=None) v3_vectoriser.fit(corpus_tr) X3 = v3_vectoriser.transform(corpus_tr) In\u00a0[7]: Copied! <pre># \u0441\u0442\u043e\u043f \u0441\u043b\u043e\u0432\u0430 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b\nv4_vectoriser = TfidfVectorizer(min_df=3,norm=None,stop_words=russian_stopwords)\nv4_vectoriser.fit(corpus_tr)\nX4 = v4_vectoriser.transform(corpus_tr)\n\nv5_vectoriser = TfidfVectorizer(min_df=2,norm=None,stop_words=russian_stopwords)\nv5_vectoriser.fit(corpus_tr)\nX5 = v5_vectoriser.transform(corpus_tr)\n\nv6_vectoriser = TfidfVectorizer(min_df=1,norm=None,stop_words=russian_stopwords)\nv6_vectoriser.fit(corpus_tr)\nX6 = v6_vectoriser.transform(corpus_tr)\n</pre> # \u0441\u0442\u043e\u043f \u0441\u043b\u043e\u0432\u0430 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b v4_vectoriser = TfidfVectorizer(min_df=3,norm=None,stop_words=russian_stopwords) v4_vectoriser.fit(corpus_tr) X4 = v4_vectoriser.transform(corpus_tr)  v5_vectoriser = TfidfVectorizer(min_df=2,norm=None,stop_words=russian_stopwords) v5_vectoriser.fit(corpus_tr) X5 = v5_vectoriser.transform(corpus_tr)  v6_vectoriser = TfidfVectorizer(min_df=1,norm=None,stop_words=russian_stopwords) v6_vectoriser.fit(corpus_tr) X6 = v6_vectoriser.transform(corpus_tr) In\u00a0[8]: Copied! <pre># \u043d-\u0433\u0440\u0430\u043c \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b\nv7_vectoriser = TfidfVectorizer(min_df=2,norm=None,stop_words=russian_stopwords,ngram_range=(1,1))\nv7_vectoriser.fit(corpus_tr)\nX7 = v7_vectoriser.transform(corpus_tr)\n\nv8_vectoriser = TfidfVectorizer(min_df=2,norm=None,stop_words=russian_stopwords,ngram_range=(1,2))\nv8_vectoriser.fit(corpus_tr)\nX8 = v8_vectoriser.transform(corpus_tr)\n\nv9_vectoriser = TfidfVectorizer(min_df=2,norm=None,stop_words=russian_stopwords,ngram_range=(1,3))\nv9_vectoriser.fit(corpus_tr)\nX9 = v9_vectoriser.transform(corpus_tr)\n\nv10_vectoriser = TfidfVectorizer(min_df=2,norm=None,stop_words=russian_stopwords,ngram_range=(2,2))\nv10_vectoriser.fit(corpus_tr)\nX10 = v10_vectoriser.transform(corpus_tr)\n\nv11_vectoriser = TfidfVectorizer(min_df=2,norm=None,stop_words=russian_stopwords,ngram_range=(3,3))\nv11_vectoriser.fit(corpus_tr)\nX11 = v11_vectoriser.transform(corpus_tr)\n\nv12_vectoriser = TfidfVectorizer(min_df=2,norm=None,stop_words=russian_stopwords,ngram_range=(4,4))\nv12_vectoriser.fit(corpus_tr)\nX12 = v12_vectoriser.transform(corpus_tr)\n</pre> # \u043d-\u0433\u0440\u0430\u043c \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b v7_vectoriser = TfidfVectorizer(min_df=2,norm=None,stop_words=russian_stopwords,ngram_range=(1,1)) v7_vectoriser.fit(corpus_tr) X7 = v7_vectoriser.transform(corpus_tr)  v8_vectoriser = TfidfVectorizer(min_df=2,norm=None,stop_words=russian_stopwords,ngram_range=(1,2)) v8_vectoriser.fit(corpus_tr) X8 = v8_vectoriser.transform(corpus_tr)  v9_vectoriser = TfidfVectorizer(min_df=2,norm=None,stop_words=russian_stopwords,ngram_range=(1,3)) v9_vectoriser.fit(corpus_tr) X9 = v9_vectoriser.transform(corpus_tr)  v10_vectoriser = TfidfVectorizer(min_df=2,norm=None,stop_words=russian_stopwords,ngram_range=(2,2)) v10_vectoriser.fit(corpus_tr) X10 = v10_vectoriser.transform(corpus_tr)  v11_vectoriser = TfidfVectorizer(min_df=2,norm=None,stop_words=russian_stopwords,ngram_range=(3,3)) v11_vectoriser.fit(corpus_tr) X11 = v11_vectoriser.transform(corpus_tr)  v12_vectoriser = TfidfVectorizer(min_df=2,norm=None,stop_words=russian_stopwords,ngram_range=(4,4)) v12_vectoriser.fit(corpus_tr) X12 = v12_vectoriser.transform(corpus_tr) In\u00a0[9]: Copied! <pre># \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0447\u0430\u0441\u0442\u043e\u0442\u0430 \u0441\u043b\u043e\u0432\nv13_vectoriser = TfidfVectorizer(min_df=2,max_df=0.9,norm=None,stop_words=russian_stopwords)\nv13_vectoriser.fit(corpus_tr)\nX13 = v13_vectoriser.transform(corpus_tr)\n\nv14_vectoriser = TfidfVectorizer(min_df=2,max_df=0.8,norm=None,stop_words=russian_stopwords)\nv14_vectoriser.fit(corpus_tr)\nX14 = v14_vectoriser.transform(corpus_tr)\n\nv15_vectoriser = TfidfVectorizer(min_df=2,max_df=0.7,norm=None,stop_words=russian_stopwords)\nv15_vectoriser.fit(corpus_tr)\nX15 = v15_vectoriser.transform(corpus_tr)\n</pre> # \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0447\u0430\u0441\u0442\u043e\u0442\u0430 \u0441\u043b\u043e\u0432 v13_vectoriser = TfidfVectorizer(min_df=2,max_df=0.9,norm=None,stop_words=russian_stopwords) v13_vectoriser.fit(corpus_tr) X13 = v13_vectoriser.transform(corpus_tr)  v14_vectoriser = TfidfVectorizer(min_df=2,max_df=0.8,norm=None,stop_words=russian_stopwords) v14_vectoriser.fit(corpus_tr) X14 = v14_vectoriser.transform(corpus_tr)  v15_vectoriser = TfidfVectorizer(min_df=2,max_df=0.7,norm=None,stop_words=russian_stopwords) v15_vectoriser.fit(corpus_tr) X15 = v15_vectoriser.transform(corpus_tr) In\u00a0[10]: Copied! <pre>from sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier\n\nmodel_srf = RandomForestClassifier(n_estimators=10,random_state=32)  # \u043d\u0435 \u0433\u043b\u0443\u0431\u043e\u043a\u0438\u0439 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441\nmodel_drf = RandomForestClassifier(n_estimators=40,random_state=32)  # \u0433\u043b\u0443\u0431\u043e\u043a\u0438\u0439 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441\nmodel_ocb = CatBoostClassifier(silent=True)  # \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433\n</pre> from sklearn.ensemble import RandomForestClassifier from catboost import CatBoostClassifier  model_srf = RandomForestClassifier(n_estimators=10,random_state=32)  # \u043d\u0435 \u0433\u043b\u0443\u0431\u043e\u043a\u0438\u0439 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441 model_drf = RandomForestClassifier(n_estimators=40,random_state=32)  # \u0433\u043b\u0443\u0431\u043e\u043a\u0438\u0439 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441 model_ocb = CatBoostClassifier(silent=True)  # \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433 In\u00a0[11]: Copied! <pre>from sklearn.metrics import accuracy_score\n\n# \u0432\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438\n\ndef evaluate_tfidf(X,vect,name,model):\n\n    print(f'case_id: :{name}')\n    print('==================================')\n    \n    # train model\n    model.fit(X,target_tr)\n    y_model = model.predict(X)\n    print(f'train: {accuracy_score(target_tr,y_model)}')\n    \n    X = vect.transform(corpus_te)\n    y_model = model.predict(X)\n    print(f'test: {accuracy_score(y_model,target_te)}')\n    \n    print('==================================','\\n')\n</pre> from sklearn.metrics import accuracy_score  # \u0432\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438  def evaluate_tfidf(X,vect,name,model):      print(f'case_id: :{name}')     print('==================================')          # train model     model.fit(X,target_tr)     y_model = model.predict(X)     print(f'train: {accuracy_score(target_tr,y_model)}')          X = vect.transform(corpus_te)     y_model = model.predict(X)     print(f'test: {accuracy_score(y_model,target_te)}')          print('==================================','\\n') In\u00a0[12]: Copied! <pre>evaluate_tfidf(X3,v3_vectoriser,'v3',model_srf)\nevaluate_tfidf(X2,v2_vectoriser,'v2',model_srf)\nevaluate_tfidf(X1,v1_vectoriser,'v1',model_srf) \n</pre> evaluate_tfidf(X3,v3_vectoriser,'v3',model_srf) evaluate_tfidf(X2,v2_vectoriser,'v2',model_srf) evaluate_tfidf(X1,v1_vectoriser,'v1',model_srf)  <pre>case_id: :v3\n==================================\ntrain: 0.9866220735785953\ntest: 0.7288888888888889\n================================== \n\ncase_id: :v2\n==================================\ntrain: 0.987736900780379\ntest: 0.7466666666666667\n================================== \n\ncase_id: :v1\n==================================\ntrain: 0.9843924191750278\ntest: 0.7111111111111111\n================================== \n\n</pre> In\u00a0[13]: Copied! <pre>evaluate_tfidf(X4,v4_vectoriser,'v6',model_srf)\nevaluate_tfidf(X5,v5_vectoriser,'v5',model_srf)\nevaluate_tfidf(X6,v6_vectoriser,'v4',model_srf) \n</pre> evaluate_tfidf(X4,v4_vectoriser,'v6',model_srf) evaluate_tfidf(X5,v5_vectoriser,'v5',model_srf) evaluate_tfidf(X6,v6_vectoriser,'v4',model_srf)  <pre>case_id: :v6\n==================================\ntrain: 0.987736900780379\ntest: 0.7155555555555555\n================================== \n\ncase_id: :v5\n==================================\ntrain: 0.9866220735785953\ntest: 0.76\n================================== \n\ncase_id: :v4\n==================================\ntrain: 0.9866220735785953\ntest: 0.7111111111111111\n================================== \n\n</pre> In\u00a0[14]: Copied! <pre>evaluate_tfidf(X9,v9_vectoriser,'v9',model_srf)\nevaluate_tfidf(X8,v8_vectoriser,'v8',model_srf)\nevaluate_tfidf(X7,v7_vectoriser,'v7',model_srf) \n</pre> evaluate_tfidf(X9,v9_vectoriser,'v9',model_srf) evaluate_tfidf(X8,v8_vectoriser,'v8',model_srf) evaluate_tfidf(X7,v7_vectoriser,'v7',model_srf)  <pre>case_id: :v9\n==================================\ntrain: 0.987736900780379\ntest: 0.6622222222222223\n================================== \n\ncase_id: :v8\n==================================\ntrain: 0.9888517279821628\ntest: 0.7155555555555555\n================================== \n\ncase_id: :v7\n==================================\ntrain: 0.9866220735785953\ntest: 0.76\n================================== \n\n</pre> <p>\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u0435\u0449\u0435 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b:</p> <ul> <li><code>v10</code> ngram_range=(1,1) (\u0422\u043e\u043b\u044c\u043a\u043e \u0443\u043d\u0438\u0433\u0440\u0430\u043c\u043c\u044b)</li> <li><code>v11</code> ngram_range=(2,2) (\u0422\u043e\u043b\u044c\u043a\u043e \u0431\u0438\u0433\u0440\u0430\u043c\u043c\u044b)</li> <li><code>v12</code> ngram_range=(3,3) (\u0422\u043e\u043b\u044c\u043a\u043e \u0442\u0440\u0438\u0433\u0440\u0430\u043c\u043c\u044b)</li> </ul> In\u00a0[15]: Copied! <pre>evaluate_tfidf(X12,v12_vectoriser,'v12',model_srf)\nevaluate_tfidf(X11,v11_vectoriser,'v11',model_srf)\nevaluate_tfidf(X10,v10_vectoriser,'v10',model_srf) \n</pre> evaluate_tfidf(X12,v12_vectoriser,'v12',model_srf) evaluate_tfidf(X11,v11_vectoriser,'v11',model_srf) evaluate_tfidf(X10,v10_vectoriser,'v10',model_srf)  <pre>case_id: :v12\n==================================\ntrain: 0.6622073578595318\ntest: 0.5466666666666666\n================================== \n\ncase_id: :v11\n==================================\ntrain: 0.9119286510590858\ntest: 0.6\n================================== \n\ncase_id: :v10\n==================================\ntrain: 0.9620958751393534\ntest: 0.6444444444444445\n================================== \n\n</pre> In\u00a0[16]: Copied! <pre>evaluate_tfidf(X13,v13_vectoriser,'v13',model_srf)\nevaluate_tfidf(X14,v14_vectoriser,'v14',model_srf)\nevaluate_tfidf(X15,v15_vectoriser,'v15',model_srf)\n</pre> evaluate_tfidf(X13,v13_vectoriser,'v13',model_srf) evaluate_tfidf(X14,v14_vectoriser,'v14',model_srf) evaluate_tfidf(X15,v15_vectoriser,'v15',model_srf) <pre>case_id: :v13\n==================================\ntrain: 0.9866220735785953\ntest: 0.76\n================================== \n\ncase_id: :v14\n==================================\ntrain: 0.9866220735785953\ntest: 0.76\n================================== \n\ncase_id: :v15\n==================================\ntrain: 0.992196209587514\ntest: 0.7155555555555555\n================================== \n\n</pre> In\u00a0[17]: Copied! <pre># 12000 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n\nX5.shape\n</pre> # 12000 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432  X5.shape Out[17]: <pre>(897, 11969)</pre> In\u00a0[18]: Copied! <pre>evaluate_tfidf(X5,v5_vectoriser,'v5_deeprf',model_drf)\nevaluate_tfidf(X5,v5_vectoriser,'v5_shallowcatboost',model_ocb)\n</pre> evaluate_tfidf(X5,v5_vectoriser,'v5_deeprf',model_drf) evaluate_tfidf(X5,v5_vectoriser,'v5_shallowcatboost',model_ocb) <pre>case_id: :v5_deeprf\n==================================\ntrain: 0.9988851727982163\ntest: 0.8044444444444444\n================================== \n\ncase_id: :v5_shallowcatboost\n==================================\ntrain: 0.9654403567447045\ntest: 0.7955555555555556\n================================== \n\n</pre> <p>\u0423 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u043b\u0435\u0441\u0430 \u043c\u043d\u043e\u0433\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0436\u043d\u043e \u043f\u0435\u0440\u0435\u043f\u043e\u0431\u043e\u0432\u0430\u0442\u044c, \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0438\u043c\u0441\u044f:</p> <ul> <li><code>max_depth</code> \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0433\u043b\u0443\u0431\u0438\u043d\u0430 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f</li> <li><code>n_estimators</code> \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0435\u0448\u0430\u0443\u0449\u0438\u0445 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432</li> <li><code>criterion</code> \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0438 \u043e\u0446\u0435\u043d\u043a\u0438 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f \u0432 \u0443\u0437\u043b\u0435</li> <li><code>min_samples_leaf</code> \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0431\u0440\u0430\u0437\u0446\u043e\u0432, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0435 \u0434\u043b\u044f \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u0432 \u043b\u0438\u0441\u0442\u043e\u0432\u043e\u043c \u0443\u0437\u043b\u0435 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0434\u0435\u0440\u0435\u0432\u0430</li> <li><code>min_samples_split</code> \u041c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0431\u0440\u0430\u0437\u0446\u043e\u0432, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0435 \u0434\u043b\u044f \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0435\u0433\u043e \u0443\u0437\u043b\u0430 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0434\u0435\u0440\u0435\u0432\u0430</li> </ul> <p>\u0418\u0437 \u0441\u043e\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0438 \u0432\u0440\u0435\u043c\u0435\u043d\u0438, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u044b\u0445 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0438\u0432\u0430\u0435\u0442\u0441\u044f (e_stimators=100)</p> In\u00a0[19]: Copied! <pre>from sklearn.model_selection import GridSearchCV\nfrom scipy.stats import randint\nimport numpy as np\n\n\ndef evaluate_cv(X,name,base_estimator):\n\n    print(f'case_id: :{name}')\n    print('==================================')\n    \n    rs_space={'max_depth':list(np.arange(10,60,10)) + [None],\n              'n_estimators':np.arange(40,120, step=20),\n              'criterion':['gini','entropy'],\n              'min_samples_leaf':[1,2,3,4],\n               'min_samples_split':np.arange(2,6, step=2),\n              }\n\n    model = GridSearchCV(base_estimator,\n                         rs_space, \n                         scoring='accuracy', \n                         n_jobs=-1, \n                         cv=3)\n    \n    # train model\n    model.fit(X,target_tr)\n    \n    print('Best hyperparameters are: '+str(model.best_params_))\n    print('Best score is: '+str(model.best_score_))\n    \n    print('==================================','\\n')\n    \n</pre> from sklearn.model_selection import GridSearchCV from scipy.stats import randint import numpy as np   def evaluate_cv(X,name,base_estimator):      print(f'case_id: :{name}')     print('==================================')          rs_space={'max_depth':list(np.arange(10,60,10)) + [None],               'n_estimators':np.arange(40,120, step=20),               'criterion':['gini','entropy'],               'min_samples_leaf':[1,2,3,4],                'min_samples_split':np.arange(2,6, step=2),               }      model = GridSearchCV(base_estimator,                          rs_space,                           scoring='accuracy',                           n_jobs=-1,                           cv=3)          # train model     model.fit(X,target_tr)          print('Best hyperparameters are: '+str(model.best_params_))     print('Best score is: '+str(model.best_score_))          print('==================================','\\n')       <p>\u041a\u0440\u043e\u0441\u0441 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u043d\u0430\u043c \u0434\u0430\u0435\u0442 \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u0445\u0443\u0436\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0447\u0435\u043c \u043d\u0430 train/test \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435; \u0432 \u0438\u0442\u043e\u0433\u0435 \u043c\u044b \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c 0.78 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0432 cv, \u0447\u0442\u043e \u043d\u0435 \u0442\u0430\u043a \u0443\u0436 \u0438 \u043f\u043b\u043e\u0445\u043e</p> In\u00a0[20]: Copied! <pre>%%time\n\nmodel_drf = RandomForestClassifier(random_state=32)\nevaluate_cv(X5,'random_forest_cv',model_srf)\n</pre> %%time  model_drf = RandomForestClassifier(random_state=32) evaluate_cv(X5,'random_forest_cv',model_srf) <pre>case_id: :random_forest_cv\n==================================\nBest hyperparameters are: {'criterion': 'entropy', 'max_depth': 40, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 100}\nBest score is: 0.7814938684503902\n================================== \n\nCPU times: user 3.13 s, sys: 604 ms, total: 3.73 s\nWall time: 1min 55s\n</pre> <p>\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043d\u0430 \u043d\u0430 \u0432\u0441\u0435\u0439 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0438 \u043e\u0442\u043b\u043e\u0436\u0435\u043d\u043d\u043e\u0439 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u043d\u0430 \u0431\u043e\u043b\u044c\u0448\u0435 \u043e\u0431\u044a\u0435\u043c\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0443 \u043d\u0430\u0441 \u043c\u043e\u0434\u0435\u043b\u044c \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u043b\u0443\u0447\u0448\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442</p> In\u00a0[22]: Copied! <pre>best_model = RandomForestClassifier(**{'criterion': 'entropy', \n                                     'max_depth': 40, \n                                     'min_samples_leaf': 3, \n                                     'min_samples_split': 2, \n                                     'n_estimators': 100, \n                                     'random_state':32})\n\nevaluate_tfidf(X5,v5_vectoriser,'v5_rf_best',best_model)\n</pre> best_model = RandomForestClassifier(**{'criterion': 'entropy',                                       'max_depth': 40,                                       'min_samples_leaf': 3,                                       'min_samples_split': 2,                                       'n_estimators': 100,                                       'random_state':32})  evaluate_tfidf(X5,v5_vectoriser,'v5_rf_best',best_model) <pre>case_id: :v5_rf_best\n==================================\ntrain: 0.9821627647714605\ntest: 0.8133333333333334\n================================== \n\n</pre> In\u00a0[23]: Copied! <pre># \u041d\u0435 \u0437\u0430\u0431\u044b\u0432\u0430\u0435\u043c \u043f\u0440\u043e \u0432\u0435\u0441\u0430 tfidf; \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u044c\ntfidf_weights = dict(zip(list(v5_vectoriser.vocabulary_.keys()),v5_vectoriser.idf_))\n</pre> # \u041d\u0435 \u0437\u0430\u0431\u044b\u0432\u0430\u0435\u043c \u043f\u0440\u043e \u0432\u0435\u0441\u0430 tfidf; \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u044c tfidf_weights = dict(zip(list(v5_vectoriser.vocabulary_.keys()),v5_vectoriser.idf_)) In\u00a0[24]: Copied! <pre>import gensim\nimport urllib.request\nimport zipfile\n\n# \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0438 URL\nwe_models = {\"geowac_lemmas_none_fasttextskipgram_300_5_2020\": \"http://vectors.nlpl.eu/repository/20/213.zip\",}\n</pre> import gensim import urllib.request import zipfile  # \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0438 URL we_models = {\"geowac_lemmas_none_fasttextskipgram_300_5_2020\": \"http://vectors.nlpl.eu/repository/20/213.zip\",} In\u00a0[25]: Copied! <pre># \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\ndef get_models(model_url, model_name, path_to_save=\"/kaggle/working/\"):\n    model_path = path_to_save + model_name + \".zip\"\n    urllib.request.urlretrieve(model_url, model_path)\n\nfor model_name, model_url in we_models.items():\n    get_models(model_url, model_name)\n</pre> # \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c def get_models(model_url, model_name, path_to_save=\"/kaggle/working/\"):     model_path = path_to_save + model_name + \".zip\"     urllib.request.urlretrieve(model_url, model_path)  for model_name, model_url in we_models.items():     get_models(model_url, model_name) In\u00a0[26]: Copied! <pre># \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0447\u0442\u0435\u043d\u0438\u044f word2vec / FastText\n\ndef open_model(model_name,model_path, is_fasttext = True):\n    \n    # word2vec (model.bin)\n    if is_fasttext == False:\n        model_file = model_path + model_name + \".zip\"\n        with zipfile.ZipFile(model_file, 'r') as archive:\n            stream = archive.open('model.bin')\n            model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)\n            \n    # fasttext (model.model)\n    else:\n        model_file = model_path + model_name\n        model = gensim.models.KeyedVectors.load(model_file + \"/model.model\")\n    return model\n</pre> # \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0447\u0442\u0435\u043d\u0438\u044f word2vec / FastText  def open_model(model_name,model_path, is_fasttext = True):          # word2vec (model.bin)     if is_fasttext == False:         model_file = model_path + model_name + \".zip\"         with zipfile.ZipFile(model_file, 'r') as archive:             stream = archive.open('model.bin')             model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)                  # fasttext (model.model)     else:         model_file = model_path + model_name         model = gensim.models.KeyedVectors.load(model_file + \"/model.model\")     return model <p>\u0420\u0430\u0441\u043f\u0430\u043a\u0443\u0435\u043c \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 \u0438 \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0432\u0435\u043a\u0442\u043e\u0440\u0430</p> In\u00a0[27]: Copied! <pre>with zipfile.ZipFile(\"/kaggle/working/geowac_lemmas_none_fasttextskipgram_300_5_2020.zip\", 'r') as zip_ref: \n    zip_ref.extractall(\"/kaggle/working/geowac_lemmas_none_fasttextskipgram_300_5_2020\")\n</pre> with zipfile.ZipFile(\"/kaggle/working/geowac_lemmas_none_fasttextskipgram_300_5_2020.zip\", 'r') as zip_ref:      zip_ref.extractall(\"/kaggle/working/geowac_lemmas_none_fasttextskipgram_300_5_2020\") In\u00a0[\u00a0]: Copied! <pre># \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c KeyedVectors \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432\u044b\u0439 \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \ngeowac_model = open_model('geowac_lemmas_none_fasttextskipgram_300_5_2020','/kaggle/working/')\n</pre> # \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c KeyedVectors \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432\u044b\u0439 \u0432\u0435\u043a\u0442\u043e\u0440\u0430  geowac_model = open_model('geowac_lemmas_none_fasttextskipgram_300_5_2020','/kaggle/working/') In\u00a0[29]: Copied! <pre>from nltk.tokenize import word_tokenize\n\n# Preprocessing, returns list instead\ndef tokenise_for_word2vec(text):\n    \n    text = text.lower() #changes to lower case\n    tokens = word_tokenize(text) #tokenize the text\n    \n    clean_list = [] \n    for token in tokens:\n        clean_list.append(token)\n        \n    return clean_list\n</pre> from nltk.tokenize import word_tokenize  # Preprocessing, returns list instead def tokenise_for_word2vec(text):          text = text.lower() #changes to lower case     tokens = word_tokenize(text) #tokenize the text          clean_list = []      for token in tokens:         clean_list.append(token)              return clean_list In\u00a0[30]: Copied! <pre>lst_corpus_tr = []\nfor doc in corpus_tr:\n    lst_corpus_tr.append(tokenise_for_word2vec(doc))\n    \nlst_corpus_te = []\nfor doc in corpus_te:\n    lst_corpus_te.append(tokenise_for_word2vec(doc))\n</pre> lst_corpus_tr = [] for doc in corpus_tr:     lst_corpus_tr.append(tokenise_for_word2vec(doc))      lst_corpus_te = [] for doc in corpus_te:     lst_corpus_te.append(tokenise_for_word2vec(doc)) In\u00a0[31]: Copied! <pre># Get average embedding vector for each text\n\ndef doc_vectoriser(doc, model):\n    \n    doc_vector = []\n    num_words = 0\n    \n    for word in doc:\n        try:\n            if num_words == 0:\n                doc_vector = model[word]\n            else:\n                doc_vector = np.add(doc_vector, model[word])\n            num_words += 1\n        except:\n            pass  # if embedding vector isn't found\n     \n    return np.asarray(doc_vector) / num_words\n</pre> # Get average embedding vector for each text  def doc_vectoriser(doc, model):          doc_vector = []     num_words = 0          for word in doc:         try:             if num_words == 0:                 doc_vector = model[word]             else:                 doc_vector = np.add(doc_vector, model[word])             num_words += 1         except:             pass  # if embedding vector isn't found           return np.asarray(doc_vector) / num_words In\u00a0[32]: Copied! <pre>X_tr = []\nfor doc in lst_corpus_tr:\n    X_tr.append(doc_vectoriser(doc,geowac_model))\n    \n\nX_te = []\nfor doc in lst_corpus_te:\n    X_te.append(doc_vectoriser(doc,geowac_model))\n</pre> X_tr = [] for doc in lst_corpus_tr:     X_tr.append(doc_vectoriser(doc,geowac_model))       X_te = [] for doc in lst_corpus_te:     X_te.append(doc_vectoriser(doc,geowac_model)) In\u00a0[33]: Copied! <pre># Get average embedding vector for each text \n\ndef doc_vectoriser_tfidf(doc, model):\n    \n    doc_vector = []\n    num_words = 0\n    \n    for word in doc:\n        try:\n            if num_words == 0:\n                \n                # if word is in tfidf dictionary\n                if(word in tfidf_weights):\n                    doc_vector = model[word]*(tfidf_weights[word])\n                else:\n                    doc_vector = model[word]\n            else:\n                \n                # if word is in tfidf dictionary\n                if(word in tfidf_weights):\n                    doc_vector = np.add(doc_vector, model[word]*(tfidf_weights[word]))\n                else:\n                    doc_vector = np.add(doc_vector, model[word])\n                    \n            num_words += 1\n        except:\n            pass  # if embedding vector isn't found\n     \n    return np.asarray(doc_vector) / num_words\n</pre> # Get average embedding vector for each text   def doc_vectoriser_tfidf(doc, model):          doc_vector = []     num_words = 0          for word in doc:         try:             if num_words == 0:                                  # if word is in tfidf dictionary                 if(word in tfidf_weights):                     doc_vector = model[word]*(tfidf_weights[word])                 else:                     doc_vector = model[word]             else:                                  # if word is in tfidf dictionary                 if(word in tfidf_weights):                     doc_vector = np.add(doc_vector, model[word]*(tfidf_weights[word]))                 else:                     doc_vector = np.add(doc_vector, model[word])                                  num_words += 1         except:             pass  # if embedding vector isn't found           return np.asarray(doc_vector) / num_words In\u00a0[34]: Copied! <pre>Xw_tr = []\nfor doc in lst_corpus_tr:\n    Xw_tr.append(doc_vectoriser_tfidf(doc,geowac_model))\n    \nXw_te = []\nfor doc in lst_corpus_te:\n    Xw_te.append(doc_vectoriser_tfidf(doc,geowac_model))\n</pre> Xw_tr = [] for doc in lst_corpus_tr:     Xw_tr.append(doc_vectoriser_tfidf(doc,geowac_model))      Xw_te = [] for doc in lst_corpus_te:     Xw_te.append(doc_vectoriser_tfidf(doc,geowac_model)) In\u00a0[35]: Copied! <pre>from sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier\n\nmodel_srf = RandomForestClassifier(n_estimators=10,random_state=32)\nmodel_drf = RandomForestClassifier(n_estimators=40,random_state=32)\nmodel_ocb = CatBoostClassifier(silent=True)\n</pre> from sklearn.ensemble import RandomForestClassifier from catboost import CatBoostClassifier  model_srf = RandomForestClassifier(n_estimators=10,random_state=32) model_drf = RandomForestClassifier(n_estimators=40,random_state=32) model_ocb = CatBoostClassifier(silent=True) In\u00a0[36]: Copied! <pre>from sklearn.metrics import accuracy_score\n\n# \u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043c\u0430\u0442\u0440\u0438\u0447\u0443 \u0444\u0438\u0447 (\u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438) X_tr, X_te\n# target_tr, target_te \u0433\u043b\u043e\u0431 \n\ndef evaluate_embedding(X_tr,X_te,name,model):\n\n    print(f'case_id: :{name}')\n    print('==================================')\n    \n    # train model\n    model.fit(X_tr,target_tr)\n    y_model = model.predict(X_tr)\n    print(f'train: {accuracy_score(target_tr,y_model)}')\n    \n    y_model = model.predict(X_te)\n    print(f'test: {accuracy_score(y_model,target_te)}')\n    \n    print('==================================','\\n')\n</pre> from sklearn.metrics import accuracy_score  # \u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043c\u0430\u0442\u0440\u0438\u0447\u0443 \u0444\u0438\u0447 (\u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438) X_tr, X_te # target_tr, target_te \u0433\u043b\u043e\u0431   def evaluate_embedding(X_tr,X_te,name,model):      print(f'case_id: :{name}')     print('==================================')          # train model     model.fit(X_tr,target_tr)     y_model = model.predict(X_tr)     print(f'train: {accuracy_score(target_tr,y_model)}')          y_model = model.predict(X_te)     print(f'test: {accuracy_score(y_model,target_te)}')          print('==================================','\\n') In\u00a0[37]: Copied! <pre>evaluate_embedding(X_tr,X_te,'geowac_rf',model_drf)\n</pre> evaluate_embedding(X_tr,X_te,'geowac_rf',model_drf) <pre>case_id: :geowac_rf\n==================================\ntrain: 1.0\ntest: 0.7422222222222222\n================================== \n\n</pre> In\u00a0[38]: Copied! <pre>evaluate_embedding(X_tr,X_te,'geowac_cat',model_ocb)\n</pre> evaluate_embedding(X_tr,X_te,'geowac_cat',model_ocb) <pre>case_id: :geowac_cat\n==================================\ntrain: 1.0\ntest: 0.8044444444444444\n================================== \n\n</pre> In\u00a0[39]: Copied! <pre>evaluate_embedding(Xw_tr,Xw_te,'geowac_rf_tfidf_weights',model_drf)\n</pre> evaluate_embedding(Xw_tr,Xw_te,'geowac_rf_tfidf_weights',model_drf) <pre>case_id: :geowac_rf_tfidf_weights\n==================================\ntrain: 1.0\ntest: 0.6977777777777778\n================================== \n\n</pre> In\u00a0[40]: Copied! <pre>evaluate_embedding(Xw_tr,Xw_te,'geowac_cat_tfidf_weights',model_ocb)\n</pre> evaluate_embedding(Xw_tr,Xw_te,'geowac_cat_tfidf_weights',model_ocb) <pre>case_id: :geowac_cat_tfidf_weights\n==================================\ntrain: 1.0\ntest: 0.7555555555555555\n================================== \n\n</pre> <ul> <li>\u0414\u043b\u044f \u0431\u043e\u043b\u0435\u0435 \u043d\u0438\u0437\u043a\u043e\u0433\u043e \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0430 \u0437\u0430\u0434\u0430\u0447\u0438, \u043c\u044b \u0432\u0438\u0434\u0438\u043c \u0447\u0442\u043e \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u043d\u0438\u043d\u0433 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u043b \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u043b\u0443\u0447\u0448\u0435 \u0447\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441</li> <li>TF-IDF \u0432\u0435\u0441\u0430 \u043d\u0435 \u043f\u043e\u043c\u043e\u0433\u043b\u0438 \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u0430\u043d\u0441\u0430\u043c\u043b\u0435\u0432\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438</li> <li>\u0418\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u043e \u0447\u0442\u043e \u043c\u043e\u0434\u0435\u043b\u044c CatBoost \u043f\u043e\u043a\u0430\u0437\u0430\u043b\u0430 \u0442\u043e\u0442 \u0442\u0443 \u0436\u0435 \u0441\u0430\u043c\u0443\u044e \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043a\u0430\u043a \u0438 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441 \u0438 Part I; \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u0447\u0442\u043e \u0441\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u043c \u043d\u0435\u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 (\u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043c \u0430\u043d\u0430\u043b\u0438\u0437 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0432 \u0440\u0430\u0437\u0434\u0435\u043b\u0435 10)</li> </ul> In\u00a0[41]: Copied! <pre>%%time\n\nmodel_drf = RandomForestClassifier(random_state=32)\nevaluate_cv(np.array(X_tr),'geowac_rf_cv',model_drf)\n</pre> %%time  model_drf = RandomForestClassifier(random_state=32) evaluate_cv(np.array(X_tr),'geowac_rf_cv',model_drf) <pre>case_id: :geowac_rf_cv\n==================================\nBest hyperparameters are: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 100}\nBest score is: 0.7725752508361204\n================================== \n\nCPU times: user 3.12 s, sys: 252 ms, total: 3.37 s\nWall time: 2min 49s\n</pre> In\u00a0[42]: Copied! <pre>best_model_rf = RandomForestClassifier(**{'criterion': 'entropy', \n                                       'max_depth': 10, \n                                       'min_samples_leaf': 1, \n                                       'min_samples_split': 4, \n                                       'n_estimators': 100,\n                                       'random_state':32})\n</pre> best_model_rf = RandomForestClassifier(**{'criterion': 'entropy',                                         'max_depth': 10,                                         'min_samples_leaf': 1,                                         'min_samples_split': 4,                                         'n_estimators': 100,                                        'random_state':32}) In\u00a0[43]: Copied! <pre>evaluate_embedding(X_tr,X_te,'geowac_rf_tfidf_weights',best_model_rf)\n</pre> evaluate_embedding(X_tr,X_te,'geowac_rf_tfidf_weights',best_model_rf) <pre>case_id: :geowac_rf_tfidf_weights\n==================================\ntrain: 1.0\ntest: 0.7822222222222223\n================================== \n\n</pre> In\u00a0[44]: Copied! <pre>%%time\n\nmodel_drf = RandomForestClassifier(random_state=32)\nevaluate_cv(np.array(Xw_tr),'geowac_rf_cv_tfidf_weights',model_drf)\n</pre> %%time  model_drf = RandomForestClassifier(random_state=32) evaluate_cv(np.array(Xw_tr),'geowac_rf_cv_tfidf_weights',model_drf) <pre>case_id: :geowac_rf_cv_tfidf_weights\n==================================\nBest hyperparameters are: {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 80}\nBest score is: 0.7647714604236343\n================================== \n\nCPU times: user 2.64 s, sys: 287 ms, total: 2.93 s\nWall time: 2min 49s\n</pre> In\u00a0[45]: Copied! <pre># tts evaluation function &amp; return model results \n\ndef evaluate_tfidf(X,vect,name,model):\n\n    print(f'case_id: :{name}')\n    print('==================================')\n    \n    # train model\n    model.fit(X,target_tr)\n    y_model_tr = model.predict(X)\n    print(f'train: {accuracy_score(target_tr,y_model_tr)}')\n    \n    X = vect.transform(corpus_te)\n    y_model_te = model.predict(X)\n    print(f'test: {accuracy_score(y_model_te,target_te)}')\n    \n    return pd.Series(y_model_tr),pd.Series(y_model_te)\n    \n    print('==================================','\\n')\n</pre> # tts evaluation function &amp; return model results   def evaluate_tfidf(X,vect,name,model):      print(f'case_id: :{name}')     print('==================================')          # train model     model.fit(X,target_tr)     y_model_tr = model.predict(X)     print(f'train: {accuracy_score(target_tr,y_model_tr)}')          X = vect.transform(corpus_te)     y_model_te = model.predict(X)     print(f'test: {accuracy_score(y_model_te,target_te)}')          return pd.Series(y_model_tr),pd.Series(y_model_te)          print('==================================','\\n') <p>\u0412\u0435\u0440\u043d\u0435\u043c\u0441\u044f \u043a TF-IDF \u043f\u043e\u0434\u0445\u043e\u0434\u0443:</p> In\u00a0[46]: Copied! <pre>ym_train,ym_test = evaluate_tfidf(X5,v5_vectoriser,'v5_rf_best',best_model)\n</pre> ym_train,ym_test = evaluate_tfidf(X5,v5_vectoriser,'v5_rf_best',best_model) <pre>case_id: :v5_rf_best\n==================================\ntrain: 0.9821627647714605\ntest: 0.8133333333333334\n</pre> In\u00a0[47]: Copied! <pre>X_train = X_train.reset_index()\nX_test = X_test.reset_index()\nX_train['ypred'] = ym_train\nX_test['ypred'] = ym_test\n</pre> X_train = X_train.reset_index() X_test = X_test.reset_index() X_train['ypred'] = ym_train X_test['ypred'] = ym_test <p>\u041d\u0430\u0439\u0434\u0435\u043c \u043f\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044e \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0430 \u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0435 \u0438 \u043d\u0435 \u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438</p> In\u00a0[48]: Copied! <pre>X_test['correct'] = (X_test['conclusion'] == X_test['ypred'])\n</pre> X_test['correct'] = (X_test['conclusion'] == X_test['ypred']) <ul> <li>\u041c\u043e\u0436\u043d\u043e \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c \u0447\u0442\u043e \u043c\u043e\u0434\u0435\u043b\u044c \u0447\u0430\u0441\u0442\u043e \u043e\u0448\u0438\u0431\u0430\u0435\u0442\u0441\u044f \u043a\u043e\u0433\u0434\u0430 \u043a\u043b\u0438\u0435\u043d\u0442\u044b \u0441\u0442\u0430\u0432\u044f\u0442 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 3 (\u0438\u0437 1-5)</li> <li>\u0421\u0440\u0435\u0434\u043d\u044f\u044f \u043e\u0446\u0435\u043d\u043a\u0430 \u0432\u043f\u043e\u043b\u043d\u0435 \u043c\u043e\u0436\u0435\u0442 \u043e\u0431\u043e\u0437\u043d\u0430\u0447\u0430\u0442\u044c \u0447\u0442\u043e \u043a\u043b\u0438\u0435\u043d\u0442 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043d\u0435\u0439\u0442\u0440\u0430\u043b\u0435\u043d; \u043c\u043e\u0436\u0435\u0442 \u043f\u0438\u0441\u0430\u0442\u044c \u043a\u0430\u043a \u0438 \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435, \u0442\u0430\u043a \u0438 \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0438 \u0432 \u043e\u0442\u0437\u044b\u0432\u0435 \u0447\u0442\u043e \u043c\u043e\u0436\u0435\u0442 \u0437\u0430\u043f\u0443\u0442\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c; \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043c\u043e\u0436\u043d\u043e \u0432\u043f\u043e\u043b\u043d\u0435 \u0443\u0432\u0435\u0440\u0435\u043d\u043e \u0441\u043a\u0430\u0437\u0430\u0442\u044c \u0447\u0442\u043e \u043f\u043e\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u0437\u0430\u0434\u0430\u0447\u0430 \u043d\u0435 \u0441\u0430\u043c\u0430\u044f \u0438\u0434\u0435\u0430\u043b\u044c\u043d\u0430\u044f</li> <li>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0443 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u0445 (True) \u0438 \u043d\u0435 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e (False) \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0445 \u043e\u0442\u0437\u044b\u0432\u043e\u0432</li> </ul> In\u00a0[49]: Copied! <pre>X_test.groupby(by=['correct','rating']).count()[['index']].T\n</pre> X_test.groupby(by=['correct','rating']).count()[['index']].T Out[49]: correct False True rating 1 2 3 4 5 1 2 3 4 5 index 10 6 12 4 10 73 17 15 13 65 <ul> <li>\u041a\u0430\u043a \u0438 \u043e\u0436\u0438\u0434\u0430\u043b\u043e\u0441\u044c, \u043a\u043b\u0438\u0435\u043d\u0442 \u043f\u0438\u0448\u0435\u0442 \u043e\u0442\u0437\u044b\u0432 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e \u0438 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u043e, \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u0442 \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0442\u0430\u043a \u0438 \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0439 \u043e\u043f\u044b\u0442</li> <li>\u0420\u0430\u0437\u043d\u0438\u0446\u0430 \u043c\u0435\u0436\u0434\u0443 \u0442\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c\u044e \u043d\u0435\u0439\u0442\u0440\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0438 \u044f\u0432\u043d\u043e \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u043e \u043d\u0430\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u043e\u0433\u043e \u043e\u0442\u0437\u044b\u0432\u0430 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043e\u0447\u0435\u0432\u0438\u0434\u043d\u0430</li> <li>\u0414\u043b\u044f \u0431\u0443\u0434\u0443\u0449\u0435\u0433\u043e, \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442\u0441\u044f \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u044c \u043c\u0435\u0442\u043a\u0443 \u043d\u0430 \u0435\u0449\u0435 \u043e\u0434\u043d\u0443 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044e (\u0447\u0442\u043e-\u0442\u043e \u043d\u0435\u0439\u0442\u0440\u0430\u043b\u044c\u043d\u043e\u0435)</li> </ul> In\u00a0[50]: Copied! <pre># \u043d\u0435\u0439\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439 \u043e\u0442\u0437\u044b\u0432 \nX_test[(X_test['correct'] == False) &amp; (X_test['rating'] == 3)]['review_cleaned'].reset_index(drop=True).iloc[2]\n</pre> # \u043d\u0435\u0439\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439 \u043e\u0442\u0437\u044b\u0432  X_test[(X_test['correct'] == False) &amp; (X_test['rating'] == 3)]['review_cleaned'].reset_index(drop=True).iloc[2] Out[50]: <pre>'\u0412\u0435\u0440\u044e \u0432\u0441\u0435\u043c\u0443 \u043e \u0447\u0435\u043c \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u043e \u0432 \u043e\u0442\u0437\u044b\u0432\u0430\u0445, \u043d\u043e \u043d\u0435 \u0441\u043e \u0432\u0441\u0435\u043c \u043c\u043e\u0433\u0443 \u0441\u043e\u0433\u043b\u0430\u0441\u0438\u0442\u044c\u0441\u044f. \u0423 \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430 \u043f\u043e\u043b\u043d\u043e \u043e\u0433\u0440\u0435\u0445\u043e\u0432, \u043d\u0435\u0434\u043e\u0440\u0430\u0431\u043e\u0442\u043e\u043a, \u0441\u0431\u043e\u0435\u0432, \u043a\u0430\u043a \u0438 \u0443 \u043c\u043d\u043e\u0433\u0438\u0445 \u0434\u0440\u0443\u0433\u0438\u0445 \u0431\u0430\u043d\u043a\u043e\u0432. \u041d\u0438\u043a\u0442\u043e \u0441\u0438\u043b\u043a\u043e\u043c \u0432 \u0431\u0430\u043d\u043a \u043d\u0435 \u0437\u0430\u0433\u043e\u043d\u044f\u0435\u0442, \u043d\u0430\u043c \u0441\u0430\u043c\u0438\u043c \u043f\u0440\u043e\u0449\u0435 \u043d\u0435 \u0438\u0434\u0442\u0438 \u043f\u0440\u043e\u0442\u0438\u0432 \u0442\u0435\u0447\u0435\u043d\u0438\u044f \u0438 \u043d\u0435 \u043e\u0442\u0441\u0442\u0430\u0438\u0432\u0430\u0442\u044c \u0441\u0432\u043e\u0438 \u043f\u0440\u0430\u0432\u0430, \u0441\u043a\u0430\u0437\u0430\u043b\u0438 \u043e\u0444\u043e\u0440\u043c\u0438\u0442\u044c \u0441\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0435 \u043f\u043e\u0441\u043e\u0431\u0438\u0435 \u0438\u043b\u0438 \u0437\u0430\u0440\u043f\u043b\u0430\u0442\u0443, \u0438\u043b\u0438 \u043f\u0435\u043d\u0441\u0438\u044e \u0447\u0435\u0440\u0435\u0437 \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a, \u043c\u044b \u0438 \u043e\u0444\u043e\u0440\u043c\u043b\u044f\u0435\u043c, \u0434\u0443\u043c\u0430\u0435\u043c \u0447\u0442\u043e \u043e\u043d \u043d\u0430\u0434\u0435\u0436\u043d\u0435\u0435, \u0438\u043b\u0438 \u043d\u0435 \u0445\u043e\u0447\u0435\u0442\u0441\u044f \u0438\u0434\u0442\u0438 \u0432 \u0431\u0443\u0445\u0433\u0430\u043b\u0442\u0435\u0440\u0438\u044e \u043f\u0440\u0430\u0432\u0430 \u043a\u0430\u0447\u0430\u0442\u044c. \u042f \u0442\u043e\u0447\u043d\u043e \u0442\u0430\u043a\u0436\u0435 \u0441\u0442\u043e\u043b\u043a\u043d\u0443\u043b\u0430\u0441\u044c \u0441 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u043e\u0439 \u043f\u0440\u0438 \u043e\u0444\u043e\u0440\u043c\u043b\u0435\u043d\u0438\u0438 \u0441\u0432\u043e\u0435\u0439 \u0441\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0439 \u043a\u0430\u0440\u0442\u044b. \u041a\u043e\u0433\u0434\u0430 \u0437\u0430\u043a\u0430\u0437\u044b\u0432\u0430\u043b\u0430 \u043a\u0430\u0440\u0442\u0443 \u043e\u0442\u043a\u0430\u0437\u0430\u043b\u0430\u0441\u044c \u043e\u0442 \u0443\u0441\u043b\u0443\u0433\u0438 \u043f\u043b\u0430\u0442\u043d\u043e\u0433\u043e \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u043e\u0433\u043e \u0431\u0430\u043d\u043a\u0430, \u043d\u043e \u043f\u0440\u043e\u0441\u0438\u043b\u0430 \u0432\u043a\u043b\u044e\u0447\u0438\u0442\u044c \u0443\u0441\u043b\u0443\u0433\u0443 \u0431\u0435\u0441\u043f\u043b\u0430\u0442\u043d\u043e\u0433\u043e \u0441\u043c\u0441 \u0438\u043d\u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0438 Online \u0431\u0430\u043d\u043a\u043e\u043c, \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u043a\u043b\u044e\u0447\u0435\u0439 \u0432\u0445\u043e\u0434\u0430 \u0438 \u043f\u0440\u043e\u0432\u043e\u0434\u0438\u043c\u044b\u0445 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u044f\u0445 \u0432 \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442 \u0431\u0430\u043d\u043a\u0435. \u042d\u0442\u043e \u0434\u0432\u0435 \u0440\u0430\u0437\u043d\u044b\u0435 \u0443\u0441\u043b\u0443\u0433\u0438, \u043e\u0434\u043d\u0430 \u043f\u043b\u0430\u0442\u043d\u0430\u044f, \u0434\u0440\u0443\u0433\u0430\u044f \u043d\u0435\u0442, \u043d\u043e \u044d\u0442\u043e \u044f \u0441\u0435\u0439\u0447\u0430\u0441 \u0442\u0430\u043a \u0441\u043a\u043b\u0430\u0434\u043d\u043e \u0438\u0437\u043b\u0430\u0433\u0430\u044e, \u0430 \u043a\u043e\u0433\u0434\u0430 \u043f\u0440\u0435\u0434\u044a\u044f\u0432\u043b\u044f\u043b\u0430 \u043f\u0440\u0435\u0442\u0435\u043d\u0437\u0438\u0438 \u0431\u0430\u043d\u043a\u0443, \u0442\u0430\u043b\u0434\u044b\u0447\u0438\u043b\u0430 \u043f\u0440\u043e \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u0439 \u0431\u0430\u043d\u043a \u0432\u043c\u0435\u0441\u0442\u043e \u0443\u0441\u043b\u0443\u0433\u0438 \u0431\u0435\u0441\u043f\u043b\u0430\u0442\u043d\u043e\u0433\u043e \u0441\u043c\u0441 \u0438\u043d\u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0438 \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442 \u0431\u0430\u043d\u043a\u043e\u043c. \u041a\u043e\u0440\u043e\u0447\u0435 \u043e\u043d\u0438 \u043c\u043d\u0435 \u043f\u0440\u043e \u0424\u043e\u043c\u0443, \u0430 \u044f \u0438\u043c \u043f\u0440\u043e \u042f\u0440\u0435\u043c\u0443. \u0422\u0435\u043f\u0435\u0440\u044c \u043f\u0440\u043e \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442 \u0431\u0430\u043d\u043a, \u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0441\u044c \u0443\u0436\u0435 \u0434\u0432\u0430 \u0433\u043e\u0434\u0430, \u0434\u043e \u044d\u0442\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u043e\u0441\u044c \u043a\u0430\u0440\u0442\u043e\u0439 \u0438 \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442 \u0431\u0430\u043d\u043a\u043e\u043c \"\u043f\u0440\u043e\u043c\u0441\u0432\u044f\u0437\u044c\u0431\u0430\u043d\u043a\u0430\" \u0435\u0441\u0442\u044c \u0441 \u0447\u0435\u043c \u0441\u0440\u0430\u0432\u043d\u0438\u0442\u044c, \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u043d\u0430\u043f\u0440\u044f\u0433\u0430\u043b\u043e \u0431\u043e\u043b\u044c\u0448\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u0442\u0440\u0430\u043d\u0438\u0446 \u0441 \u043f\u043e\u0432\u0442\u043e\u0440\u044f\u044e\u0449\u0438\u043c\u0438\u0441\u044f \u0440\u0435\u043a\u0432\u0438\u0437\u0438\u0442\u0430\u043c\u0438 \u043f\u0440\u0438 \u043e\u043f\u043b\u0430\u0442\u0435 \u0416\u041a\u0425, \u043f\u0440\u0438\u0447\u0435\u043c \u043f\u0435\u0440\u0432\u044b\u0439 \u0440\u0430\u0437 \u043d\u0443\u0436\u043d\u043e \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u0448\u0430\u0431\u043b\u043e\u043d, \u0437\u0430\u0442\u0435\u043c \u0435\u0433\u043e \u043f\u043e\u0434\u0442\u0432\u0435\u0440\u0434\u0438\u0442\u044c \u0438 \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u043e\u0442\u043e\u043c \u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0441\u0442\u0438 \u043e\u043f\u043b\u0430\u0442\u0443. \u0417\u0434\u0435\u0441\u044c \u043e\u043d\u0438 \u043d\u0430\u043a\u0440\u0443\u0442\u0438\u043b\u0438 \u0431\u0443\u0434\u044c \u0437\u0434\u043e\u0440\u043e\u0432, \u0432 \"\u043f\u0440\u043e\u043c\u0441\u0432\u044f\u0437\u044c\u0431\u0430\u043d\u043a\u0435\" \u044d\u0442\u0430 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u044f \u0432\u044b\u0433\u043b\u044f\u0434\u0438\u0442 \u043a\u0430\u043a \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435 \u043e\u0431\u044b\u0447\u043d\u043e\u0439 \u043a\u0432\u0438\u0442\u0430\u043d\u0446\u0438\u0438, \u043d\u0438\u043a\u0430\u043a\u0438\u0445 \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u0432 \u0438 \u043f\u0440\u043e\u0431\u043b\u0435\u043c. \u0427\u0442\u043e \u043f\u043e\u043d\u0440\u0430\u0432\u0438\u043b\u043e\u0441\u044c \u0443 \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430 - \u044d\u0442\u043e \u043f\u043b\u0430\u0442\u0438\u0442\u044c \u043d\u0430\u043b\u043e\u0433\u0438 \u0437\u0430 \u043a\u0432\u0430\u0440\u0442\u0438\u0440\u0443, \u0434\u0430\u0447\u0443 \u0438 \u0442.\u0434., \u043f\u043e \u043d\u043e\u043c\u0435\u0440\u0443 \u0438\u0437\u0432\u0435\u0449\u0435\u043d\u0438\u044f \u0441\u0440\u0430\u0437\u0443 \u0432\u044b\u0441\u0432\u0435\u0447\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u0432\u0430\u0448\u0430 \u043a\u0432\u0438\u0442\u0430\u043d\u0446\u0438\u044f, \u043e\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u0435\u0442\u0435 \u043e\u0434\u043d\u0438\u043c \u043a\u043b\u0438\u043a\u043e\u043c. \u0412 \"\u043f\u0440\u043e\u043c\u0441\u0432\u044f\u0437\u044c\u0431\u0430\u043d\u043a\u0435\" \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u043d\u0443\u0436\u043d\u043e \u0437\u0430\u043f\u043e\u043b\u043d\u044f\u0442\u044c \u0432\u0441\u0435 \u043f\u043e\u043b\u044f \u0441\u0430\u043c\u043e\u0441\u0442\u043e\u044f\u0442\u0435\u043b\u044c\u043d\u043e, \u043f\u043e\u043a\u0430 \u0437\u0430\u043f\u043e\u043b\u043d\u0438\u0448\u044c-\u0441\u043e\u0441\u0442\u0430\u0440\u0438\u0448\u044c\u0441\u044f. \u0422\u0430\u043a\u0436\u0435 \u043b\u0435\u0433\u043a\u043e \u0432 \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0435 \u043e\u043f\u043b\u0430\u0442\u0438\u0442\u044c \u0434\u043e\u043c\u0430\u0448\u043d\u0438\u0439 \u0442\u0435\u043b\u0435\u0444\u043e\u043d, \u043d\u0435 \u043d\u0430\u0434\u043e \u0436\u0434\u0430\u0442\u044c \u043a\u0432\u0438\u0442\u0430\u043d\u0446\u0438\u0438 \u043d\u0430 \u043e\u043f\u043b\u0430\u0442\u0443, \u043d\u0430\u0431\u0438\u0440\u0430\u0435\u0448\u044c \u043d\u043e\u043c\u0435\u0440 \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u0430 \u0438 \u0441\u0443\u043c\u043c\u0430 \u0434\u043e\u043b\u0433\u0430 \u043d\u0430 \u044d\u043a\u0440\u0430\u043d\u0435. \u041a\u043e\u0433\u0434\u0430 \u0431\u043e\u043b\u0435\u0435 \u0438\u043b\u0438 \u043c\u0435\u043d\u0435\u0435 \u043e\u0441\u0432\u043e\u0438\u0448\u044c\u0441\u044f \u0438 \u0441\u043e\u0437\u0434\u0430\u0448\u044c \u0448\u0430\u0431\u043b\u043e\u043d\u044b, \u043f\u0440\u043e\u0431\u043b\u0435\u043c \u043d\u0435 \u0432\u043e\u0437\u043d\u0438\u043a\u0430\u0435\u0442. \u041a\u0430\u0440\u0442\u043e\u0439 \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430 \u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0441\u044c \u043f\u0440\u0438 \u043e\u043f\u043b\u0430\u0442\u0435 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432 \u0438\u043b\u0438 \u0442\u043e\u0432\u0430\u0440\u043e\u0432 \u0432 \u043b\u044e\u0431\u043e\u043c \u043c\u0430\u0433\u0430\u0437\u0438\u043d\u0435 , \u0433\u0434\u0435 \u043a\u0430\u0440\u0442\u044b \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u044e\u0442 \u043a \u043e\u043f\u043b\u0430\u0442\u0435 \u0443\u0436\u0435 \u0434\u0432\u0430 \u0433\u043e\u0434\u0430. \u0421\u0440\u0430\u0437\u0443 \u043a\u0430\u043a \u043f\u043e\u044f\u0432\u0438\u043b\u0430\u0441\u044c \u0443\u0441\u043b\u0443\u0433\u0430 \"\u0421\u043f\u0430\u0441\u0438\u0431\u043e \u043e\u0442 \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430\", \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0438\u043b\u0430\u0441\u044c \u0438 \u043f\u043e\u0442\u0438\u0445\u043e\u043d\u0435\u0447\u043a\u0443 \u043a\u043e\u043f\u044f\u0442\u0441\u044f \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u0438\u0435 \u0434\u0435\u043d\u0435\u0436\u043a\u0438, \u0443\u0436\u0435 \u0440\u0430\u0441\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u043b\u0430\u0441\u044c \u0438\u043c\u0438 \u043f\u0440\u0438 \u043f\u043e\u043a\u0443\u043f\u043a\u0435 \u043c\u0435\u043b\u043e\u0447\u0435\u0432\u043a\u0438 \u0432 \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442 \u043c\u0430\u0433\u0430\u0437\u0438\u043d\u0430\u0445, \u043f\u0443\u0441\u0442\u044f\u0447\u043e\u043a, \u0430 \u043f\u0440\u0438\u044f\u0442\u043d\u043e. \u0415\u0441\u043b\u0438 \u0443 \u0432\u0430\u0441 \u0432 \u0441\u0435\u043c\u044c\u0435 \u0434\u0432\u0435 \u043a\u0430\u0440\u0442\u044b \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430, \u043b\u0435\u0433\u043a\u043e \u043f\u0435\u0440\u0435\u0431\u0440\u043e\u0441\u0438\u0442\u044c \u0434\u0435\u043d\u044c\u0433\u0438 \u0441 \u043e\u0434\u043d\u043e\u0439 \u043a\u0430\u0440\u0442\u044b \u043d\u0430 \u0434\u0440\u0443\u0433\u0443\u044e \u0431\u0435\u0441\u043f\u043b\u0430\u0442\u043d\u043e. \u0412\u044b \u043e\u0431\u0440\u0430\u0449\u0430\u043b\u0438 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u043d\u0430 \u043e\u0433\u0440\u043e\u043c\u043d\u044b\u0435 \u043e\u0447\u0435\u0440\u0435\u0434\u0438 \u0432 \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430\u0445, \u043f\u0440\u0438\u0447\u0435\u043c \u044d\u0442\u0438 \u043e\u0447\u0435\u0440\u0435\u0434\u0438 \u0441\u043e\u0441\u0442\u043e\u044f\u0442 \u043d\u0435 \u0438\u0437 \u043e\u0434\u043d\u0438\u0445 \u0441\u0442\u0430\u0440\u0443\u0448\u0435\u043a, \u043d\u0435 \u0443\u043c\u0435\u044e\u0449\u0438\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442\u043e\u043c, \u0442\u0430\u043c \u043f\u043e\u043b\u043d\u043e \u0438 \u043c\u043e\u043b\u043e\u0434\u0435\u0436\u0438. \u0421\u043f\u0440\u0430\u0448\u0438\u0432\u0430\u0435\u0442\u0441\u044f, \u043f\u043e\u0447\u0435\u043c\u0443 \u043d\u0435 \u043f\u0440\u043e\u0432\u0435\u0441\u0442\u0438 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u043f\u043b\u0430\u0442\u0435\u0436\u0438 \u0434\u043e\u043c\u0430 \u0431\u0435\u0437 \u043e\u0447\u0435\u0440\u0435\u0434\u0438, \u0441\u0438\u0434\u044f \u0437\u0430 \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043e\u043c \u0441 \u0447\u0430\u0448\u043a\u043e\u0439 \u043a\u043e\u0444\u0435. \u041f\u043e\u0447\u0435\u043c\u0443 \u043c\u043e\u043b\u043e\u0434\u044b\u0435 \u043b\u044e\u0434\u0438 \u043d\u0435 \u043c\u043e\u0433\u0443\u0442 \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u0441\u0432\u043e\u0438\u0445 \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u0435\u0439 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u0430\u0440\u043d\u044b\u043c \u043d\u0430\u0432\u044b\u043a\u0430\u043c \u0440\u0430\u0431\u043e\u0442\u044b \u0432 \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442\u0435, \u0430 \u0437\u0430\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u0441\u0442\u043e\u044f\u0442\u044c \u0432 \u043e\u0447\u0435\u0440\u0435\u0434\u044f\u0445? \u0422\u0430 \u0436\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u0430 \u0438 \u0432 \u043c\u0430\u0433\u0430\u0437\u0438\u043d\u0430\u0445, \u0435\u0434\u0438\u043d\u0438\u0446\u044b \u0440\u0430\u0441\u043f\u043b\u0430\u0447\u0438\u0432\u0430\u044e\u0442\u0441\u044f \u043a\u0430\u0440\u0442\u043e\u0447\u043a\u0430\u043c\u0438. \u0416\u0438\u0437\u043d\u044c \u043d\u0435 \u0441\u0442\u043e\u0438\u0442 \u043d\u0430 \u043c\u0435\u0441\u0442\u0435, \u0430 \u0441 \u043a\u0430\u0436\u0434\u044b\u043c \u0433\u043e\u0434\u043e\u043c \u0442\u0435\u043c\u043f \u0436\u0438\u0437\u043d\u0438 \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u0438 \u043d\u0430\u0434\u043e \u0441\u0442\u0430\u0440\u0430\u0442\u044c\u0441\u044f \u0438\u0434\u0442\u0438 \u0432 \u043d\u043e\u0433\u0443, \u0438\u043d\u0430\u0447\u0435 \u043e\u0447\u0435\u0440\u0435\u0434\u0438 \u0431\u0443\u0434\u0443\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0441\u0442\u0438. \u041c\u043d\u043e\u0433\u0438\u0445 \u043d\u0435\u043f\u0440\u0438\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439 \u043c\u043e\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u0431\u044b \u0438\u0437\u0431\u0435\u0436\u0430\u0442\u044c, \u0435\u0441\u043b\u0438 \u0431\u044b \u043d\u0430\u0448\u0438 \u043b\u044e\u0434\u0438 \u0432\u043d\u0438\u043c\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u0447\u0438\u0442\u0430\u043b\u0438 \u0442\u0435\u043a\u0441\u0442\u044b \u0434\u043e\u0433\u043e\u0432\u043e\u0440\u043e\u0432, \u0442\u0430\u0440\u0438\u0444\u043e\u0432 \u043e\u0431\u0441\u043b\u0443\u0436\u0438\u0432\u0430\u043d\u0438\u044f, \u0430 \u043f\u0440\u043e\u0449\u0435 \u0438\u043d\u0441\u0442\u0440\u0443\u043a\u0446\u0438\u0438 \u043f\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044e \u0431\u0430\u043d\u043a\u043e\u043c, \u043a\u0430\u0440\u0442\u043e\u0439 \u0438 \u0442.\u0434. \u0438 \u0442.\u043f. \u0418 \u0435\u0449\u0435 \u043d\u0435 \u0437\u0430\u0431\u044b\u0432\u0430\u0435\u0442\u0435, \u0447\u0442\u043e \u0432 \u0431\u0430\u043d\u043a\u0430\u0445 \u0440\u0430\u0431\u043e\u0442\u0430\u044e\u0442 \u043d\u0435 \u0438\u043d\u043e\u043f\u043b\u0430\u043d\u0435\u0442\u044f\u043d\u0435, \u0430 \u043d\u0430\u0448\u0438 \u0441 \u0432\u0430\u043c\u0438 \u0437\u043d\u0430\u043a\u043e\u043c\u044b\u0435, \u0434\u0440\u0443\u0437\u044c\u044f, \u0440\u043e\u0434\u0441\u0442\u0432\u0435\u043d\u043d\u0438\u043a\u0438 \u0438 \u043a\u0430\u043a \u043d\u0430\u0443\u0447\u0438\u043b\u0438 \u0438\u0445 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c, \u0442\u0430\u043a \u043e\u043d\u0438 \u0438 \u0440\u0430\u0431\u043e\u0442\u0430\u044e\u0442. \u041a\u043e\u043c\u0443 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u043e, \u043c\u043e\u0439 \u043e\u0442\u0437\u044b\u0432 \u043e \u0431\u0430\u043d\u043a\u0435 \"\u0425\u043e\u0443\u043c \u041a\u0440\u0435\u0434\u0438\u0442\" \u043f\u043e \u0441\u0441\u044b\u043b\u043a\u0435.'</pre> In\u00a0[51]: Copied! <pre># \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u043e\u0442\u0437\u044b\u0432\nX_test[(X_test['correct'] == True) &amp; (X_test['rating'] == 1)]['review_cleaned'].reset_index(drop=True).iloc[2]\n</pre> # \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u043e\u0442\u0437\u044b\u0432 X_test[(X_test['correct'] == True) &amp; (X_test['rating'] == 1)]['review_cleaned'].reset_index(drop=True).iloc[2] Out[51]: <pre>'\u041d\u0430\u0433\u043b\u043e\u0441\u0442\u044c \"\u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430\" \u043d\u0435 \u0437\u043d\u0430\u0435\u0442 \u0433\u0440\u0430\u043d\u0438\u0446! \u0421\u043d\u044f\u0442\u0438\u0435 \u0434\u0435\u043d\u0435\u0436\u043d\u044b\u0445 \u0441\u0440\u0435\u0434\u0441\u0442\u0432 \u0431\u0435\u0437 \u0443\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u044f \u043a\u043b\u0438\u0435\u043d\u0442\u0430, \u0431\u043b\u043e\u043a\u0438\u0440\u043e\u0432\u043a\u0430 \u0431\u0430\u043d\u043a\u043e\u0432\u0441\u043a\u0438\u0445 \u043a\u0430\u0440\u0442 \u0431\u0435\u0437 \u043a\u0430\u043a\u0438\u0445-\u043b\u0438\u0431\u043e \u043d\u0430 \u0442\u043e \u043f\u0440\u0438\u0447\u0438\u043d \u0438 \u0441\u0430\u043c\u043e\u0435 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u043e\u0435, \u0447\u0442\u043e \u043f\u0440\u0438\u0447\u0438\u043d\u0443 \u0431\u043b\u043e\u043a\u0438\u0440\u043e\u0432\u043a\u0438 \u043a\u0430\u0440\u0442\u044b \u043d\u0435\u043b\u044c\u0437\u044f \u0443\u0437\u043d\u0430\u0442\u044c \u043f\u043e \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u0443 \u0433\u043e\u0440\u044f\u0447\u0435\u0439 \u043b\u0438\u043d\u0438\u0438, \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u043d\u0443\u0436\u043d\u043e \u043f\u043e\u0441\u0435\u0442\u0438\u0442\u044c \u043e\u0444\u0438\u0441 \u0431\u0430\u043d\u043a\u0430! \u041e\u0442\u0432\u0440\u0430\u0442\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435 \u043e\u0431\u0441\u043b\u0443\u0436\u0438\u0432\u0430\u043d\u0438\u0435! \u0411\u043e\u043b\u044c\u0448\u0435 \u043d\u0435\u0442 \u043d\u0438\u043a\u0430\u043a\u043e\u0433\u043e \u0434\u043e\u0432\u0435\u0440\u0438\u044f \u043a \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0443, \u0431\u043e\u0438\u0448\u044c\u0441\u044f \u0445\u0440\u0430\u043d\u0438\u0442\u044c \u0441\u0432\u043e\u0438 \u0434\u0435\u043d\u0435\u0436\u043d\u044b\u0435 \u0441\u0440\u0435\u0434\u0441\u0442\u0432\u0430 \u043d\u0430 \u0431\u0430\u043d\u043a\u043e\u0432\u0441\u043a\u043e\u0439 \u043a\u0430\u0440\u0442\u0435, \u0442.\u043a. \u0432 \u043e\u0434\u0438\u043d \u043f\u0440\u0435\u043a\u0440\u0430\u0441\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442 \u0438\u0445 \u043c\u043e\u0433\u0443\u0442 \u0441\u043f\u0438\u0441\u0430\u0442\u044c \u0437\u0430 \u043a\u0430\u043a\u0438\u0435-\u043b\u0438\u0431\u043e \u0443\u0441\u043b\u0443\u0433\u0438, \u043d\u0435 \u0443\u0432\u0435\u0434\u043e\u043c\u0438\u0432 \u043a\u043b\u0438\u0435\u043d\u0442\u0430, \u043b\u0438\u0431\u043e \u0443\u0432\u0435\u0434\u043e\u043c\u0438\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u043e\u0441\u043b\u0435 \u0441\u043f\u0438\u0441\u0430\u043d\u0438\u044f \u0438 \u0431\u0435\u0437 \u0432\u0430\u0448\u0435\u0433\u043e \u0441\u043e\u0433\u043b\u0430\u0441\u0438\u044f. \u041c\u043e\u0433\u0443\u0442 \u0437\u0430\u0431\u043b\u043e\u043a\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043a\u0430\u0440\u0442\u0443 \u043a \u043f\u0440\u0438\u043c\u0435\u0440\u0443 \u043f\u043e\u0441\u043b\u0435 \u043f\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0431\u0430\u043b\u0430\u043d\u0441\u0430 \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043a\u0441\u0442\u0430\u0442\u0438 \u043f\u0440\u0438\u0432\u044f\u0437\u0430\u043d \u043a \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u043e\u043c\u0443 \u0431\u0430\u043d\u043a\u0443! \u0412 \u043e\u0431\u0449\u0435\u043c \u0442\u0432\u043e\u0440\u044f\u0442 \u0447\u0442\u043e \u0445\u043e\u0442\u044f\u0442, \u0431\u043e\u043b\u044c\u0448\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c \u0441\u0432\u043e\u0438 \u0434\u0435\u043d\u044c\u0433\u0438 \u0432 \u043a\u0430\u043a\u043e\u0439-\u043d\u0438\u0431\u0443\u0434\u044c \u0448\u0430\u0440\u0430\u0448\u043a\u0438\u043d\u043e\u0439 \u043a\u043e\u043d\u0442\u043e\u0440\u0435, \u0447\u0435\u043c \u0432 \"\u0433\u043e\u0441\u0443\u0434\u0430\u0440\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0439 \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u0438\" \u0442\u0430\u043a\u043e\u0439 \u043a\u0430\u043a \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a!'</pre>"},{"location":"portfolio/course_nlp/3_product-reviews.html","title":"\u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/course_nlp/3_product-reviews.html","title":"\u0426\u0435\u043b\u044c\u00b6","text":"<p>\u0412 \u044d\u0442\u043e\u043c \u0414\u0417 \u0432\u044b \u043e\u0441\u0432\u043e\u0438\u0442\u0435 \u0440\u0430\u0431\u043e\u0442\u0443 \u0441 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u043c\u0438 \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u044b\u043c\u0438 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f\u043c\u0438.</p>"},{"location":"portfolio/course_nlp/3_product-reviews.html","title":"\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435\u00b6","text":"<p>\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432\u043e\u0437\u044c\u043c\u0438\u0442\u0435 \u043b\u0438\u0431\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442, \u0441\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0439 \u0432 \u043f\u0435\u0440\u0432\u043e\u043c \u0437\u0430\u043d\u044f\u0442\u0438\u0438 (\u043f\u0440\u0435\u0434\u043f\u043e\u0447\u0442\u0438\u0442\u0435\u043b\u044c\u043d\u043e), \u043b\u0438\u0431\u043e \u0441\u043a\u0430\u0447\u0430\u0439\u0442\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0441 \u043e\u0442\u0437\u044b\u0432\u0430\u043c\u0438 \u043d\u0430 \u0444\u0438\u043b\u044c\u043c\u044b \u0441 \u0441\u0430\u0439\u0442\u0430 IMDB (https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews), \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043e\u0442\u0437\u044b\u0432\u0430 \u043f\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0430 \u0441\u0435\u043c\u0430\u043d\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043e\u0446\u0435\u043d\u043a\u0430 - \"\u043f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u044b\u0439\" \u0438\u043b\u0438 \"\u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0439\".</p> <ul> <li>\u0420\u0430\u0437\u0431\u0435\u0439\u0442\u0435 \u0441\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 train/test, \u043e\u0442\u043b\u043e\u0436\u0438\u0432 20-30% \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u0439 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f. \u041f\u0440\u0438\u043c\u0435\u043d\u0438\u0442\u0435 tf-idf \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u0433\u043e \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u044f. \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435 \u043a\u0430\u043a \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0435 \u0442\u043e\u043a\u0435\u043d\u044b, \u0442\u0430\u043a \u0438 \u0431\u0438\u0433\u0440\u0430\u043c\u043c\u044b, \u043e\u0442\u0441\u0435\u0439\u0442\u0435 \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430, \u0430 \u0442\u0430\u043a\u0436\u0435 \u0441\u043b\u043e\u0432\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0442\u0441\u044f \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0440\u0435\u0434\u043a\u043e \u0438\u043b\u0438 \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0447\u0430\u0441\u0442\u043e (\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b min/max_df), \u043d\u0435 \u0437\u0430\u0431\u0443\u0434\u044c\u0442\u0435 \u0443\u0431\u0440\u0430\u0442\u044c l2 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044e, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u0430.</li> <li>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 random forest \u0438\u043b\u0438 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433 (LightGBM \u0438\u043b\u0438 catboost) \u043d\u0430 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0432\u0435\u043a\u0442\u043e\u0440\u0430\u0445 \u0438 \u043f\u043e\u0434\u0431\u0435\u0440\u0438\u0442\u0435 \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u0443\u044e \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u044e \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e GridSearch</li> <li>\u0422\u0435\u043f\u0435\u0440\u044c \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435\u0441\u044c \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u043c\u0438 word2vec/fasttext \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0430\u043c\u0438 \u0434\u043b\u044f \u0432\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0442\u0430. \u0412\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0443\u0439\u0442\u0435 \u0442\u0435\u043a\u0441\u0442\u044b \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u043e\u0434\u0430 word2vec/fasttext c \u0432\u0435\u0441\u0430\u043c\u0438 tf-idf</li> <li>\u0421\u043e\u0432\u0435\u0442: \u0434\u043b\u044f \u0442\u0435\u043a\u0441\u0442\u043e\u0432 \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u043e\u043c \u044f\u0437\u044b\u043a\u0435 \u043c\u043e\u0436\u043d\u043e \u0432\u0437\u044f\u0442\u044c \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 \u0441 \u0441\u0430\u0439\u0442\u0430 rusvectores https://rusvectores.org/ru/models/ (\u0432\u0430\u043c \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0442 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u043c \u0442\u044d\u0433\u0441\u0435\u0442 \u041d\u0415\u0422). \u0414\u043b\u044f \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 \u043c\u043e\u0436\u0435\u0442\u0435 \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f word2vec, \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u043c\u0438 \u043d\u0430 Google News \u041f\u043e\u0432\u0442\u043e\u0440\u0438\u0442\u0435 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442 \u0438\u0437 \u043f\u0443\u043d\u043a\u0442\u0430 4 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0432 \u043f\u0443\u043d\u043a\u0442\u0435 5 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432</li> </ul>"},{"location":"portfolio/course_nlp/3_product-reviews.html","title":"\u041a\u0440\u0438\u0442\u0435\u0440\u0438\u0438 \u043e\u0446\u0435\u043d\u043a\u0438\u00b6","text":"<ul> <li>\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 train/test - 1 \u0431\u0430\u043b\u043b</li> <li>\u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0442\u0435\u043a\u0441\u0442\u0430 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 tf-idf - 2 \u0431\u0430\u043b\u043b\u0430</li> <li>\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 tf-idf \u0432\u0435\u043a\u0442\u043e\u0440\u0430\u0445 - 2 \u0431\u0430\u043b\u043b\u0430</li> <li>\u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0442\u0435\u043a\u0441\u0442\u0430 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u043f\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432 word2vec/fasttext - 3 \u0431\u0430\u043b\u043b\u0430</li> <li>\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0430\u0445 - 2 \u0431\u0430\u043b\u043b\u0430</li> </ul>"},{"location":"portfolio/course_nlp/3_product-reviews.html","title":"\u0427\u0442\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":""},{"location":"portfolio/course_nlp/3_product-reviews.html","title":"\u041e\u0442\u0437\u044b\u0432\u044b \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430\u00b6","text":"<ul> <li>\u041d\u0430\u0447\u043d\u0435\u043c \u0441 \u0447\u0442\u0435\u043d\u0438\u0435\u043c \u043d\u0430\u0448\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043f\u0430\u0440\u0441\u0435\u0440\u0430 \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u044b \u043e\u0442\u0437\u044b\u0432\u043e\u0432</li> <li>\u0412 \u043d\u0430\u0448\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u043c\u044b \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043b\u0438 \u043e\u0442\u0437\u044b\u0432\u044b \u043e \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0435 \u0441 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c\u0438\u0412:<ul> <li><code>user</code> - \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c/\u043a\u043b\u0438\u0435\u043d\u0442</li> <li><code>time</code> - \u0432\u0440\u0435\u043c\u044f \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u0438\u044f \u043e\u0442\u0437\u044b\u0432\u0430</li> <li><code>conclusion</code> - \u0438\u0442\u043e\u0433\u043e\u0432\u044b\u0439 \u0432\u044b\u0432\u043e\u0434 \u043e\u0442\u0437\u044b\u0432\u0430</li> <li><code>rating</code> - \u0440\u0435\u0439\u0442\u0438\u043d\u0433</li> <li><code>review_cleaned</code> - \u043e\u0442\u0437\u044b\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c/\u043a\u043b\u0438\u0435\u043d\u0442</li> </ul> </li> </ul> <p>\u041a\u043e\u0440\u043f\u0443\u0441 (corpus) \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>review_cleaned</code> \u0443\u0436\u0435 \u0431\u044b\u043b \u043e\u0447\u0438\u0449\u0435\u043d \u0432 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u043c \u0437\u0430\u0434\u0430\u043d\u0438\u0438</p>"},{"location":"portfolio/course_nlp/3_product-reviews.html","title":"\u041f\u043e\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u0417\u0430\u0434\u0430\u0447\u0438\u00b6","text":"<ul> <li>\u0414\u043b\u044f \u0443\u043f\u0440\u043e\u0449\u0435\u043d\u0438\u0435 \u0437\u0430\u0434\u0430\u0447\u0438, \u0432\u043e\u0437\u044c\u043c\u0435\u043c \u0446\u0435\u043b\u0435\u0432\u0443\u044e \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0443\u044e conclusion</li> <li>\u042d\u0442\u043e \u043e\u0434\u0438\u043d \u0438\u0437 \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0438 \u043e\u0446\u0435\u043d\u043a\u0438 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432 (\u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442 \u043b\u0438\u0431\u043e \u043d\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442), \u0432 \u0434\u0430\u043d\u043d\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u043c\u044b \u0441\u043f\u0430\u0440\u0441\u0438\u043b\u0438 \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0431\u0449\u0438\u0435 \u043e\u0442\u0437\u044b\u0432\u044b \u043e \u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0435, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u0432 \u043d\u0435\u0433\u043e \u043c\u043e\u0436\u0435\u0442 \u0432\u0445\u043e\u0434\u0438\u0442\u044c \u0440\u0430\u0437\u043d\u044b\u0435 \u0443\u0441\u043b\u0443\u0433\u0438 \u0431\u0430\u043d\u043a\u0430</li> <li>\u0411\u0443\u0434\u0435\u043c \u0441\u043e\u0437\u0434\u0430\u0432\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u043e \u043e\u0442\u0437\u044b\u0432\u0443 \u043a\u043b\u0438\u0435\u043d\u0442\u0430 review_cleaned \u0431\u0443\u0434\u0435\u0442 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0442\u044c \u0442\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u043e\u0442\u0437\u044b\u0432\u0430; \u0437\u0430\u0434\u0430\u0447\u0430 \u0431\u0438\u043d\u0430\u0440\u043d\u043e\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0432 natural language processing (NLP)</li> </ul>"},{"location":"portfolio/course_nlp/3_product-reviews.html","title":"\u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439\u00b6","text":"<p>\u0412 \u043d\u0430\u0448\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u0430\u044f \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f, <code>accuracy</code> \u0431\u0443\u0434\u0435\u0442 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0446\u0435\u043b\u0435\u0441\u043e\u043e\u0431\u0440\u0430\u0437\u043d\u0430\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0430 \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043c\u043e\u0434\u0435\u043b\u0435\u0439</p>"},{"location":"portfolio/course_nlp/3_product-reviews.html","title":"\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\u00b6","text":"<p>\u0418\u0437 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u044f:</p> <p>\u0420\u0430\u0437\u0431\u0435\u0439\u0442\u0435 \u0441\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 train/test, \u043e\u0442\u043b\u043e\u0436\u0438\u0432 20-30% \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u0439 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f</p> <ul> <li>\u0420\u0430\u0437\u043e\u0431\u044c\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 \u0434\u0432\u0435 \u0447\u0430\u0441\u0442\u0438; \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u0443\u044e (train) \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443 (test)</li> <li>\u041d\u0430 test \u0431\u0443\u0434\u0435\u043c \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0442\u044c \u043e\u0431\u043e\u0431\u0449\u0430\u044e\u0449\u0430\u044f \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438</li> </ul>"},{"location":"portfolio/course_nlp/3_product-reviews.html#part-i-tf-idf","title":"PART I - TF-IDF \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f\u00b6","text":"<p>\u0418\u0437 \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u0438 \u0437\u0430\u0434\u0430\u043d\u0438\u044f:</p> <p>\u041f\u0440\u0438\u043c\u0435\u043d\u0438\u0442\u0435 <code>tf-idf</code> \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u0433\u043e \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u044f. \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435 \u043a\u0430\u043a <code>\u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0435 \u0442\u043e\u043a\u0435\u043d\u044b</code> (unigram), \u0442\u0430\u043a \u0438 <code>\u0431\u0438\u0433\u0440\u0430\u043c\u043c\u044b</code>, \u043e\u0442\u0441\u0435\u0439\u0442\u0435 <code>\u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430</code>, \u0430 \u0442\u0430\u043a\u0436\u0435 \u0441\u043b\u043e\u0432\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0442\u0441\u044f \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0440\u0435\u0434\u043a\u043e \u0438\u043b\u0438 \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0447\u0430\u0441\u0442\u043e (\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b <code>min_df</code>/<code>max_df</code>), \u043d\u0435 \u0437\u0430\u0431\u0443\u0434\u044c\u0442\u0435 \u0443\u0431\u0440\u0430\u0442\u044c l2 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044e, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u0430</p> <ul> <li>\u0417\u0430\u0434\u0430\u0447\u0443 \u0440\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u043c \u043d\u0430 \u0434\u0432\u0435 \u0447\u0430\u0441\u0442\u0438, \u0432 \u043f\u0435\u0440\u0432\u043e\u0439 \u0447\u0430\u0441\u0442\u0438 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043f\u043e\u0434\u0445\u043e\u0434 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0442\u0435\u043a\u0441\u0442\u0430 \u0432 \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u043e\u0434\u0430 (TF-IDF)</li> <li>\u0412\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432 \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0438 TF-IDF \u043d\u0435 \u043c\u0430\u043b\u043e, \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043d\u0430\u0439\u0442\u0438 \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u044e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043d\u0430\u043c \u0434\u0430\u0441\u0442 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0442\u043e\u0447\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c</li> </ul>"},{"location":"portfolio/course_nlp/3_product-reviews.html#tf-idf","title":"\u0412\u0430\u0440\u0438\u0430\u043d\u0442\u044b TF-IDF\u00b6","text":"<p>\u0412\u0441\u0435 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u043a\u043e\u0440\u043f\u0443\u0441\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f TF-IDF \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043b\u043e\u0432 \u043f\u0435\u0440\u0435\u0447\u0438\u0441\u043b\u0435\u043d\u044b \u043d\u0438\u0436\u0435</p>"},{"location":"portfolio/course_nlp/3_product-reviews.html#part-i-tf-idf","title":"PART I - TF-IDF \u041c\u043e\u0434\u0435\u043b\u0438\u00b6","text":""},{"location":"portfolio/course_nlp/3_product-reviews.html","title":"\u0412\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u041c\u043e\u0434\u0435\u043b\u0435\u0439\u00b6","text":"<ul> <li>\u0412 \u043f\u043b\u0430\u043d\u0435 \u043c\u043e\u0434\u0435\u043b\u0435\u0439, \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0434\u0432\u0430 \u043f\u043e\u0434\u0445\u043e\u0434\u0430 \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u044c\u044f:<ul> <li>\u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441 (\u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u0431\u044d\u0433\u0433\u0438\u043d\u0433 \u0438 \u043c\u0435\u0442\u043e\u0434 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0430)</li> <li>\u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433 \u0421atBoost</li> </ul> </li> </ul> <p>\u041d\u0430\u0447\u043d\u0435\u043c \u0441 \u0431\u0430\u0437\u043e\u0432\u043e\u0433\u043e RandomForest (\u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u0433\u043b\u0443\u0431\u043e\u043a\u043e\u0433\u043e) <code>model_srf</code>, \u0430 \u043f\u043e\u0442\u043e\u043c \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e <code>model_drf</code> \u0438 <code>model_ocb</code></p>"},{"location":"portfolio/course_nlp/3_product-reviews.html","title":"\u041c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0447\u0430\u0441\u0442\u043e\u0442\u0430 \u0441\u043b\u043e\u0432\u00b6","text":"<p>\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043a\u0430\u043a \u0432\u043b\u0438\u044f\u0435\u0442 \u0444\u0438\u043b\u044c\u0442\u0440 \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0447\u0430\u0441\u0442\u043e\u0442\u044b \u0441\u043b\u043e\u0432 (<code>min_df</code>) \u0432 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0435 \u043d\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442</p> <ul> <li><code>v1</code> (<code>min_df=3</code> \u0444\u0438\u043b\u044c\u0442\u0440\u0443\u0435\u043c \u0441\u043b\u043e\u0432\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0432 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u043c\u0435\u043d\u044c\u0448\u0435 3 \u0440\u0430\u0437)</li> <li><code>v2</code> (<code>min_df=2</code> \u0444\u0438\u043b\u044c\u0442\u0440\u0443\u0435\u043c \u0441\u043b\u043e\u0432\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0432 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u043c\u0435\u043d\u044c\u0448\u0435 2 \u0440\u0430\u0437)</li> <li><code>v3</code> (<code>min_df=1</code>)</li> </ul> <p>\u041a\u0430\u043a \u043c\u044b \u0432\u0438\u0434\u0438\u043c, \u044d\u0444\u0444\u0435\u043a\u0442 \u043e\u0442 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438 \u0447\u0430\u0441\u0442\u043e \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0449\u0438\u0445\u0441\u044f \u0441\u043b\u043e\u0432 \u0440\u0430\u0437\u043d\u044b\u0439</p>"},{"location":"portfolio/course_nlp/3_product-reviews.html","title":"\u0421\u0442\u043e\u043f \u0421\u043b\u043e\u0432\u0430\u00b6","text":"<ul> <li>\u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043f\u043e\u0432\u044b\u0441\u0438\u0442\u044c \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438 \u043d\u0435 \u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u044b\u0445 \u0441\u043b\u043e\u0432 (\u0441\u0442\u043e\u043f \u0441\u043b\u043e\u0432\u0430)</li> <li>\u041f\u0440\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0438 \u0440\u0430\u0437\u043d\u044b\u0445 \u043d\u0430\u0441\u0442\u0440\u043e\u0435\u043a <code>min_df</code> \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438 \u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f \u043f\u043e \u0440\u0430\u0437\u043d\u043e\u043c\u0443; <code>min_df=2</code> \u0434\u0430\u0435\u0442 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0445\u043e\u0440\u043e\u0448\u0438\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442</li> </ul> <p>\u0412\u0430\u0440\u0438\u0430\u0442\u043d\u044b:</p> <ul> <li><code>v4</code> (\u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u044f \u0441\u0442\u043e\u043f \u0441\u043b\u043e\u0432 \u0441 <code>min_df=3</code>)</li> <li><code>v5</code> (\u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u044f \u0441\u0442\u043e\u043f \u0441\u043b\u043e\u0432 \u0441 <code>min_df=2</code>)</li> <li><code>v6</code> (\u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u044f \u0441\u0442\u043e\u043f \u0441\u043b\u043e\u0432 \u0441 <code>min_df=1</code>)</li> </ul>"},{"location":"portfolio/course_nlp/3_product-reviews.html#-","title":"\u043d-\u0433\u0440\u0430\u043c\u043c\u044b\u00b6","text":"<ul> <li>\u0412\u0430\u0440\u0438\u0430\u043d\u0442 <code>v5</code> \u0434\u0430\u0435\u0442 \u043d\u0430\u043c \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c 0.76 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u044d\u0442\u043e\u0442 \u0432\u0430\u0440\u0438\u0430\u043d\u0442 \u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u0440\u0430\u0437\u043d\u044b\u0435 \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0438 \u043d-\u0433\u0440\u0430\u043c\u043c</li> <li>\u041a\u0430\u043a \u043c\u044b \u0432\u0438\u0434\u0438\u043c, \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u0438 \u043d\u0438\u043a\u0430\u043a\u0438\u0445 \u043d\u0435\u0442 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435, \u043f\u0440\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0438 bigram,trigram, \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u0441\u0438\u043b\u044c\u043d\u043e \u0441\u043d\u0438\u0437\u0438\u043b\u043e\u0441\u044c,  \u0431\u0430\u0437\u043e\u0432\u044b\u0435 \u0442\u043e\u043a\u0435\u043d\u044b (unigram) \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043a\u0440\u0438\u0442\u0438\u0447\u043d\u044b\u043c \u0434\u043b\u044f \u0432\u044b\u0441\u043e\u043a\u043e\u0439 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u0438</li> </ul> <p>\u0412\u0430\u0440\u0438\u0430\u043d\u0442\u044b:</p> <ul> <li><code>v7</code> ngram_range=(1,1) (\u0422\u043e\u043b\u044c\u043a\u043e \u0443\u043d\u0438\u0433\u0440\u0430\u043c\u043c\u044b)</li> <li><code>v8</code> ngram_range=(1,2) (\u0423\u043d\u0438\u0433\u0440\u0430\u043c\u043c\u044b \u0438 \u0431\u0438\u0433\u0440\u0430\u043c\u043c\u044b)</li> <li><code>v9</code> ngram_range=(1,3) (\u0443\u043d\u0438\u0433\u0440\u0430\u043c\u043c\u044b, \u0431\u0438\u0433\u0440\u0430\u043c\u043c\u044b \u0438 \u0442\u0440\u0438\u0433\u0440\u0430\u043c\u043c\u044b)</li> </ul>"},{"location":"portfolio/course_nlp/3_product-reviews.html","title":"\u041c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0447\u0430\u0441\u0442\u043e\u0442\u0430\u00b6","text":"<p>\u0412 \u0437\u0430\u0434\u0430\u043d\u0438\u0438 \u0442\u0430\u043a \u0436\u0435 \u0443\u043f\u043e\u043c\u0438\u043d\u0430\u0435\u0442\u0441\u044f \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0447\u0430\u0441\u0442\u043e\u0442\u0430 \u0441\u043b\u043e\u0432 <code>max_df</code> \u0432 \u0432\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0442\u043e\u0440\u0435, \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b:</p> <ul> <li><code>v13</code> <code>max_df=0.9</code> (\u0444\u0438\u043b\u044c\u0442\u0440\u0443\u0435\u043c \u0441\u043b\u043e\u0432\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0432 90% \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432)</li> <li><code>v14</code> <code>max_df=0.8</code> (\u0444\u0438\u043b\u044c\u0442\u0440\u0443\u0435\u043c \u0441\u043b\u043e\u0432\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0432 80% \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432)</li> <li><code>v15</code> <code>max_df=0.7</code> (\u0444\u0438\u043b\u044c\u0442\u0440\u0443\u0435\u043c \u0441\u043b\u043e\u0432\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0432 70% \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432)</li> </ul> <p>\u041a\u0430\u043a \u043c\u044b \u0432\u0438\u0434\u0438\u043c, \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u0438 \u043d\u0438\u043a\u0430\u043a\u0438\u0445 \u043d\u0435\u0442, \u043f\u0440\u0438 \u0443\u043c\u0435\u043d\u044c\u0448\u0435\u043d\u0438\u0438 \u0447\u0430\u0441\u0442\u043e \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0448\u0438\u0445\u0441\u044f \u0441\u043b\u043e\u0432, \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043f\u0430\u0434\u0430\u0435\u0442</p>"},{"location":"portfolio/course_nlp/3_product-reviews.html","title":"\u0412\u044b\u0431\u043e\u0440 \u043c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<ul> <li>\u0411\u0430\u0437\u043e\u0432\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u043b\u0435\u0441\u0430 \u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u0433\u043b\u0443\u0431\u043e\u043a\u0430\u044f, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u043c\u043e\u0436\u0435\u0442 \u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u0445\u043e\u0440\u043e\u0448\u043e \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c \u0437\u0430\u043a\u043e\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445, \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0434\u0440\u0443\u0433\u0438\u0435 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u043f\u0435\u0440\u0435\u0447\u0438\u0441\u043b\u0435\u043d\u044b \u0432 \u043d\u0430\u0447\u0430\u043b\u0435 \u0440\u0430\u0437\u0434\u0435\u043b\u0430</li> <li>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u043c \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u043e\u043a\u0430\u0437\u0430\u043b \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0443\u044e \u043e\u0431\u043e\u0431\u0449\u0430\u044e\u0449\u0438\u044e \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 (<code>v5</code>)</li> <li>CatBoost \u0445\u043e\u0440\u043e\u0448\u043e \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u043f\u043e\u0434 \u043b\u044e\u0431\u044b\u0435 \u0442\u0430\u0431\u043b\u0438\u0447\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435, \u0438 \u043a\u0430\u043a \u043c\u044b \u0432\u0438\u0434\u0438\u043c \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f \u043d\u0435\u0442, \u043e\u0431\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0442 \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u0438\u0434\u0435\u043d\u0442\u0438\u0447\u043d\u044b\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442</li> <li>\u0421\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u0447\u0442\u043e \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u0438 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0441 TF-IDF (\u043f\u043e\u043c\u0438\u043c\u043e \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u0438) \u043e\u0441\u043e\u0431\u043e \u043d\u0435\u0442, \u0438 \u043d\u0443\u0436\u043d\u043e \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043c\u044b \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0432 Part II</li> </ul>"},{"location":"portfolio/course_nlp/3_product-reviews.html#part-i-tf-idf-gridsearchcv","title":"PART I - TF-IDF GridSearchCV\u00b6","text":"<p>\u0418\u0437 \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u0438 \u0437\u0430\u0434\u0430\u043d\u0438\u044f:</p> <p>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 random forest \u0438\u043b\u0438 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433 (LightGBM \u0438\u043b\u0438 catboost) \u043d\u0430 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0432\u0435\u043a\u0442\u043e\u0440\u0430\u0445 \u0438 \u043f\u043e\u0434\u0431\u0435\u0440\u0438\u0442\u0435 \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u0443\u044e \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u044e \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e GridSearch</p> <ul> <li>\u0412 \u0440\u0443\u0447\u043d\u0443\u044e \u043f\u043e\u0434\u0431\u043e\u0440\u043a\u0430 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043c\u043e\u0436\u0435\u0442 \u043e\u043a\u0430\u0437\u0430\u0442\u044c\u0441\u044f \u043d\u0435 \u0441\u0430\u043c\u044b\u043c \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u043c \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f <code>GridSearchCV</code> \u043c\u0435\u0442\u043e\u0434\u043e\u043c \u043a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u043f\u043e\u0434\u0431\u043e\u0440\u043a\u0438 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0438</li> <li>\u041a\u0430\u043a\u0443\u044e \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u044b\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043d\u0443\u0436\u043d\u043e \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043d\u0435 \u0443\u0442\u043e\u0447\u043d\u044f\u0435\u0442\u0441\u044f, \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043d\u0430\u0439\u0442\u0438 \u0431\u043e\u043b\u0435\u0435 \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u043b\u0435\u0441\u0430, \u0442\u0430\u043a \u043a\u0430\u043a CatBoost \u043f\u043e\u043a\u0430\u0437\u0430\u043b \u0445\u0443\u0436\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442</li> </ul>"},{"location":"portfolio/course_nlp/3_product-reviews.html#part-ii-","title":"PART II - \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438\u00b6","text":"<p>\u0418\u0437 \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u0438 \u0437\u0430\u0434\u0430\u043d\u0438\u044f:</p> <p>\u0422\u0435\u043f\u0435\u0440\u044c \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435\u0441\u044c \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u043c\u0438 word2vec/fasttext \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0430\u043c\u0438 \u0434\u043b\u044f \u0432\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0442\u0430. \u0412\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0443\u0439\u0442\u0435 \u0442\u0435\u043a\u0441\u0442\u044b \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u043e\u0434\u0430 word2vec/fasttext c \u0432\u0435\u0441\u0430\u043c\u0438 tf-idf</p> <ul> <li>\u0412 Part II \u0443 \u043d\u0430\u0441 \u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f \u043f\u043e\u0434\u0445\u043e\u0434 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439 \u0444\u043e\u0440\u043c\u0430\u0442, \u043e\u0442 \u043d\u0430\u0441 \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0440\u0430\u043d\u0435\u0435 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 (\u043d\u0430 \u0434\u0440\u0443\u0433\u043e\u043c \u043a\u043e\u0440\u043f\u0443\u0441\u0435) \u043e\u0442 rusvectores.org \u0438 \u0441\u0440\u0430\u0432\u043d\u0438\u0442\u044c \u0441 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u043c \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u043c TF-IDF.</li> <li>\u041a\u0430\u043a \u0438 \u0440\u0430\u043d\u044c\u0448\u0435 \u0431\u0443\u0434\u0435\u043c \u043e\u0440\u0438\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u043d\u0430 \u043e\u0431\u043e\u0431\u0449\u0430\u044e\u0449\u0443\u044e \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435</li> </ul> <p>\u041d\u0430\u0447\u043d\u0435\u043c \u0441 \u0432\u044b\u0433\u0440\u0443\u0437\u043a\u0438 TF-IDF \u0432\u0435\u0441\u043e\u0432 \u0438\u0437 \u0432\u0435\u0441\u0442\u043e\u0440\u0438\u0437\u0430\u0442\u043e\u0440\u0430 <code>v5_vectoriser</code></p>"},{"location":"portfolio/course_nlp/3_product-reviews.html","title":"\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432\u00b6","text":"<p>\u0414\u043b\u044f \u043d\u0430\u0448\u0435\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 \u0432\u043e\u0437\u044c\u043c\u0435\u043c \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 (\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u0431\u0435\u0437 \u0442\u0430\u0433\u0441\u0435\u0442)</p> <ul> <li>geowac_lemmas_none_fasttextskipgram_300_5_2020</li> </ul>"},{"location":"portfolio/course_nlp/3_product-reviews.html","title":"\u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432\u00b6","text":"<ul> <li>\u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0432\u0441\u0435 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u044b \u0432 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435/\u043a\u043e\u043f\u0440\u0443\u0441\u0435, \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0439 \u043c\u0435\u0442\u043e\u0434 \u0438\u0437 nltk <code>word_tokenize</code></li> <li>\u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u044b \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u0432 <code>lst_corpus_tr</code> \u0438 <code>lst_corpus_te</code> \u0434\u043b\u044f <code>train</code> \u0438 <code>test</code> \u0432\u044b\u0431\u043e\u0440\u043a\u0438</li> </ul>"},{"location":"portfolio/course_nlp/3_product-reviews.html#tf-idf","title":"\u0423\u0441\u0440\u0435\u0434\u043d\u0435\u043d\u043d\u044b\u0435 \u0412\u0435\u043a\u0442\u043e\u0440\u0430 (\u0431\u0435\u0437 TF-IDF \u0432\u0435\u0441\u043e\u0432)\u00b6","text":"<ul> <li>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0441\u0430\u043c\u044b\u043c \u043f\u0440\u043e\u0441\u0442\u044b\u043c \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u043c; \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430 \u043c\u044b \u0432\u043e\u0437\u044c\u043c\u0435\u043c \u0443\u0441\u0440\u0435\u0434\u043d\u0435\u043d\u043d\u044b\u0439 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432\u044b\u0439 \u0432\u0435\u043a\u0442\u043e\u0440 \u0432\u0441\u0435\u0445 \u0441\u043b\u043e\u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432 \u043d\u0435\u0433\u043e \u0432\u0445\u043e\u0434\u044f\u0442</li> <li>\u0420\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0430\u043c\u0438 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u043d\u0430\u043c\u043d\u043e\u0433\u043e \u043d\u0438\u0436\u0435 (300) \u0447\u0435\u043c \u0432 TF-IDF (11969) \u0447\u0442\u043e \u0434\u043e\u043b\u0436\u043d\u043e \u0441\u043a\u0430\u0437\u0430\u0442\u044c\u0441\u044f \u043d\u0430 \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438</li> <li>\u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430 \u0432 <code>X_tr</code> \u0438 <code>X_te</code> \u0434\u043b\u044f <code>train</code> \u0438 <code>test</code> \u0432\u044b\u0431\u043e\u0440\u043a\u0438</li> </ul>"},{"location":"portfolio/course_nlp/3_product-reviews.html#c-tf-idf","title":"\u0423\u0441\u0440\u0435\u0434\u043d\u0435\u043d\u043d\u044b\u0435 \u0412\u0435\u043a\u0442\u043e\u0440\u0430 (c TF-IDF \u0432\u0435\u0441\u0430\u043c\u0438)\u00b6","text":"<ul> <li>\u041c\u044b \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043b\u0438 \u0432\u0435\u0441\u0430 \u0437\u043d\u0430\u0447\u0438\u043c\u043e\u0441\u0442\u0438 \u0441\u043b\u043e\u0432 (\u0442\u043e\u043a\u0435\u043d\u043e\u0432) \u0432 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0445 \u043a\u043e\u0440\u043f\u0443\u0441\u0430 \u0432 <code>tfidf_weights</code></li> <li>\u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u0432\u0437\u0432\u0435\u0448\u0435\u043d\u043d\u044b\u0435 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430 \u0432 <code>Xw_tr</code> \u0438 <code>Xw_te</code> \u0434\u043b\u044f <code>train</code> \u0438 <code>test</code> \u0432\u044b\u0431\u043e\u0440\u043a\u0438</li> </ul>"},{"location":"portfolio/course_nlp/3_product-reviews.html#part-ii-","title":"PART II - \u042d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433 \u043c\u043e\u0434\u0435\u043b\u0438\u00b6","text":""},{"location":"portfolio/course_nlp/3_product-reviews.html","title":"\u0412\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u041c\u043e\u0434\u0435\u043b\u0435\u0439\u00b6","text":"<p>\u041f\u043e\u0432\u0442\u043e\u0440\u0438\u043c \u0442\u043e \u0447\u0442\u043e \u043c\u044b \u0434\u0435\u043b\u0430\u043b\u0438 \u0432 Part I:</p> <ul> <li>\u0412 \u043f\u043b\u0430\u043d\u0435 \u043c\u043e\u0434\u0435\u043b\u0435\u0439, \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0434\u0432\u0430 \u043f\u043e\u0434\u0445\u043e\u0434\u0430 \u0430\u043d\u0441\u0430\u0431\u043b\u044c\u044f, \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441 (\u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u0431\u044d\u0433\u0433\u0438\u043d\u0442 \u0438 \u043c\u0435\u0442\u043e\u0434 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0430) \u0438 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433 \u0421atBoost</li> <li>\u041d\u0430\u0447\u043d\u0435\u043c \u0441 \u0431\u0430\u0437\u043e\u0432\u043e\u0433\u043e RandomForest (\u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u0433\u043b\u0443\u0431\u043e\u043a\u043e\u0433\u043e) <code>model_srf</code>, \u0430 \u043f\u043e\u0442\u043e\u043c \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0441 \u043f\u043e\u043c\u043e\u0448\u044c\u044e <code>model_drf</code> \u0438 <code>model_ocb</code></li> </ul>"},{"location":"portfolio/course_nlp/3_product-reviews.html#tf-idf","title":"\u0412\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u041c\u043e\u0434\u0435\u043b\u0435\u0439 (\u0431\u0435\u0437 TF-IDF \u0432\u0435\u0441\u043e\u0432)\u00b6","text":"<ul> <li>\u041c\u043e\u0434\u0435\u043b\u0438 \u0431\u0435\u0437 TF-IDF \u0432\u0435\u0441\u0430\u043c\u0438 \u0434\u043b\u044f \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432, \u043a\u0430\u043a \u0438 \u0440\u0430\u043d\u044c\u0448\u0435 \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0431\u0430\u0437\u043e\u0432\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438, \u043f\u043e\u0441\u043b\u0435 \u0447\u0435\u0433\u043e \u043c\u044b \u043f\u043e\u043f\u044b\u0442\u0430\u0435\u043c\u0441\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b</li> <li>\u0418\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u043e \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c \u0447\u0442\u043e \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438 \u043e\u0447\u0435\u043d\u044c \u0440\u0435\u0434\u043a\u043e \u043f\u0440\u0435\u0432\u044b\u0448\u0430\u0435\u0442 0.804</li> </ul>"},{"location":"portfolio/course_nlp/3_product-reviews.html#c-tf-idf","title":"\u0412\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u041c\u043e\u0434\u0435\u043b\u0435\u0439 (c TF-IDF \u0432\u0435\u0441\u0430\u043c\u0438)\u00b6","text":"<ul> <li>\u041c\u043e\u0434\u0435\u043b\u0438 \u0441 TF-IDF \u0432\u0435\u0441\u0430\u043c\u0438 \u0434\u043b\u044f \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432</li> <li>\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u0441 \u0432\u0435\u0441\u0430\u043c\u0438 \u043d\u0435 \u0443\u043b\u0443\u0447\u0448\u0438\u043b\u0430\u0441\u044c</li> </ul>"},{"location":"portfolio/course_nlp/3_product-reviews.html#part-ii-gridsearchcv","title":"PART II - \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433 GridSearchCV\u00b6","text":""},{"location":"portfolio/course_nlp/3_product-reviews.html#gridsearchcv-tf-idf","title":"GridSearchCV (\u0431\u0435\u0437 TF-IDF \u0432\u0435\u0441\u043e\u0432)\u00b6","text":"<ul> <li>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0442\u043e\u0439 \u0436\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0447\u0442\u043e \u0438 \u0440\u0430\u043d\u044c\u0448\u0435 \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438</li> <li>\u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b \u043b\u0435\u0441, \u0438 \u043d\u0430\u0439\u0434\u0435\u043c \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u0435 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e <code>GridSearchCV</code></li> </ul>"},{"location":"portfolio/course_nlp/3_product-reviews.html#gridsearchcv-c-tf-idf","title":"GridSearchCV (c TF-IDF \u0432\u0435\u0441\u0430\u043c\u0438)\u00b6","text":"<ul> <li>\u041f\u043e\u0432\u0442\u043e\u0440\u044f\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044e \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0434\u043b\u044f \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432 \u0441 \u0432\u0435\u0441\u0430\u043c\u0438 TF-IDF</li> <li>\u0423\u043c\u043d\u043e\u0436\u0438\u0432 \u0432\u0435\u0441\u0430 \u0438\u0437 TF-IDF \u043d\u0430 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 \u043d\u0435 \u0443\u043b\u0443\u0447\u0448\u0438\u043b\u043e \u043c\u043e\u0434\u0435\u043b\u044c</li> </ul>"},{"location":"portfolio/course_nlp/3_product-reviews.html","title":"\u0410\u043d\u0430\u043b\u0438\u0437 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u00b6","text":""},{"location":"portfolio/course_nlp/3_product-reviews.html","title":"\u0410\u043d\u0430\u043b\u0438\u0437 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\u00b6","text":"<ul> <li>\u041f\u043e \u0438\u0442\u043e\u0433\u0430\u043c \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0438 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0432\u043e\u0437\u043d\u0438\u043a\u0430\u0435\u0442 \u043d\u0435\u043a\u0438\u0439 \u0432\u043e\u043f\u0440\u043e\u0441, \u043f\u043e\u0447\u0435\u043c\u0443 \u043d\u0430\u0448\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0438\u0432\u0430\u044e\u0442\u0441\u044f \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c\u044e \u0432 \u0440\u0430\u0439\u043e\u043d\u0435 80%</li> <li>\u041f\u0440\u0438\u0447\u0438\u043d \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e, \u0434\u0430\u0432\u0430\u0439\u0442\u0435 \u0441\u043e\u0441\u0440\u0435\u0434\u043e\u0442\u043e\u0447\u0438\u043c\u0441\u044f \u043d\u0430 \u0441\u0430\u043c\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043f\u043e\u043f\u044b\u0442\u0430\u0435\u043c\u0441\u044f \u043f\u043e\u043d\u044f\u0442\u044c \u043f\u043e\u0447\u0435\u043c\u0443 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438 \u0441\u0442\u0430\u0433\u043d\u0438\u0440\u0443\u0435\u0442</li> <li>\u0412\u043f\u043e\u043b\u043d\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e \u0447\u0442\u043e \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0430 \u0432 \u0441\u0430\u043c\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0432 \u043f\u043e\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0435 \u0437\u0430\u0434\u0430\u0447\u0438 \u0431\u0438\u043d\u0430\u0440\u043d\u043e\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438</li> </ul>"},{"location":"portfolio/course_nlp/gru_NER.html","title":"gru NER","text":"In\u00a0[69]: Copied! <pre>pip install ipymarkup pymorphy2 livelossplot\n</pre> pip install ipymarkup pymorphy2 livelossplot <pre>Requirement already satisfied: ipymarkup in /opt/conda/lib/python3.10/site-packages (0.9.0)\nRequirement already satisfied: pymorphy2 in /opt/conda/lib/python3.10/site-packages (0.9.1)\nRequirement already satisfied: livelossplot in /opt/conda/lib/python3.10/site-packages (0.5.5)\nRequirement already satisfied: intervaltree&gt;=3 in /opt/conda/lib/python3.10/site-packages (from ipymarkup) (3.1.0)\nRequirement already satisfied: dawg-python&gt;=0.7.1 in /opt/conda/lib/python3.10/site-packages (from pymorphy2) (0.7.2)\nRequirement already satisfied: pymorphy2-dicts-ru&lt;3.0,&gt;=2.4 in /opt/conda/lib/python3.10/site-packages (from pymorphy2) (2.4.417127.4579844)\nRequirement already satisfied: docopt&gt;=0.6 in /opt/conda/lib/python3.10/site-packages (from pymorphy2) (0.6.2)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from livelossplot) (3.7.1)\nRequirement already satisfied: bokeh in /opt/conda/lib/python3.10/site-packages (from livelossplot) (3.1.1)\nRequirement already satisfied: sortedcontainers&lt;3.0,&gt;=2.0 in /opt/conda/lib/python3.10/site-packages (from intervaltree&gt;=3-&gt;ipymarkup) (2.4.0)\nRequirement already satisfied: Jinja2&gt;=2.9 in /opt/conda/lib/python3.10/site-packages (from bokeh-&gt;livelossplot) (3.1.2)\nRequirement already satisfied: contourpy&gt;=1 in /opt/conda/lib/python3.10/site-packages (from bokeh-&gt;livelossplot) (1.1.0)\nRequirement already satisfied: numpy&gt;=1.16 in /opt/conda/lib/python3.10/site-packages (from bokeh-&gt;livelossplot) (1.23.5)\nRequirement already satisfied: packaging&gt;=16.8 in /opt/conda/lib/python3.10/site-packages (from bokeh-&gt;livelossplot) (21.3)\nRequirement already satisfied: pandas&gt;=1.2 in /opt/conda/lib/python3.10/site-packages (from bokeh-&gt;livelossplot) (1.5.3)\nRequirement already satisfied: pillow&gt;=7.1.0 in /opt/conda/lib/python3.10/site-packages (from bokeh-&gt;livelossplot) (9.5.0)\nRequirement already satisfied: PyYAML&gt;=3.10 in /opt/conda/lib/python3.10/site-packages (from bokeh-&gt;livelossplot) (6.0)\nRequirement already satisfied: tornado&gt;=5.1 in /opt/conda/lib/python3.10/site-packages (from bokeh-&gt;livelossplot) (6.3.2)\nRequirement already satisfied: xyzservices&gt;=2021.09.1 in /opt/conda/lib/python3.10/site-packages (from bokeh-&gt;livelossplot) (2023.5.0)\nRequirement already satisfied: cycler&gt;=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib-&gt;livelossplot) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib-&gt;livelossplot) (4.40.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib-&gt;livelossplot) (1.4.4)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib-&gt;livelossplot) (3.0.9)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib-&gt;livelossplot) (2.8.2)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2&gt;=2.9-&gt;bokeh-&gt;livelossplot) (2.1.3)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas&gt;=1.2-&gt;bokeh-&gt;livelossplot) (2023.3)\nRequirement already satisfied: six&gt;=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;livelossplot) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[70]: Copied! <pre>import os\nimport sys\nfrom tqdm import tqdm\nimport typing\nfrom functools import lru_cache\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom livelossplot import PlotLosses\n\nimport random\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ntorch.manual_seed(1)\n\nimport pymorphy2\nmorph = pymorphy2.MorphAnalyzer()\n</pre> import os import sys from tqdm import tqdm import typing from functools import lru_cache import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from livelossplot import PlotLosses  import random import re import numpy as np import matplotlib.pyplot as plt %matplotlib inline  torch.manual_seed(1)  import pymorphy2 morph = pymorphy2.MorphAnalyzer() In\u00a0[71]: Copied! <pre>import lxml\nfrom lxml import etree\n\n# get spans in xml for &lt;category&gt;\ndef get_aspect_spans(root):\n    return get_tag_spans(root,'category')\n\n# get the start &amp; ending in &lt;text&gt;\ndef get_tag_spans(root,tag):\n    attributes = root.attrib\n    start = int(attributes['from'])\n    end   = int(attributes['to'])\n    tag   = attributes[tag]\n    span  = (start,end,tag)\n    return span\n\ndef parse_xml_aspect(xml_file):\n    return parseXML(xml_file,\n                    get_aspect_spans)\n\n# helper; parse XML &amp; get marked spans in &lt;text&gt;\n# span_func : parse_xml_aspect\n\ndef parseXML(xmlFile, span_func):\n\n    xml  = open(xmlFile).read()\n    root = etree.fromstring(xml)\n\n    result = []\n    for review in root.getchildren():\n        for elem in review.getchildren():\n            if elem.tag == 'text':\n                text  = elem.text\n            if elem.tag == 'aspects':\n                spans = [span_func(xml_span) for xml_span in elem.getchildren()]\n                spans = span_sanity_filter(spans)\n        result.append((text, spans))\n    return result\n\ndef span_sanity_filter(spans):\n    result = [spans[0]]\n\n    for span in spans[1:]:\n        _,prev_span_end,_   = result[-1]\n        curr_span_start,_,_ = span\n\n        if prev_span_end &lt; curr_span_start:\n            result.append(span)\n    return result\n</pre> import lxml from lxml import etree  # get spans in xml for  def get_aspect_spans(root):     return get_tag_spans(root,'category')  # get the start &amp; ending in  def get_tag_spans(root,tag):     attributes = root.attrib     start = int(attributes['from'])     end   = int(attributes['to'])     tag   = attributes[tag]     span  = (start,end,tag)     return span  def parse_xml_aspect(xml_file):     return parseXML(xml_file,                     get_aspect_spans)  # helper; parse XML &amp; get marked spans in  # span_func : parse_xml_aspect  def parseXML(xmlFile, span_func):      xml  = open(xmlFile).read()     root = etree.fromstring(xml)      result = []     for review in root.getchildren():         for elem in review.getchildren():             if elem.tag == 'text':                 text  = elem.text             if elem.tag == 'aspects':                 spans = [span_func(xml_span) for xml_span in elem.getchildren()]                 spans = span_sanity_filter(spans)         result.append((text, spans))     return result  def span_sanity_filter(spans):     result = [spans[0]]      for span in spans[1:]:         _,prev_span_end,_   = result[-1]         curr_span_start,_,_ = span          if prev_span_end &lt; curr_span_start:             result.append(span)     return result In\u00a0[72]: Copied! <pre># show tags from natasha library\nfrom ipymarkup         import show_box_markup\nfrom ipymarkup.palette import palette, PALETTE, BLUE, RED, GREEN, PURPLE, BROWN, ORANGE\n\n# aspect\nPALETTE.set('Comfort',ORANGE)\nPALETTE.set('B-Comfort',ORANGE)\nPALETTE.set('I-Comfort',ORANGE)\nPALETTE.set('Appearance',BLUE)\nPALETTE.set('B-Appearance',BLUE)\nPALETTE.set('I-Appearance',BLUE)\nPALETTE.set('Reliability',GREEN)\nPALETTE.set('B-Reliability',GREEN)\nPALETTE.set('I-Reliability',GREEN)\nPALETTE.set('Safety',PURPLE)\nPALETTE.set('B-Safety',PURPLE)\nPALETTE.set('I-Safety',PURPLE)\nPALETTE.set('Driveability',BLUE)\nPALETTE.set('B-Driveability',BLUE)\nPALETTE.set('I-Driveability',BLUE)\nPALETTE.set('Whole',RED)\nPALETTE.set('B-Whole',RED)\nPALETTE.set('I-Whole',RED)\nPALETTE.set('Costs',RED)\nPALETTE.set('B-Costs',RED)\nPALETTE.set('I-Costs',RED)\n\ndef show_markup(text,spans):\n    show_box_markup(text, spans, palette=PALETTE)\n</pre> # show tags from natasha library from ipymarkup         import show_box_markup from ipymarkup.palette import palette, PALETTE, BLUE, RED, GREEN, PURPLE, BROWN, ORANGE  # aspect PALETTE.set('Comfort',ORANGE) PALETTE.set('B-Comfort',ORANGE) PALETTE.set('I-Comfort',ORANGE) PALETTE.set('Appearance',BLUE) PALETTE.set('B-Appearance',BLUE) PALETTE.set('I-Appearance',BLUE) PALETTE.set('Reliability',GREEN) PALETTE.set('B-Reliability',GREEN) PALETTE.set('I-Reliability',GREEN) PALETTE.set('Safety',PURPLE) PALETTE.set('B-Safety',PURPLE) PALETTE.set('I-Safety',PURPLE) PALETTE.set('Driveability',BLUE) PALETTE.set('B-Driveability',BLUE) PALETTE.set('I-Driveability',BLUE) PALETTE.set('Whole',RED) PALETTE.set('B-Whole',RED) PALETTE.set('I-Whole',RED) PALETTE.set('Costs',RED) PALETTE.set('B-Costs',RED) PALETTE.set('I-Costs',RED)  def show_markup(text,spans):     show_box_markup(text, spans, palette=PALETTE) In\u00a0[73]: Copied! <pre>xml = 'SentiRuEval_car_markup_train.xml'\ntexts_w_aspect_spans    = parse_xml_aspect('/kaggle/input/araneum/'+xml)\n\namount = len(texts_w_aspect_spans)\nprint('\u0417\u0430\u0433\u0440\u0443\u0436\u0435\u043d\u043e {} \u0442\u0435\u043a\u0441\u0442\u043e\u0432 \u0441 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u043e\u0439\\n'.format(amount))\n</pre> xml = 'SentiRuEval_car_markup_train.xml' texts_w_aspect_spans    = parse_xml_aspect('/kaggle/input/araneum/'+xml)  amount = len(texts_w_aspect_spans) print('\u0417\u0430\u0433\u0440\u0443\u0436\u0435\u043d\u043e {} \u0442\u0435\u043a\u0441\u0442\u043e\u0432 \u0441 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u043e\u0439\\n'.format(amount)) <pre>\u0417\u0430\u0433\u0440\u0443\u0436\u0435\u043d\u043e 217 \u0442\u0435\u043a\u0441\u0442\u043e\u0432 \u0441 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u043e\u0439\n\n</pre> In\u00a0[74]: Copied! <pre>texts_w_aspect_spans[0]\n</pre> texts_w_aspect_spans[0] Out[74]: <pre>('\u041d\u0435\u0434\u0430\u0432\u043d\u043e \u043a\u0443\u043f\u0438\u043b \u044d\u0442\u043e\u0442 \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c. \u0410\u0432\u0442\u043e \u043e\u0442\u043b\u0438\u0447\u043d\u043e\u0435! \u0414\u0432\u0438\u0433\u0430\u0442\u0435\u043b\u044c 2,5 \u043b\u0438\u0442\u0440\u0430, \u0442\u0443\u0440\u0431\u043e\u0434\u0438\u0437\u0435\u043b\u044c. \u041f\u0440\u0435\u0436\u043d\u0438\u0439 \u0445\u043e\u0437\u044f\u0438\u043d \u0441\u043a\u0430\u0437\u0430\u043b \u043f\u0440\u0438 \u043f\u0440\u043e\u0434\u0430\u0436\u0435, \u0447\u0442\u043e \u043c\u0430\u0441\u043b\u043e \u043d\u0435 \u0436\u0440\u0435\u0442, \u0441\u043e\u043b\u044f\u0440\u043a\u0443 \u0442\u043e\u0436\u0435, \u043b\u0435\u0442\u0438\u0442 \u043a\u0430\u043a \u0443\u0433\u043e\u0440\u0435\u043b\u0430\u044f! \u0422\u0430\u043a \u043e\u043d\u043e \u0438 \u0435\u0441\u0442\u044c. 140 \u043a\u043c/\u0447 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u043a\u0440\u0435\u0439\u0441\u0435\u0440\u0441\u043a\u0430\u044f \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c. \u0412\u043e\u043e\u0431\u0449\u0435 \u043d\u0435\u043c\u0446\u044b \u0443\u043c\u0435\u044e\u0442 \u0434\u0435\u043b\u0430\u0442\u044c \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u0438. \u0414\u043e\u0440\u043e\u0433\u0443 \u0434\u0435\u0440\u0436\u0438\u0442 \u043e\u0442\u043b\u0438\u0447\u043d\u043e, \u0442\u0430\u043a \u043a\u0430\u043a \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0448\u0438\u0440\u043e\u043a\u0430\u044f \u043c\u0430\u0448\u0438\u043d\u0430. \u0422\u043e\u0440\u043c\u043e\u0437\u0430 \u0432\u0441\u0435 \u0434\u0438\u0441\u043a\u043e\u0432\u044b\u0435. \u0413\u043b\u0430\u0432\u043d\u043e\u0435 \u043f\u0435\u0440\u0435\u0434\u043d\u0438\u0439 \u043f\u0440\u0438\u0432\u043e\u0434, \u043f\u043e \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044e \u0441 \u0434\u0440\u0443\u0433\u0438\u043c\u0438 \u043d\u0435\u043c\u0435\u0446\u043a\u0438\u043c\u0438 \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044f\u043c\u0438. \u0422\u0430\u043a\u0438\u043c\u0438 \u043a\u0430\u043a \u043c\u0435\u0440\u0441\u0435\u0434\u0435\u0441 \u0438 \u0431\u043c\u0432 \u044d\u0442\u043e\u0433\u043e \u0436\u0435 \u043a\u043b\u0430\u0441\u0441\u0430. \u041c\u0435\u0441\u0442\u0430 \u0432 \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u0435 \u043e\u0447\u0435\u043d\u044c \u043c\u043d\u043e\u0433\u043e. \u041d\u0435 \u0442\u0435\u0441\u043d\u043e \u0434\u0430\u0436\u0435, \u043a\u043e\u0433\u0434\u0430 \u0441\u0438\u0434\u044f\u0442 \u043f\u044f\u0442\u044c \u0432\u0437\u0440\u043e\u0441\u043b\u044b\u0445 \u0447\u0435\u043b\u043e\u0432\u0435\u043a. \u0411\u0430\u0433\u0430\u0436\u043d\u043e\u0435 \u043e\u0442\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0442\u043e\u0436\u0435 \u043e\u0433\u0440\u043e\u043c\u043d\u043e. \u0412\u043b\u0435\u0437\u043b\u0430 \u0441\u0442\u0438\u0440\u0430\u043b\u044c\u043d\u0430\u044f \u043c\u0430\u0448\u0438\u043d\u0430. \u041f\u043e \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044e \u0446\u0435\u043d\u0430 - \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e, \u043e\u0442\u043b\u0438\u0447\u043d\u044b\u0439 \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c. \u0411\u043e\u043b\u044c\u0448\u0435 \u043d\u0438\u043a\u043e\u0433\u0434\u0430 \u043d\u0435 \u0441\u044f\u0434\u0443 \u0437\u0430 \u0440\u0443\u043b\u044c \u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u0430\u0432\u0442\u043e\u043c\u0431\u0438\u043b\u044f! \u0412\u0441\u0435\u043c \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u044e \u0410\u0443\u0434\u0438. ',\n [(19, 29, 'Whole'),\n  (31, 35, 'Whole'),\n  (46, 65, 'Driveability'),\n  (67, 78, 'Driveability'),\n  (125, 132, 'Driveability'),\n  (134, 141, 'Driveability'),\n  (148, 166, 'Driveability'),\n  (204, 224, 'Driveability'),\n  (252, 262, 'Whole'),\n  (264, 277, 'Driveability'),\n  (306, 320, 'Driveability'),\n  (322, 329, 'Driveability'),\n  (334, 342, 'Driveability'),\n  (352, 367, 'Driveability'),\n  (392, 414, 'Driveability'),\n  (427, 435, 'Driveability'),\n  (438, 441, 'Driveability'),\n  (459, 477, 'Comfort'),\n  (491, 499, 'Comfort'),\n  (541, 559, 'Comfort'),\n  (603, 630, 'Costs'),\n  (641, 651, 'Costs'),\n  (684, 705, 'Whole'),\n  (723, 727, 'Whole')])</pre> In\u00a0[75]: Copied! <pre>random_picks = [random.randint(0,amount-1) for _ in range(0,1)]\nrandom_picks\n</pre> random_picks = [random.randint(0,amount-1) for _ in range(0,1)] random_picks Out[75]: <pre>[195]</pre> In\u00a0[76]: Copied! <pre>text,spans = texts_w_aspect_spans[random_picks[0]]\nshow_markup(text,spans)\n</pre> text,spans = texts_w_aspect_spans[random_picks[0]] show_markup(text,spans) \u041e \u043f\u043b\u044e\u0441\u0430\u0445 \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044fWhole. \u0414\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u0435 \u0437\u0430\u043f\u0447\u0430\u0441\u0442\u0438Reliability, \u043c\u043e\u0436\u043d\u043e \u043d\u0430\u0439\u0442\u0438 \u0432 \u043b\u044e\u0431\u043e\u043c \u0433\u043e\u0440\u043e\u0434\u0435, \u0432 \u043b\u044e\u0431\u043e\u043c \u0433\u0430\u0440\u0430\u0436\u0435, \u0433\u0434\u0435 \u0437\u0430\u043d\u0438\u043c\u0430\u044e\u0442\u0441\u044f \u043c\u0430\u0448\u0438\u043d\u0430\u043c\u0438. \u041c\u0430\u0448\u0438\u043d\u0430Driveability \u0445\u043e\u0440\u043e\u0448\u0430\u044f, \u0442\u043e\u043b\u044c\u043a\u043e \u043d\u0430 \u0434\u0430\u043b\u044c\u043d\u044e\u044e \u0434\u043e\u0440\u043e\u0433\u0443, \u0431\u0435\u043d\u0437\u0438\u043d \u043d\u0430 \u0442\u0440\u0430\u0441\u0441\u0435 \u043f\u0440\u043e\u0441\u0442\u043e \u043d\u044e\u0445\u0430\u0435\u0442, \u0432 \u0433\u043e\u0440\u043e\u0434\u0441\u043a\u043e\u043c \u0440\u0435\u0436\u0438\u043c\u0435Driveability \u0440\u0430\u0441\u0445\u043e\u0434 \u0442\u043e\u043f\u043b\u0438\u0432\u0430Driveability \u0440\u0435\u0437\u043a\u043e \u043f\u043e\u0434\u0441\u043a\u0430\u043a\u0438\u0432\u0430\u0435\u0442. \u0425\u043e\u0440\u043e\u0448\u0430\u044f \u043f\u043e\u0434\u0432\u0435\u0441\u043a\u0430Driveability \u0438 \u0432\u044b\u0441\u043e\u043a\u0438\u0439 \u043a\u043b\u0438\u0440\u0435\u043d\u0441Driveability. \u0412 \u043e\u0431\u0441\u043b\u0443\u0436\u0438\u0432\u0430\u043d\u0438\u0438Reliability \u043d\u0435\u043f\u0440\u0438\u0445\u043e\u0442\u043b\u0438\u0432Reliability. \u0421\u043b\u0430\u0431\u044b\u0439 \u0434\u0432\u0438\u0433\u0430\u0442\u0435\u043b\u044cDriveability, \u043d\u0435 \u0442\u044f\u043d\u0435\u0442 \u0431\u043e\u043b\u044c\u0448\u0435 3000 \u043e\u0431\u043e\u0440\u043e\u0442\u043e\u0432, \u043e\u0447\u0435\u043d\u044c \u0441\u043b\u0430\u0431\u0430\u044f \u0434\u0438\u043d\u0430\u043c\u0438\u043a\u0430Driveability. \u041f\u043b\u043e\u0445\u043e \u0440\u0430\u0441\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u044b \u0432 \u043a\u043e\u0440\u043e\u0431\u043a\u0435 \u043f\u0435\u0440\u0435\u0434\u0430\u0447Driveability. \u041e\u0447\u0435\u043d\u044c \u0441\u043b\u0430\u0431\u0430\u044f \u0440\u0443\u043b\u0435\u0432\u0430\u044f \u0440\u0435\u0439\u043a\u0430Reliability, \u0442\u0440\u0435\u0431\u0443\u0435\u0442 \u043f\u043e\u0441\u0442\u043e\u044f\u043d\u043d\u043e\u0433\u043e \u0440\u0435\u043c\u043e\u043d\u0442\u0430Reliability \u043f\u043e\u0441\u043b\u0435 \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0435\u0440\u044c\u0435\u0437\u043d\u043e\u0439 \u044f\u043c\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u0440\u0435\u0434\u043a\u043e \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0442\u0441\u044f \u043d\u0430 \u0434\u043e\u0440\u043e\u0433\u0430\u0445. \u041e\u0447\u0435\u043d\u044c \u043f\u043b\u043e\u0445\u043e\u0439 \u043c\u0435\u0442\u0430\u043b\u043bReliability, \u0440\u0436\u0430\u0432\u0435\u0435\u0442Reliability \u043c\u043e\u043c\u0435\u043d\u0442\u0430\u043b\u044c\u043d\u043e. \u041f\u043b\u043e\u0445\u0430\u044f \u0448\u0443\u043c\u043e\u0438\u0437\u043e\u043b\u044f\u0446\u0438\u044fComfort. \u041f\u043e\u0434\u043e\u0439\u0434\u0435\u0442 \u043a\u0430\u043a \u043f\u0435\u0440\u0432\u0430\u044f \u043c\u0430\u0448\u0438\u043d\u0430Whole, \u0441\u0430\u043c \u0441\u043b\u043e\u043c\u0430\u043b \u0441\u0430\u043c \u043f\u043e\u0447\u0438\u043d\u0438\u043b. \u041e\u0431\u0449\u0435\u0435 \u0432\u043f\u0435\u0447\u0430\u0442\u043b\u0435\u043d\u0438\u0435 : \u0414\u0435\u0448\u0435\u0432\u044b\u0439Costs, \u043d\u0435\u043f\u0440\u0438\u0445\u043e\u0442\u043b\u0438\u0432\u044b\u0439Reliability, \u043d\u0438\u0437\u043a\u0438\u0439 \u0440\u0430\u0441\u0445\u043e\u0434 \u0442\u043e\u043f\u043b\u0438\u0432\u0430Driveability In\u00a0[77]: Copied! <pre>\"\"\"\n\n\u0417\u0430\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043f\u0440\u043e\u0431\u0435\u043b\u044b \u0432 \u0430\u0432\u0442\u043e\u0440\u0441\u043a\u043e\u0439 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0435\n\n\"\"\"\n\ndef fill_gaps(text, source_spans):\n\n    chunks = []\n    text_pos = 0\n    for span in source_spans:\n        s,e,t = span\n        if text_pos&lt;s:\n            chunks.append((text_pos,s,'Other'))\n        chunks.append((s,e,t))\n        text_pos=e\n\n    if text_pos&lt;len(text)-1:\n        chunks.append((text_pos,len(text),'Other'))\n\n    return chunks\n\n\"\"\"\n\n\u0420\u0430\u0437\u043e\u0431\u044c\u0435\u043c \u043d\u0430 bio-\u0442\u043e\u043a\u0435\u043d\u044b \u043f\u043e \u0431\u0435\u0437\u043f\u0440\u043e\u0431\u0435\u043b\u044c\u043d\u043e\u0439 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0435\n\n\"\"\"\n\ndef extract_BIO_tagged_tokens(text, source_spans, tokenizer):\n\n    tokens_w_tags = []\n    for span in source_spans:\n        s,e,tag  = span\n        tokens   = tokenizer(text[s:e])\n\n        if tag == 'Other':\n            tokens_w_tags += [(token,tag) for token in tokens]\n        else:\n            tokens_w_tags.append((tokens[0],'B-'+tag))\n            for token in tokens[1:]:\n                tokens_w_tags.append((token,'I-'+tag))\n\n    return tokens_w_tags\n</pre> \"\"\"  \u0417\u0430\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043f\u0440\u043e\u0431\u0435\u043b\u044b \u0432 \u0430\u0432\u0442\u043e\u0440\u0441\u043a\u043e\u0439 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0435  \"\"\"  def fill_gaps(text, source_spans):      chunks = []     text_pos = 0     for span in source_spans:         s,e,t = span         if text_pos In\u00a0[78]: Copied! <pre>text, aspect_spans = texts_w_aspect_spans[random_picks[0]]\ncover_spans       = fill_gaps(text, aspect_spans)\n\nprint('\\n\u041f\u043e\u043b\u043d\u043e\u0435 \u043f\u043e\u043a\u0440\u044b\u0442\u0438\u0435 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u043e\u0439 \u0442\u0435\u043a\u0441\u0442\u0430 :','\\n')\nshow_markup(text, cover_spans)\n</pre> text, aspect_spans = texts_w_aspect_spans[random_picks[0]] cover_spans       = fill_gaps(text, aspect_spans)  print('\\n\u041f\u043e\u043b\u043d\u043e\u0435 \u043f\u043e\u043a\u0440\u044b\u0442\u0438\u0435 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u043e\u0439 \u0442\u0435\u043a\u0441\u0442\u0430 :','\\n') show_markup(text, cover_spans) <pre>\n\u041f\u043e\u043b\u043d\u043e\u0435 \u043f\u043e\u043a\u0440\u044b\u0442\u0438\u0435 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u043e\u0439 \u0442\u0435\u043a\u0441\u0442\u0430 : \n\n</pre> \u041e \u043f\u043b\u044e\u0441\u0430\u0445 Other\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044fWhole. \u0414\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u0435 Other\u0437\u0430\u043f\u0447\u0430\u0441\u0442\u0438Reliability, \u043c\u043e\u0436\u043d\u043e \u043d\u0430\u0439\u0442\u0438 \u0432 \u043b\u044e\u0431\u043e\u043c \u0433\u043e\u0440\u043e\u0434\u0435, \u0432 \u043b\u044e\u0431\u043e\u043c \u0433\u0430\u0440\u0430\u0436\u0435, \u0433\u0434\u0435 \u0437\u0430\u043d\u0438\u043c\u0430\u044e\u0442\u0441\u044f \u043c\u0430\u0448\u0438\u043d\u0430\u043c\u0438. Other\u041c\u0430\u0448\u0438\u043d\u0430Driveability \u0445\u043e\u0440\u043e\u0448\u0430\u044f, \u0442\u043e\u043b\u044c\u043a\u043e \u043d\u0430 \u0434\u0430\u043b\u044c\u043d\u044e\u044e \u0434\u043e\u0440\u043e\u0433\u0443, \u0431\u0435\u043d\u0437\u0438\u043d \u043d\u0430 \u0442\u0440\u0430\u0441\u0441\u0435 \u043f\u0440\u043e\u0441\u0442\u043e \u043d\u044e\u0445\u0430\u0435\u0442, \u0432 Other\u0433\u043e\u0440\u043e\u0434\u0441\u043a\u043e\u043c \u0440\u0435\u0436\u0438\u043c\u0435Driveability Other\u0440\u0430\u0441\u0445\u043e\u0434 \u0442\u043e\u043f\u043b\u0438\u0432\u0430Driveability \u0440\u0435\u0437\u043a\u043e \u043f\u043e\u0434\u0441\u043a\u0430\u043a\u0438\u0432\u0430\u0435\u0442. \u0425\u043e\u0440\u043e\u0448\u0430\u044f Other\u043f\u043e\u0434\u0432\u0435\u0441\u043a\u0430Driveability \u0438 Other\u0432\u044b\u0441\u043e\u043a\u0438\u0439 \u043a\u043b\u0438\u0440\u0435\u043d\u0441Driveability. Other\u0412 \u043e\u0431\u0441\u043b\u0443\u0436\u0438\u0432\u0430\u043d\u0438\u0438Reliability Other\u043d\u0435\u043f\u0440\u0438\u0445\u043e\u0442\u043b\u0438\u0432Reliability. Other\u0421\u043b\u0430\u0431\u044b\u0439 \u0434\u0432\u0438\u0433\u0430\u0442\u0435\u043b\u044cDriveability, \u043d\u0435 \u0442\u044f\u043d\u0435\u0442 \u0431\u043e\u043b\u044c\u0448\u0435 3000 \u043e\u0431\u043e\u0440\u043e\u0442\u043e\u0432, \u043e\u0447\u0435\u043d\u044c Other\u0441\u043b\u0430\u0431\u0430\u044f \u0434\u0438\u043d\u0430\u043c\u0438\u043a\u0430Driveability. \u041f\u043b\u043e\u0445\u043e \u0440\u0430\u0441\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b Other\u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u044b \u0432 \u043a\u043e\u0440\u043e\u0431\u043a\u0435 \u043f\u0435\u0440\u0435\u0434\u0430\u0447Driveability. \u041e\u0447\u0435\u043d\u044c \u0441\u043b\u0430\u0431\u0430\u044f Other\u0440\u0443\u043b\u0435\u0432\u0430\u044f \u0440\u0435\u0439\u043a\u0430Reliability, Other\u0442\u0440\u0435\u0431\u0443\u0435\u0442 \u043f\u043e\u0441\u0442\u043e\u044f\u043d\u043d\u043e\u0433\u043e \u0440\u0435\u043c\u043e\u043d\u0442\u0430Reliability \u043f\u043e\u0441\u043b\u0435 \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0435\u0440\u044c\u0435\u0437\u043d\u043e\u0439 \u044f\u043c\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u0440\u0435\u0434\u043a\u043e \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0442\u0441\u044f \u043d\u0430 \u0434\u043e\u0440\u043e\u0433\u0430\u0445. \u041e\u0447\u0435\u043d\u044c \u043f\u043b\u043e\u0445\u043e\u0439 Other\u043c\u0435\u0442\u0430\u043b\u043bReliability, Other\u0440\u0436\u0430\u0432\u0435\u0435\u0442Reliability \u043c\u043e\u043c\u0435\u043d\u0442\u0430\u043b\u044c\u043d\u043e. \u041f\u043b\u043e\u0445\u0430\u044f Other\u0448\u0443\u043c\u043e\u0438\u0437\u043e\u043b\u044f\u0446\u0438\u044fComfort. \u041f\u043e\u0434\u043e\u0439\u0434\u0435\u0442 \u043a\u0430\u043a \u043f\u0435\u0440\u0432\u0430\u044f Other\u043c\u0430\u0448\u0438\u043d\u0430Whole, \u0441\u0430\u043c \u0441\u043b\u043e\u043c\u0430\u043b \u0441\u0430\u043c \u043f\u043e\u0447\u0438\u043d\u0438\u043b. \u041e\u0431\u0449\u0435\u0435 \u0432\u043f\u0435\u0447\u0430\u0442\u043b\u0435\u043d\u0438\u0435 : Other\u0414\u0435\u0448\u0435\u0432\u044b\u0439Costs, Other\u043d\u0435\u043f\u0440\u0438\u0445\u043e\u0442\u043b\u0438\u0432\u044b\u0439Reliability, Other\u043d\u0438\u0437\u043a\u0438\u0439 \u0440\u0430\u0441\u0445\u043e\u0434 \u0442\u043e\u043f\u043b\u0438\u0432\u0430Driveability In\u00a0[79]: Copied! <pre># \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043a\u0430 \u043f\u043e \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u043c\u044b \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \ndef regex_sentence_detector(text):\n    sentences = re.split(r'(?&lt;!\\w\\.\\w.)(?&lt;![A-Z][a-z]\\.)(?&lt;=\\.|\\?)\\s', text)\n    return sentences\n\n\"\"\"\n\n\u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u043d\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u0438 \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443 - \u0434\u043b\u044f \u043a\u0440\u0430\u0441\u043e\u0442\u044b\n\n\"\"\"\n\ndef sentence_spans(text, sentence_detector):\n\n    sentences = sentence_detector(text)\n    spans = []\n\n    sent_start= 0\n    idx       = 1\n    for sent in sentences:\n        sent_end = sent_start + len(sent)\n        spans.append((sent_start,sent_end, 's{}'.format(idx)))\n\n        sent_start = 1+sent_end\n        idx       += 1\n\n    return spans\n\n\n\"\"\"\n\n\u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u043d\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u0438 \u043f\u0435\u0440\u0435\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443\n\n\"\"\"\n\ndef sentence_splitter(text, spans):\n    result = []\n    sentences = regex_sentence_detector(text)\n\n    sent_start = 0\n    span_idx   = 0\n\n    for sent in sentences:\n        sent_end = sent_start + len(sent)\n\n        sent_spans = []\n        for span in spans[span_idx:]:\n            s,e,t = span\n            if e &lt;= sent_end:\n                sent_spans.append((s-sent_start,e-sent_start,t))\n                span_idx+=1\n            else:\n                continue\n\n        result.append((text[sent_start:sent_end], sent_spans))\n\n        sent_start = 1+sent_end\n\n    return result\n</pre> # \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043a\u0430 \u043f\u043e \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u043c\u044b \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c  def regex_sentence_detector(text):     sentences = re.split(r'(? In\u00a0[80]: Copied! <pre>from nltk.tokenize import RegexpTokenizer\nword_tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n</pre> from nltk.tokenize import RegexpTokenizer word_tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+') In\u00a0[81]: Copied! <pre>text, aspect_spans = texts_w_aspect_spans[random_picks[0]]\nprint('\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u0438 BIO \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u0430') \nfor sentence, spans in sentence_splitter(text, aspect_spans):\n\n    cover_spans      = fill_gaps(sentence,spans)\n    tokens_w_biotags = extract_BIO_tagged_tokens(sentence, \n                                                     cover_spans, \n                                                     word_tokenizer.tokenize)\n\n    show_markup(sentence, cover_spans)\n    print(tokens_w_biotags[:10],'\\n')\n</pre> text, aspect_spans = texts_w_aspect_spans[random_picks[0]] print('\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u0438 BIO \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u0430')  for sentence, spans in sentence_splitter(text, aspect_spans):      cover_spans      = fill_gaps(sentence,spans)     tokens_w_biotags = extract_BIO_tagged_tokens(sentence,                                                       cover_spans,                                                       word_tokenizer.tokenize)      show_markup(sentence, cover_spans)     print(tokens_w_biotags[:10],'\\n') <pre>\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u0438 BIO \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u0430\n</pre> \u041e \u043f\u043b\u044e\u0441\u0430\u0445 Other\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044fWhole. <pre>[('\u041e', 'Other'), ('\u043f\u043b\u044e\u0441\u0430\u0445', 'Other'), ('\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044f', 'B-Whole')] \n\n</pre> \u0414\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u0435 Other\u0437\u0430\u043f\u0447\u0430\u0441\u0442\u0438Reliability, \u043c\u043e\u0436\u043d\u043e \u043d\u0430\u0439\u0442\u0438 \u0432 \u043b\u044e\u0431\u043e\u043c \u0433\u043e\u0440\u043e\u0434\u0435, \u0432 \u043b\u044e\u0431\u043e\u043c \u0433\u0430\u0440\u0430\u0436\u0435, \u0433\u0434\u0435 \u0437\u0430\u043d\u0438\u043c\u0430\u044e\u0442\u0441\u044f \u043c\u0430\u0448\u0438\u043d\u0430\u043c\u0438.Other <pre>[('\u0414\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u0435', 'Other'), ('\u0437\u0430\u043f\u0447\u0430\u0441\u0442\u0438', 'B-Reliability'), (',', 'Other'), ('\u043c\u043e\u0436\u043d\u043e', 'Other'), ('\u043d\u0430\u0439\u0442\u0438', 'Other'), ('\u0432', 'Other'), ('\u043b\u044e\u0431\u043e\u043c', 'Other'), ('\u0433\u043e\u0440\u043e\u0434\u0435', 'Other'), (',', 'Other'), ('\u0432', 'Other')] \n\n</pre> \u041c\u0430\u0448\u0438\u043d\u0430Driveability \u0445\u043e\u0440\u043e\u0448\u0430\u044f, \u0442\u043e\u043b\u044c\u043a\u043e \u043d\u0430 \u0434\u0430\u043b\u044c\u043d\u044e\u044e \u0434\u043e\u0440\u043e\u0433\u0443, \u0431\u0435\u043d\u0437\u0438\u043d \u043d\u0430 \u0442\u0440\u0430\u0441\u0441\u0435 \u043f\u0440\u043e\u0441\u0442\u043e \u043d\u044e\u0445\u0430\u0435\u0442, \u0432 Other\u0433\u043e\u0440\u043e\u0434\u0441\u043a\u043e\u043c \u0440\u0435\u0436\u0438\u043c\u0435Driveability Other\u0440\u0430\u0441\u0445\u043e\u0434 \u0442\u043e\u043f\u043b\u0438\u0432\u0430Driveability \u0440\u0435\u0437\u043a\u043e \u043f\u043e\u0434\u0441\u043a\u0430\u043a\u0438\u0432\u0430\u0435\u0442.Other <pre>[('\u041c\u0430\u0448\u0438\u043d\u0430', 'B-Driveability'), ('\u0445\u043e\u0440\u043e\u0448\u0430\u044f', 'Other'), (',', 'Other'), ('\u0442\u043e\u043b\u044c\u043a\u043e', 'Other'), ('\u043d\u0430', 'Other'), ('\u0434\u0430\u043b\u044c\u043d\u044e\u044e', 'Other'), ('\u0434\u043e\u0440\u043e\u0433\u0443', 'Other'), (',', 'Other'), ('\u0431\u0435\u043d\u0437\u0438\u043d', 'Other'), ('\u043d\u0430', 'Other')] \n\n</pre> \u0425\u043e\u0440\u043e\u0448\u0430\u044f Other\u043f\u043e\u0434\u0432\u0435\u0441\u043a\u0430Driveability \u0438 Other\u0432\u044b\u0441\u043e\u043a\u0438\u0439 \u043a\u043b\u0438\u0440\u0435\u043d\u0441Driveability. <pre>[('\u0425\u043e\u0440\u043e\u0448\u0430\u044f', 'Other'), ('\u043f\u043e\u0434\u0432\u0435\u0441\u043a\u0430', 'B-Driveability'), ('\u0438', 'Other'), ('\u0432\u044b\u0441\u043e\u043a\u0438\u0439', 'B-Driveability'), ('\u043a\u043b\u0438\u0440\u0435\u043d\u0441', 'I-Driveability')] \n\n</pre> \u0412 \u043e\u0431\u0441\u043b\u0443\u0436\u0438\u0432\u0430\u043d\u0438\u0438Reliability Other\u043d\u0435\u043f\u0440\u0438\u0445\u043e\u0442\u043b\u0438\u0432Reliability. <pre>[('\u0412', 'B-Reliability'), ('\u043e\u0431\u0441\u043b\u0443\u0436\u0438\u0432\u0430\u043d\u0438\u0438', 'I-Reliability'), ('\u043d\u0435\u043f\u0440\u0438\u0445\u043e\u0442\u043b\u0438\u0432', 'B-Reliability')] \n\n</pre> \u0421\u043b\u0430\u0431\u044b\u0439 \u0434\u0432\u0438\u0433\u0430\u0442\u0435\u043b\u044cDriveability, \u043d\u0435 \u0442\u044f\u043d\u0435\u0442 \u0431\u043e\u043b\u044c\u0448\u0435 3000 \u043e\u0431\u043e\u0440\u043e\u0442\u043e\u0432, \u043e\u0447\u0435\u043d\u044c Other\u0441\u043b\u0430\u0431\u0430\u044f \u0434\u0438\u043d\u0430\u043c\u0438\u043a\u0430Driveability. <pre>[('\u0421\u043b\u0430\u0431\u044b\u0439', 'B-Driveability'), ('\u0434\u0432\u0438\u0433\u0430\u0442\u0435\u043b\u044c', 'I-Driveability'), (',', 'Other'), ('\u043d\u0435', 'Other'), ('\u0442\u044f\u043d\u0435\u0442', 'Other'), ('\u0431\u043e\u043b\u044c\u0448\u0435', 'Other'), ('3000', 'Other'), ('\u043e\u0431\u043e\u0440\u043e\u0442\u043e\u0432', 'Other'), (',', 'Other'), ('\u043e\u0447\u0435\u043d\u044c', 'Other')] \n\n</pre> \u041f\u043b\u043e\u0445\u043e \u0440\u0430\u0441\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b Other\u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u044b \u0432 \u043a\u043e\u0440\u043e\u0431\u043a\u0435 \u043f\u0435\u0440\u0435\u0434\u0430\u0447Driveability. <pre>[('\u041f\u043b\u043e\u0445\u043e', 'Other'), ('\u0440\u0430\u0441\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b', 'Other'), ('\u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u044b', 'B-Driveability'), ('\u0432', 'I-Driveability'), ('\u043a\u043e\u0440\u043e\u0431\u043a\u0435', 'I-Driveability'), ('\u043f\u0435\u0440\u0435\u0434\u0430\u0447', 'I-Driveability')] \n\n</pre> \u041e\u0447\u0435\u043d\u044c \u0441\u043b\u0430\u0431\u0430\u044f Other\u0440\u0443\u043b\u0435\u0432\u0430\u044f \u0440\u0435\u0439\u043a\u0430Reliability, Other\u0442\u0440\u0435\u0431\u0443\u0435\u0442 \u043f\u043e\u0441\u0442\u043e\u044f\u043d\u043d\u043e\u0433\u043e \u0440\u0435\u043c\u043e\u043d\u0442\u0430Reliability \u043f\u043e\u0441\u043b\u0435 \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0435\u0440\u044c\u0435\u0437\u043d\u043e\u0439 \u044f\u043c\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u0440\u0435\u0434\u043a\u043e \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0442\u0441\u044f \u043d\u0430 \u0434\u043e\u0440\u043e\u0433\u0430\u0445.Other <pre>[('\u041e\u0447\u0435\u043d\u044c', 'Other'), ('\u0441\u043b\u0430\u0431\u0430\u044f', 'Other'), ('\u0440\u0443\u043b\u0435\u0432\u0430\u044f', 'B-Reliability'), ('\u0440\u0435\u0439\u043a\u0430', 'I-Reliability'), (',', 'Other'), ('\u0442\u0440\u0435\u0431\u0443\u0435\u0442', 'B-Reliability'), ('\u043f\u043e\u0441\u0442\u043e\u044f\u043d\u043d\u043e\u0433\u043e', 'I-Reliability'), ('\u0440\u0435\u043c\u043e\u043d\u0442\u0430', 'I-Reliability'), ('\u043f\u043e\u0441\u043b\u0435', 'Other'), ('\u043a\u0430\u0436\u0434\u043e\u0439', 'Other')] \n\n</pre> \u041e\u0447\u0435\u043d\u044c \u043f\u043b\u043e\u0445\u043e\u0439 Other\u043c\u0435\u0442\u0430\u043b\u043bReliability, Other\u0440\u0436\u0430\u0432\u0435\u0435\u0442Reliability \u043c\u043e\u043c\u0435\u043d\u0442\u0430\u043b\u044c\u043d\u043e.Other <pre>[('\u041e\u0447\u0435\u043d\u044c', 'Other'), ('\u043f\u043b\u043e\u0445\u043e\u0439', 'Other'), ('\u043c\u0435\u0442\u0430\u043b\u043b', 'B-Reliability'), (',', 'Other'), ('\u0440\u0436\u0430\u0432\u0435\u0435\u0442', 'B-Reliability'), ('\u043c\u043e\u043c\u0435\u043d\u0442\u0430\u043b\u044c\u043d\u043e', 'Other'), ('.', 'Other')] \n\n</pre> \u041f\u043b\u043e\u0445\u0430\u044f Other\u0448\u0443\u043c\u043e\u0438\u0437\u043e\u043b\u044f\u0446\u0438\u044fComfort. <pre>[('\u041f\u043b\u043e\u0445\u0430\u044f', 'Other'), ('\u0448\u0443\u043c\u043e\u0438\u0437\u043e\u043b\u044f\u0446\u0438\u044f', 'B-Comfort')] \n\n</pre> \u041f\u043e\u0434\u043e\u0439\u0434\u0435\u0442 \u043a\u0430\u043a \u043f\u0435\u0440\u0432\u0430\u044f Other\u043c\u0430\u0448\u0438\u043d\u0430Whole, \u0441\u0430\u043c \u0441\u043b\u043e\u043c\u0430\u043b \u0441\u0430\u043c \u043f\u043e\u0447\u0438\u043d\u0438\u043b.Other <pre>[('\u041f\u043e\u0434\u043e\u0439\u0434\u0435\u0442', 'Other'), ('\u043a\u0430\u043a', 'Other'), ('\u043f\u0435\u0440\u0432\u0430\u044f', 'Other'), ('\u043c\u0430\u0448\u0438\u043d\u0430', 'B-Whole'), (',', 'Other'), ('\u0441\u0430\u043c', 'Other'), ('\u0441\u043b\u043e\u043c\u0430\u043b', 'Other'), ('\u0441\u0430\u043c', 'Other'), ('\u043f\u043e\u0447\u0438\u043d\u0438\u043b', 'Other'), ('.', 'Other')] \n\n</pre> \u041e\u0431\u0449\u0435\u0435 \u0432\u043f\u0435\u0447\u0430\u0442\u043b\u0435\u043d\u0438\u0435 : Other\u0414\u0435\u0448\u0435\u0432\u044b\u0439Costs, Other\u043d\u0435\u043f\u0440\u0438\u0445\u043e\u0442\u043b\u0438\u0432\u044b\u0439Reliability, Other\u043d\u0438\u0437\u043a\u0438\u0439 \u0440\u0430\u0441\u0445\u043e\u0434 \u0442\u043e\u043f\u043b\u0438\u0432\u0430Driveability <pre>[('\u041e\u0431\u0449\u0435\u0435', 'Other'), ('\u0432\u043f\u0435\u0447\u0430\u0442\u043b\u0435\u043d\u0438\u0435', 'Other'), (':', 'Other'), ('\u0414\u0435\u0448\u0435\u0432\u044b\u0439', 'B-Costs'), (',', 'Other'), ('\u043d\u0435\u043f\u0440\u0438\u0445\u043e\u0442\u043b\u0438\u0432\u044b\u0439', 'B-Reliability'), (',', 'Other'), ('\u043d\u0438\u0437\u043a\u0438\u0439', 'B-Driveability'), ('\u0440\u0430\u0441\u0445\u043e\u0434', 'I-Driveability'), ('\u0442\u043e\u043f\u043b\u0438\u0432\u0430', 'I-Driveability')] \n\n</pre> In\u00a0[82]: Copied! <pre>def form_vocabulary_and_tagset(words_w_tags):\n    \n    dictionary = set()\n    tagset     = set()\n\n    for words,tags in words_w_tags:\n        for word, tag in zip(words, tags):\n            dictionary.add(word)\n            tagset.add(tag)\n\n    return dictionary,tagset\n\n\n# create (token,tag) data\ndef prepare_data(texts_with_spans, tokenize_func):\n    \n    result = []\n    for text, spans in texts_with_spans:\n        for sent, sent_spans in sentence_splitter(text, spans):\n            if len(sent)&gt;1:\n                cover_spans      = fill_gaps(sent, sent_spans)\n                try:\n                    tokens_w_biotags = extract_BIO_tagged_tokens(sent, cover_spans, tokenize_func)\n                    tokens, biotags  = list(zip(*tokens_w_biotags))\n                    result.append((tokens, biotags))\n\n                except Exception as e:\n                    continue\n\n    return result\n</pre> def form_vocabulary_and_tagset(words_w_tags):          dictionary = set()     tagset     = set()      for words,tags in words_w_tags:         for word, tag in zip(words, tags):             dictionary.add(word)             tagset.add(tag)      return dictionary,tagset   # create (token,tag) data def prepare_data(texts_with_spans, tokenize_func):          result = []     for text, spans in texts_with_spans:         for sent, sent_spans in sentence_splitter(text, spans):             if len(sent)&gt;1:                 cover_spans      = fill_gaps(sent, sent_spans)                 try:                     tokens_w_biotags = extract_BIO_tagged_tokens(sent, cover_spans, tokenize_func)                     tokens, biotags  = list(zip(*tokens_w_biotags))                     result.append((tokens, biotags))                  except Exception as e:                     continue      return result In\u00a0[83]: Copied! <pre>xml_train = 'SentiRuEval_car_markup_train.xml'\nxml_test  = 'SentiRuEval_car_markup_test.xml'\n\ntexts_w_aspect_spans = parse_xml_aspect('/kaggle/input/araneum/'+xml_train)\ntraining_data        = prepare_data(texts_w_aspect_spans, word_tokenizer.tokenize)\n\ntexts_w_aspect_spans = parse_xml_aspect('/kaggle/input/araneum/'+xml_test)\ntest_data            = prepare_data(texts_w_aspect_spans, word_tokenizer.tokenize)\n</pre> xml_train = 'SentiRuEval_car_markup_train.xml' xml_test  = 'SentiRuEval_car_markup_test.xml'  texts_w_aspect_spans = parse_xml_aspect('/kaggle/input/araneum/'+xml_train) training_data        = prepare_data(texts_w_aspect_spans, word_tokenizer.tokenize)  texts_w_aspect_spans = parse_xml_aspect('/kaggle/input/araneum/'+xml_test) test_data            = prepare_data(texts_w_aspect_spans, word_tokenizer.tokenize) In\u00a0[84]: Copied! <pre>texts_w_aspect_spans[0]\n</pre> texts_w_aspect_spans[0] Out[84]: <pre>('\u0412 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u0435 \u043c\u0430\u0448\u0438\u043d\u043a\u0430 \u043d\u0435 \u043f\u043b\u043e\u0445\u0430\u044f, \u043e\u0431\u044a\u0435\u043c\u043e\u043c \u0432 2.0 \u043a\u0443\u0431\u0430, \u043b\u0435\u0433\u043a\u0438\u0439 \u043a\u0443\u0437\u043e\u0432, \u0434\u043e\u0440\u043e\u0433\u0443 \u0434\u0435\u0440\u0436\u0438\u0442 \u043d\u0435 \u043f\u043b\u043e\u0445\u043e, \u043f\u0440\u0438\u044f\u0442\u043d\u0430 \u043f\u043e \u0441\u0430\u043b\u043e\u043d\u0443, \u0441\u0434\u0435\u043b\u0430\u043d\u0430 \u0432 \u0441\u043f\u043e\u0440\u0442 \u0441\u0442\u0438\u043b\u0435, \u043f\u043e \u0433\u043e\u0440\u043e\u0434\u0443 9 \u043b\u0438\u0442\u0440\u043e\u0432 \u043d\u0435 \u0431\u043e\u043b\u044c\u0448\u0435, \u043f\u043e \u0442\u0440\u0430\u0441\u0441\u0435 7, \u043d\u043e \u0435\u0441\u0442\u044c \u0438 \u043c\u0438\u043d\u0443\u0441\u044b \u0447\u0442\u043e \u043f\u043e \u0445\u043e\u0434\u043e\u0432\u043e\u0439 \u0441\u043b\u0430\u0431\u0435\u043d\u044c\u043a\u0430\u044f \u043c\u0430\u0448\u0438\u043d\u0430 \u0438 \u043a\u0443\u0437\u043e\u0432\u0430 \u043f\u043e\u0447\u0442\u0438 \u0443 \u0432\u0441\u0435\u0445 \u0433\u043d\u0438\u044e\u0442 \u0441\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0435\u043c, \u043c\u043d\u043e\u0433\u0438\u0435 \u0432\u043e\u0434\u0438\u0442\u0435\u043b\u0438 \u044d\u0442\u043e\u0433\u043e \u043d\u0435 \u0437\u0430\u043c\u0435\u0447\u0430\u044e\u0442, \u043d\u0435 \u0433\u043e\u0432\u043e\u0440\u044e \u0447\u0442\u043e \u0432\u0441\u0435 100% \u0433\u043d\u0438\u044e\u0442. \u041e\u0431\u0449\u0435\u0435 \u0432\u043f\u0435\u0447\u0430\u0442\u043b\u0435\u043d\u0438\u0435 : \u0421\u043b\u0430\u0431\u0430\u044f \u043f\u043e \u0445\u043e\u0434\u043e\u0432\u043a\u0435,\u0446\u0432\u0435\u0442\u0435\u0442 \u043a\u0443\u0437\u043e\u0432.',\n [(11, 18, 'Whole'),\n  (57, 62, 'Driveability'),\n  (64, 77, 'Driveability'),\n  (99, 105, 'Appearance'),\n  (115, 128, 'Appearance'),\n  (130, 139, 'Driveability'),\n  (160, 169, 'Driveability'),\n  (197, 204, 'Driveability'),\n  (216, 222, 'Driveability'),\n  (225, 231, 'Reliability'),\n  (245, 250, 'Reliability'),\n  (322, 327, 'Reliability'),\n  (359, 366, 'Driveability'),\n  (367, 373, 'Reliability'),\n  (374, 379, 'Reliability')])</pre> In\u00a0[85]: Copied! <pre>training_data[0]\n</pre> training_data[0] Out[85]: <pre>(('\u041d\u0435\u0434\u0430\u0432\u043d\u043e', '\u043a\u0443\u043f\u0438\u043b', '\u044d\u0442\u043e\u0442', '\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c'),\n ('Other', 'Other', 'Other', 'B-Whole'))</pre> <p>\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u0434\u0430\u043b\u043e \u043d\u0430\u043c \u0441\u0442\u043e\u043b\u044c\u043a\u043e \u043a\u043e\u0440\u043e\u0442\u043a\u0438\u0445 \u0442\u0435\u043a\u0441\u0442\u043e\u0432</p> In\u00a0[86]: Copied! <pre>len(training_data), len(test_data)\n</pre> len(training_data), len(test_data) Out[86]: <pre>(2210, 1922)</pre> In\u00a0[87]: Copied! <pre>all_data = training_data + test_data\nvocabulary, labels = form_vocabulary_and_tagset(all_data)\n</pre> all_data = training_data + test_data vocabulary, labels = form_vocabulary_and_tagset(all_data) In\u00a0[88]: Copied! <pre>labels = sorted(labels)\nlabels\n</pre> labels = sorted(labels) labels Out[88]: <pre>['B-Appearance',\n 'B-Comfort',\n 'B-Costs',\n 'B-Driveability',\n 'B-Reliability',\n 'B-Safety',\n 'B-Whole',\n 'I-Appearance',\n 'I-Comfort',\n 'I-Costs',\n 'I-Driveability',\n 'I-Reliability',\n 'I-Safety',\n 'I-Whole',\n 'Other']</pre> In\u00a0[89]: Copied! <pre>len(vocabulary), len(labels)\n</pre> len(vocabulary), len(labels) Out[89]: <pre>(11333, 15)</pre> In\u00a0[92]: Copied! <pre>import gensim\nimport urllib.request\nimport zipfile\n\n# \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0438 URL\nwe_models = {\"geowac_lemmas_none_fasttextskipgram_300_5_2020\": \"http://vectors.nlpl.eu/repository/20/213.zip\",}\n</pre> import gensim import urllib.request import zipfile  # \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0438 URL we_models = {\"geowac_lemmas_none_fasttextskipgram_300_5_2020\": \"http://vectors.nlpl.eu/repository/20/213.zip\",} In\u00a0[93]: Copied! <pre># \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\ndef get_models(model_url, model_name, path_to_save=\"/kaggle/working/\"):\n    model_path = path_to_save + model_name + \".zip\"\n    urllib.request.urlretrieve(model_url, model_path)\n\nfor model_name, model_url in we_models.items():\n    get_models(model_url, model_name)\n</pre> # \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c def get_models(model_url, model_name, path_to_save=\"/kaggle/working/\"):     model_path = path_to_save + model_name + \".zip\"     urllib.request.urlretrieve(model_url, model_path)  for model_name, model_url in we_models.items():     get_models(model_url, model_name) In\u00a0[94]: Copied! <pre># \u0440\u0430\u0437\u0430\u0440\u0445\u0438\u0432\u0438\u0440\u0435\u043c \nwith zipfile.ZipFile(\"/kaggle/working/geowac_lemmas_none_fasttextskipgram_300_5_2020.zip\", 'r') as zip_ref: \n    zip_ref.extractall(\"/kaggle/working/geowac_lemmas_none_fasttextskipgram_300_5_2020\")\n    \n# \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c KeyedVectors \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432\u044b\u0439 \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \n# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0447\u0442\u0435\u043d\u0438\u044f word2vec / FastText\n\ndef open_model(model_name,model_path, is_fasttext = True):\n    \n    # word2vec (model.bin)\n    if is_fasttext == False:\n        model_file = model_path + model_name + \".zip\"\n        with zipfile.ZipFile(model_file, 'r') as archive:\n            stream = archive.open('model.bin')\n            model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)\n            \n    # fasttext (model.model)\n    else:\n        model_file = model_path + model_name\n        model = gensim.models.KeyedVectors.load(model_file + \"/model.model\")\n    return model\n\nw2v_model = open_model('geowac_lemmas_none_fasttextskipgram_300_5_2020','/kaggle/working/')\n</pre> # \u0440\u0430\u0437\u0430\u0440\u0445\u0438\u0432\u0438\u0440\u0435\u043c  with zipfile.ZipFile(\"/kaggle/working/geowac_lemmas_none_fasttextskipgram_300_5_2020.zip\", 'r') as zip_ref:      zip_ref.extractall(\"/kaggle/working/geowac_lemmas_none_fasttextskipgram_300_5_2020\")      # \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c KeyedVectors \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432\u044b\u0439 \u0432\u0435\u043a\u0442\u043e\u0440\u0430  # \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0447\u0442\u0435\u043d\u0438\u044f word2vec / FastText  def open_model(model_name,model_path, is_fasttext = True):          # word2vec (model.bin)     if is_fasttext == False:         model_file = model_path + model_name + \".zip\"         with zipfile.ZipFile(model_file, 'r') as archive:             stream = archive.open('model.bin')             model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)                  # fasttext (model.model)     else:         model_file = model_path + model_name         model = gensim.models.KeyedVectors.load(model_file + \"/model.model\")     return model  w2v_model = open_model('geowac_lemmas_none_fasttextskipgram_300_5_2020','/kaggle/working/') <p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432</p> In\u00a0[95]: Copied! <pre>words = ['\u0442\u0430\u0447\u043a\u0430', '\u0434\u0432\u0438\u0433\u0430\u0442\u0435\u043b\u044c', '\u0430\u0443\u0434\u0438']\n\nfor word in words:\n    \n    for i in w2v_model.most_similar(positive=[word], topn=10):\n        nearest_word      = i[0]\n        cosine_similarity = i[1]\n        print(nearest_word, cosine_similarity)\n    print('\\n')\n\n    #else: print(word + ' is not present in the model')\n</pre> words = ['\u0442\u0430\u0447\u043a\u0430', '\u0434\u0432\u0438\u0433\u0430\u0442\u0435\u043b\u044c', '\u0430\u0443\u0434\u0438']  for word in words:          for i in w2v_model.most_similar(positive=[word], topn=10):         nearest_word      = i[0]         cosine_similarity = i[1]         print(nearest_word, cosine_similarity)     print('\\n')      #else: print(word + ' is not present in the model') <pre>\u043c\u0430\u0448\u0438\u043d\u0430 0.5936635732650757\n\u043c\u0430\u0448\u0438\u043d\u043a\u0430 0.58965665102005\n\u0447\u0435\u0440\u0435\u043f\u0430\u0448\u043a\u0430 0.5865190029144287\n\u0433\u0440\u0443\u0437\u043e\u0432\u0438\u0447\u043e\u043a 0.5823847055435181\n\u0433\u0440\u0443\u0437\u043e\u0432\u0438\u0447\u043a\u0430 0.5814732909202576\n\u0442\u0440\u0430\u043a\u0442\u043e\u0440 0.578829824924469\n\u043c\u0430\u0448\u044b\u043d\u0430 0.5752986073493958\n\u043c\u043e\u0442\u043e\u0440\u043e\u043b\u043b\u0435\u0440 0.5746884942054749\n\u0431\u0430\u0439\u043a 0.574382483959198\n\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c\u0447\u0438\u043a 0.5617275238037109\n\n\n\u043c\u043e\u0442\u043e\u0440 0.8536396622657776\n\u0442\u0443\u0440\u0431\u043e\u0434\u0432\u0438\u0433\u0430\u0442\u0435\u043b\u044c 0.7599603533744812\n\u044d\u043b\u0435\u043a\u0442\u0440\u043e\u0434\u0432\u0438\u0433\u0430\u0442\u0435\u043b\u044c 0.7526513338088989\n\u0434\u0432\u0441 0.7494674921035767\n\u0442\u0443\u0440\u0431\u0438\u043d\u0430 0.744778037071228\n\u0447\u0435\u0442\u044b\u0440\u0435\u0445\u0442\u0430\u043a\u0442\u043d\u044b\u0439 0.7394916415214539\n\u043a\u0430\u0440\u0431\u044e\u0440\u0430\u0442\u043e\u0440\u043d\u044b\u0439 0.7300528883934021\n\u0431\u0435\u043d\u0437\u0438\u043d\u043e\u0432\u044b\u0439 0.7256711721420288\n\u0442\u0440\u0430\u043d\u0441\u043c\u0438\u0441\u0441\u0438\u044f 0.7220973968505859\n\u043e\u0434\u043d\u043e\u0446\u0438\u043b\u0438\u043d\u0434\u0440\u043e\u0432\u044b\u0439 0.7176888585090637\n\n\nbmw 0.778282105922699\nskoda 0.7711601853370667\ncitroen 0.7693266272544861\nvolkswagen 0.7655405402183533\nsportback 0.7623247504234314\nopel 0.759402334690094\nmercedes 0.7484782934188843\nvw 0.7478039860725403\nq7 0.7471405863761902\npassat 0.7310269474983215\n\n\n</pre> In\u00a0[97]: Copied! <pre># \u0412\u044b\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0432\u0435\u043a\u0442\u043e\u0440\u0430\n\ndef get_vector(w):\n    try:\n        return w2v_model[w]\n    except:\n        \n        x = np.zeros(300)\n        elems = {}\n        c = 0\n        for i in w2v_model.index2word:\n            \n            # \u0435\u0441\u043b\u0438 \u0447\u0430\u0441\u0442\u044c \u0442\u043e\u043a\u0435\u043d\u0430 \u043d\u0430\u0439\u0434\u0435\u043d\u0430 \u0432 vocab\n            if i in w:\n                elems[i] = w2v_model[i]\n                c += 1\n                \n        if c &gt; 0:\n            vect = sum(elems.values())\n        else:\n            return x  # \u0435\u0441\u043b\u0438 \u0432\u043e\u043e\u0431\u0449\u0435 \u043d\u0438\u0447\u0435\u0433\u043e \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d\u043e\n         \n        # \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0432\u0435\u043a\u0442\u043e\u0440\u0430\n        n_vect = np.linalg.norm(vect)\n        if n_vect &lt; 0.05:\n            return np.zeros(300)\n        else:\n            return vect / n_vect\n</pre> # \u0412\u044b\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0432\u0435\u043a\u0442\u043e\u0440\u0430  def get_vector(w):     try:         return w2v_model[w]     except:                  x = np.zeros(300)         elems = {}         c = 0         for i in w2v_model.index2word:                          # \u0435\u0441\u043b\u0438 \u0447\u0430\u0441\u0442\u044c \u0442\u043e\u043a\u0435\u043d\u0430 \u043d\u0430\u0439\u0434\u0435\u043d\u0430 \u0432 vocab             if i in w:                 elems[i] = w2v_model[i]                 c += 1                          if c &gt; 0:             vect = sum(elems.values())         else:             return x  # \u0435\u0441\u043b\u0438 \u0432\u043e\u043e\u0431\u0449\u0435 \u043d\u0438\u0447\u0435\u0433\u043e \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d\u043e                   # \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0432\u0435\u043a\u0442\u043e\u0440\u0430         n_vect = np.linalg.norm(vect)         if n_vect &lt; 0.05:             return np.zeros(300)         else:             return vect / n_vect In\u00a0[99]: Copied! <pre>@lru_cache(10000)\ndef lemmatize(s):\n    s = str(s).lower()\n    return morph.parse(s)[0].normal_form\n</pre> @lru_cache(10000) def lemmatize(s):     s = str(s).lower()     return morph.parse(s)[0].normal_form In\u00a0[100]: Copied! <pre>def vectorize_word_seq(word_seq: typing.Iterable[str]):\n    vects = []\n    for w in word_seq:\n        w_l = lemmatize(w)\n        vects.append(get_vector(w_l))\n\n    return torch.tensor(np.vstack(vects), requires_grad=False, dtype=torch.float32)\n</pre> def vectorize_word_seq(word_seq: typing.Iterable[str]):     vects = []     for w in word_seq:         w_l = lemmatize(w)         vects.append(get_vector(w_l))      return torch.tensor(np.vstack(vects), requires_grad=False, dtype=torch.float32) <p>\u041f\u0440\u043e\u0442\u0435\u0441\u0442\u0438\u0440\u0443\u0435\u043c \u0432\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u0443\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043a\u043e\u0442\u043e\u0440\u044e \u0431\u0443\u0434\u0435\u043c \u043f\u043e\u0434\u0430\u0432\u0430\u0442\u044c \u0432 \u043c\u043e\u0434\u0435\u043b\u044c</p> In\u00a0[138]: Copied! <pre>test = vectorize_word_seq('\u043c\u0430\u043c\u0430 \u043c\u044b\u043b\u0430 \u0440\u0430\u043c\u0443'.split())\ntest.size()\n</pre> test = vectorize_word_seq('\u043c\u0430\u043c\u0430 \u043c\u044b\u043b\u0430 \u0440\u0430\u043c\u0443'.split()) test.size() Out[138]: <pre>torch.Size([3, 300])</pre> In\u00a0[102]: Copied! <pre>class RecTagger(nn.Module):\n\n    def __init__(self, embedding_dim, \n                 hidden_dim, \n                 tagset_size, \n                 rec_lalers=1, \n                 labels=None):\n        \n        super(RecTagger, self).__init__()\n        assert labels\n\n        self.labels = sorted(labels)\n\n        self.labels_to_id = {l: i for i, l in zip(range(len(self.labels)), self.labels)}\n\n        self.target_size     = tagset_size\n        self.hidden_dim      = hidden_dim\n\n        self.rec             = nn.GRU(embedding_dim, hidden_dim, rec_lalers,\n                                      bidirectional=True, batch_first=True)\n\n        self.vect_projection = nn.Linear(embedding_dim, hidden_dim)\n        self.hidden2tag      = nn.Linear(3*hidden_dim, tagset_size)\n\n    def forward(self, words_seq, device=None):\n        \n        if device:\n            embeds = vectorize_word_seq(words_seq).to(device)\n        else:\n            embeds = vectorize_word_seq(words_seq)\n            \n        rec_out, _ = self.rec(embeds)\n        vect_proj = self.vect_projection(embeds)\n        \n        concat_vect = torch.concat([rec_out, vect_proj], dim=-1)\n        tag_space   = self.hidden2tag(concat_vect)\n        tag_scores  = F.log_softmax(tag_space, dim=-1)\n\n        return tag_scores\n\n    def encode_lebels(self, labels_seq, device=None):\n        res = []\n        for l in labels_seq:\n            res.append(self.labels_to_id[l])\n        if device:\n            return torch.tensor(res, requires_grad=False).to(device)\n        else:\n            return torch.tensor(res, requires_grad=False)\n\n    def decode_labels(self, labels_id: np.array):\n        res = []\n        for i in list(labels_id):\n            res.append(self.labels[i])\n        return res\n\n    def predict_tags(self, words_seq):\n        with torch.no_grad():\n            tags_pred = self.forward(words_seq).numpy()\n            tags_pred = np.argmax(tags_pred, axis=-1)\n\n        return tags_pred\n</pre> class RecTagger(nn.Module):      def __init__(self, embedding_dim,                   hidden_dim,                   tagset_size,                   rec_lalers=1,                   labels=None):                  super(RecTagger, self).__init__()         assert labels          self.labels = sorted(labels)          self.labels_to_id = {l: i for i, l in zip(range(len(self.labels)), self.labels)}          self.target_size     = tagset_size         self.hidden_dim      = hidden_dim          self.rec             = nn.GRU(embedding_dim, hidden_dim, rec_lalers,                                       bidirectional=True, batch_first=True)          self.vect_projection = nn.Linear(embedding_dim, hidden_dim)         self.hidden2tag      = nn.Linear(3*hidden_dim, tagset_size)      def forward(self, words_seq, device=None):                  if device:             embeds = vectorize_word_seq(words_seq).to(device)         else:             embeds = vectorize_word_seq(words_seq)                      rec_out, _ = self.rec(embeds)         vect_proj = self.vect_projection(embeds)                  concat_vect = torch.concat([rec_out, vect_proj], dim=-1)         tag_space   = self.hidden2tag(concat_vect)         tag_scores  = F.log_softmax(tag_space, dim=-1)          return tag_scores      def encode_lebels(self, labels_seq, device=None):         res = []         for l in labels_seq:             res.append(self.labels_to_id[l])         if device:             return torch.tensor(res, requires_grad=False).to(device)         else:             return torch.tensor(res, requires_grad=False)      def decode_labels(self, labels_id: np.array):         res = []         for i in list(labels_id):             res.append(self.labels[i])         return res      def predict_tags(self, words_seq):         with torch.no_grad():             tags_pred = self.forward(words_seq).numpy()             tags_pred = np.argmax(tags_pred, axis=-1)          return tags_pred In\u00a0[105]: Copied! <pre>EMBEDDING_DIM = 300\nHIDDEN_DIM = 12\nTAGSET_SIZE = len(labels)\n\n# \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c (\u0442\u0435\u0441\u0442)\nm_test = RecTagger(EMBEDDING_DIM, \n                   HIDDEN_DIM, \n                   TAGSET_SIZE, \n                   1, \n                   labels)\n</pre> EMBEDDING_DIM = 300 HIDDEN_DIM = 12 TAGSET_SIZE = len(labels)  # \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c (\u0442\u0435\u0441\u0442) m_test = RecTagger(EMBEDDING_DIM,                     HIDDEN_DIM,                     TAGSET_SIZE,                     1,                     labels) In\u00a0[107]: Copied! <pre># \u0412\u0441\u0435 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0438\nm_test.labels\n</pre> # \u0412\u0441\u0435 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0438 m_test.labels Out[107]: <pre>['B-Appearance',\n 'B-Comfort',\n 'B-Costs',\n 'B-Driveability',\n 'B-Reliability',\n 'B-Safety',\n 'B-Whole',\n 'I-Appearance',\n 'I-Comfort',\n 'I-Costs',\n 'I-Driveability',\n 'I-Reliability',\n 'I-Safety',\n 'I-Whole',\n 'Other']</pre> <p>\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432</p> In\u00a0[139]: Copied! <pre>def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ncount_parameters(m_test)\n</pre> def count_parameters(model):     return sum(p.numel() for p in model.parameters() if p.requires_grad)  count_parameters(m_test) Out[139]: <pre>26775</pre> <p>\u0423\u0431\u0435\u0434\u0438\u043c\u0441\u044f \u0447\u0442\u043e \u0432\u0441\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442</p> In\u00a0[149]: Copied! <pre># \u0442\u044d\u0440\u043e\u0432\u044b\u0435 \u043b\u043e\u0433\u0430\u0440\u0438\u0444\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0439 softmax\npred = m_test('\u043c\u0430\u043c\u0430 \u043c\u044b\u043b\u0430 \u0440\u0430\u043c\u0443 \u043d\u0430 \u0430\u0443\u0434\u0438 q5, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0445\u043e\u0440\u043e\u0448\u043e \u0440\u0443\u043b\u0438\u0442\u0441\u044f'.split())\npred\n</pre> # \u0442\u044d\u0440\u043e\u0432\u044b\u0435 \u043b\u043e\u0433\u0430\u0440\u0438\u0444\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0439 softmax pred = m_test('\u043c\u0430\u043c\u0430 \u043c\u044b\u043b\u0430 \u0440\u0430\u043c\u0443 \u043d\u0430 \u0430\u0443\u0434\u0438 q5, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0445\u043e\u0440\u043e\u0448\u043e \u0440\u0443\u043b\u0438\u0442\u0441\u044f'.split()) pred Out[149]: <pre>tensor([[-2.7464, -2.8194, -2.9020, -2.6654, -2.6564, -2.5888, -2.4695, -2.6416,\n         -2.8311, -2.5134, -2.6439, -2.7907, -2.6945, -2.8152, -2.9826],\n        [-2.6151, -2.7963, -3.0382, -2.6598, -2.6631, -2.6316, -2.3924, -2.3449,\n         -2.7286, -2.9495, -2.7651, -2.7599, -2.8032, -2.7408, -2.9980],\n        [-2.6639, -2.7700, -2.9969, -2.7356, -2.4825, -2.7246, -2.4907, -2.4942,\n         -2.9998, -2.7316, -2.5690, -2.7424, -2.5333, -2.8730, -3.0690],\n        [-2.7045, -2.8588, -2.8543, -2.7703, -2.4608, -2.7426, -2.4342, -2.6316,\n         -2.9949, -2.5709, -2.5761, -2.8725, -2.5081, -2.9079, -2.9766],\n        [-2.6249, -2.9957, -3.0788, -2.5937, -2.3531, -2.6346, -2.6320, -2.6055,\n         -2.9043, -2.7025, -2.4937, -2.8921, -2.5229, -2.9110, -3.0031],\n        [-2.7078, -3.0814, -3.0198, -2.8580, -2.5065, -2.4220, -2.4222, -2.7285,\n         -3.0709, -2.6330, -2.4823, -2.7425, -2.4711, -3.0152, -2.8621],\n        [-2.6051, -2.8346, -3.0058, -2.8829, -2.6562, -2.5346, -2.4479, -2.7801,\n         -3.2070, -2.6199, -2.4534, -2.6809, -2.5417, -3.0330, -2.6712],\n        [-2.6725, -2.9667, -2.9198, -2.9955, -2.5889, -2.3924, -2.4630, -2.7588,\n         -3.0745, -2.5800, -2.4089, -2.7896, -2.4784, -3.0491, -2.8859],\n        [-2.5770, -3.0401, -2.8716, -2.8527, -2.5293, -2.5777, -2.5332, -2.8035,\n         -3.0880, -2.6367, -2.5506, -2.7004, -2.3085, -3.0203, -2.8894]],\n       grad_fn=&lt;LogSoftmaxBackward0&gt;)</pre> In\u00a0[151]: Copied! <pre>pred = m_test.predict_tags('\u043c\u0430\u043c\u0430 \u043c\u044b\u043b\u0430 \u0440\u0430\u043c\u0443 \u043d\u0430 \u0430\u0443\u0434\u0438 q5, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0445\u043e\u0440\u043e\u0448\u043e \u0440\u0443\u043b\u0438\u0442\u0441\u044f'.split())\n</pre> pred = m_test.predict_tags('\u043c\u0430\u043c\u0430 \u043c\u044b\u043b\u0430 \u0440\u0430\u043c\u0443 \u043d\u0430 \u0430\u0443\u0434\u0438 q5, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0445\u043e\u0440\u043e\u0448\u043e \u0440\u0443\u043b\u0438\u0442\u0441\u044f'.split()) In\u00a0[152]: Copied! <pre>m_test.decode_labels(pred)\n</pre> m_test.decode_labels(pred) Out[152]: <pre>['B-Whole',\n 'I-Appearance',\n 'B-Reliability',\n 'B-Whole',\n 'B-Reliability',\n 'B-Safety',\n 'B-Whole',\n 'B-Safety',\n 'I-Safety']</pre> In\u00a0[153]: Copied! <pre>m_test.encode_lebels(['I-Reliability', 'I-Whole'])\n</pre> m_test.encode_lebels(['I-Reliability', 'I-Whole']) Out[153]: <pre>tensor([11, 13])</pre> In\u00a0[114]: Copied! <pre>from collections import Counter\n\ntag_counter = Counter()\nfor _,tokens in training_data:\n    for token in tokens:\n        tag_counter[token]+=1\n\ntag_counter.most_common()\n</pre> from collections import Counter  tag_counter = Counter() for _,tokens in training_data:     for token in tokens:         tag_counter[token]+=1  tag_counter.most_common() Out[114]: <pre>[('Other', 23529),\n ('B-Driveability', 1188),\n ('B-Comfort', 1092),\n ('I-Driveability', 773),\n ('B-Reliability', 769),\n ('B-Whole', 768),\n ('I-Comfort', 480),\n ('I-Reliability', 457),\n ('B-Costs', 392),\n ('B-Appearance', 354),\n ('I-Whole', 310),\n ('I-Costs', 177),\n ('I-Appearance', 146),\n ('B-Safety', 87),\n ('I-Safety', 63)]</pre> In\u00a0[166]: Copied! <pre>sum((e[1] for e in tag_counter.items()))\n</pre> sum((e[1] for e in tag_counter.items())) Out[166]: <pre>30585</pre> In\u00a0[168]: Copied! <pre>class_weights = torch.ones(len(labels))\nclass_divs    = torch.ones(len(labels))\n\nfor tag, inv_weight in tag_counter.most_common():    \n    tag_idx             = m_test.labels_to_id[tag]\n    class_divs[tag_idx] = inv_weight  # division value\n\n# normalise division value\nnorm       = torch.norm(class_divs, p=2, dim=0).detach()\nclass_divs = class_divs.div(norm.expand_as(class_divs))\n\n# calculate class weights\nclass_weights /= class_divs\n\nprint(class_weights.detach())\n</pre> class_weights = torch.ones(len(labels)) class_divs    = torch.ones(len(labels))  for tag, inv_weight in tag_counter.most_common():         tag_idx             = m_test.labels_to_id[tag]     class_divs[tag_idx] = inv_weight  # division value  # normalise division value norm       = torch.norm(class_divs, p=2, dim=0).detach() class_divs = class_divs.div(norm.expand_as(class_divs))  # calculate class weights class_weights /= class_divs  print(class_weights.detach()) <pre>tensor([ 66.7812,  21.6488,  60.3075,  19.8994,  30.7419, 271.7303,  30.7819,\n        161.9215,  49.2511, 133.5623,  30.5828,  51.7298, 375.2466,  76.2598,\n          1.0047])\n</pre> In\u00a0[118]: Copied! <pre>device = torch.device('cpu')\n\nmodel         = RecTagger(EMBEDDING_DIM, HIDDEN_DIM, TAGSET_SIZE, 1, labels)\nmodel.to(device)\n\nloss_function = nn.NLLLoss(class_weights).to(device)\noptimizer     = optim.SGD(model.parameters(), lr=0.1)\n</pre> device = torch.device('cpu')  model         = RecTagger(EMBEDDING_DIM, HIDDEN_DIM, TAGSET_SIZE, 1, labels) model.to(device)  loss_function = nn.NLLLoss(class_weights).to(device) optimizer     = optim.SGD(model.parameters(), lr=0.1) <p>\u0423\u0431\u0435\u0440\u0435\u043c \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0438</p> In\u00a0[121]: Copied! <pre>shuffled = list(range(len(training_data)))\nrandom.shuffle(shuffled)\nshuffled[:10]\n</pre> shuffled = list(range(len(training_data))) random.shuffle(shuffled) shuffled[:10] Out[121]: <pre>[1903, 1202, 1833, 933, 224, 1747, 2126, 551, 43, 978]</pre> In\u00a0[122]: Copied! <pre>print(training_data[0][0])\nprint(training_data[0][1])\n</pre> print(training_data[0][0]) print(training_data[0][1]) <pre>('\u041d\u0435\u0434\u0430\u0432\u043d\u043e', '\u043a\u0443\u043f\u0438\u043b', '\u044d\u0442\u043e\u0442', '\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c')\n('Other', 'Other', 'Other', 'B-Whole')\n</pre> In\u00a0[170]: Copied! <pre>import matplotlib\n\nmatplotlib.rcParams['figure.figsize'] = (5, 3)\n</pre> import matplotlib  matplotlib.rcParams['figure.figsize'] = (5, 3) In\u00a0[174]: Copied! <pre>%%time\n\nliveplot = PlotLosses()\n\nfor epoch in range(10): \n    \n    shuffled = list(range(len(training_data)))\n    random.shuffle(shuffled)\n    losses = []\n    for i, ind in enumerate(shuffled):\n        \n        # preprocessing\n        words_seq = training_data[ind][0]\n        labels_seq = training_data[ind][1]\n        labels_tensor = model.encode_lebels(labels_seq, device)\n        \n        model.zero_grad()\n        \n        tag_scores = model(words_seq, device)\n        loss = loss_function(tag_scores, labels_tensor)\n        \n        loss.backward()\n        optimizer.step()\n        \n        loss_copy = loss.detach().to('cpu')\n        losses.append(loss_copy.item())\n        \n        \n        if(i % 100 == 0):\n            liveplot.update({'negative log likelihood loss': loss_copy})\n            liveplot.draw()\n</pre> %%time  liveplot = PlotLosses()  for epoch in range(10):           shuffled = list(range(len(training_data)))     random.shuffle(shuffled)     losses = []     for i, ind in enumerate(shuffled):                  # preprocessing         words_seq = training_data[ind][0]         labels_seq = training_data[ind][1]         labels_tensor = model.encode_lebels(labels_seq, device)                  model.zero_grad()                  tag_scores = model(words_seq, device)         loss = loss_function(tag_scores, labels_tensor)                  loss.backward()         optimizer.step()                  loss_copy = loss.detach().to('cpu')         losses.append(loss_copy.item())                           if(i % 100 == 0):             liveplot.update({'negative log likelihood loss': loss_copy})             liveplot.draw()  <pre>negative log likelihood loss\n\tnegative log likelihood loss \t (min:    0.000, max:    0.766, cur:    0.018)\nCPU times: user 7min 45s, sys: 1min 2s, total: 8min 48s\nWall time: 4min 22s\n</pre> In\u00a0[175]: Copied! <pre>sum(losses) / len(losses)\n</pre> sum(losses) / len(losses) Out[175]: <pre>0.061214469474302924</pre> In\u00a0[154]: Copied! <pre>%%time\n\ntest_losses = []\nwith torch.no_grad():\n    for ind in range(len(test_data)):\n        words_seq = test_data[ind][0]\n        labels_seq = test_data[ind][1]\n\n        labels_tensor = model.encode_lebels(labels_seq, device)\n\n        tag_scores = model(words_seq, device)\n        loss = loss_function(tag_scores, labels_tensor)\n\n        loss_copy = loss.detach().to('cpu')\n        test_losses.append(loss_copy.item())\n        \nsum(test_losses) / len(test_losses)\n</pre> %%time  test_losses = [] with torch.no_grad():     for ind in range(len(test_data)):         words_seq = test_data[ind][0]         labels_seq = test_data[ind][1]          labels_tensor = model.encode_lebels(labels_seq, device)          tag_scores = model(words_seq, device)         loss = loss_function(tag_scores, labels_tensor)          loss_copy = loss.detach().to('cpu')         test_losses.append(loss_copy.item())          sum(test_losses) / len(test_losses) <pre>CPU times: user 5.85 s, sys: 7.75 ms, total: 5.86 s\nWall time: 2.93 s\n</pre> Out[154]: <pre>1.152625141903092</pre> In\u00a0[128]: Copied! <pre>model.to('cpu')\ntorch.save(model.state_dict(), './model1.pth')\n</pre> model.to('cpu') torch.save(model.state_dict(), './model1.pth') In\u00a0[129]: Copied! <pre>model1 = RecTagger(EMBEDDING_DIM, \n                   HIDDEN_DIM, TAGSET_SIZE, \n                   1, \n                   labels)\n\nmodel1.load_state_dict(torch.load('./model1.pth'))\nmodel1.eval()\n</pre> model1 = RecTagger(EMBEDDING_DIM,                     HIDDEN_DIM, TAGSET_SIZE,                     1,                     labels)  model1.load_state_dict(torch.load('./model1.pth')) model1.eval() Out[129]: <pre>RecTagger(\n  (rec): GRU(300, 12, batch_first=True, bidirectional=True)\n  (vect_projection): Linear(in_features=300, out_features=12, bias=True)\n  (hidden2tag): Linear(in_features=36, out_features=15, bias=True)\n)</pre> In\u00a0[130]: Copied! <pre># \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u0442\u044d\u0433\u0438 \u0438 \u0438\u0445 \u0434\u0435\u043a\u043e\u0434\u0438\u0440\u0443\u0435\u043c\ndef predict_tags(model, words_seq):\n    with torch.no_grad():\n        tags = model.predict_tags(words_seq)\n\n    return model.decode_labels(tags)\n</pre> # \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u0442\u044d\u0433\u0438 \u0438 \u0438\u0445 \u0434\u0435\u043a\u043e\u0434\u0438\u0440\u0443\u0435\u043c def predict_tags(model, words_seq):     with torch.no_grad():         tags = model.predict_tags(words_seq)      return model.decode_labels(tags) In\u00a0[134]: Copied! <pre>def generate_markup(tokens,tags):\n    \n    mapper = lambda tag: tag[2:] if tag!='Other' else tag\n    tags  = [mapper(tag) for tag in tags]\n    text  = ' '.join(tokens)\n    spans = []\n\n    start,end,tag = 0,len(tokens[0]),tags[0]\n\n    for word, ttag in zip(tokens[1:], tags[1:]):\n\n        if tag == ttag:\n            end  += 1+len(word)\n\n        else:\n            span  = (start, end, tag)\n            spans.append(span)\n\n            start = 1+end\n            end  += 1+len(word)\n            tag   = ttag\n\n    span  = (start, end, tag)\n    spans.append(span)\n\n    return text, spans\n</pre> def generate_markup(tokens,tags):          mapper = lambda tag: tag[2:] if tag!='Other' else tag     tags  = [mapper(tag) for tag in tags]     text  = ' '.join(tokens)     spans = []      start,end,tag = 0,len(tokens[0]),tags[0]      for word, ttag in zip(tokens[1:], tags[1:]):          if tag == ttag:             end  += 1+len(word)          else:             span  = (start, end, tag)             spans.append(span)              start = 1+end             end  += 1+len(word)             tag   = ttag      span  = (start, end, tag)     spans.append(span)      return text, spans In\u00a0[158]: Copied! <pre>test_text, test_tags = training_data[1211]\ntext, spans = generate_markup(test_text, test_tags)\nshow_markup(text, spans)\n</pre> test_text, test_tags = training_data[1211] text, spans = generate_markup(test_text, test_tags) show_markup(text, spans) \u0417\u0430 \u043f\u0435\u0440\u0432\u044b\u0445 \u0442\u0440\u0438 \u0433\u043e\u0434\u0430 \u043d\u0435 \u0431\u044b\u043b\u043eOther \u043d\u0438 \u043e\u0434\u043d\u043e\u0439 \u043f\u043e\u043b\u043e\u043c\u043a\u0438Reliability , \u043f\u043e\u0442\u043e\u043c \u0441\u0442\u0430\u043b\u0438 \u043f\u0440\u043e\u044f\u0432\u043b\u044f\u0442\u044c\u0441\u044f \u043c\u0435\u043b\u043a\u0438\u0435Other \u043d\u0435\u0438\u0441\u043f\u0440\u0430\u0432\u043d\u043e\u0441\u0442\u0438Reliability :Other \u043f\u043e\u0434\u0448\u0438\u043f\u043d\u0438\u043a \u043d\u0430 \u043f\u043e\u043b\u0443\u043e\u0441\u0438Reliability ,Other \u043b\u0430\u043c\u043f\u043e\u0447\u043a\u0438 \u0434\u0430\u043b\u044c\u043d\u0435\u0433\u043e \u0431\u043b\u0438\u0436\u043d\u0435\u0433\u043e \u0441\u0432\u0435\u0442\u0430Reliability ,Other \u043a\u0440\u0435\u0441\u0442\u043e\u0432\u0438\u043d\u044bReliability ,Other \u0430\u043a\u043a\u0443\u043c\u0443\u043b\u044f\u0442\u043e\u0440Reliability ,Other \u0441\u0432\u0435\u0447\u0438Reliability In\u00a0[181]: Copied! <pre>for i,j in zip(test_text,test_tags):\n    print(i,j)\n</pre> for i,j in zip(test_text,test_tags):     print(i,j) <pre>\u0417\u0430 Other\n\u043f\u0435\u0440\u0432\u044b\u0445 Other\n\u0442\u0440\u0438 Other\n\u0433\u043e\u0434\u0430 Other\n\u043d\u0435 Other\n\u0431\u044b\u043b\u043e Other\n\u043d\u0438 B-Reliability\n\u043e\u0434\u043d\u043e\u0439 I-Reliability\n\u043f\u043e\u043b\u043e\u043c\u043a\u0438 I-Reliability\n, Other\n\u043f\u043e\u0442\u043e\u043c Other\n\u0441\u0442\u0430\u043b\u0438 Other\n\u043f\u0440\u043e\u044f\u0432\u043b\u044f\u0442\u044c\u0441\u044f Other\n\u043c\u0435\u043b\u043a\u0438\u0435 Other\n\u043d\u0435\u0438\u0441\u043f\u0440\u0430\u0432\u043d\u043e\u0441\u0442\u0438 B-Reliability\n: Other\n\u043f\u043e\u0434\u0448\u0438\u043f\u043d\u0438\u043a B-Reliability\n\u043d\u0430 I-Reliability\n\u043f\u043e\u043b\u0443\u043e\u0441\u0438 I-Reliability\n, Other\n\u043b\u0430\u043c\u043f\u043e\u0447\u043a\u0438 B-Reliability\n\u0434\u0430\u043b\u044c\u043d\u0435\u0433\u043e I-Reliability\n\u0431\u043b\u0438\u0436\u043d\u0435\u0433\u043e I-Reliability\n\u0441\u0432\u0435\u0442\u0430 I-Reliability\n, Other\n\u043a\u0440\u0435\u0441\u0442\u043e\u0432\u0438\u043d\u044b B-Reliability\n, Other\n\u0430\u043a\u043a\u0443\u043c\u0443\u043b\u044f\u0442\u043e\u0440 B-Reliability\n, Other\n\u0441\u0432\u0435\u0447\u0438 B-Reliability\n</pre> In\u00a0[178]: Copied! <pre>for i in range(0,3):\n    \n    print(f'\u041f\u0440\u0438\u043c\u0435\u0440 {i}')\n\n    recipe, tags = test_data[np.random.randint(0,1000)]\n\n    tags_pred    = predict_tags(model1, recipe)\n\n    print('\\n\u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u0442\u044d\u0433\u0438:')\n    text, spans = generate_markup(recipe, tags)\n    show_markup(text, spans)\n\n    print('\\n\u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0442\u044d\u0433\u0438:')\n    text, spans = generate_markup(recipe, tags_pred)\n\n    show_markup(text, spans)\n    print()\n</pre> for i in range(0,3):          print(f'\u041f\u0440\u0438\u043c\u0435\u0440 {i}')      recipe, tags = test_data[np.random.randint(0,1000)]      tags_pred    = predict_tags(model1, recipe)      print('\\n\u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u0442\u044d\u0433\u0438:')     text, spans = generate_markup(recipe, tags)     show_markup(text, spans)      print('\\n\u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0442\u044d\u0433\u0438:')     text, spans = generate_markup(recipe, tags_pred)      show_markup(text, spans)     print() <pre>\u041f\u0440\u0438\u043c\u0435\u0440 0\n\n\u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u0442\u044d\u0433\u0438:\n</pre> \u0417\u0430\u043f\u043b\u0430\u0442\u0438\u043b 1250Other \u0433\u0440\u0438\u0432\u0435\u043dCosts \u0437\u0430Other \u0440\u0435\u043c\u043e\u043d\u0442Costs <pre>\n\u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0442\u044d\u0433\u0438:\n</pre> \u0417\u0430\u043f\u043b\u0430\u0442\u0438\u043bCosts 1250Other \u0433\u0440\u0438\u0432\u0435\u043dCosts \u0437\u0430 \u0440\u0435\u043c\u043e\u043d\u0442Reliability <pre>\n\u041f\u0440\u0438\u043c\u0435\u0440 1\n\n\u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u0442\u044d\u0433\u0438:\n</pre> \u0415\u0449\u0435 \u043a\u043e\u043d\u0435\u0447\u043d\u043eOther \u0434\u0432\u0438\u0436\u043e\u043aDriveability \u0441\u043b\u0430\u0431\u043e\u0432\u0430\u0442 \u0438 \u043d\u0430 \u043f\u043b\u043e\u0445\u0438\u0445 \u0434\u043e\u0440\u043e\u0433\u0430\u0445 \u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u0445\u043e\u0447\u0435\u0442\u0441\u044fOther \u0433\u043e\u043d\u044f\u0442\u044c \u043d\u0430 \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044f\u0445Driveability , \u0445\u043e\u0442\u044fOther \u043c\u0430\u0448\u0438\u043d\u0443Driveability \u043c\u043e\u0436\u043d\u043eOther \u0440\u0430\u0437\u043e\u0433\u043d\u0430\u0442\u044cDriveability \u0438 \u0434\u043e 190 \u043a\u043c /\u0447\u0430\u0441 \u043f\u043e \u043b\u0438\u0447\u043d\u043e\u043c\u0443 \u043e\u043f\u044b\u0442\u0443 .Other <pre>\n\u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0442\u044d\u0433\u0438:\n</pre> \u0415\u0449\u0435Other \u043a\u043e\u043d\u0435\u0447\u043d\u043e \u0434\u0432\u0438\u0436\u043e\u043a \u0441\u043b\u0430\u0431\u043e\u0432\u0430\u0442 \u0438 \u043d\u0430Driveability \u043f\u043b\u043e\u0445\u0438\u0445Other \u0434\u043e\u0440\u043e\u0433\u0430\u0445 \u043d\u0435Driveability \u043e\u0447\u0435\u043d\u044c \u0445\u043e\u0447\u0435\u0442\u0441\u044fOther \u0433\u043e\u043d\u044f\u0442\u044c \u043d\u0430 \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044f\u0445Driveability ,Other \u0445\u043e\u0442\u044f \u043c\u0430\u0448\u0438\u043d\u0443Driveability \u043c\u043e\u0436\u043d\u043eOther \u0440\u0430\u0437\u043e\u0433\u043d\u0430\u0442\u044cDriveability \u0438 \u0434\u043e 190 \u043a\u043c /\u0447\u0430\u0441 \u043f\u043e \u043b\u0438\u0447\u043d\u043e\u043c\u0443 \u043e\u043f\u044b\u0442\u0443 .Other <pre>\n\u041f\u0440\u0438\u043c\u0435\u0440 2\n\n\u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u0442\u044d\u0433\u0438:\n</pre> \u041c\u0430\u0448\u0438\u043d\u0430Whole \u0441\u0442\u043e\u044f\u0449\u0430\u044f , \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u044e \u0432\u0441\u0435\u043c \u0430\u043a\u0442\u0438\u0432\u043d\u044b\u043c \u043b\u044e\u0434\u044f\u043c , \u043a\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u0443\u0435\u0442 \u043e\u0445\u043e\u0442\u0430 , \u0440\u044b\u0431\u0430\u043b\u043a\u0430 , \u043a\u0442\u043e \u043b\u044e\u0431\u0438\u0442Other \u0435\u0437\u0434\u0438\u0442\u044c \u043d\u0430 \u043f\u0440\u0438\u0440\u043e\u0434\u0443Driveability \u043f\u043e\u0434\u0430\u043b\u044c\u0448\u0435 \u043e\u0442 \u0446\u0438\u0432\u0438\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u0440\u043e\u0441\u0442\u043e \u043e\u0442\u0434\u043e\u0445\u043d\u0443\u0442\u044c .Other <pre>\n\u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0442\u044d\u0433\u0438:\n</pre> \u041c\u0430\u0448\u0438\u043d\u0430Whole \u0441\u0442\u043e\u044f\u0449\u0430\u044f , \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u044e \u0432\u0441\u0435\u043c \u0430\u043a\u0442\u0438\u0432\u043d\u044b\u043c \u043b\u044e\u0434\u044f\u043c , \u043a\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u0443\u0435\u0442 \u043e\u0445\u043e\u0442\u0430 , \u0440\u044b\u0431\u0430\u043b\u043a\u0430 , \u043a\u0442\u043e \u043b\u044e\u0431\u0438\u0442 \u0435\u0437\u0434\u0438\u0442\u044c \u043d\u0430 \u043f\u0440\u0438\u0440\u043e\u0434\u0443 \u043f\u043e\u0434\u0430\u043b\u044c\u0448\u0435 \u043e\u0442 \u0446\u0438\u0432\u0438\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u0440\u043e\u0441\u0442\u043e \u043e\u0442\u0434\u043e\u0445\u043d\u0443\u0442\u044c .Other <pre>\n</pre>"},{"location":"portfolio/course_nlp/gru_NER.html","title":"\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u041c\u043e\u0434\u0435\u043b\u044c \u0420\u0430\u0441\u043f\u043e\u0437\u043d\u043e\u0432\u0430\u043d\u0438\u044f \u0418\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0421\u0443\u0449\u043d\u043e\u0441\u0442\u0435\u0439\u00b6","text":""},{"location":"portfolio/course_nlp/gru_NER.html","title":"\u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/course_nlp/gru_NER.html#ner","title":"<code>NER</code> \u043e\u0442\u0437\u044b\u0432\u043e\u0432 \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u0435\u0439\u00b6","text":"<p>\u0412 \u044d\u0442\u043e\u043c \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435 \u043c\u044b \u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0438\u043c \u0442\u0435\u043c\u0443 \u0437\u0430\u0434\u0430\u0447\u0443 \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0432\u0430\u043d\u0438\u044f \u0438\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u0435\u0439 <code>NER</code></p> <ul> <li>\u041c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0440\u0430\u0437\u043c\u0435\u0447\u0435\u043d\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043e\u0442\u0437\u0432\u043e\u0432 \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u0435\u0439, \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u043d\u044b\u0435 \u0432 \u0443\u0434\u043e\u0431\u043d\u043e\u043c \u0444\u043e\u0440\u043c\u0430\u0442\u0435 <code>xml</code>.</li> <li>\u0412 \u043f\u0440\u043e\u0448\u043b\u043e\u043c \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435 \u043c\u044b \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u0438\u0441\u044c \u0433\u043e\u0442\u043e\u0432\u044b\u043c \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0430\u043c\u0438 \u0434\u043b\u044f <code>NER</code>.</li> <li>\u0412 \u0434\u0430\u043d\u043d\u043e\u043c \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u043e\u0431\u0443\u0447\u0430\u0442\u044c \u0441\u0432\u043e\u044e \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u0443\u044e <code>NER</code> \u043c\u043e\u0434\u0435\u043b\u044c (GRU) \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u043e\u0432\u0430\u043d\u0438\u044f \u0438\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u0435\u0439 \u043f\u043e \u0442\u0435\u043c\u0430\u043c \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u043c \u0441 \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044f\u043c\u0438</li> <li>\u0423 \u043d\u0430\u0441 \u0434\u0432\u0435 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0438 <code>sentiment</code> \u0438 <code>category</code>, \u043d\u043e \u0434\u043b\u044f \u0442\u0435\u043a\u0443\u0449\u0435\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 NER, \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u043c\u0435\u0442\u043a\u043e\u0439 <code>category</code></li> </ul>"},{"location":"portfolio/course_nlp/gru_NER.html","title":"\u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438\u00b6","text":""},{"location":"portfolio/course_nlp/gru_NER.html#pipeline","title":"PIPELINE: \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":""},{"location":"portfolio/course_nlp/gru_NER.html#xml","title":"\u0427\u0442\u0435\u043d\u0438\u0435 <code>XML</code> \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<p>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0432\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 <code>dlnlputils</code> \u0438\u0437 github</p> <ul> <li>\u0414\u043b\u044f \u0447\u0442\u0435\u043d\u0438\u044f <code>xml</code> \u0434\u0430\u043d\u043d\u044b\u0445 \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044f \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u043e\u0439 <code>lxml</code></li> <li><code>ipymarkup</code> \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 <code>natasha</code> \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0434\u043b\u044f \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438</li> </ul>"},{"location":"portfolio/course_nlp/gru_NER.html#xml","title":"\u041f\u0430\u0440\u0441\u0438\u043c <code>XML</code>\u00b6","text":"<ul> <li>\u0414\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u044b \u0441 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u043e\u0439 \u0430\u0441\u043f\u0435\u043a\u0442\u043e\u0432 \u043f\u0430\u0440\u0441\u0438\u043c \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e <code>parse_xml_aspec</code></li> <li>\u041d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u043a\u043e\u0440\u0442\u0435\u0436\u043e\u0432 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u043e\u0432 \u0432 <code>xml</code> (\u0442\u0435\u043a\u0441\u0442 \u0438 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0438)</li> </ul>"},{"location":"portfolio/course_nlp/gru_NER.html#preprocessing-pipeline","title":"\u0412\u044b\u0431\u0435\u0440\u0435\u043c \u0442\u0435\u043a\u0441\u0442, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0431\u0443\u0434\u0435\u043c \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0442\u044c preprocessing pipeline\u00b6","text":""},{"location":"portfolio/course_nlp/gru_NER.html#ner","title":"\u0420\u0430\u0437\u043c\u0435\u0442\u043a\u0430 \u0434\u043b\u044f NER\u00b6","text":"<p>\u0422\u044d\u0433\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f NER - aspect</p>"},{"location":"portfolio/course_nlp/gru_NER.html#pipeline-bio-","title":"PIPELINE: BIO-\u0442\u044d\u0433\u0438\u00b6","text":"<ul> <li>\u0414\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c BIO \u0442\u044d\u0433\u043e\u0432\u0443\u044e \u0441\u0438\u0441\u0442\u0435\u043c\u0443 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0438, ie. <code>beginner</code>, <code>inner</code> \u0438 <code>outer</code></li> <li>\u041f\u043e \u043d\u0430\u0447\u0430\u043b\u0443 \u043e\u0442\u043c\u0435\u0442\u0438\u043c \u0442\u043e \u0447\u0442\u043e \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0442\u044d\u0433\u043e\u043c \"\u0432\u0441\u0435 \u043f\u0440\u043e\u0447\u0438\u0435\", \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c span \u0434\u043b\u044f \u0432\u0441\u0435 \u0447\u0442\u043e \u043d\u0435 \u0431\u044b\u043b\u043e \u0440\u0430\u0437\u043c\u0435\u0447\u0435\u043d\u043d\u044b\u043c \u0441 \u043f\u043c\u043e\u0449\u044c\u044e <code>fill_gaps</code></li> </ul>"},{"location":"portfolio/course_nlp/gru_NER.html#pipeline","title":"Pipeline: \u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/course_nlp/gru_NER.html","title":"\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435\u00b6","text":"<ul> <li>\u0414\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u043f\u0435\u0440\u0435\u0434\u0435\u043b\u0430\u0442\u044c \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443 (spans)</li> <li>\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430 \u043d\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f, \u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439 - \u043d\u0430 \u0441\u043b\u043e\u0432\u0430.</li> </ul>"},{"location":"portfolio/course_nlp/gru_NER.html","title":"\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c\u0441\u044f \u0441 \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0442\u043e\u0440\u043e\u043c\u00b6","text":"<p>\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c\u0441\u044f \u0441 \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0442\u043e\u0440\u043e\u043c, \u0434\u043b\u044f \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0442\u043e\u0440\u0430 \u0432\u043e\u0437\u043c\u0435\u043c <code>RegexpTokenizer</code></p>"},{"location":"portfolio/course_nlp/gru_NER.html#pipeline","title":"\u0413\u0440\u0443\u043f\u043f\u0438\u0440\u0443\u0435\u043c \u0432\u0441\u0435 \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u044b pipeline\u00b6","text":"<p>\u041d\u0430 \u043e\u0434\u043d\u043e\u043c \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u0432\u0435\u0441\u044c \u043f\u0430\u0439\u043f\u043b\u0430\u0438\u043d:</p> <ul> <li>\u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442 \u043d\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435 <code>sentence_splitter</code></li> <li>\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043f\u0440\u043e\u0447\u0438\u0435 \u0442\u044d\u0433\u0438 <code>fill_gaps</code></li> <li>\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043e\u0431\u0449\u0438\u0435 BIO \u0442\u044d\u0433\u0438 (\u043c\u0435\u0442\u043a\u0438 \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438)</li> </ul>"},{"location":"portfolio/course_nlp/gru_NER.html#pipeline","title":"\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c pipeline\u00b6","text":""},{"location":"portfolio/course_nlp/gru_NER.html#pipeline","title":"\u0413\u043e\u0442\u043e\u0432\u044b\u0439 pipeline\u00b6","text":"<p>\u0422\u0435\u043f\u0435\u0440\u044c \u043f\u0440\u043e\u0439\u0434\u0435\u043c \u0447\u0435\u0440\u0435\u0437 \u0432\u0441\u044e \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443 \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438</p> <p>\u041f\u0430\u0439\u043f\u043b\u0430\u0438\u043d \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 <code>xml</code> \u0442\u0435\u043a\u0441\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f NER:</p> <ul> <li><code>parse_xml_aspect</code> - \u043f\u0430\u0440\u0441\u0438\u043c <code>xml</code> \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043a\u043e\u0440\u0434\u0435\u0436\u044b \u0442\u0435\u043a\u0441\u0442\u0430 \u0438 \u043c\u0435\u0442\u043e\u043a \u0432 \u0442\u0435\u043a\u0441\u0442\u0435</li> <li><code>sentence_splitter</code> - \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442 \u043d\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0438 \u0438 \u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u043c \u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u043c\u0435\u0442\u043e\u043a</li> <li><code>fill_gaps</code> - \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u043c\u0435\u0442\u043a\u0438 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0445 \u0442\u043e\u043a\u0435\u043d\u043e\u0432</li> <li><code>extract_BIO_tagged_tokens</code> - \u0441\u043e\u0437\u0434\u0430\u0435\u043c BIO \u0442\u044d\u0433\u0438 \u0434\u043b\u044f \u0442\u043e\u043a\u0435\u043d\u043e\u0432</li> </ul>"},{"location":"portfolio/course_nlp/gru_NER.html","title":"\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\u00b6","text":"<p>\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043e\u044e\u0443\u0447\u0430\u044e\u0449\u0438\u044e \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 \u0433\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u0430\u0439\u043f\u043b\u0430\u0438\u043d \u0438 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0441\u043f\u0438\u0441\u043a\u0438 \u0442\u043e\u043a\u0435\u043d\u043e\u0432 \u0438 \u0435\u0435 \u043c\u0435\u0442\u043a\u0430</p>"},{"location":"portfolio/course_nlp/gru_NER.html","title":"\u0421\u043b\u043e\u0432\u0430\u0440\u044c \u0438 \u041c\u0435\u0442\u043a\u0430\u00b6","text":"<p>\u0421\u043b\u043e\u0432\u0430\u0440\u044c \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u0439 \u0432\u0441\u0435 \u0442\u043e\u043a\u0435\u043d\u044b (\u0441\u043b\u043e\u0432\u0430) \u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0430\u0445 \u0438 \u0432\u0441\u0435 \u043c\u0435\u0442\u043a\u0438</p>"},{"location":"portfolio/course_nlp/gru_NER.html#pipeline-embedding","title":"Pipeline: Embedding\u00b6","text":""},{"location":"portfolio/course_nlp/gru_NER.html#fasttext","title":"\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c fasttext \u0432\u0435\u043a\u0442\u043e\u0440\u0430\u00b6","text":"<ul> <li>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u043c\u0438 \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u044b\u043c\u0438 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f\u043c\u0438 \u0441\u043b\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0438 (araneum_none_fasttextcbow_300_5_2018)</li> <li>\u0410\u043b\u0433\u043e\u0440\u0438\u0442\u043c fasttext \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0439 \u043d\u0430 \u043a\u043e\u0440\u043f\u0443\u0441\u0435 \u0422\u0430\u0439\u0433\u0430, \u0441\u043c\u043e\u0442\u0440\u0438\u0442\u0435 \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u043e\u0441\u0442\u0438 \u043d\u0430 \u0441\u0430\u0439\u0442\u0435: https://rusvectores.org/ru/models/</li> </ul>"},{"location":"portfolio/course_nlp/gru_NER.html","title":"\u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0430 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432\u00b6","text":"<p>\u0412\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0432\u0435\u043a\u0442\u043e\u0440 \u0434\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0442\u043e\u043a\u0435\u043d\u0430, \u0435\u0441\u043b\u0438 \u0435\u0433\u043e \u043d\u0435\u0442 \u0432 \u0441\u043b\u043e\u0432\u0430\u0440\u0435, \u043f\u0440\u043e\u0445\u043e\u0434\u0438\u043c \u043f\u043e \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u044e \u0442\u043e\u043a\u0435\u043d\u0430 \u0438 \u0432\u044b\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043d-\u0433\u0440\u0430\u043c\u043c\u043e\u0432\u044b\u0435 \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u0438 \u0438\u0445 \u0443\u0441\u0440\u0435\u0434\u043d\u044f\u0435\u043c</p>"},{"location":"portfolio/course_nlp/gru_NER.html","title":"\u041c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/course_nlp/gru_NER.html#ner","title":"\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c NER \u043c\u043e\u0434\u0435\u043b\u044c\u00b6","text":"<p>\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c NER \u043c\u043e\u0434\u0435\u043b\u044c \u0438 \u043f\u0440\u043e\u0442\u0435\u0441\u0442\u0438\u0440\u0443\u0435\u043c \u0447\u0442\u043e\u0431\u044b \u0443\u0431\u0435\u0434\u0438\u0442\u0441\u044f \u0447\u0442\u043e \u0432\u0441\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442</p>"},{"location":"portfolio/course_nlp/gru_NER.html","title":"\u0412\u0437\u0432\u0435\u0448\u0435\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c\u00b6","text":"<ul> <li>\u0412 \u0437\u0430\u0434\u0430\u0447\u0438 NER \u0443 \u043d\u0430\u0441 \u043f\u043e\u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435 \u0434\u0438\u0441\u0431\u0430\u043b\u0430\u043d\u0441 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 (\u043d \u043d\u0430\u0441 \u043f\u043e\u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043e\u0447\u0435\u043d\u044c \u043c\u043d\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 <code>other</code>)</li> <li>\u041c\u044b \u043c\u043e\u0436\u0435\u043c \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0441\u044f \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u0447\u043d\u044b\u043c\u0438 \u0432\u0435\u0441\u0430\u043c\u0438 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430, \u0432 \u0434\u0430\u043d\u043d\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u0432\u0435\u0441\u0430 \u0443\u0441\u043b\u043e\u0432\u043d\u043e \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u043f\u0440\u043e\u043f\u043e\u0440\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b \u0447\u0430\u0441\u0442\u043e\u0442\u0435 \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u0438 \u0438\u0445 \u043f\u0435\u0440\u0435\u0434\u0430\u0434\u0438\u043c \u0432 \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u044e \u043e\u0446\u0435\u043d\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 <code>loss_function</code></li> </ul>"},{"location":"portfolio/course_nlp/gru_NER.html","title":"\u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u041c\u043e\u0434\u0435\u043b\u044c\u00b6","text":"<p>\u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c, \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c <code>NNLLoss</code> \u0438 \u043f\u043e\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440 <code>SGD</code></p>"},{"location":"portfolio/course_nlp/gru_NER.html","title":"\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\u00b6","text":"<p>\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 10 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445</p>"},{"location":"portfolio/course_nlp/gru_NER.html","title":"\u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u043e\u0431\u043e\u0431\u0449\u0430\u044e\u0449\u0438\u044e \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c\u00b6","text":"<p>\u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u0441\u0440\u0435\u0434\u043d\u0438\u0439 loss \u043f\u043e batch \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435</p>"},{"location":"portfolio/course_nlp/gru_NER.html","title":"\u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u041c\u043e\u0434\u0435\u043b\u044c\u00b6","text":"<p>\u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u0443\u0435\u043c \u043a\u0430\u043a \u043e\u043d\u0430 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u043d\u0430 \u043d\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445</p>"},{"location":"portfolio/course_nlp/gru_NER.html#ner","title":"\u0422\u0435\u0441\u0442\u0438\u0440\u0443\u0435\u043c NER \u0442\u044d\u0433\u0435\u0440\u0430\u00b6","text":""},{"location":"portfolio/course_nlp/gru_NER.html","title":"\u0422\u0435\u0441\u0442\u0438\u0440\u0443\u0435\u043c \u041c\u043e\u0434\u0435\u043b\u044c\u00b6","text":"<p>\u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u0443\u0435\u043c NER \u043a\u0430\u043a \u043e\u043d\u0430 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u043d\u0430 \u043d\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445</p>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html","title":"Khazah news sentiment","text":"In\u00a0[2]: Copied! <pre>%matplotlib inline\n! pip install pymorphy2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\nfrom sklearn.metrics import *\nimport warnings\nwarnings.filterwarnings(\"ignore\") \n</pre> %matplotlib inline ! pip install pymorphy2 import matplotlib.pyplot as plt import numpy as np import pandas as pd import json from tqdm import tqdm from sklearn.metrics import * import warnings warnings.filterwarnings(\"ignore\")  <pre>Collecting pymorphy2\n  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 55.5/55.5 kB 3.2 MB/s eta 0:00:00\nCollecting dawg-python&gt;=0.7.1 (from pymorphy2)\n  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\nCollecting pymorphy2-dicts-ru&lt;3.0,&gt;=2.4 (from pymorphy2)\n  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8.2/8.2 MB 59.8 MB/s eta 0:00:0000:0100:01\nRequirement already satisfied: docopt&gt;=0.6 in /opt/conda/lib/python3.10/site-packages (from pymorphy2) (0.6.2)\nInstalling collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\nSuccessfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n</pre> <pre>/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}\"\n</pre> In\u00a0[3]: Copied! <pre>import pandas as pd\nimport nltk  \nimport re\nimport json\n\nwith open('/kaggle/input/khazah-news/train.json', encoding = 'utf-8') as json_file:\n    data = json.load(json_file)\n\ndf = pd.read_json('/kaggle/input/khazah-news/train.json')\ndf.tail()\n</pre> import pandas as pd import nltk   import re import json  with open('/kaggle/input/khazah-news/train.json', encoding = 'utf-8') as json_file:     data = json.load(json_file)  df = pd.read_json('/kaggle/input/khazah-news/train.json') df.tail() Out[3]: text id sentiment 8258 \u041a\u0430\u043a \u043c\u044b \u043f\u0438\u0441\u0430\u043b\u0438 \u0435\u0449\u0435 \u0432\u0435\u0441\u043d\u043e\u0439, \u0434\u043b\u044f \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u044f \u0441\u0431\u044b\u0442\u0430... 10312 positive 8259 \u041d\u043e \u043c\u043e\u043b\u043e\u0434\u043e\u0439 \u043c\u0438\u043d\u0438\u0441\u0442\u0440 \u043d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0439 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0438 \u0411\u0438\u0448\u0438... 10313 negative 8260 \\n \\n\u0412 \u0415\u041d\u041f\u0424 \u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d \u043d\u043e\u0432\u044b\u0439 \u043f\u0440\u0435\u0434\u0441\u0435\u0434\u0430\u0442\u0435\u043b\u044c \u043f\u0440\u0430\u0432\u043b\u0435... 10314 neutral 8261 \u0412 \u0410\u043b\u043c\u0430\u0442\u044b \u0443 \u043e\u0442\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0431\u0430\u043d\u043a\u0430 \u043f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u043e \u043d\u0430\u043f\u0430\u0434\u0435\u043d\u0438\u0435... 10315 negative 8262 \u041d\u041f\u041f \u0420\u041a \u00ab\u0410\u0442\u0430\u043c\u0435\u043a\u0435\u043d\u00bb \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u0442 \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u041d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c... 10316 neutral In\u00a0[4]: Copied! <pre>df['sentiment'].unique()\n</pre> df['sentiment'].unique() Out[4]: <pre>array(['negative', 'positive', 'neutral'], dtype=object)</pre> <ul> <li>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432. \u0412\u0438\u0434\u0438\u043c, \u0447\u0442\u043e \u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0432\u0443\u0435\u0442 \u0441\u0438\u043b\u044c\u043d\u044b\u0439 \u0434\u0438\u0441\u0431\u0430\u043b\u0430\u043d\u0441.</li> <li>\u041f\u043e\u0447\u0442\u0438 \u043f\u043e\u043b\u043e\u0432\u0438\u043d\u0430 \u0432\u0441\u0435\u0445 \u043d\u043e\u0432\u043e\u0441\u0442\u0435\u0439 - \u043d\u0435\u0439\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0435, \u0430 \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u043d\u043e\u0432\u043e\u0441\u0442\u0435\u0439 \u043c\u0435\u043d\u044c\u0448\u0435 \u0432\u0441\u0435\u0433\u043e.</li> </ul> In\u00a0[5]: Copied! <pre>df['sentiment'].value_counts()\n</pre> df['sentiment'].value_counts() Out[5]: <pre>neutral     4034\npositive    2795\nnegative    1434\nName: sentiment, dtype: int64</pre> In\u00a0[6]: Copied! <pre># \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432 \u0434\u043b\u044f \u0440\u0443\u0441\u0441\u043a\u043e\u0433\u043e\nnltk.download('stopwords')\nstop_words = nltk.corpus.stopwords.words('russian')\nprint(f'C\u0442\u043e\u043f \u0441\u043b\u043e\u0432\u0430: {len(stop_words)}')\nprint(stop_words)\n</pre> # \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432 \u0434\u043b\u044f \u0440\u0443\u0441\u0441\u043a\u043e\u0433\u043e nltk.download('stopwords') stop_words = nltk.corpus.stopwords.words('russian') print(f'C\u0442\u043e\u043f \u0441\u043b\u043e\u0432\u0430: {len(stop_words)}') print(stop_words) <pre>[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nC\u0442\u043e\u043f \u0441\u043b\u043e\u0432\u0430: 151\n['\u0438', '\u0432', '\u0432\u043e', '\u043d\u0435', '\u0447\u0442\u043e', '\u043e\u043d', '\u043d\u0430', '\u044f', '\u0441', '\u0441\u043e', '\u043a\u0430\u043a', '\u0430', '\u0442\u043e', '\u0432\u0441\u0435', '\u043e\u043d\u0430', '\u0442\u0430\u043a', '\u0435\u0433\u043e', '\u043d\u043e', '\u0434\u0430', '\u0442\u044b', '\u043a', '\u0443', '\u0436\u0435', '\u0432\u044b', '\u0437\u0430', '\u0431\u044b', '\u043f\u043e', '\u0442\u043e\u043b\u044c\u043a\u043e', '\u0435\u0435', '\u043c\u043d\u0435', '\u0431\u044b\u043b\u043e', '\u0432\u043e\u0442', '\u043e\u0442', '\u043c\u0435\u043d\u044f', '\u0435\u0449\u0435', '\u043d\u0435\u0442', '\u043e', '\u0438\u0437', '\u0435\u043c\u0443', '\u0442\u0435\u043f\u0435\u0440\u044c', '\u043a\u043e\u0433\u0434\u0430', '\u0434\u0430\u0436\u0435', '\u043d\u0443', '\u0432\u0434\u0440\u0443\u0433', '\u043b\u0438', '\u0435\u0441\u043b\u0438', '\u0443\u0436\u0435', '\u0438\u043b\u0438', '\u043d\u0438', '\u0431\u044b\u0442\u044c', '\u0431\u044b\u043b', '\u043d\u0435\u0433\u043e', '\u0434\u043e', '\u0432\u0430\u0441', '\u043d\u0438\u0431\u0443\u0434\u044c', '\u043e\u043f\u044f\u0442\u044c', '\u0443\u0436', '\u0432\u0430\u043c', '\u0432\u0435\u0434\u044c', '\u0442\u0430\u043c', '\u043f\u043e\u0442\u043e\u043c', '\u0441\u0435\u0431\u044f', '\u043d\u0438\u0447\u0435\u0433\u043e', '\u0435\u0439', '\u043c\u043e\u0436\u0435\u0442', '\u043e\u043d\u0438', '\u0442\u0443\u0442', '\u0433\u0434\u0435', '\u0435\u0441\u0442\u044c', '\u043d\u0430\u0434\u043e', '\u043d\u0435\u0439', '\u0434\u043b\u044f', '\u043c\u044b', '\u0442\u0435\u0431\u044f', '\u0438\u0445', '\u0447\u0435\u043c', '\u0431\u044b\u043b\u0430', '\u0441\u0430\u043c', '\u0447\u0442\u043e\u0431', '\u0431\u0435\u0437', '\u0431\u0443\u0434\u0442\u043e', '\u0447\u0435\u0433\u043e', '\u0440\u0430\u0437', '\u0442\u043e\u0436\u0435', '\u0441\u0435\u0431\u0435', '\u043f\u043e\u0434', '\u0431\u0443\u0434\u0435\u0442', '\u0436', '\u0442\u043e\u0433\u0434\u0430', '\u043a\u0442\u043e', '\u044d\u0442\u043e\u0442', '\u0442\u043e\u0433\u043e', '\u043f\u043e\u0442\u043e\u043c\u0443', '\u044d\u0442\u043e\u0433\u043e', '\u043a\u0430\u043a\u043e\u0439', '\u0441\u043e\u0432\u0441\u0435\u043c', '\u043d\u0438\u043c', '\u0437\u0434\u0435\u0441\u044c', '\u044d\u0442\u043e\u043c', '\u043e\u0434\u0438\u043d', '\u043f\u043e\u0447\u0442\u0438', '\u043c\u043e\u0439', '\u0442\u0435\u043c', '\u0447\u0442\u043e\u0431\u044b', '\u043d\u0435\u0435', '\u0441\u0435\u0439\u0447\u0430\u0441', '\u0431\u044b\u043b\u0438', '\u043a\u0443\u0434\u0430', '\u0437\u0430\u0447\u0435\u043c', '\u0432\u0441\u0435\u0445', '\u043d\u0438\u043a\u043e\u0433\u0434\u0430', '\u043c\u043e\u0436\u043d\u043e', '\u043f\u0440\u0438', '\u043d\u0430\u043a\u043e\u043d\u0435\u0446', '\u0434\u0432\u0430', '\u043e\u0431', '\u0434\u0440\u0443\u0433\u043e\u0439', '\u0445\u043e\u0442\u044c', '\u043f\u043e\u0441\u043b\u0435', '\u043d\u0430\u0434', '\u0431\u043e\u043b\u044c\u0448\u0435', '\u0442\u043e\u0442', '\u0447\u0435\u0440\u0435\u0437', '\u044d\u0442\u0438', '\u043d\u0430\u0441', '\u043f\u0440\u043e', '\u0432\u0441\u0435\u0433\u043e', '\u043d\u0438\u0445', '\u043a\u0430\u043a\u0430\u044f', '\u043c\u043d\u043e\u0433\u043e', '\u0440\u0430\u0437\u0432\u0435', '\u0442\u0440\u0438', '\u044d\u0442\u0443', '\u043c\u043e\u044f', '\u0432\u043f\u0440\u043e\u0447\u0435\u043c', '\u0445\u043e\u0440\u043e\u0448\u043e', '\u0441\u0432\u043e\u044e', '\u044d\u0442\u043e\u0439', '\u043f\u0435\u0440\u0435\u0434', '\u0438\u043d\u043e\u0433\u0434\u0430', '\u043b\u0443\u0447\u0448\u0435', '\u0447\u0443\u0442\u044c', '\u0442\u043e\u043c', '\u043d\u0435\u043b\u044c\u0437\u044f', '\u0442\u0430\u043a\u043e\u0439', '\u0438\u043c', '\u0431\u043e\u043b\u0435\u0435', '\u0432\u0441\u0435\u0433\u0434\u0430', '\u043a\u043e\u043d\u0435\u0447\u043d\u043e', '\u0432\u0441\u044e', '\u043c\u0435\u0436\u0434\u0443']\n</pre> In\u00a0[7]: Copied! <pre># \u041d\u0430\u0448 \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0442\u043e\u0440\nword_tokenizer = nltk.WordPunctTokenizer()\ntokens = word_tokenizer.tokenize('\u043a\u0430\u0437\u043d\u0438\u0442\u044c, \u043d\u0435\u043b\u044c\u0437\u044f \u043f\u043e\u043c\u0438\u043b\u043e\u0432\u0430\u0442\u044c!!!')\nprint(tokens)\n</pre> # \u041d\u0430\u0448 \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0442\u043e\u0440 word_tokenizer = nltk.WordPunctTokenizer() tokens = word_tokenizer.tokenize('\u043a\u0430\u0437\u043d\u0438\u0442\u044c, \u043d\u0435\u043b\u044c\u0437\u044f \u043f\u043e\u043c\u0438\u043b\u043e\u0432\u0430\u0442\u044c!!!') print(tokens) <pre>['\u043a\u0430\u0437\u043d\u0438\u0442\u044c', ',', '\u043d\u0435\u043b\u044c\u0437\u044f', '\u043f\u043e\u043c\u0438\u043b\u043e\u0432\u0430\u0442\u044c', '!!!']\n</pre> <p>\u0417\u0430\u043f\u0438\u0448\u0435\u043c \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443 \u0442\u0435\u043a\u0441\u0442\u0430 \u0432 \u0432\u0438\u0434\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438</p> In\u00a0[8]: Copied! <pre># \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043d\u0430\u0445\u043e\u0434\u0438\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u0441\u043b\u043e\u0432\u0430\ndef words_only(text):\n    try:\n        return \" \".join(re.findall(r'[\u0410-\u042f\u0430-\u044fA-z\u0451\u0401-]+',text)).lower()\n    except:\n        return \"\"\n\nwords_only('\u041a\u0430\u0437\u043d\u0438\u0442\u044c, \u043d\u0435\u043b\u044c\u0437\u044f \u043f\u043e\u043c\u0438\u043b\u043e\u0432\u0430\u0442\u044c!!!')\n</pre> # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043d\u0430\u0445\u043e\u0434\u0438\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u0441\u043b\u043e\u0432\u0430 def words_only(text):     try:         return \" \".join(re.findall(r'[\u0410-\u042f\u0430-\u044fA-z\u0451\u0401-]+',text)).lower()     except:         return \"\"  words_only('\u041a\u0430\u0437\u043d\u0438\u0442\u044c, \u043d\u0435\u043b\u044c\u0437\u044f \u043f\u043e\u043c\u0438\u043b\u043e\u0432\u0430\u0442\u044c!!!') Out[8]: <pre>'\u043a\u0430\u0437\u043d\u0438\u0442\u044c \u043d\u0435\u043b\u044c\u0437\u044f \u043f\u043e\u043c\u0438\u043b\u043e\u0432\u0430\u0442\u044c'</pre> In\u00a0[9]: Copied! <pre># \u0440\u0430\u0441\u0448\u0438\u0440\u0438\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432, \u0441\u043b\u043e\u0432\u0430\u043c\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u044f\u0432\u043b\u044f\u0435\u044e\u0442\u0441\u044f \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430\u043c\u0438 \u0432 \u0434\u0430\u043d\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0435\nadd_stop_words = ['kz', '\u043a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d', '\u0430\u0441\u0442\u0430\u043d\u0430', '\u043a\u0430\u0437\u0430\u0445\u0441\u043a\u0438\u0439', '\u0430\u043b\u043c\u0430\u0442\u044b', '\u0430\u043e', '\u043e\u0430\u043e', '\u043e\u043e\u043e']\nmonths = ['\u044f\u043d\u0432\u0430\u0440\u044c', '\u0444\u0435\u0432\u0440\u0430\u043b\u044c', '\u043c\u0430\u0440\u0442', '\u0430\u043f\u0440\u0435\u043b\u044c', '\u043c\u0430\u0439', '\u0438\u044e\u043d\u044c', '\u0438\u044e\u043b\u044c', '\u0430\u0432\u0433\u0443\u0441\u0442', '\u0441\u0435\u043d\u0442\u044f\u0431\u0440\u044c', '\u043e\u043a\u0442\u044f\u0431\u0440\u044c', '\u043d\u043e\u044f\u0431\u0440\u044c', '\u0434\u0435\u043a\u0430\u0431\u0440\u044c',]\nall_stop_words = stop_words + add_stop_words + months\n</pre> # \u0440\u0430\u0441\u0448\u0438\u0440\u0438\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432, \u0441\u043b\u043e\u0432\u0430\u043c\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u044f\u0432\u043b\u044f\u0435\u044e\u0442\u0441\u044f \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430\u043c\u0438 \u0432 \u0434\u0430\u043d\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0435 add_stop_words = ['kz', '\u043a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d', '\u0430\u0441\u0442\u0430\u043d\u0430', '\u043a\u0430\u0437\u0430\u0445\u0441\u043a\u0438\u0439', '\u0430\u043b\u043c\u0430\u0442\u044b', '\u0430\u043e', '\u043e\u0430\u043e', '\u043e\u043e\u043e'] months = ['\u044f\u043d\u0432\u0430\u0440\u044c', '\u0444\u0435\u0432\u0440\u0430\u043b\u044c', '\u043c\u0430\u0440\u0442', '\u0430\u043f\u0440\u0435\u043b\u044c', '\u043c\u0430\u0439', '\u0438\u044e\u043d\u044c', '\u0438\u044e\u043b\u044c', '\u0430\u0432\u0433\u0443\u0441\u0442', '\u0441\u0435\u043d\u0442\u044f\u0431\u0440\u044c', '\u043e\u043a\u0442\u044f\u0431\u0440\u044c', '\u043d\u043e\u044f\u0431\u0440\u044c', '\u0434\u0435\u043a\u0430\u0431\u0440\u044c',] all_stop_words = stop_words + add_stop_words + months In\u00a0[\u00a0]: Copied! <pre>def process_data(data):\n    \n    texts = []\n    targets = []\n    \n    # \u043f\u043e\u043e\u0447\u0435\u0440\u0435\u0434\u043d\u043e \u043f\u0440\u043e\u0445\u043e\u0434\u0438\u043c \u043f\u043e \u0432\u0441\u0435\u043c \u043d\u043e\u0432\u043e\u0441\u0442\u044f\u043c \u0432 \u0441\u043f\u0438\u0441\u043a\u0435\n    for item in tqdm(data):\n               \n        text_lower = words_only(item['text'])            # \u043e\u0441\u0442\u0430\u0432\u0438\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0441\u043b\u043e\u0432\u0430 (str)\n        tokens     = word_tokenizer.tokenize(text_lower) # \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0442\u0435\u043a\u0441\u0442 \u043d\u0430 \u0441\u043b\u043e\u0432\u0430 (lst of str)\n        \n        # \u0443\u0434\u0430\u043b\u044f\u0435\u043c \u043f\u0443\u043d\u043a\u0442\u0443\u0430\u0446\u0438\u044e \u0438 \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430\n        tokens = [word for word in tokens if (word not in all_stop_words and not word.isnumeric())]\n        \n        texts.append(tokens) # \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0432 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u044b\u0439 \u0441\u043f\u0438\u0441\u043e\u043a\n    \n    return texts\n</pre> def process_data(data):          texts = []     targets = []          # \u043f\u043e\u043e\u0447\u0435\u0440\u0435\u0434\u043d\u043e \u043f\u0440\u043e\u0445\u043e\u0434\u0438\u043c \u043f\u043e \u0432\u0441\u0435\u043c \u043d\u043e\u0432\u043e\u0441\u0442\u044f\u043c \u0432 \u0441\u043f\u0438\u0441\u043a\u0435     for item in tqdm(data):                         text_lower = words_only(item['text'])            # \u043e\u0441\u0442\u0430\u0432\u0438\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0441\u043b\u043e\u0432\u0430 (str)         tokens     = word_tokenizer.tokenize(text_lower) # \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0442\u0435\u043a\u0441\u0442 \u043d\u0430 \u0441\u043b\u043e\u0432\u0430 (lst of str)                  # \u0443\u0434\u0430\u043b\u044f\u0435\u043c \u043f\u0443\u043d\u043a\u0442\u0443\u0430\u0446\u0438\u044e \u0438 \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430         tokens = [word for word in tokens if (word not in all_stop_words and not word.isnumeric())]                  texts.append(tokens) # \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0432 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u044b\u0439 \u0441\u043f\u0438\u0441\u043e\u043a          return texts In\u00a0[10]: Copied! <pre># \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u043d\u0430\u0448\u0443 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443\ny = [item['sentiment'] for item in data]\ntexts = process_data(data)\n</pre> # \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u043d\u0430\u0448\u0443 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443 y = [item['sentiment'] for item in data] texts = process_data(data) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8263/8263 [00:08&lt;00:00, 941.30it/s] \n</pre> In\u00a0[14]: Copied! <pre># example\ni = 1\nprint(\"Label: \", y[i])\nprint(\"Tokens: \", texts[i])\n</pre> # example i = 1 print(\"Label: \", y[i]) print(\"Tokens: \", texts[i]) <pre>Label:  negative\nTokens:  ['\u043c\u0435\u0434\u0438\u043a\u0438', '\u0440\u0430\u0441\u0441\u043a\u0430\u0437\u0430\u043b\u0438', '\u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0438', '\u043f\u043e\u0441\u0442\u0440\u0430\u0434\u0430\u0432\u0448\u0435\u0433\u043e', '\u043c\u0443\u0436\u0447\u0438\u043d\u044b', '\u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e', '\u0441\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u043e', '\u043d\u0430\u043f\u0430\u0434\u0435\u043d\u0438\u0435', '\u0432\u043e\u0437\u043b\u0435', '\u043e\u0442\u0434\u0435\u043b\u0435\u043d\u0438\u044f', '\u0431\u0430\u043d\u043a\u0430', '\u0442\u0438\u043c\u0438\u0440\u044f\u0437\u0435\u0432\u0430', '\u043f\u0440\u043e\u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043b\u0438', 'tengrinews', '\u043f\u0440\u0435\u0441\u0441', '-', '\u0441\u043b\u0443\u0436\u0431\u0435', '\u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f', '\u0437\u0434\u0440\u0430\u0432\u043e\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f', '\u043c\u0435\u0441\u0442\u0430', '\u043f\u0440\u043e\u0438\u0441\u0448\u0435\u0441\u0442\u0432\u0438\u044f', '\u0441\u043b\u0443\u0436\u0431\u0443', '\u0441\u043a\u043e\u0440\u043e\u0439', '\u043f\u043e\u043c\u043e\u0449\u0438', '\u043e\u0431\u0440\u0430\u0442\u0438\u043b\u0438\u0441\u044c', '\u0434\u0432\u043e\u0435', '\u0447\u0435\u043b\u043e\u0432\u0435\u043a', '\u043e\u0434\u043d\u043e\u043c\u0443', '\u043c\u0435\u0441\u0442\u0435', '\u043e\u043a\u0430\u0437\u0430\u043d\u0430', '\u043c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u0430\u044f', '\u043f\u043e\u043c\u043e\u0449\u044c', '\u0433\u043e\u0441\u043f\u0438\u0442\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438', '\u043e\u0442\u043a\u0430\u0437\u0430\u043b\u0441\u044f', '\u0432\u0442\u043e\u0440\u043e\u0439', '\u043f\u0430\u0446\u0438\u0435\u043d\u0442', '\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d', '\u0431\u043e\u043b\u044c\u043d\u0438\u0446\u0443', '\u0441\u043a\u043e\u0440\u043e\u0439', '\u043d\u0435\u043e\u0442\u043b\u043e\u0436\u043d\u043e\u0439', '\u043f\u043e\u043c\u043e\u0449\u0438', '\u0431\u0441\u043d\u043f', '\u0441\u043e\u0442\u0440\u044f\u0441\u0435\u043d\u0438\u0435\u043c', '\u0433\u043e\u043b\u043e\u0432\u043d\u043e\u0433\u043e', '\u043c\u043e\u0437\u0433\u0430', '\u0443\u0448\u0438\u0431\u043b\u0435\u043d\u043d\u043e\u0439', '\u0440\u0430\u043d\u043e\u0439', '\u0433\u043e\u043b\u043e\u0432\u044b', '\u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435', '\u0434\u0430\u043d\u043d\u044b\u0439', '\u043c\u043e\u043c\u0435\u043d\u0442', '\u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u0442\u0441\u044f', '\u0431\u043b\u0438\u0436\u0435', '\u0443\u0434\u043e\u0432\u043b\u0435\u0442\u0432\u043e\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u043c\u0443', '\u043f\u043e\u043a\u0430', '\u043f\u0440\u043e\u0445\u043e\u0434\u0438\u0442', '\u043e\u0431\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0435', '\u0431\u043e\u043b\u044c\u043d\u0438\u0446\u0435', '\u0441\u043e\u043e\u0431\u0449\u0438\u043b\u0438', '\u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0438', '\u0437\u0434\u0440\u0430\u0432\u043e\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f', '\u043d\u0430\u043f\u043e\u043c\u043d\u0438\u043c', '\u043f\u0435\u0440\u0435\u0441\u0435\u0447\u0435\u043d\u0438\u0438', '\u0443\u043b\u0438\u0446', '\u0442\u0438\u043c\u0438\u0440\u044f\u0437\u0435\u0432\u0430', '\u043c\u0430\u0440\u043a\u043e\u0432\u0430', '\u0432\u043e\u0437\u043b\u0435', '\u0431\u0446', '\u0430\u043b\u0430\u0442\u0430\u0443', '\u0433\u0440\u0430\u043d\u0434', '\u043f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u0430', '\u0441\u0442\u0440\u0435\u043b\u044c\u0431\u0430', '\u043e\u0440\u0438\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e', '\u043e\u0431\u0435\u0434\u0435\u043d\u043d\u043e\u0435', '\u0432\u0440\u0435\u043c\u044f', '\u0437\u0434\u0430\u043d\u0438\u0438', '\u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u044b', '\u043e\u0442\u0434\u0435\u043b\u0435\u043d\u0438\u044f', '\u0431\u0430\u043d\u043a\u043e\u0432', '\u0432\u0442\u0431', '\u0441\u0431\u0435\u0440\u0431\u0430\u043d\u043a', '\u043d\u0430\u0441\u0442\u043e\u044f\u0449\u0435\u0435', '\u0432\u0440\u0435\u043c\u044f', '\u043f\u043e\u043b\u0438\u0446\u0435\u0439\u0441\u043a\u0438\u0435', '\u0440\u0430\u0437\u044b\u0441\u043a\u0438\u0432\u0430\u044e\u0442', '\u043f\u043e\u0434\u043e\u0437\u0440\u0435\u0432\u0430\u0435\u043c\u044b\u0445', '\u0441\u0442\u0440\u0435\u043b\u044c\u0431\u0435', '\u0444\u0430\u043a\u0442\u0443', '\u043d\u0430\u043f\u0430\u0434\u0435\u043d\u0438\u044f', '\u043c\u0435\u0441\u0442\u043d\u043e\u043c', '\u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0438', '\u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0438\u0445', '\u0434\u0435\u043b', '\u043d\u0430\u0447\u0430\u0442\u043e', '\u0434\u043e\u0441\u0443\u0434\u0435\u0431\u043d\u043e\u0435', '\u0440\u0430\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0435', '\u0441\u0442\u0430\u0442\u044c\u0435', '\u0443\u043a', '\u0440\u043a', '\u0440\u0430\u0437\u0431\u043e\u0439', '\u0441\u043e\u0437\u0434\u0430\u043d\u0430', '\u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u0430\u044f', '\u0441\u043b\u0435\u0434\u0441\u0442\u0432\u0435\u043d\u043d\u043e', '-', '\u043e\u043f\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u0430\u044f', '\u0433\u0440\u0443\u043f\u043f\u0430', '\u0447\u0438\u0441\u043b\u0430', '\u043e\u043f\u044b\u0442\u043d\u044b\u0445', '\u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u043e\u0432', '\u043f\u043e\u0434\u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0439', '\u043a\u0440\u0438\u043c\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0439', '\u043f\u043e\u043b\u0438\u0446\u0438\u0438', '\u043d\u0430\u0441\u0442\u043e\u044f\u0449\u0438\u0439', '\u043c\u043e\u043c\u0435\u043d\u0442', '\u043f\u0440\u043e\u0432\u043e\u0434\u0438\u0442\u0441\u044f', '\u043a\u043e\u043c\u043f\u043b\u0435\u043a\u0441', '\u043e\u043f\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u044b\u0445', '\u0441\u043b\u0435\u0434\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0445', '\u043c\u0435\u0440\u043e\u043f\u0440\u0438\u044f\u0442\u0438\u0439', '\u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0445', '\u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435', '\u043b\u0438\u0447\u043d\u043e\u0441\u0442\u0435\u0439', '\u043d\u0430\u043f\u0430\u0434\u0430\u0432\u0448\u0438\u0445', '\u0437\u0430\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u0435', '\u0440\u0430\u043d\u0435\u0435', '\u0431\u0430\u043d\u043a', '\u0432\u0442\u0431', '\u043f\u0440\u043e\u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043b\u0438', '\u043d\u0430\u043f\u0430\u0434\u0435\u043d\u0438\u0435', '\u043c\u0443\u0436\u0447\u0438\u043d\u0443', '\u0434\u0430\u043d\u043d\u044b\u043c', '\u043f\u0440\u0435\u0441\u0441', '-', '\u0441\u043b\u0443\u0436\u0431\u044b', '\u0431\u0430\u043d\u043a\u0430', '\u043e\u0442\u0434\u0435\u043b\u0435\u043d\u0438\u044f', '\u0431\u0430\u043d\u043a\u0430', '\u0440\u0430\u0431\u043e\u0442\u0430\u044e\u0442', '\u0448\u0442\u0430\u0442\u043d\u043e\u043c', '\u0440\u0435\u0436\u0438\u043c\u0435', '\u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435\u043c', '\u0443\u0441\u0438\u043b\u0435\u043d\u043d\u044b\u0445', '\u043c\u0435\u0440', '\u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u0438', '\u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u0438', '\u043f\u043e\u043b\u0438\u0446\u0438\u0438', '\u0440\u0430\u0431\u043e\u0442\u0430\u044e\u0442', '\u0432\u043d\u0443\u0442\u0440\u0438', '\u043e\u0442\u0434\u0435\u043b\u0435\u043d\u0438\u044f', '\u0431\u0430\u043d\u043a\u0430', '\u0432\u0442\u0431', '\u043c\u0435\u0441\u0442\u043e', '\u043f\u0440\u043e\u0438\u0441\u0448\u0435\u0441\u0442\u0432\u0438\u044f', '\u043e\u0446\u0435\u043f\u043b\u0435\u043d\u043e']\n</pre> In\u00a0[33]: Copied! <pre>from nltk.stem.snowball import SnowballStemmer \n\n# \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0441\u0442\u0435\u043c\u043c\u0435\u0440\nstemmer = SnowballStemmer(\"russian\")\n</pre> from nltk.stem.snowball import SnowballStemmer   # \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0441\u0442\u0435\u043c\u043c\u0435\u0440 stemmer = SnowballStemmer(\"russian\") <p>\u041f\u0440\u0438\u043c\u0435\u0440 \u0441\u0442\u0435\u043c\u043c\u0438\u043d\u0433\u0430 \u0434\u043b\u044f \u0440\u0443\u0441\u0441\u043a\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430</p> In\u00a0[44]: Copied! <pre>from nltk.stem.snowball import SnowballStemmer \nstemmer = SnowballStemmer(\"english\")\ntext = 'tonight we are listening to a webinar in otus'\nstemmed_text = ' '.join([stemmer.stem(x) for x in text.split(' ')])\nstemmed_text\n</pre> from nltk.stem.snowball import SnowballStemmer  stemmer = SnowballStemmer(\"english\") text = 'tonight we are listening to a webinar in otus' stemmed_text = ' '.join([stemmer.stem(x) for x in text.split(' ')]) stemmed_text Out[44]: <pre>'tonight we are listen to a webinar in otus'</pre> In\u00a0[34]: Copied! <pre># \u043f\u0440\u0438\u043c\u0435\u0440\u044b \u0441\u0442\u0435\u043c\u043c\u0438\u043d\u0433\u0430\n\ni = 1\nfor aword in texts[i][:10]:\n    aword_stem = stemmer.stem(aword)\n    print(\"Before: %s, After: %s\" % (aword, aword_stem))\n</pre> # \u043f\u0440\u0438\u043c\u0435\u0440\u044b \u0441\u0442\u0435\u043c\u043c\u0438\u043d\u0433\u0430  i = 1 for aword in texts[i][:10]:     aword_stem = stemmer.stem(aword)     print(\"Before: %s, After: %s\" % (aword, aword_stem)) <pre>Before: \u043c\u0435\u0434\u0438\u043a\u0438, After: \u043c\u0435\u0434\u0438\u043a\nBefore: \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u0430\u043b\u0438, After: \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u0430\nBefore: \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0438, After: \u0441\u043e\u0441\u0442\u043e\u044f\u043d\nBefore: \u043f\u043e\u0441\u0442\u0440\u0430\u0434\u0430\u0432\u0448\u0435\u0433\u043e, After: \u043f\u043e\u0441\u0442\u0440\u0430\u0434\u0430\nBefore: \u043c\u0443\u0436\u0447\u0438\u043d\u044b, After: \u043c\u0443\u0436\u0447\u0438\u043d\nBefore: \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e, After: \u043a\u043e\u0442\u043e\u0440\nBefore: \u0441\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u043e, After: \u0441\u043e\u0432\u0435\u0440\u0448\nBefore: \u043d\u0430\u043f\u0430\u0434\u0435\u043d\u0438\u0435, After: \u043d\u0430\u043f\u0430\u0434\u0435\u043d\nBefore: \u0432\u043e\u0437\u043b\u0435, After: \u0432\u043e\u0437\u043b\nBefore: \u043e\u0442\u0434\u0435\u043b\u0435\u043d\u0438\u044f, After: \u043e\u0442\u0434\u0435\u043b\u0435\u043d\n</pre> In\u00a0[35]: Copied! <pre>text = '\u0432 \u044d\u0442\u043e\u0442 \u0432\u0435\u0447\u0435\u0440 \u043c\u044b \u0441\u043b\u0443\u0448\u0430\u0435\u043c \u0441\u043b\u0443\u0448\u0430\u0442\u044c \u0441\u043b\u0443\u0448\u0430\u043b\u0438 \u0432\u0435\u0431\u0438\u043d\u0430\u0440 \u043f\u043e \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 \u0432 \u043e\u0442\u0443\u0441'\nstemmed_text = ' '.join([stemmer.stem(x) for x in text.split(' ')])\n\nprint('Original text:\\t',text)\nprint('Stemmed text:\\t',stemmed_text)\n</pre> text = '\u0432 \u044d\u0442\u043e\u0442 \u0432\u0435\u0447\u0435\u0440 \u043c\u044b \u0441\u043b\u0443\u0448\u0430\u0435\u043c \u0441\u043b\u0443\u0448\u0430\u0442\u044c \u0441\u043b\u0443\u0448\u0430\u043b\u0438 \u0432\u0435\u0431\u0438\u043d\u0430\u0440 \u043f\u043e \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 \u0432 \u043e\u0442\u0443\u0441' stemmed_text = ' '.join([stemmer.stem(x) for x in text.split(' ')])  print('Original text:\\t',text) print('Stemmed text:\\t',stemmed_text) <pre>Original text:\t \u0432 \u044d\u0442\u043e\u0442 \u0432\u0435\u0447\u0435\u0440 \u043c\u044b \u0441\u043b\u0443\u0448\u0430\u0435\u043c \u0441\u043b\u0443\u0448\u0430\u0442\u044c \u0441\u043b\u0443\u0448\u0430\u043b\u0438 \u0432\u0435\u0431\u0438\u043d\u0430\u0440 \u043f\u043e \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 \u0432 \u043e\u0442\u0443\u0441\nStemmed text:\t \u0432 \u044d\u0442\u043e\u0442 \u0432\u0435\u0447\u0435\u0440 \u043c\u044b \u0441\u043b\u0443\u0448\u0430 \u0441\u043b\u0443\u0448\u0430 \u0441\u043b\u0443\u0448\u0430 \u0432\u0435\u0431\u0438\u043d\u0430\u0440 \u043f\u043e \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d \u044f\u0437\u044b\u043a \u0432 \u043e\u0442\u0443\u0441\n</pre> <p>\u0414\u043b\u044f \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u0433\u043e \u043d\u0435 \u0442\u0430\u043a \u0445\u043e\u0440\u043e\u0433\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0441\u0442\u0435\u043c\u043c\u0435\u0440</p> In\u00a0[45]: Copied! <pre>stemmer = SnowballStemmer(\"english\")\ntext = 'tonight we listening to a webinar in otus'\n\nstemmed_text = ' '.join([stemmer.stem(x) for x in text.split(' ')])\nprint('Original text:\\t',text)\nprint('Stemmed text:\\t',stemmed_text)\n</pre> stemmer = SnowballStemmer(\"english\") text = 'tonight we listening to a webinar in otus'  stemmed_text = ' '.join([stemmer.stem(x) for x in text.split(' ')]) print('Original text:\\t',text) print('Stemmed text:\\t',stemmed_text) <pre>Original text:\t tonight we listening to a webinar in otus\nStemmed text:\t tonight we listen to a webinar in otus\n</pre> In\u00a0[19]: Copied! <pre># \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443 \u0434\u043b\u044f \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u0438\nimport pymorphy2 # \u041c\u043e\u0440\u0444\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0430\u043d\u0430\u043b\u0438\u0437\u0430\u0442\u043e\u0440\n\n# \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0442\u043e\u0440 :)\nmorph = pymorphy2.MorphAnalyzer()\n</pre> # \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443 \u0434\u043b\u044f \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u0438 import pymorphy2 # \u041c\u043e\u0440\u0444\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0430\u043d\u0430\u043b\u0438\u0437\u0430\u0442\u043e\u0440  # \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0442\u043e\u0440 :) morph = pymorphy2.MorphAnalyzer() In\u00a0[38]: Copied! <pre>i = 1\nfor aword in texts[i][:10]:\n    aword_norm = morph.parse(aword)[0].normal_form\n    print(\"\u0418\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \u0441\u043b\u043e\u0432\u043e: %s\\t\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0435: %s\" % (aword, aword_norm))\n</pre> i = 1 for aword in texts[i][:10]:     aword_norm = morph.parse(aword)[0].normal_form     print(\"\u0418\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \u0441\u043b\u043e\u0432\u043e: %s\\t\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0435: %s\" % (aword, aword_norm)) <pre>\u0418\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \u0441\u043b\u043e\u0432\u043e: \u043c\u0435\u0434\u0438\u043a\u0438\t\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0435: \u043c\u0435\u0434\u0438\u043a\n\u0418\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \u0441\u043b\u043e\u0432\u043e: \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u0430\u043b\u0438\t\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0435: \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u0430\u0442\u044c\n\u0418\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \u0441\u043b\u043e\u0432\u043e: \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0438\t\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0435: \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\n\u0418\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \u0441\u043b\u043e\u0432\u043e: \u043f\u043e\u0441\u0442\u0440\u0430\u0434\u0430\u0432\u0448\u0435\u0433\u043e\t\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0435: \u043f\u043e\u0441\u0442\u0440\u0430\u0434\u0430\u0442\u044c\n\u0418\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \u0441\u043b\u043e\u0432\u043e: \u043c\u0443\u0436\u0447\u0438\u043d\u044b\t\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0435: \u043c\u0443\u0436\u0447\u0438\u043d\u0430\n\u0418\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \u0441\u043b\u043e\u0432\u043e: \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e\t\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0435: \u043a\u043e\u0442\u043e\u0440\u044b\u0439\n\u0418\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \u0441\u043b\u043e\u0432\u043e: \u0441\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u043e\t\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0435: \u0441\u043e\u0432\u0435\u0440\u0448\u0438\u0442\u044c\n\u0418\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \u0441\u043b\u043e\u0432\u043e: \u043d\u0430\u043f\u0430\u0434\u0435\u043d\u0438\u0435\t\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0435: \u043d\u0430\u043f\u0430\u0434\u0435\u043d\u0438\u0435\n\u0418\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \u0441\u043b\u043e\u0432\u043e: \u0432\u043e\u0437\u043b\u0435\t\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0435: \u0432\u043e\u0437\u043b\u0435\n\u0418\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \u0441\u043b\u043e\u0432\u043e: \u043e\u0442\u0434\u0435\u043b\u0435\u043d\u0438\u044f\t\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0435: \u043e\u0442\u0434\u0435\u043b\u0435\u043d\u0438\u0435\n</pre> In\u00a0[39]: Copied! <pre>text = '\u0432 \u044d\u0442\u043e\u0442 \u0432\u0435\u0447\u0435\u0440 \u043c\u044b \u0441\u043b\u0443\u0448\u0430\u0435\u043c \u0441\u043b\u0443\u0448\u0430\u0442\u044c \u0441\u043b\u0443\u0448\u0430\u043b\u0438 \u0432\u0435\u0431\u0438\u043d\u0430\u0440 \u043f\u043e \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 \u0432 \u043e\u0442\u0443\u0441'\nstemmed_text = ' '.join([morph.parse(x)[0].normal_form for x in text.split(' ')])\nprint('\u041e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0442\u0435\u043a\u0441\u0442:\\t',text)\nprint('\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u0442\u0435\u043a\u0441\u0442:\\t',stemmed_text)\n</pre> text = '\u0432 \u044d\u0442\u043e\u0442 \u0432\u0435\u0447\u0435\u0440 \u043c\u044b \u0441\u043b\u0443\u0448\u0430\u0435\u043c \u0441\u043b\u0443\u0448\u0430\u0442\u044c \u0441\u043b\u0443\u0448\u0430\u043b\u0438 \u0432\u0435\u0431\u0438\u043d\u0430\u0440 \u043f\u043e \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 \u0432 \u043e\u0442\u0443\u0441' stemmed_text = ' '.join([morph.parse(x)[0].normal_form for x in text.split(' ')]) print('\u041e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0442\u0435\u043a\u0441\u0442:\\t',text) print('\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u0442\u0435\u043a\u0441\u0442:\\t',stemmed_text) <pre>\u041e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0442\u0435\u043a\u0441\u0442:\t \u0432 \u044d\u0442\u043e\u0442 \u0432\u0435\u0447\u0435\u0440 \u043c\u044b \u0441\u043b\u0443\u0448\u0430\u0435\u043c \u0441\u043b\u0443\u0448\u0430\u0442\u044c \u0441\u043b\u0443\u0448\u0430\u043b\u0438 \u0432\u0435\u0431\u0438\u043d\u0430\u0440 \u043f\u043e \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 \u0432 \u043e\u0442\u0443\u0441\n\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u0442\u0435\u043a\u0441\u0442:\t \u0432 \u044d\u0442\u043e\u0442 \u0432\u0435\u0447\u0435\u0440 \u043c\u044b \u0441\u043b\u0443\u0448\u0430\u0442\u044c \u0441\u043b\u0443\u0448\u0430\u0442\u044c \u0441\u043b\u0443\u0448\u0430\u0442\u044c \u0432\u0435\u0431\u0438\u043d\u0430\u0440 \u043f\u043e \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439 \u044f\u0437\u044b\u043a \u0432 \u043e\u0442\u0443\u0441\n</pre> In\u00a0[47]: Copied! <pre>from tqdm import tqdm_notebook\n\n# \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044e \u043a\u043e \u0432\u0441\u0435\u043c \u0442\u0435\u043a\u0441\u0442\u0430\u043c\nfor i in tqdm_notebook(range(len(texts))):          \n    text_lemmatized = [morph.parse(x)[0].normal_form for x in texts[i]] # \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044e \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430 \u0432 \u0442\u0435\u043a\u0441\u0442\u0435\n    texts[i] = ' '.join(text_lemmatized)                # \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u0432\u0441\u0435 \u0441\u043b\u043e\u0432\u0430 \u0432 \u043e\u0434\u043d\u0443 \u0441\u0442\u0440\u043e\u043a\u0443 \u0447\u0435\u0440\u0435\u0437 \u043f\u0440\u043e\u0431\u0435\u043b\n</pre> from tqdm import tqdm_notebook  # \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044e \u043a\u043e \u0432\u0441\u0435\u043c \u0442\u0435\u043a\u0441\u0442\u0430\u043c for i in tqdm_notebook(range(len(texts))):               text_lemmatized = [morph.parse(x)[0].normal_form for x in texts[i]] # \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044e \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430 \u0432 \u0442\u0435\u043a\u0441\u0442\u0435     texts[i] = ' '.join(text_lemmatized)                # \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u0432\u0441\u0435 \u0441\u043b\u043e\u0432\u0430 \u0432 \u043e\u0434\u043d\u0443 \u0441\u0442\u0440\u043e\u043a\u0443 \u0447\u0435\u0440\u0435\u0437 \u043f\u0440\u043e\u0431\u0435\u043b <pre>  0%|          | 0/8263 [00:00&lt;?, ?it/s]</pre> In\u00a0[50]: Copied! <pre>texts[0]\n</pre> texts[0] Out[50]: <pre>'\u0434\u043e\u0441\u0443\u0434\u0435\u0431\u043d\u044b\u0439 \u0440\u0430\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0435 \u0444\u0430\u043a\u0442 \u043f\u043e\u043a\u0443\u043f\u043a\u0430 \u0435\u043d\u043f\u0444 \u043f\u0430\u043a\u0435\u0442 \u043e\u0431\u043b\u0438\u0433\u0430\u0446\u0438\u044f \u0442\u043e\u043e \u0431\u0443\u0437\u0433\u0443\u043b \u0430\u0443\u0440\u0443\u043c \u043d\u0430\u0447\u0430\u0442\u044c \u0438\u043d\u0438\u0446\u0438\u0430\u0442\u0438\u0432\u0430 \u043d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0431\u0430\u043d\u043a \u0440\u043a \u0441\u043e\u043e\u0431\u0449\u0438\u0442\u044c \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440 \u0434\u0435\u043f\u0430\u0440\u0442\u0430\u043c\u0435\u043d\u0442 \u0437\u0430\u0449\u0438\u0442\u0430 \u043f\u0440\u0430\u0432\u043e \u043f\u043e\u0442\u0440\u0435\u0431\u0438\u0442\u0435\u043b\u044c \u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u044b\u0439 \u0443\u0441\u043b\u0443\u0433\u0430 \u043d\u0430\u0446\u0431\u0430\u043d\u043a \u043a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d \u0430\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440 \u0442\u0435\u0440\u0435\u043d\u0442\u044c\u0435\u0432 \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u043e\u0441\u0443\u0434\u0435\u0431\u043d\u044b\u0439 \u0440\u0430\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0435 \u0441\u0442\u0430\u0442\u044c \u043e\u0431\u0440\u0430\u0449\u0435\u043d\u0438\u0435 \u043d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0431\u0430\u043d\u043a \u043f\u0438\u0441\u044c\u043c\u043e \u043d\u043e\u044f\u0431\u0440\u044c \u0433\u043e\u0434 \u043e\u0431\u0440\u0430\u0449\u0435\u043d\u0438\u0435 \u043d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0431\u0430\u043d\u043a \u043f\u0440\u0430\u0432\u043e\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u043e\u0440\u0433\u0430\u043d \u043c\u044b \u044d\u0442\u043e\u0442 \u0441\u0434\u0435\u043b\u043a\u0430 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u044c\u0441\u044f \u0441\u043e\u043c\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u043d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u044b\u0439 \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043d\u0430\u0446\u0431\u0430\u043d\u043a \u043d\u043e\u044f\u0431\u0440\u044c \u0433\u043e\u0434 \u043e\u0431\u0440\u0430\u0442\u0438\u0442\u044c\u0441\u044f \u043f\u0440\u0430\u0432\u043e\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u043e\u0440\u0433\u0430\u043d \u044d\u0442\u043e \u043c\u043e\u0447\u044c \u043e\u0437\u0432\u0443\u0447\u0438\u0442\u044c \u0441\u0435\u0433\u043e\u0434\u043d\u044f \u0438\u0434\u0442\u0438 \u0441\u043b\u0435\u0434\u0441\u0442\u0432\u0438\u0435 \u043f\u0440\u043e\u0432\u043e\u0434\u0438\u0442\u044c\u0441\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0441\u043a\u0430\u0437\u0430\u0442\u044c \u0442\u0435\u0440\u0435\u043d\u0442\u044c\u0435\u0432 \u0434\u0435\u043a\u0430\u0431\u0440\u044c \u043d\u0430\u0446\u0431\u0430\u043d\u043a \u0437\u0430\u044f\u0432\u0438\u0442\u044c \u0437\u043d\u0430\u0442\u044c \u0441\u0442\u0430\u0442\u044c \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0435\u043d\u043f\u0444 \u0434\u0435\u043a\u0430\u0431\u0440\u044c \u0444\u0430\u043a\u0442 \u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0435\u0434\u0438\u043d\u044b\u0439 \u043d\u0430\u043a\u043e\u043f\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u043f\u0435\u043d\u0441\u0438\u043e\u043d\u043d\u044b\u0439 \u0444\u043e\u043d\u0434 \u043f\u043e\u0434\u0442\u0432\u0435\u0440\u0434\u0438\u0442\u044c\u0441\u044f \u043f\u0440\u0435\u0441\u0441 - \u0441\u043b\u0443\u0436\u0431\u0430 \u043d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0431\u0430\u043d\u043a \u0441\u043e\u043e\u0431\u0449\u0438\u0442\u044c \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043f\u0440\u043e\u0432\u043e\u0434\u0438\u0442\u044c \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u044f \u0441\u043e\u0432\u0435\u0440\u0448\u0438\u0442\u044c \u0435\u043d\u043f\u0444 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u0438\u043d\u0432\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439 \u0430\u043a\u0442\u0438\u0432 \u0442\u0430\u043a\u0436\u0435 \u0444\u0438\u043d\u0440\u0435\u0433\u0443\u043b\u044f\u0442\u043e\u0440 \u0441\u043e\u043e\u0431\u0449\u0430\u0442\u044c \u0441\u0434\u0435\u043b\u043a\u0430 \u0435\u043d\u043f\u0444 \u0441\u0443\u043c\u043c\u0430 \u043f\u044f\u0442\u044c \u043c\u043b\u0440\u0434 \u0437\u0430\u0432\u0435\u0441\u0442\u0438 \u0443\u0433\u043e\u043b\u043e\u0432\u043d\u044b\u0439 \u0434\u0435\u043b\u043e \u043d\u0430\u0446\u0431\u0430\u043d\u043a \u0437\u0430\u0432\u0435\u0440\u044f\u0442\u044c \u0432\u0441\u0451 \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442\u044c \u0437\u0430\u0442\u0440\u0430\u0433\u0438\u0432\u0430\u0442\u044c \u043f\u0435\u043d\u0441\u0438\u043e\u043d\u043d\u044b\u0439 \u043d\u0430\u043a\u043e\u043f\u043b\u0435\u043d\u0438\u0435 \u043a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d\u0435\u0446 \u043d\u0430\u0441\u043b\u0430\u0442\u044c \u043e\u0448\u0438\u0431\u043a\u0430 \u0442\u0435\u043a\u0441\u0442 \u0432\u044b\u0434\u0435\u043b\u0438\u0442\u044c \u043c\u044b\u0448\u0438\u0439 \u043d\u0430\u0436\u0430\u0442\u044c ctrl enter'</pre> In\u00a0[55]: Copied! <pre># \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043b\u0435\u0439\u0431\u043b\u043e\u0432\ndef label2num(y):\n    if y == 'positive':\n        return 1\n    if y == 'negative':\n        return -1\n    if y == 'neutral':\n        return 0\n\nencoded_y = [label2num(yy) for yy in y]\n</pre> # \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043b\u0435\u0439\u0431\u043b\u043e\u0432 def label2num(y):     if y == 'positive':         return 1     if y == 'negative':         return -1     if y == 'neutral':         return 0  encoded_y = [label2num(yy) for yy in y] <p>\u042d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 \u043d\u0435\u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u043e \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0442\u044c \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445! \u042d\u0442\u043e \u0432\u0441\u0435 \u0440\u0430\u0432\u043d\u043e \u0447\u0442\u043e \u043d\u0430 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0443\u0447\u0435\u043d\u0438\u043a\u0443 \u0434\u0430\u0432\u0430\u0442\u044c \u0437\u0430\u0434\u0430\u0447\u0438, \u0440\u0430\u0437\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0435 \u0432 \u043a\u043b\u0430\u0441\u0441\u0435</p> <ul> <li>\u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u043c\u044b \u043e\u0442\u043b\u043e\u0436\u0438\u043c \u0447\u0430\u0441\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0438 \u043e\u0446\u0435\u043d\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430.</li> <li>\u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0435\u0439 <code>train_test_split</code> \u0438 \u043e\u0442\u043c\u0435\u0442\u0438\u043c <code>stratify</code> \u0442\u0430\u043a \u043a\u0430\u043a \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043c\u0435\u0442\u043e\u043a \u043d\u0435 \u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b</li> </ul> In\u00a0[56]: Copied! <pre>#train test_split\nfrom sklearn.model_selection import train_test_split\ntrain_texts, test_texts, train_y, test_y = train_test_split(texts, encoded_y, \n                                                            test_size=0.2, \n                                                            random_state=42, \n                                                            stratify = y)\n</pre> #train test_split from sklearn.model_selection import train_test_split train_texts, test_texts, train_y, test_y = train_test_split(texts, encoded_y,                                                              test_size=0.2,                                                              random_state=42,                                                              stratify = y) In\u00a0[96]: Copied! <pre># \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0432\u0435\u043a\u0442\u043e\u0440\u0430\u0439\u0437\u0435\u0440\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_features = 1000)\nvectorizer.fit(train_texts)\n\n# \u0422\u043e\u043f-10 \u0441\u043b\u043e\u0432\nvectorizer.get_feature_names_out()[:10]\n</pre> # \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0432\u0435\u043a\u0442\u043e\u0440\u0430\u0439\u0437\u0435\u0440 from sklearn.feature_extraction.text import CountVectorizer vectorizer = CountVectorizer(max_features = 1000) vectorizer.fit(train_texts)  # \u0422\u043e\u043f-10 \u0441\u043b\u043e\u0432 vectorizer.get_feature_names_out()[:10] Out[96]: <pre>array(['bank', 'invest', 'kase', 'kazakhstan', 'lada', 'today', '\u0430\u0432\u0433\u0443\u0441\u0442',\n       '\u0430\u0432\u0438\u0430\u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f', '\u0430\u0432\u0442\u043e', '\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c'], dtype=object)</pre> In\u00a0[97]: Copied! <pre># \u041f\u043e\u0434\u0442\u0432\u0435\u0440\u0434\u0438\u043c \u0447\u0442\u043e max_features \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b\u043e\nlen(vectorizer.get_feature_names_out())\n</pre> # \u041f\u043e\u0434\u0442\u0432\u0435\u0440\u0434\u0438\u043c \u0447\u0442\u043e max_features \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b\u043e len(vectorizer.get_feature_names_out()) Out[97]: <pre>1000</pre> <ul> <li>\u041e\u0431\u0443\u0447\u0430\u0435\u043c vectorizer \u043d\u0430 train-\u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0441\u0440\u0430\u0437\u0443 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0435\u043c \u0438\u0445 \u0432 \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u043e\u0434\u0430 <code>fit_transform</code></li> <li>\u0422\u0430\u043a\u0436\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0439 <code>vectorizer</code> \u043a \u0434\u0430\u043d\u043d\u044b\u043c \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f</li> </ul> In\u00a0[98]: Copied! <pre># \u041e\u0431\u0443\u0447\u0430\u0435\u043c vectorizer \u043d\u0430 train-\u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0441\u0440\u0430\u0437\u0443 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0435\u043c \u0438\u0445 \u0432 \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u043e\u0434\u0430 fit_transform\ntrain_X = vectorizer.transform(train_texts)\ntest_X  = vectorizer.transform(test_texts)\ntrain_X.todense().shape\n</pre> # \u041e\u0431\u0443\u0447\u0430\u0435\u043c vectorizer \u043d\u0430 train-\u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0441\u0440\u0430\u0437\u0443 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0435\u043c \u0438\u0445 \u0432 \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u043e\u0434\u0430 fit_transform train_X = vectorizer.transform(train_texts) test_X  = vectorizer.transform(test_texts) train_X.todense().shape Out[98]: <pre>(6610, 1000)</pre> In\u00a0[93]: Copied! <pre>from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators = 500) # \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\nclf = clf.fit(train_X, train_y) # \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u0435\u0435 \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\npred = clf.predict(test_X) # \u0434\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\n</pre> from sklearn.ensemble import RandomForestClassifier  clf = RandomForestClassifier(n_estimators = 500) # \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c clf = clf.fit(train_X, train_y) # \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u0435\u0435 \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 pred = clf.predict(test_X) # \u0434\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 In\u00a0[66]: Copied! <pre>print('\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043a\u0438: ', pred[0:20], \".....\")\nprint('\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043a\u0438: ', test_y[0:20], \".....\")\n</pre> print('\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043a\u0438: ', pred[0:20], \".....\") print('\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043a\u0438: ', test_y[0:20], \".....\") <pre>\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043a\u0438:  [ 0  0  0  0  0  0  0  1  1  0  0  0  0  0  0 -1 -1  0  0  0] .....\n\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043a\u0438:  [1, 0, 1, 0, 0, -1, 0, 1, 0, 0, 0, -1, -1, 1, -1, -1, -1, 0, 0, 0] .....\n</pre> In\u00a0[94]: Copied! <pre># \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043b\u0435\u0439\u0431\u043b\u043e\u0432\ndef num2label(y):\n    if y == 1:\n        return 'positive'\n    if y == -1:\n        return 'negative'\n    if y == 0:\n        return 'neutral'\n    \ndecoded_pred = [num2label(y) for y in pred]\ndecoded_test_y = [num2label(y) for y in test_y]\nprint('\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043a\u0438: ', decoded_pred[0:20], \".....\",'\\n')\nprint('\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043a\u0438: ', decoded_test_y [0:20], \".....\")\n</pre> # \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043b\u0435\u0439\u0431\u043b\u043e\u0432 def num2label(y):     if y == 1:         return 'positive'     if y == -1:         return 'negative'     if y == 0:         return 'neutral'      decoded_pred = [num2label(y) for y in pred] decoded_test_y = [num2label(y) for y in test_y] print('\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043a\u0438: ', decoded_pred[0:20], \".....\",'\\n') print('\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043a\u0438: ', decoded_test_y [0:20], \".....\") <pre>\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043a\u0438:  ['neutral', 'neutral', 'positive', 'neutral', 'neutral', 'negative', 'positive', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'negative', 'negative', 'neutral', 'neutral', 'neutral'] ..... \n\n\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043a\u0438:  ['positive', 'neutral', 'positive', 'neutral', 'neutral', 'negative', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'neutral', 'neutral', 'neutral'] .....\n</pre> In\u00a0[95]: Copied! <pre>print('Accuracy: ', accuracy_score(test_y, pred))\nprint('F1: ', f1_score(test_y, pred, average = 'macro'))\n</pre> print('Accuracy: ', accuracy_score(test_y, pred)) print('F1: ', f1_score(test_y, pred, average = 'macro')) <pre>Accuracy:  0.6346037507562009\nF1:  0.6107372194655939\n</pre> In\u00a0[70]: Copied! <pre>for i in range(10):\n    print('\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b:',decoded_test_y[i])\n    print('\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b:',decoded_pred[i])\n    print('\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438: ', train_texts[i][:500]+'...')\n    print('\\n')\n</pre> for i in range(10):     print('\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b:',decoded_test_y[i])     print('\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b:',decoded_pred[i])     print('\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438: ', train_texts[i][:500]+'...')     print('\\n') <pre>\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: positive\n\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438:  \u0441\u043d\u0438\u043c\u043e\u043a \u0447\u0435\u0442\u044b\u0440\u0451\u0445\u043a\u0440\u0430\u0442\u043d\u044b\u0439 \u0447\u0435\u043c\u043f\u0438\u043e\u043d\u043a\u0430 \u043c\u0438\u0440 \u0448\u0430\u0445\u043c\u0430\u0442\u044b \u0441\u0440\u0435\u0434\u0438 \u0440\u0435\u0431\u0451\u043d\u043e\u043a \u0431\u0438\u0431\u0438\u0441\u0430\u0440\u0430 \u0430\u0441\u0430\u0443\u0431\u0430\u0435\u0432 ria ru \u0448\u0430\u0445\u043c\u0430\u0442\u0438\u0441\u0442\u043a\u0430 \u0431\u0438\u0431\u0438\u0441\u0430\u0440\u0430 \u0430\u0441\u0430\u0443\u0431\u0430\u0435\u0432 \u043f\u0435\u0440\u0435\u0435\u0445\u0430\u0442\u044c \u0445\u0438\u043c\u043a\u0438 \u043a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d - \u043f\u043e\u0434\u043c\u043e\u0441\u043a\u043e\u0432\u043d\u044b\u0439 \u0445\u0438\u043c\u043a\u0438 \u043a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d \u043f\u0435\u0440\u0435\u0435\u0445\u0430\u0442\u044c \u0447\u0435\u0442\u044b\u0440\u0451\u0445\u043a\u0440\u0430\u0442\u043d\u044b\u0439 \u0447\u0435\u043c\u043f\u0438\u043e\u043d\u043a\u0430 \u043c\u0438\u0440 \u0448\u0430\u0445\u043c\u0430\u0442\u044b \u0441\u0440\u0435\u0434\u0438 \u0440\u0435\u0431\u0451\u043d\u043e\u043a \u0431\u0438\u0431\u0438\u0441\u0430\u0440\u0430 \u0430\u0441\u0430\u0443\u0431\u0430\u0435\u0432 \u0441\u043e\u043e\u0431\u0449\u0438\u0442\u044c \u0442\u0435\u043b\u0435\u043a\u0430\u043d\u0430\u043b \u043f\u0435\u0440\u0435\u0435\u0437\u0434 \u0434\u0435\u0432\u0443\u0448\u043a\u0430 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0430\u0442\u044c \u0433\u0443\u0431\u0435\u0440\u043d\u0430\u0442\u043e\u0440 \u0440\u0435\u0433\u0438\u043e\u043d \u0430\u043d\u0434\u0440\u0435\u0439 \u0432\u043e\u0440\u043e\u0431\u044c\u0451\u0432 russian news cn \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043f\u043e\u044f\u0441 \u0448\u0451\u043b\u043a\u043e\u0432\u044b\u0439 \u043f\u0443\u0442\u044c \u043d\u0443\u0440\u043b\u0430 \u0436\u043e\u043b \u043e\u0442\u0440\u0430\u0436\u0430\u0442\u044c \u0435\u0434\u0438\u043d\u0441\u0442\u0432\u043e \u043a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d\u0441\u043a\u0438\u0439 \u043a\u0438\u0442\u0430\u0439\u0441\u043a\u0438\u0439 \u043d\u0430\u0440\u043e\u0434 - \u043a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d\u0441\u043a\u0438\u0439 \u043f\u043e\u043b\u0438\u0442\u043e\u043b\u043e\u0433 - \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043f\u043e\u044f\u0441 \u0448\u0451\u043b\u043a\u043e\u0432\u044b\u0439 \u043f\u0443\u0442\u044c \u043d\u0443\u0440...\n\n\n\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438:  \u0431\u043e\u043b\u044c\u0448\u0438\u0439 \u0441\u0442\u0440\u0430\u043d\u0430 - \u0431\u043e\u043b\u044c\u0448\u0438\u0439 \u0441\u0435\u043c\u044c\u044f \u043f\u0435\u0440\u0435\u0434\u0430\u0447\u0430 \u043d\u043e\u044f\u0431\u0440\u044c...\n\n\n\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: positive\n\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438:  \u0442\u043e\u0440\u0436\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439 \u0433\u0430\u0448\u0435\u043d\u0438\u0435 \u043c\u0430\u0440\u043a\u0430 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a \u0433\u0440\u0443\u043f\u043f\u0430 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f \u0441\u0430\u043c\u0440\u0443\u043a\u0430 \u043a\u0430\u0437\u044b\u043d \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u0440\u043e\u0442\u044f\u0436\u0435\u043d\u0438\u0435 \u0433\u043e\u0434 \u0442\u0440\u0443\u0434\u0438\u0442\u044c\u0441\u044f \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0432\u0430\u0436\u043d\u044b\u0439 \u043e\u0442\u0440\u0430\u0441\u043b\u044c \u0441\u0442\u0440\u0430\u043d\u0430 \u0446\u0435\u0440\u0435\u043c\u043e\u043d\u0438\u044f \u0433\u0430\u0448\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0441\u0442\u0438 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a \u043a\u0430\u0437\u043f\u043e\u0447\u0442\u0430 \u043a\u0430\u0437\u0430\u0445\u0442\u0435\u043b\u0435\u043a \u043d\u0430\u043a \u043a\u0430\u0437\u0430\u0442\u043e\u043c\u043f\u0440\u043e\u043c \u0430\u0437 \u0441\u0442\u0430\u043d \u0442\u0435\u043ci\u0440 \u0436\u043e\u043b \u0441\u0435\u0433\u043e\u0434\u043d\u044f \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u043d\u043e \u0440\u0430\u0431\u043e\u0442\u043d\u0438\u043a \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u043d\u044b\u0439 \u0441\u0432\u044f\u0437\u044c \u0436\u0435\u043b\u0435\u0437\u043d\u044b\u0439 \u0434\u043e\u0440\u043e\u0433\u0430 \u0430\u0442\u043e\u043c\u043d\u044b\u0439 \u044d\u043d\u0435\u0440\u0433\u0435\u0442\u0438\u043a\u0430 \u0432\u044b\u043f\u0443\u0441\u0442\u0438\u0442\u044c \u043f\u043e\u0447\u0442\u043e\u0432\u044b\u0439 \u043e\u0431\u043e\u0440\u043e\u0442 \u043c\u0430\u0440\u043a\u0430 \u043f\u043e\u0441\u0432\u044f\u0442\u0438\u0442\u044c - \u043b\u0435\u0442\u0438\u0435 \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u044c \u0440\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430 \u043a\u0430\u0436\u0434\u044b\u0439 \u0443\u0447\u0430\u0441\u0442\u043d\u0438\u043a \u0442\u043e\u0440\u0436\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u0441\u0432\u044f\u0442\u0438\u0442\u044c \u0433\u043e\u0434 \u0441\u0432\u043e\u0439 \u0436\u0438\u0437\u043d\u044c \u0440\u0430\u0431\u043e\u0442\u0430 \u0431\u043b\u0430\u0433\u043e \u0440\u043e\u0434\u0438\u043d\u0430 \u0433\u043e\u0434 \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u044c \u043d\u0430\u0448 \u0441\u0442\u0440\u0430\u043d\u0430 \u043f...\n\n\n\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438:  \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440 \u0431\u0435\u043a\u0433\u0430\u043b\u0438 \u0436\u0443\u0431\u0430\u043d \u0434\u0435\u0444\u0438\u0446\u0438\u0442 \u0431\u0435\u043d\u0437\u0438\u043d \u0436\u0430\u043d\u0430\u043e\u0437\u0435\u043d \u0441\u0432\u044f\u0437\u0430\u0442\u044c \u0437\u0430\u0434\u0435\u0440\u0436\u043a\u0430 \u043f\u043e\u0441\u0442\u0430\u0432\u043a\u0430 \u0442\u043e\u043f\u043b\u0438\u0432\u043e \u0430\u0442\u044b\u0440\u0430\u0443\u0441\u043a\u0438\u0439 \u0437\u0430\u0432\u043e\u0434 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c lada \u043f\u043e\u0436\u0430\u043b\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0434\u0435\u0444\u0438\u0446\u0438\u0442 \u0431\u0435\u043d\u0437\u0438\u043d \u0437\u0430\u043f\u0440\u0430\u0432\u043a\u0430 \u0436\u0430\u043d\u0430\u043e\u0437\u0442\u044c \u0436\u0438\u0442\u0435\u043b\u044c \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u0447\u0435\u0442\u044b\u0440\u0435 \u0437\u0430\u043f\u0440\u0430\u0432\u043a\u0430 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043b\u0438\u0448\u044c \u0434\u0432\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u044c \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0435 - \u043b\u0438\u0442\u0440 \u0442\u043e\u043f\u043b\u0438\u0432\u043e \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044f \u043f\u0440\u043e\u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0431\u0435\u043a\u0433\u0430\u043b\u0438 \u0436\u0443\u0431\u0430\u043d \u0437\u0430\u043c\u0435\u0441\u0442\u0438\u0442\u0435\u043b\u044c \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u043e \u0442\u043e\u0440\u0433\u043e\u0432\u043b\u044f \u043c\u0430\u043d\u0433\u0438\u0441\u0442\u0430\u0443\u0441\u043a\u0430 \u043e\u0431\u043b\u0430\u0441\u0442\u044c \u0444\u043e\u0442\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u0441\u0430\u0439\u0442 lada \u0441\u043e\u043e\u0431\u0449\u0438\u0442\u044c \u0430\u0432\u0442\u043e\u0432\u043b\u0430\u0434\u0435\u043b\u0435\u0446 \u0436\u0430\u043d\u0430\u043e\u0437\u0442\u044c \u0437\u0430\u043f\u0440\u0430\u0432\u043a\u0430 \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c \u0432\u044b\u043d\u0443\u0434\u0438\u0442\u044c \u0442\u0440\u0430\u0442\u0438\u0442\u044c \u043d\u0435\u0441\u043a\u043e\u043b...\n\n\n\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438:  \u0430\u0440\u0435\u0441\u0442\u043e\u0432\u0430\u0442\u044c \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u0435\u043d\u043f\u0444 \u0434\u0430\u0442\u044c \u043d\u043e\u0432\u043e\u0441\u0442\u044c \u044f\u0432\u043b\u044f\u0442\u044c\u0441\u044f \u0437\u0430\u043a\u0440\u044b\u0442\u044b\u0439 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u0439 \u043f\u043b\u0430\u0442\u043d\u044b\u0439 \u043f\u043e\u0434\u043f\u0438\u0441\u0447\u0438\u043a \u0442\u0430\u043a\u0436\u0435 \u043c\u043e\u0447\u044c \u043f\u0440\u0438\u043e\u0431\u0440\u0435\u0441\u0442\u0438 \u0434\u043e\u0441\u0442\u0443\u043f \u0434\u0430\u0442\u044c \u043d\u043e\u0432\u043e\u0441\u0442\u044c \u0446\u0435\u043d\u0430 \u0442\u0435\u043d\u0433 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u0440\u043e\u0439\u0442\u0438 \u0430\u0432\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u044f...\n\n\n\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: negative\n\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438:  \u043a\u044b\u0437\u044b\u043b\u043e\u0440\u0434 \u043a\u0430\u0437\u0438\u043d\u0444\u043e\u0440\u043c - \u043a\u044b\u0437\u044b\u043b\u043e\u0440\u0434 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u0440\u0430\u043c\u043a\u0430 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u0434\u0430\u043c\u0430 - \u043e\u043f\u0442\u0438\u043c \u043f\u0440\u043e\u0435\u043a\u0442 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f \u0435\u0440 - \u0431\u0430\u043a \u043f\u0435\u0440\u0435\u0434\u0430\u0432\u0430\u0442\u044c \u043a\u043e\u0440\u0440\u0435\u0441\u043f\u043e\u043d\u0434\u0435\u043d\u0442 \u043c\u0438\u0430 \u043a\u0430\u0437\u0438\u043d\u0444\u043e\u0440\u043c \u0441\u0441\u044b\u043b\u043a\u0430 \u043f\u0440\u0435\u0441\u0441 - \u0441\u043b\u0443\u0436\u0431\u0430 \u0444\u043e\u043d\u0434 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u043f\u0440\u0435\u0434\u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u043e \u0434\u0430\u043c\u0430 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043a\u0440\u0435\u0434\u0438\u0442 \u043f\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435 \u043e\u0431\u043e\u0440\u043e\u0442\u043d\u044b\u0439 \u0441\u0440\u0435\u0434\u0441\u0442\u0432\u043e \u0444\u043e\u0440\u0442\u0435\u0431\u0430\u043d\u043a\u0430 \u0444\u043e\u043d\u0434 \u0434\u0430\u043c\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0438\u0442\u044c \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f \u0435\u0440 - \u0431\u0430\u043a \u0437\u0430\u043d\u0438\u043c\u0430\u0442\u044c\u0441\u044f \u043e\u043f\u0442\u043e\u0432\u044b\u0439 \u0440\u043e\u0437\u043d\u0438\u0447\u043d\u044b\u0439 \u0442\u043e\u0440\u0433\u043e\u0432\u043b\u044f \u043c\u0435\u0442\u0430\u043b\u043b\u043e\u0438\u0437\u0434\u0435\u043b\u0438\u0435 - \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u043d\u044b\u0439 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f \u0435\u0440 - \u0431\u0430\u043a \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0433\u043e\u0434 \u044f\u0432\u043b\u044f\u0442\u044c\u0441\u044f \u043e\u0434\u0438\u043d \u043a\u0440\u0443\u043f\u043d\u044b\u0439 \u043f\u0440\u0435\u0434\u043f\u0440\u0438\u044f\u0442\u0438\u0435 \u0441\u0432\u043e\u0439 \u0441\u0444\u0435\u0440\u0430 \u043f\u043e\u043c\u043e\u0449\u044c \u043a\u0440\u0435\u0434\u0438\u0442\u043d\u044b\u0439 \u0441\u0440\u0435\u0434\u0441\u0442\u0432\u043e \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f \u043f\u043e\u043b\u0443\u0447\u0438\u0442...\n\n\n\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438:  \u043c\u043e\u0441\u043a\u0432\u0430 \u0434\u0435\u043a \u0440\u0438\u0430 \u043d\u043e\u0432\u043e\u0441\u0442\u044c \u043f\u0440\u0430\u0439\u043c\u0430 \u0443\u0445\u043e\u0434\u0438\u0442\u044c \u0433\u043e\u0434 \u043e\u0437\u043d\u0430\u043c\u0435\u043d\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a\u0438\u0439 \u043d\u0435\u0444\u0442\u0435\u0433\u0430\u0437\u043e\u0432\u044b\u0439 \u043e\u0442\u0440\u0430\u0441\u043b\u044c \u0434\u0432\u0430 \u043a\u0440\u0443\u043f\u043d\u044b\u0439 \u043f\u0440\u0438\u0432\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u0441\u0434\u0435\u043b\u043a\u0430 \u043f\u0435\u0440\u0432\u044b\u0439 \u0433\u043e\u0434 \u0431\u0440\u0438\u0442\u0430\u043d\u0441\u043a\u0438\u0439 \u0432\u0440 \u0441\u0442\u0430\u0442\u044c \u0432\u043b\u0430\u0434\u0435\u043b\u0435\u0446 \u0430\u043a\u0446\u0438\u044f \u0440\u043e\u0441\u043d\u0435\u0444\u0442\u044c \u0433\u0435\u0440\u043e\u0439 \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u043a\u0440\u0443\u043f\u043d\u044b\u0439 \u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a\u0438\u0439 \u043d\u0435\u0444\u0442\u044f\u043d\u043e\u0439 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f \u043a\u0443\u043f\u0438\u0442\u044c \u0433\u043e\u0441\u0443\u0434\u0430\u0440\u0441\u0442\u0432\u043e \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u044b\u0439 \u043f\u0430\u043a\u0435\u0442 \u0431\u0430\u0448\u043d\u0435\u0444\u0442\u044c \u0437\u0430\u0442\u0435\u043c \u0440\u0430\u043c\u043a\u0430 \u043f\u0440\u0438\u0432\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439 \u0430\u043a\u0446\u0438\u044f \u043e\u0431\u0440\u0435\u0441\u0442\u0438 \u0434\u0432\u0430 \u043d\u043e\u0432\u044b\u0439 \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0439 \u0438\u043d\u0432\u0435\u0441\u0442\u043e\u0440 \u0438\u0442\u043e\u0433 \u0441\u043e\u0431\u0440\u0430\u0442\u044c \u043f\u043b\u0430\u043d\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u044d\u0442\u043e\u0442 \u0434\u0432\u0430 \u0441\u0434\u0435\u043b\u043a\u0430 \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0442\u044c \u043f\u043e\u0441\u0442\u0443\u043f\u043b\u0435\u043d\u0438\u0435 \u0431\u044e\u0434\u0436\u0435\u0442 \u0442\u0440\u0438\u043b\u043b\u0438\u043e\u043d \u0440\u0443\u0431\u043b\u044c \u0437\u0430\u043a\u0440\u044b\u0432\u0430\u0442\u044c \u043d\u0430\u0448 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0430 \u0442\u0435\u043a\u0443\u0449\u0438\u0439 \u0431\u044e\u0434\u0436\u0435\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0442\u044c ...\n\n\n\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: positive\n\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: positive\n\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438:  \u043a\u0430\u0437\u0438\u043d\u0444\u043e\u0440\u043c - \u043f\u0440\u0435\u0441\u0441 - \u0441\u043b\u0443\u0436\u0431\u0430 \u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u0441\u043a\u0438\u0439 \u043f\u0435\u0440\u0435\u0432\u043e\u0437\u043a\u0430 \u0441\u043e\u043e\u0431\u0449\u0438\u0442\u044c \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u0447\u043f \u0432\u043e\u043a\u0437\u0430\u043b - \u044f\u043d\u0432\u0430\u0440\u044c \u0442\u0435\u0440\u0440\u0438\u0442\u043e\u0440\u0438\u044f \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043f\u0430\u0440\u043a \u0441\u0442\u0430\u043d\u0446\u0438\u044f \u043f\u0440\u043e\u0438\u0437\u043e\u0439\u0442\u0438 \u0437\u0430\u0434\u044b\u043c\u043b\u0435\u043d\u0438\u0435 \u0441\u043f\u0438\u0441\u0430\u0442\u044c \u0432\u0430\u0433\u043e\u043d \u0436\u0435\u0440\u0442\u0432\u0430 \u043f\u043e\u0441\u0442\u0440\u0430\u0434\u0430\u0442\u044c \u0432\u0430\u0433\u043e\u043d \u043f\u043e\u0441\u0442\u0440\u043e\u0439\u043a\u0430 \u043d\u043e\u044f\u0431\u0440\u044c \u0433\u043e\u0434 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435 \u0441\u043d\u044f\u0442\u043e\u0439 \u044d\u043a\u0441\u043f\u043b\u0443\u0430\u0442\u0430\u0446\u0438\u044f \u0433\u043e\u0442\u043e\u0432\u0438\u0442\u044c\u0441\u044f \u0443\u0442\u0438\u043b\u0438\u0437\u0430\u0446\u0438\u044f - \u0433\u043e\u0432\u043e\u0440\u0438\u0442\u044c\u0441\u044f \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435 \u0441\u0440\u0435\u0434\u043d\u0438\u0439 \u0441\u0440\u043e\u043a \u044d\u043a\u0441\u043f\u043b\u0443\u0430\u0442\u0430\u0446\u0438\u044f \u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u0441\u043a\u0438\u0439 \u0432\u0430\u0433\u043e\u043d \u0441\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0442\u044c \u0433\u043e\u0434 \u043f\u0440\u0438\u0447\u0438\u043d\u0430 \u0441\u043b\u0443\u0447\u0438\u0442\u044c\u0441\u044f \u0432\u044b\u044f\u0441\u043d\u044f\u0442\u044c\u0441\u044f \u0438\u0442\u043e\u0433 \u0441\u043b\u0443\u0436\u0435\u0431\u043d\u044b\u0439 \u0440\u0430\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0435 \u0431\u044b\u0442\u044c \u043f\u0440\u0438\u043d\u044f\u0442\u044c \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0439 \u043c\u0435\u0440\u0430 \u0441\u043e\u043e\u0431\u0449\u0430\u0442\u044c \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f \u043d\u0430\u043f\u043e\u043c\u043d\u0438\u0442\u044c - \u0447\u0430\u0441 \u043f\u0443\u043b\u044c\u0442 \u0434\u0447\u0441 \u0433 \u043f...\n\n\n\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: positive\n\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438:  \u043b\u0438\u0448\u0435\u043d\u0438\u0435 \u043e\u043b\u0438\u043c\u043f\u0438\u0439\u0441\u043a\u0438\u0439 \u0437\u043e\u043b\u043e\u0442\u043e \u0438\u043b\u044c\u044f \u0438\u043b\u044c\u0438\u043d \u043e\u0441\u043f\u0430\u0440\u0438\u0432\u0430\u0442\u044c \u0431\u0435\u0441\u043f\u043e\u043b\u0435\u0437\u043d\u043e \u0441\u043a\u0430\u0437\u0430\u0442\u044c \u0432\u0438\u0446\u0430 - \u043c\u0438\u043d\u0438\u0441\u0442\u0440 \u043a\u0443\u043b\u044c\u0442\u0443\u0440\u0430 \u0441\u043f\u043e\u0440\u0442 \u0440\u043a \u0441\u0430\u043a\u0435\u043d \u043c\u0443\u0441\u0430\u0439\u0431\u0435\u043a \u043a\u0443\u043b\u0443\u0430\u0440 \u043c\u0430\u0436\u0438\u043b\u0438\u0441 \u0440\u043a \u043f\u0435\u0440\u0435\u0434\u0430\u0432\u0430\u0442\u044c \u043a\u043e\u0440\u0440\u0435\u0441\u043f\u043e\u043d\u0434\u0435\u043d\u0442 \u0432\u0435\u0440\u043d\u0443\u0442\u044c \u043c\u0435\u0434\u0430\u043b\u044c \u043f\u0435\u043a\u0438\u043d \u043f\u0440\u0438\u043d\u0446\u0438\u043f \u0441\u043e\u0433\u043b\u0430\u0441\u043d\u044b\u0439 \u0440\u0435\u0448\u0435\u043d\u0438\u0435 \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u044b\u0439 \u043e\u043b\u0438\u043c\u043f\u0438\u0439\u0441\u043a\u0438\u0439 \u043a\u043e\u043c\u0438\u0442\u0435\u0442 \u043c\u043e\u043a\u043d\u0443\u0442\u044c \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0432\u044b\u043d\u0435\u0441\u0442\u0438 \u043e\u043d \u0448\u043e\u043a \u0441\u0430\u043c \u0448\u043e\u043a \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u044c\u0441\u044f \u0441\u0435\u043c\u044c\u044f \u043c\u043e\u043a\u043d\u0443\u0442\u044c \u0432\u0435\u0441\u044c \u043f\u0440\u0430\u0432\u0438\u043b\u043e \u0437\u0430\u043a\u043e\u043d \u0441\u043e\u0433\u043b\u0430\u0441\u043d\u044b\u0439 \u043e\u0441\u043f\u0430\u0440\u0438\u0432\u0430\u0442\u044c \u0431\u0435\u0441\u043f\u043e\u043b\u0435\u0437\u043d\u043e - \u0441\u043a\u0430\u0437\u0430\u0442\u044c \u043c\u0443\u0441\u0430\u0439\u0431\u0435\u043a \u0441\u043b\u043e\u0432\u043e \u0432\u0438\u0446\u0430 - \u043c\u0438\u043d\u0438\u0441\u0442\u0440 \u0432\u0435\u0434\u043e\u043c\u0441\u0442\u0432\u043e \u0433\u043b\u0430\u0432\u0430 \u0432\u0435\u0434\u043e\u043c\u0441\u0442\u0432\u043e \u0430\u0440\u044b\u0441\u0442\u0430\u043d\u0431\u0435\u043a \u043c\u0443\u0445\u0430\u043c\u0435\u0434\u0438\u0443\u043b \u0442\u044f\u0436\u0435\u043b\u043e \u0432\u043e\u0441\u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0442\u044c \u043d\u043e\u0432\u043e\u0441\u0442\u044c \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u043b\u044e\u0431\u044f\u0449\u0438\u0439 \u0441\u043f\u043e\u0440\u0442 \u0447\u0435\u043b\u043e\u0432\u0435\u043a...\n\n\n\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438:  \u0431\u044b\u0432\u0448\u0438\u0439 \u0437\u0430\u043c\u0435\u0441\u0442\u0438\u0442\u0435\u043b\u044c \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c \u0430\u0434\u043c\u0438\u043d\u0438\u0441\u0442\u0440\u0430\u0446\u0438\u044f \u043f\u0440\u0435\u0437\u0438\u0434\u0435\u043d\u0442 \u043a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d \u0431\u0430\u0433\u043b\u0430\u043d \u043c\u0430\u0439\u043b\u044b\u0431\u0430\u0435\u0432 \u0430\u0440\u0435\u0441\u0442\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u043a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d\u0441\u043a\u0438\u0439 \u0441\u043c\u0438 \u043a\u043e\u043d\u0435\u0446 \u043f\u0440\u043e\u0448\u043b\u044b\u0439 \u043d\u0435\u0434\u0435\u043b\u044f \u0445\u0438\u0449\u0435\u043d\u0438\u0435 \u043c\u043b\u0440\u0434 \u0442\u0435\u043d\u0433 \u043c\u043b\u043d \u0440\u0443\u0431\u043b\u044c \u0441\u0440\u0435\u0434\u0441\u0442\u0432\u043e \u0440\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430\u043d\u0441\u043a\u0438\u0439 \u0431\u044e\u0434\u0436\u0435\u0442 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0430\u0440\u0435\u0441\u0442\u043e\u0432\u0430\u0442\u044c \u043a\u043e\u043c\u0438\u0442\u0435\u0442 \u043d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u044c \u0430\u043d\u0430\u043b\u043e\u0433 \u0444\u0441\u0431 \u043a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0438\u0442\u044c \u0441\u0435\u0433\u043e\u0434\u043d\u044f \u044f\u043d\u0432\u0430\u0440\u044c \u043e\u0444\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0439 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435 \u0437\u0430\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u0435 \u0431\u0430\u0433\u043b\u0430\u043d \u043c\u0430\u0439\u043b\u044b\u0431\u0430\u0435\u0432 \u0434\u0440\u0443\u0433\u043e\u0439 \u0431\u044b\u0432\u0448\u0438\u0439 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a \u0430\u0434\u043c\u0438\u043d\u0438\u0441\u0442\u0440\u0430\u0446\u0438\u044f \u043f\u0440\u0435\u0437\u0438\u0434\u0435\u043d\u0442 \u043d\u0438\u043a\u043e\u043b\u0430\u0439 \u0433\u0430\u043b\u0438\u0445\u0438\u043d\u0430 \u043f\u0440\u043e\u0438\u0437\u043e\u0439\u0442\u0438 \u044f\u043d\u0432\u0430\u0440\u044c \u043a\u043e\u043c\u0438\u0442\u0435\u0442 \u043d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u044c \u0440\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430 \u043f\u043e\u0434\u043e\u0437\u0440\u0435\u043d\u0438\u0435 \u043d\u0435...\n\n\n</pre> In\u00a0[82]: Copied! <pre>from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(max_features = 1000, norm = None) \nvectorizer.fit(train_texts)\nvectorizer.get_feature_names_out()[:10]\n</pre> from sklearn.feature_extraction.text import TfidfVectorizer  vectorizer = TfidfVectorizer(max_features = 1000, norm = None)  vectorizer.fit(train_texts) vectorizer.get_feature_names_out()[:10] Out[82]: <pre>array(['bank', 'invest', 'kase', 'kazakhstan', 'lada', 'today', '\u0430\u0432\u0433\u0443\u0441\u0442',\n       '\u0430\u0432\u0438\u0430\u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f', '\u0430\u0432\u0442\u043e', '\u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c'], dtype=object)</pre> In\u00a0[99]: Copied! <pre># \u041e\u0431\u0443\u0447\u0430\u0435\u043c TF-IDF \u043d\u0430 train, \u0430 \u0437\u0430\u0442\u0435\u043c \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u043a train \u0438 test\ntrain_X = vectorizer.fit_transform(train_texts)\ntest_X  = vectorizer.transform(test_texts)\n\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators = 500) # \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\nclf = clf.fit(train_X, train_y) # \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u0435\u0435 \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\npred = clf.predict(test_X) # \u0434\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\n</pre> # \u041e\u0431\u0443\u0447\u0430\u0435\u043c TF-IDF \u043d\u0430 train, \u0430 \u0437\u0430\u0442\u0435\u043c \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u043a train \u0438 test train_X = vectorizer.fit_transform(train_texts) test_X  = vectorizer.transform(test_texts)  from sklearn.ensemble import RandomForestClassifier clf = RandomForestClassifier(n_estimators = 500) # \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c clf = clf.fit(train_X, train_y) # \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u0435\u0435 \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 pred = clf.predict(test_X) # \u0434\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 In\u00a0[100]: Copied! <pre>print('Accuracy: ', accuracy_score(test_y, pred))\nprint('F1: ', f1_score(test_y, pred, average = 'macro'))\n</pre> print('Accuracy: ', accuracy_score(test_y, pred)) print('F1: ', f1_score(test_y, pred, average = 'macro')) <pre>Accuracy:  0.6684815486993345\nF1:  0.642218515263824\n</pre> In\u00a0[85]: Copied! <pre>for i in range(10):\n    print('\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b:',decoded_test_y[i])\n    print('\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b:',decoded_pred[i])\n    print('\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438: ', train_texts[i][:500]+'...')\n    print('\\n')\n</pre> for i in range(10):     print('\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b:',decoded_test_y[i])     print('\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b:',decoded_pred[i])     print('\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438: ', train_texts[i][:500]+'...')     print('\\n') <pre>\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: positive\n\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438:  \u0441\u043d\u0438\u043c\u043e\u043a \u0447\u0435\u0442\u044b\u0440\u0451\u0445\u043a\u0440\u0430\u0442\u043d\u044b\u0439 \u0447\u0435\u043c\u043f\u0438\u043e\u043d\u043a\u0430 \u043c\u0438\u0440 \u0448\u0430\u0445\u043c\u0430\u0442\u044b \u0441\u0440\u0435\u0434\u0438 \u0440\u0435\u0431\u0451\u043d\u043e\u043a \u0431\u0438\u0431\u0438\u0441\u0430\u0440\u0430 \u0430\u0441\u0430\u0443\u0431\u0430\u0435\u0432 ria ru \u0448\u0430\u0445\u043c\u0430\u0442\u0438\u0441\u0442\u043a\u0430 \u0431\u0438\u0431\u0438\u0441\u0430\u0440\u0430 \u0430\u0441\u0430\u0443\u0431\u0430\u0435\u0432 \u043f\u0435\u0440\u0435\u0435\u0445\u0430\u0442\u044c \u0445\u0438\u043c\u043a\u0438 \u043a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d - \u043f\u043e\u0434\u043c\u043e\u0441\u043a\u043e\u0432\u043d\u044b\u0439 \u0445\u0438\u043c\u043a\u0438 \u043a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d \u043f\u0435\u0440\u0435\u0435\u0445\u0430\u0442\u044c \u0447\u0435\u0442\u044b\u0440\u0451\u0445\u043a\u0440\u0430\u0442\u043d\u044b\u0439 \u0447\u0435\u043c\u043f\u0438\u043e\u043d\u043a\u0430 \u043c\u0438\u0440 \u0448\u0430\u0445\u043c\u0430\u0442\u044b \u0441\u0440\u0435\u0434\u0438 \u0440\u0435\u0431\u0451\u043d\u043e\u043a \u0431\u0438\u0431\u0438\u0441\u0430\u0440\u0430 \u0430\u0441\u0430\u0443\u0431\u0430\u0435\u0432 \u0441\u043e\u043e\u0431\u0449\u0438\u0442\u044c \u0442\u0435\u043b\u0435\u043a\u0430\u043d\u0430\u043b \u043f\u0435\u0440\u0435\u0435\u0437\u0434 \u0434\u0435\u0432\u0443\u0448\u043a\u0430 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0430\u0442\u044c \u0433\u0443\u0431\u0435\u0440\u043d\u0430\u0442\u043e\u0440 \u0440\u0435\u0433\u0438\u043e\u043d \u0430\u043d\u0434\u0440\u0435\u0439 \u0432\u043e\u0440\u043e\u0431\u044c\u0451\u0432 russian news cn \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043f\u043e\u044f\u0441 \u0448\u0451\u043b\u043a\u043e\u0432\u044b\u0439 \u043f\u0443\u0442\u044c \u043d\u0443\u0440\u043b\u0430 \u0436\u043e\u043b \u043e\u0442\u0440\u0430\u0436\u0430\u0442\u044c \u0435\u0434\u0438\u043d\u0441\u0442\u0432\u043e \u043a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d\u0441\u043a\u0438\u0439 \u043a\u0438\u0442\u0430\u0439\u0441\u043a\u0438\u0439 \u043d\u0430\u0440\u043e\u0434 - \u043a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d\u0441\u043a\u0438\u0439 \u043f\u043e\u043b\u0438\u0442\u043e\u043b\u043e\u0433 - \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043f\u043e\u044f\u0441 \u0448\u0451\u043b\u043a\u043e\u0432\u044b\u0439 \u043f\u0443\u0442\u044c \u043d\u0443\u0440...\n\n\n\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438:  \u0431\u043e\u043b\u044c\u0448\u0438\u0439 \u0441\u0442\u0440\u0430\u043d\u0430 - \u0431\u043e\u043b\u044c\u0448\u0438\u0439 \u0441\u0435\u043c\u044c\u044f \u043f\u0435\u0440\u0435\u0434\u0430\u0447\u0430 \u043d\u043e\u044f\u0431\u0440\u044c...\n\n\n\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: positive\n\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438:  \u0442\u043e\u0440\u0436\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439 \u0433\u0430\u0448\u0435\u043d\u0438\u0435 \u043c\u0430\u0440\u043a\u0430 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a \u0433\u0440\u0443\u043f\u043f\u0430 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f \u0441\u0430\u043c\u0440\u0443\u043a\u0430 \u043a\u0430\u0437\u044b\u043d \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u0440\u043e\u0442\u044f\u0436\u0435\u043d\u0438\u0435 \u0433\u043e\u0434 \u0442\u0440\u0443\u0434\u0438\u0442\u044c\u0441\u044f \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0432\u0430\u0436\u043d\u044b\u0439 \u043e\u0442\u0440\u0430\u0441\u043b\u044c \u0441\u0442\u0440\u0430\u043d\u0430 \u0446\u0435\u0440\u0435\u043c\u043e\u043d\u0438\u044f \u0433\u0430\u0448\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0441\u0442\u0438 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a \u043a\u0430\u0437\u043f\u043e\u0447\u0442\u0430 \u043a\u0430\u0437\u0430\u0445\u0442\u0435\u043b\u0435\u043a \u043d\u0430\u043a \u043a\u0430\u0437\u0430\u0442\u043e\u043c\u043f\u0440\u043e\u043c \u0430\u0437 \u0441\u0442\u0430\u043d \u0442\u0435\u043ci\u0440 \u0436\u043e\u043b \u0441\u0435\u0433\u043e\u0434\u043d\u044f \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u043d\u043e \u0440\u0430\u0431\u043e\u0442\u043d\u0438\u043a \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u043d\u044b\u0439 \u0441\u0432\u044f\u0437\u044c \u0436\u0435\u043b\u0435\u0437\u043d\u044b\u0439 \u0434\u043e\u0440\u043e\u0433\u0430 \u0430\u0442\u043e\u043c\u043d\u044b\u0439 \u044d\u043d\u0435\u0440\u0433\u0435\u0442\u0438\u043a\u0430 \u0432\u044b\u043f\u0443\u0441\u0442\u0438\u0442\u044c \u043f\u043e\u0447\u0442\u043e\u0432\u044b\u0439 \u043e\u0431\u043e\u0440\u043e\u0442 \u043c\u0430\u0440\u043a\u0430 \u043f\u043e\u0441\u0432\u044f\u0442\u0438\u0442\u044c - \u043b\u0435\u0442\u0438\u0435 \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u044c \u0440\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430 \u043a\u0430\u0436\u0434\u044b\u0439 \u0443\u0447\u0430\u0441\u0442\u043d\u0438\u043a \u0442\u043e\u0440\u0436\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u0441\u0432\u044f\u0442\u0438\u0442\u044c \u0433\u043e\u0434 \u0441\u0432\u043e\u0439 \u0436\u0438\u0437\u043d\u044c \u0440\u0430\u0431\u043e\u0442\u0430 \u0431\u043b\u0430\u0433\u043e \u0440\u043e\u0434\u0438\u043d\u0430 \u0433\u043e\u0434 \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u044c \u043d\u0430\u0448 \u0441\u0442\u0440\u0430\u043d\u0430 \u043f...\n\n\n\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438:  \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440 \u0431\u0435\u043a\u0433\u0430\u043b\u0438 \u0436\u0443\u0431\u0430\u043d \u0434\u0435\u0444\u0438\u0446\u0438\u0442 \u0431\u0435\u043d\u0437\u0438\u043d \u0436\u0430\u043d\u0430\u043e\u0437\u0435\u043d \u0441\u0432\u044f\u0437\u0430\u0442\u044c \u0437\u0430\u0434\u0435\u0440\u0436\u043a\u0430 \u043f\u043e\u0441\u0442\u0430\u0432\u043a\u0430 \u0442\u043e\u043f\u043b\u0438\u0432\u043e \u0430\u0442\u044b\u0440\u0430\u0443\u0441\u043a\u0438\u0439 \u0437\u0430\u0432\u043e\u0434 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c lada \u043f\u043e\u0436\u0430\u043b\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0434\u0435\u0444\u0438\u0446\u0438\u0442 \u0431\u0435\u043d\u0437\u0438\u043d \u0437\u0430\u043f\u0440\u0430\u0432\u043a\u0430 \u0436\u0430\u043d\u0430\u043e\u0437\u0442\u044c \u0436\u0438\u0442\u0435\u043b\u044c \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u0447\u0435\u0442\u044b\u0440\u0435 \u0437\u0430\u043f\u0440\u0430\u0432\u043a\u0430 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043b\u0438\u0448\u044c \u0434\u0432\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u044c \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0435 - \u043b\u0438\u0442\u0440 \u0442\u043e\u043f\u043b\u0438\u0432\u043e \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044f \u043f\u0440\u043e\u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0431\u0435\u043a\u0433\u0430\u043b\u0438 \u0436\u0443\u0431\u0430\u043d \u0437\u0430\u043c\u0435\u0441\u0442\u0438\u0442\u0435\u043b\u044c \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u043e \u0442\u043e\u0440\u0433\u043e\u0432\u043b\u044f \u043c\u0430\u043d\u0433\u0438\u0441\u0442\u0430\u0443\u0441\u043a\u0430 \u043e\u0431\u043b\u0430\u0441\u0442\u044c \u0444\u043e\u0442\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u0441\u0430\u0439\u0442 lada \u0441\u043e\u043e\u0431\u0449\u0438\u0442\u044c \u0430\u0432\u0442\u043e\u0432\u043b\u0430\u0434\u0435\u043b\u0435\u0446 \u0436\u0430\u043d\u0430\u043e\u0437\u0442\u044c \u0437\u0430\u043f\u0440\u0430\u0432\u043a\u0430 \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u044c \u0432\u044b\u043d\u0443\u0434\u0438\u0442\u044c \u0442\u0440\u0430\u0442\u0438\u0442\u044c \u043d\u0435\u0441\u043a\u043e\u043b...\n\n\n\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438:  \u0430\u0440\u0435\u0441\u0442\u043e\u0432\u0430\u0442\u044c \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u0435\u043d\u043f\u0444 \u0434\u0430\u0442\u044c \u043d\u043e\u0432\u043e\u0441\u0442\u044c \u044f\u0432\u043b\u044f\u0442\u044c\u0441\u044f \u0437\u0430\u043a\u0440\u044b\u0442\u044b\u0439 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u0439 \u043f\u043b\u0430\u0442\u043d\u044b\u0439 \u043f\u043e\u0434\u043f\u0438\u0441\u0447\u0438\u043a \u0442\u0430\u043a\u0436\u0435 \u043c\u043e\u0447\u044c \u043f\u0440\u0438\u043e\u0431\u0440\u0435\u0441\u0442\u0438 \u0434\u043e\u0441\u0442\u0443\u043f \u0434\u0430\u0442\u044c \u043d\u043e\u0432\u043e\u0441\u0442\u044c \u0446\u0435\u043d\u0430 \u0442\u0435\u043d\u0433 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u0440\u043e\u0439\u0442\u0438 \u0430\u0432\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u044f...\n\n\n\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: negative\n\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438:  \u043a\u044b\u0437\u044b\u043b\u043e\u0440\u0434 \u043a\u0430\u0437\u0438\u043d\u0444\u043e\u0440\u043c - \u043a\u044b\u0437\u044b\u043b\u043e\u0440\u0434 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u0440\u0430\u043c\u043a\u0430 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u0434\u0430\u043c\u0430 - \u043e\u043f\u0442\u0438\u043c \u043f\u0440\u043e\u0435\u043a\u0442 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f \u0435\u0440 - \u0431\u0430\u043a \u043f\u0435\u0440\u0435\u0434\u0430\u0432\u0430\u0442\u044c \u043a\u043e\u0440\u0440\u0435\u0441\u043f\u043e\u043d\u0434\u0435\u043d\u0442 \u043c\u0438\u0430 \u043a\u0430\u0437\u0438\u043d\u0444\u043e\u0440\u043c \u0441\u0441\u044b\u043b\u043a\u0430 \u043f\u0440\u0435\u0441\u0441 - \u0441\u043b\u0443\u0436\u0431\u0430 \u0444\u043e\u043d\u0434 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u043f\u0440\u0435\u0434\u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u043e \u0434\u0430\u043c\u0430 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043a\u0440\u0435\u0434\u0438\u0442 \u043f\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435 \u043e\u0431\u043e\u0440\u043e\u0442\u043d\u044b\u0439 \u0441\u0440\u0435\u0434\u0441\u0442\u0432\u043e \u0444\u043e\u0440\u0442\u0435\u0431\u0430\u043d\u043a\u0430 \u0444\u043e\u043d\u0434 \u0434\u0430\u043c\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0438\u0442\u044c \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f \u0435\u0440 - \u0431\u0430\u043a \u0437\u0430\u043d\u0438\u043c\u0430\u0442\u044c\u0441\u044f \u043e\u043f\u0442\u043e\u0432\u044b\u0439 \u0440\u043e\u0437\u043d\u0438\u0447\u043d\u044b\u0439 \u0442\u043e\u0440\u0433\u043e\u0432\u043b\u044f \u043c\u0435\u0442\u0430\u043b\u043b\u043e\u0438\u0437\u0434\u0435\u043b\u0438\u0435 - \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u043d\u044b\u0439 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f \u0435\u0440 - \u0431\u0430\u043a \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0433\u043e\u0434 \u044f\u0432\u043b\u044f\u0442\u044c\u0441\u044f \u043e\u0434\u0438\u043d \u043a\u0440\u0443\u043f\u043d\u044b\u0439 \u043f\u0440\u0435\u0434\u043f\u0440\u0438\u044f\u0442\u0438\u0435 \u0441\u0432\u043e\u0439 \u0441\u0444\u0435\u0440\u0430 \u043f\u043e\u043c\u043e\u0449\u044c \u043a\u0440\u0435\u0434\u0438\u0442\u043d\u044b\u0439 \u0441\u0440\u0435\u0434\u0441\u0442\u0432\u043e \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f \u043f\u043e\u043b\u0443\u0447\u0438\u0442...\n\n\n\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438:  \u043c\u043e\u0441\u043a\u0432\u0430 \u0434\u0435\u043a \u0440\u0438\u0430 \u043d\u043e\u0432\u043e\u0441\u0442\u044c \u043f\u0440\u0430\u0439\u043c\u0430 \u0443\u0445\u043e\u0434\u0438\u0442\u044c \u0433\u043e\u0434 \u043e\u0437\u043d\u0430\u043c\u0435\u043d\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a\u0438\u0439 \u043d\u0435\u0444\u0442\u0435\u0433\u0430\u0437\u043e\u0432\u044b\u0439 \u043e\u0442\u0440\u0430\u0441\u043b\u044c \u0434\u0432\u0430 \u043a\u0440\u0443\u043f\u043d\u044b\u0439 \u043f\u0440\u0438\u0432\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u0441\u0434\u0435\u043b\u043a\u0430 \u043f\u0435\u0440\u0432\u044b\u0439 \u0433\u043e\u0434 \u0431\u0440\u0438\u0442\u0430\u043d\u0441\u043a\u0438\u0439 \u0432\u0440 \u0441\u0442\u0430\u0442\u044c \u0432\u043b\u0430\u0434\u0435\u043b\u0435\u0446 \u0430\u043a\u0446\u0438\u044f \u0440\u043e\u0441\u043d\u0435\u0444\u0442\u044c \u0433\u0435\u0440\u043e\u0439 \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u043a\u0440\u0443\u043f\u043d\u044b\u0439 \u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a\u0438\u0439 \u043d\u0435\u0444\u0442\u044f\u043d\u043e\u0439 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f \u043a\u0443\u043f\u0438\u0442\u044c \u0433\u043e\u0441\u0443\u0434\u0430\u0440\u0441\u0442\u0432\u043e \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u044b\u0439 \u043f\u0430\u043a\u0435\u0442 \u0431\u0430\u0448\u043d\u0435\u0444\u0442\u044c \u0437\u0430\u0442\u0435\u043c \u0440\u0430\u043c\u043a\u0430 \u043f\u0440\u0438\u0432\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439 \u0430\u043a\u0446\u0438\u044f \u043e\u0431\u0440\u0435\u0441\u0442\u0438 \u0434\u0432\u0430 \u043d\u043e\u0432\u044b\u0439 \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0439 \u0438\u043d\u0432\u0435\u0441\u0442\u043e\u0440 \u0438\u0442\u043e\u0433 \u0441\u043e\u0431\u0440\u0430\u0442\u044c \u043f\u043b\u0430\u043d\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u044d\u0442\u043e\u0442 \u0434\u0432\u0430 \u0441\u0434\u0435\u043b\u043a\u0430 \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0442\u044c \u043f\u043e\u0441\u0442\u0443\u043f\u043b\u0435\u043d\u0438\u0435 \u0431\u044e\u0434\u0436\u0435\u0442 \u0442\u0440\u0438\u043b\u043b\u0438\u043e\u043d \u0440\u0443\u0431\u043b\u044c \u0437\u0430\u043a\u0440\u044b\u0432\u0430\u0442\u044c \u043d\u0430\u0448 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0430 \u0442\u0435\u043a\u0443\u0449\u0438\u0439 \u0431\u044e\u0434\u0436\u0435\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0442\u044c ...\n\n\n\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: positive\n\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: positive\n\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438:  \u043a\u0430\u0437\u0438\u043d\u0444\u043e\u0440\u043c - \u043f\u0440\u0435\u0441\u0441 - \u0441\u043b\u0443\u0436\u0431\u0430 \u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u0441\u043a\u0438\u0439 \u043f\u0435\u0440\u0435\u0432\u043e\u0437\u043a\u0430 \u0441\u043e\u043e\u0431\u0449\u0438\u0442\u044c \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u0447\u043f \u0432\u043e\u043a\u0437\u0430\u043b - \u044f\u043d\u0432\u0430\u0440\u044c \u0442\u0435\u0440\u0440\u0438\u0442\u043e\u0440\u0438\u044f \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043f\u0430\u0440\u043a \u0441\u0442\u0430\u043d\u0446\u0438\u044f \u043f\u0440\u043e\u0438\u0437\u043e\u0439\u0442\u0438 \u0437\u0430\u0434\u044b\u043c\u043b\u0435\u043d\u0438\u0435 \u0441\u043f\u0438\u0441\u0430\u0442\u044c \u0432\u0430\u0433\u043e\u043d \u0436\u0435\u0440\u0442\u0432\u0430 \u043f\u043e\u0441\u0442\u0440\u0430\u0434\u0430\u0442\u044c \u0432\u0430\u0433\u043e\u043d \u043f\u043e\u0441\u0442\u0440\u043e\u0439\u043a\u0430 \u043d\u043e\u044f\u0431\u0440\u044c \u0433\u043e\u0434 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435 \u0441\u043d\u044f\u0442\u043e\u0439 \u044d\u043a\u0441\u043f\u043b\u0443\u0430\u0442\u0430\u0446\u0438\u044f \u0433\u043e\u0442\u043e\u0432\u0438\u0442\u044c\u0441\u044f \u0443\u0442\u0438\u043b\u0438\u0437\u0430\u0446\u0438\u044f - \u0433\u043e\u0432\u043e\u0440\u0438\u0442\u044c\u0441\u044f \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435 \u0441\u0440\u0435\u0434\u043d\u0438\u0439 \u0441\u0440\u043e\u043a \u044d\u043a\u0441\u043f\u043b\u0443\u0430\u0442\u0430\u0446\u0438\u044f \u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u0441\u043a\u0438\u0439 \u0432\u0430\u0433\u043e\u043d \u0441\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0442\u044c \u0433\u043e\u0434 \u043f\u0440\u0438\u0447\u0438\u043d\u0430 \u0441\u043b\u0443\u0447\u0438\u0442\u044c\u0441\u044f \u0432\u044b\u044f\u0441\u043d\u044f\u0442\u044c\u0441\u044f \u0438\u0442\u043e\u0433 \u0441\u043b\u0443\u0436\u0435\u0431\u043d\u044b\u0439 \u0440\u0430\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0435 \u0431\u044b\u0442\u044c \u043f\u0440\u0438\u043d\u044f\u0442\u044c \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0439 \u043c\u0435\u0440\u0430 \u0441\u043e\u043e\u0431\u0449\u0430\u0442\u044c \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f \u043d\u0430\u043f\u043e\u043c\u043d\u0438\u0442\u044c - \u0447\u0430\u0441 \u043f\u0443\u043b\u044c\u0442 \u0434\u0447\u0441 \u0433 \u043f...\n\n\n\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: positive\n\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438:  \u043b\u0438\u0448\u0435\u043d\u0438\u0435 \u043e\u043b\u0438\u043c\u043f\u0438\u0439\u0441\u043a\u0438\u0439 \u0437\u043e\u043b\u043e\u0442\u043e \u0438\u043b\u044c\u044f \u0438\u043b\u044c\u0438\u043d \u043e\u0441\u043f\u0430\u0440\u0438\u0432\u0430\u0442\u044c \u0431\u0435\u0441\u043f\u043e\u043b\u0435\u0437\u043d\u043e \u0441\u043a\u0430\u0437\u0430\u0442\u044c \u0432\u0438\u0446\u0430 - \u043c\u0438\u043d\u0438\u0441\u0442\u0440 \u043a\u0443\u043b\u044c\u0442\u0443\u0440\u0430 \u0441\u043f\u043e\u0440\u0442 \u0440\u043a \u0441\u0430\u043a\u0435\u043d \u043c\u0443\u0441\u0430\u0439\u0431\u0435\u043a \u043a\u0443\u043b\u0443\u0430\u0440 \u043c\u0430\u0436\u0438\u043b\u0438\u0441 \u0440\u043a \u043f\u0435\u0440\u0435\u0434\u0430\u0432\u0430\u0442\u044c \u043a\u043e\u0440\u0440\u0435\u0441\u043f\u043e\u043d\u0434\u0435\u043d\u0442 \u0432\u0435\u0440\u043d\u0443\u0442\u044c \u043c\u0435\u0434\u0430\u043b\u044c \u043f\u0435\u043a\u0438\u043d \u043f\u0440\u0438\u043d\u0446\u0438\u043f \u0441\u043e\u0433\u043b\u0430\u0441\u043d\u044b\u0439 \u0440\u0435\u0448\u0435\u043d\u0438\u0435 \u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u044b\u0439 \u043e\u043b\u0438\u043c\u043f\u0438\u0439\u0441\u043a\u0438\u0439 \u043a\u043e\u043c\u0438\u0442\u0435\u0442 \u043c\u043e\u043a\u043d\u0443\u0442\u044c \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0432\u044b\u043d\u0435\u0441\u0442\u0438 \u043e\u043d \u0448\u043e\u043a \u0441\u0430\u043c \u0448\u043e\u043a \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u044c\u0441\u044f \u0441\u0435\u043c\u044c\u044f \u043c\u043e\u043a\u043d\u0443\u0442\u044c \u0432\u0435\u0441\u044c \u043f\u0440\u0430\u0432\u0438\u043b\u043e \u0437\u0430\u043a\u043e\u043d \u0441\u043e\u0433\u043b\u0430\u0441\u043d\u044b\u0439 \u043e\u0441\u043f\u0430\u0440\u0438\u0432\u0430\u0442\u044c \u0431\u0435\u0441\u043f\u043e\u043b\u0435\u0437\u043d\u043e - \u0441\u043a\u0430\u0437\u0430\u0442\u044c \u043c\u0443\u0441\u0430\u0439\u0431\u0435\u043a \u0441\u043b\u043e\u0432\u043e \u0432\u0438\u0446\u0430 - \u043c\u0438\u043d\u0438\u0441\u0442\u0440 \u0432\u0435\u0434\u043e\u043c\u0441\u0442\u0432\u043e \u0433\u043b\u0430\u0432\u0430 \u0432\u0435\u0434\u043e\u043c\u0441\u0442\u0432\u043e \u0430\u0440\u044b\u0441\u0442\u0430\u043d\u0431\u0435\u043a \u043c\u0443\u0445\u0430\u043c\u0435\u0434\u0438\u0443\u043b \u0442\u044f\u0436\u0435\u043b\u043e \u0432\u043e\u0441\u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0442\u044c \u043d\u043e\u0432\u043e\u0441\u0442\u044c \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u043b\u044e\u0431\u044f\u0449\u0438\u0439 \u0441\u043f\u043e\u0440\u0442 \u0447\u0435\u043b\u043e\u0432\u0435\u043a...\n\n\n\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043b\u0435\u0439\u0431\u043b: neutral\n\u0422\u0435\u043a\u0441\u0442 \u043d\u043e\u0432\u043e\u0441\u0442\u0438:  \u0431\u044b\u0432\u0448\u0438\u0439 \u0437\u0430\u043c\u0435\u0441\u0442\u0438\u0442\u0435\u043b\u044c \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c \u0430\u0434\u043c\u0438\u043d\u0438\u0441\u0442\u0440\u0430\u0446\u0438\u044f \u043f\u0440\u0435\u0437\u0438\u0434\u0435\u043d\u0442 \u043a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d \u0431\u0430\u0433\u043b\u0430\u043d \u043c\u0430\u0439\u043b\u044b\u0431\u0430\u0435\u0432 \u0430\u0440\u0435\u0441\u0442\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u043a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d\u0441\u043a\u0438\u0439 \u0441\u043c\u0438 \u043a\u043e\u043d\u0435\u0446 \u043f\u0440\u043e\u0448\u043b\u044b\u0439 \u043d\u0435\u0434\u0435\u043b\u044f \u0445\u0438\u0449\u0435\u043d\u0438\u0435 \u043c\u043b\u0440\u0434 \u0442\u0435\u043d\u0433 \u043c\u043b\u043d \u0440\u0443\u0431\u043b\u044c \u0441\u0440\u0435\u0434\u0441\u0442\u0432\u043e \u0440\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430\u043d\u0441\u043a\u0438\u0439 \u0431\u044e\u0434\u0436\u0435\u0442 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0430\u0440\u0435\u0441\u0442\u043e\u0432\u0430\u0442\u044c \u043a\u043e\u043c\u0438\u0442\u0435\u0442 \u043d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u044c \u0430\u043d\u0430\u043b\u043e\u0433 \u0444\u0441\u0431 \u043a\u0430\u0437\u0430\u0445\u0441\u0442\u0430\u043d \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0438\u0442\u044c \u0441\u0435\u0433\u043e\u0434\u043d\u044f \u044f\u043d\u0432\u0430\u0440\u044c \u043e\u0444\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0439 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435 \u0437\u0430\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u0435 \u0431\u0430\u0433\u043b\u0430\u043d \u043c\u0430\u0439\u043b\u044b\u0431\u0430\u0435\u0432 \u0434\u0440\u0443\u0433\u043e\u0439 \u0431\u044b\u0432\u0448\u0438\u0439 \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a \u0430\u0434\u043c\u0438\u043d\u0438\u0441\u0442\u0440\u0430\u0446\u0438\u044f \u043f\u0440\u0435\u0437\u0438\u0434\u0435\u043d\u0442 \u043d\u0438\u043a\u043e\u043b\u0430\u0439 \u0433\u0430\u043b\u0438\u0445\u0438\u043d\u0430 \u043f\u0440\u043e\u0438\u0437\u043e\u0439\u0442\u0438 \u044f\u043d\u0432\u0430\u0440\u044c \u043a\u043e\u043c\u0438\u0442\u0435\u0442 \u043d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u044c \u0440\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430 \u043f\u043e\u0434\u043e\u0437\u0440\u0435\u043d\u0438\u0435 \u043d\u0435...\n\n\n</pre>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html","title":"\u0410\u043d\u0430\u043b\u0438\u0437 \u0442\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438\u00b6","text":""},{"location":"portfolio/course_nlp/khazah-news-sentiment.html","title":"\u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/course_nlp/khazah-news-sentiment.html#sentiment-analysis","title":"\u0417\u0430\u0434\u0430\u0447\u0430 Sentiment Analysis\u00b6","text":"<ul> <li>\u0421\u0435\u0433\u043e\u0434\u043d\u044f \u043c\u044b \u043f\u043e\u0437\u043d\u0430\u043a\u043e\u043c\u0438\u043c\u0441\u044f \u0441 \u043e\u0441\u043d\u043e\u0432\u0430\u043c\u0438 NLP \u043d\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u0437\u0430\u0434\u0430\u0447\u0438 \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0442\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438 (Sentiment Analysis) \u0441 \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u044f Kaggle Competition</li> <li>\u0417\u0430\u0434\u0430\u0447\u0430 \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u044f - \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c ML \u043c\u043e\u0434\u0435\u043b\u044c, \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u0443\u044e \u0440\u0430\u0437\u043b\u0438\u0447\u0430\u0442\u044c \u0442\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c (\u043f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u0430\u044f, \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u0430\u044f, \u043d\u0435\u0439\u0442\u0440\u0430\u043b\u044c\u043d\u0430\u044f) \u043d\u043e\u0432\u043e\u0441\u0442\u0435\u0439</li> </ul>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html","title":"\u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<p>\u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0442\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438</p> <ul> <li><p>\u0414\u043b\u044f \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u043a\u0438 \u0442\u0435\u043a\u0441\u0442\u0430:</p> <ul> <li>\u041e\u0441\u0442\u0430\u0432\u0438\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0441\u043b\u043e\u0432\u0430 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043a\u0438</li> <li>\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0435 \u0442\u043e\u043a\u0435\u043d\u044b \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u043d\u043e \u0441 <code>WordPunctTokenizer</code></li> <li>\u0424\u0438\u043b\u044c\u0442\u0440\u0443\u0435\u043c \u0441\u0442\u043e\u043f \u0441\u043b\u043e\u0432\u0430</li> </ul> </li> <li><p>\u0414\u043b\u044f \u043f\u0440\u0435\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u0430 \u0432 \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u043c\u0435\u0442\u043e\u0434\u0430\u043c\u0438 <code>BoW</code> \u0438 <code>TF-IDF</code></p> </li> <li><p>\u041e\u0446\u0435\u043d\u0438\u043c \u043e\u0431\u043e\u0431\u0449\u0430\u044e\u0449\u0438\u044e \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f <code>train_test_split</code> \u0438 <code>f1_score</code> \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435</p> </li> <li><p>\u0414\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044f \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u043c \u043b\u0435\u0441\u043e\u043c, \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043a\u0430\u043a\u043e\u0439 \u043c\u0435\u0442\u043e\u0434 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u043b\u0443\u0447\u0448\u0435 \u0432\u0441\u0435\u0433\u043e</p> </li> </ul>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html","title":"\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0414\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<p>\u0414\u0430\u043d\u043d\u044b\u0435 \u0437\u0430\u043f\u0438\u0441\u0430\u043d\u044b \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0435 <code>json</code>. \u0414\u043b\u044f \u0435\u0433\u043e \u0447\u0442\u0435\u043d\u0438\u044f \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u043e\u0439 <code>json</code> \u0438 \u043c\u0435\u0442\u043e\u0434\u043e\u043c <code>open</code>.</p>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html","title":"\u041c\u0435\u0442\u043a\u0430\u00b6","text":"<p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u0435 \u0438 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u043d\u0430\u0448\u0435\u0439 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u0444\u0438\u0447\u0438</p> <ul> <li>\u0411\u0443\u0434\u0435\u043c \u0440\u0435\u0448\u0430\u0442\u044c \u0437\u0430\u0434\u0430\u0447\u0443 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u043d\u0430 3 \u043a\u043b\u0430\u0441\u0441\u0430</li> </ul> <p>\u041a\u0430\u0436\u0434\u044b\u0439 \u043f\u0440\u0438\u043c\u0435\u0440 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0438\u0437 id, \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u0433\u043e \u0444\u0440\u0430\u0433\u043c\u0435\u043d\u0442\u0430 \u043d\u043e\u0432\u043e\u0441\u0442\u0438, \u0438 \u043b\u0435\u0439\u0431\u043b\u0430 (<code>'positive', 'negative', 'neutral'</code>). \u0417\u0430\u0434\u0430\u0447\u0430 - \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u043b\u0435\u0439\u0431\u043b</p>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html","title":"\u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<p>\u041f\u0440\u0435\u0436\u0434\u0435, \u0447\u0435\u043c \u043f\u0435\u0440\u0435\u0439\u0442\u0438 \u043a ML, \u0442\u0435\u043a\u0441\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c</p>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html#1-","title":"\u0428\u0430\u0433 1. \u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u0438 \u0443\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u00b6","text":"<p>\u041f\u0435\u0440\u0432\u044b\u0439 \u0448\u0430\u0433 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 - \u0440\u0430\u0437\u0431\u0438\u0442\u044c \u0442\u0435\u043a\u0441\u0442 \u043d\u0430 \u0435\u0434\u0438\u043d\u0438\u0446\u044b, \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c\u0438 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c. \u042d\u0442\u0438 \u044e\u043d\u0438\u0442\u044b \u043d\u0430\u0437\u044b\u0432\u0430\u044e\u0442\u0441\u044f \u0442\u043e\u043a\u0435\u043d\u0430\u043c\u0438 (tokens), \u0430 \u043f\u0440\u043e\u0446\u0435\u0441\u0441 - \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f (tokenization). \u0412 \u0431\u043e\u043b\u044c\u0448\u0438\u043d\u0441\u0442\u0432\u0435 \u0441\u043b\u0443\u0447\u0430\u0435\u0432 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0442\u043e\u043a\u0435\u043d\u043e\u0432 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442 \u0441\u043b\u043e\u0432\u0430, \u043d\u043e \u0438\u043d\u043e\u0433\u0434\u0430 \u0440\u0430\u0431\u043e\u0442\u0430\u044e\u0442 \u0441 \u0431\u0443\u043a\u0432\u0430\u043c\u0438.</p> <p>\u0421\u0435\u0433\u043e\u0434\u043d\u044f \u043d\u0430 \u0432\u0435\u0431\u0438\u043d\u0430\u0440\u0435 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441\u043e \u0441\u043b\u043e\u0432\u0430\u043c\u0438. \u041f\u0440\u043e\u0449\u0435 \u0432\u0441\u0435\u0433\u043e \u0440\u0430\u0437\u0431\u0438\u0442\u044c \u0442\u0435\u043a\u0441\u0442 \u043d\u0430 \u0441\u043b\u043e\u0432\u0430 \u043f\u043e \u043f\u0440\u043e\u0431\u0435\u043b\u0430\u043c (\u043d\u0435 \u0437\u0430\u0431\u044b\u0432\u0430\u044f \u043f\u0440\u043e \u043f\u0443\u043d\u043a\u0442\u0443\u0430\u0446\u0438\u044e).</p>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html#2-","title":"\u0428\u0430\u0433 2. \u0423\u0434\u0430\u043b\u044f\u0435\u043c \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430\u00b6","text":"<p>\u0421\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430 \u2013 \u044d\u0442\u043e \u0441\u043b\u043e\u0432\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u044b\u043a\u0438\u0434\u044b\u0432\u0430\u044e\u0442\u0441\u044f \u0438\u0437 \u0442\u0435\u043a\u0441\u0442\u0430 \u043f\u0440\u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0435 \u0442\u0435\u043a\u0441\u0442\u0430. \u041a\u043e\u0433\u0434\u0430 \u043c\u044b \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043a \u0442\u0435\u043a\u0441\u0442\u0430\u043c, \u0442\u0430\u043a\u0438\u0435 \u0441\u043b\u043e\u0432\u0430 \u043c\u043e\u0433\u0443\u0442 \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u043c\u043d\u043e\u0433\u043e \u0448\u0443\u043c\u0430, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0438\u0437\u0431\u0430\u0432\u043b\u044f\u0442\u044c\u0441\u044f \u043e\u0442 \u043d\u0435\u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u044b\u0445 \u0441\u043b\u043e\u0432.</p> <p>\u0421\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430 \u044d\u0442\u043e \u043e\u0431\u044b\u0447\u043d\u043e \u043f\u043e\u043d\u0438\u043c\u0430\u044e\u0442 \u0430\u0440\u0442\u0438\u043a\u043b\u0438, \u043c\u0435\u0436\u0434\u043e\u043c\u0435\u0442\u0438\u044f, \u0441\u043e\u044e\u0437\u044b \u0438 \u0442.\u0434., \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u043d\u0435\u0441\u0443\u0442 \u0441\u043c\u044b\u0441\u043b\u043e\u0432\u043e\u0439 \u043d\u0430\u0433\u0440\u0443\u0437\u043a\u0438. \u041f\u0440\u0438 \u044d\u0442\u043e\u043c \u043d\u0430\u0434\u043e \u043f\u043e\u043d\u0438\u043c\u0430\u0442\u044c, \u0447\u0442\u043e \u043d\u0435 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442 \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0441\u043f\u0438\u0441\u043a\u0430 \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432, \u0432\u0441\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u0433\u043e \u0441\u043b\u0443\u0447\u0430\u044f.</p>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html","title":"\u0412\u044b\u0431\u043e\u0440 \u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0442\u043e\u0440\u0430\u00b6","text":"<p>\u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c <code>WordPunctTokenizer</code>, \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u0437\u0430\u0442\u0435\u043c \u0440\u0430\u0437\u043e\u0431\u044c\u0435\u043c \u0442\u0435\u043a\u0441\u0442 \u043d\u0430 \u0441\u043b\u043e\u0432\u0430import re</p>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html","title":"\u0414\u043e\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u0441\u0442\u043e\u043f \u0441\u043b\u043e\u0432\u0430\u00b6","text":"<p>\u0421\u0430\u043c\u0438\u043c \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432, \u0441\u043b\u043e\u0432\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u0431\u0443\u0434\u0443\u0442 \u043d\u0435\u0441\u0442\u0438 \u0442\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c</p>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html","title":"\u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0442\u0435\u043a\u0441\u0442\u0430\u00b6","text":"<p>\u041f\u0440\u043e\u0433\u043e\u043d\u044f\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 <code>json</code> \u0447\u0435\u0440\u0435\u0437 \u0447\u0438\u0441\u0442\u043a\u0443 <code>str</code>, \u0437\u0430\u0442\u0435\u043c \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438 \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u0441\u0442\u043e\u043f \u0441\u043b\u043e\u0432\u0430 \u0438 \u0447\u0438\u0441\u043b\u0430</p>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html#3","title":"\u0428\u0430\u0433 3 . \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0441\u043b\u043e\u0432\u00b6","text":"<ul> <li>\u041e\u0431\u044b\u0447\u043d\u043e \u0442\u0435\u043a\u0441\u0442\u044b \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 \u0440\u0430\u0437\u043d\u044b\u0435 \u0433\u0440\u0430\u043c\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0444\u043e\u0440\u043c\u044b \u043e\u0434\u043d\u043e\u0433\u043e \u0438 \u0442\u043e\u0433\u043e \u0436\u0435 \u0441\u043b\u043e\u0432\u0430, \u0430 \u0442\u0430\u043a\u0436\u0435 \u043c\u043e\u0433\u0443\u0442 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0442\u044c\u0441\u044f \u043e\u0434\u043d\u043e\u043a\u043e\u0440\u0435\u043d\u043d\u044b\u0435 \u0441\u043b\u043e\u0432\u0430.</li> <li>\u0427\u0442\u043e\u0431\u044b \u0443\u043d\u0438\u0444\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441\u043b\u043e\u0432\u0430 \u0432 \u0442\u0435\u043a\u0441\u0442\u0435 \u0438 \u0438\u0437\u0431\u0430\u0432\u0438\u0442\u044c\u0441\u044f \u043e\u0442 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0444\u043e\u0440\u043c \u0441\u043b\u043e\u0432\u0430, \u0441\u043b\u043e\u0432\u0430 \u0432 \u0442\u0435\u043a\u0441\u0442\u0435 \u043c\u043e\u0436\u043d\u043e \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c.</li> </ul> <p>\u0421\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442 2 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0441\u043f\u043e\u0441\u043e\u0431\u0430 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0441\u043b\u043e\u0432:</p> <ul> <li>\u0441\u0442\u0435\u043c\u043c\u0438\u043d\u0433 (stemming)</li> <li>\u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f (\u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f).</li> </ul> <p>\u0412 \u043e\u0431\u0449\u0438\u0445 \u0447\u0435\u0440\u0442\u0430\u0445 \u043e\u043d\u0438 \u043f\u043e\u0445\u043e\u0436\u0435, \u043d\u043e \u043c\u0435\u0436\u0434\u0443 \u044d\u0442\u0438\u043c\u0438 \u043c\u0435\u0442\u043e\u0434\u0430\u043c\u0438 \u0435\u0441\u0442\u044c \u0440\u0430\u0437\u043b\u0438\u0447\u0438\u044f. \u0412 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 \u044f\u0437\u044b\u043a\u0430 \u0438 \u0437\u0430\u0434\u0430\u0447\u0438 \u0442\u043e\u0442 \u0438\u043b\u0438 \u0438\u043d\u043e\u0439 \u043c\u0435\u0442\u043e\u0434 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043f\u0440\u0435\u0434\u043f\u043e\u0447\u0442\u0438\u0442\u0435\u043b\u044c\u043d\u0435\u0435.</p> <ul> <li><p><code>\u0421\u0442\u0435\u043c\u043c\u0438\u043d\u0433</code> (\u0430\u043d\u0433\u043b. stemming \u2014 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u044c \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435) \u2014 \u044d\u0442\u043e \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u043e\u0441\u043d\u043e\u0432\u044b \u0441\u043b\u043e\u0432\u0430 \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430. \u041e\u0441\u043d\u043e\u0432\u0430 \u0441\u043b\u043e\u0432\u0430 \u043d\u0435 \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u0441\u043e\u0432\u043f\u0430\u0434\u0430\u0435\u0442 \u0441 \u043c\u043e\u0440\u0444\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u043c \u043a\u043e\u0440\u043d\u0435\u043c \u0441\u043b\u043e\u0432\u0430 \u0438 \u043d\u0435 \u043e\u0431\u044f\u0437\u0430\u043d\u0430 \u044f\u0432\u043b\u044f\u0442\u044c\u0441\u044f \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u043c \u0441\u043b\u043e\u0432\u043e\u043c \u0432 \u044f\u0437\u044b\u043a\u0435. \u0421\u0442\u0435\u043c\u043c\u0438\u043d\u0433 \u2013 \u044d\u0442\u043e \u0433\u0440\u0443\u0431\u044b\u0439 \u044d\u0432\u0440\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043f\u0440\u043e\u0446\u0435\u0441\u0441, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043e\u0442\u0440\u0435\u0437\u0430\u0435\u0442 \u00ab\u043b\u0438\u0448\u043d\u0435\u0435\u00bb \u043e\u0442 \u043a\u043e\u0440\u043d\u044f \u0441\u043b\u043e\u0432, \u0447\u0430\u0441\u0442\u043e \u044d\u0442\u043e \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u0442 \u043a \u043f\u043e\u0442\u0435\u0440\u0435 \u0441\u043b\u043e\u0432\u043e\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0441\u0443\u0444\u0444\u0438\u043a\u0441\u043e\u0432</p> </li> <li><p><code>\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f</code> \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u0442 \u0432\u0441\u0435 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0449\u0438\u0435\u0441\u044f \u0441\u043b\u043e\u0432\u043e\u0444\u043e\u0440\u043c\u044b \u043a \u043e\u0434\u043d\u043e\u0439, \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0441\u043b\u043e\u0432\u0430\u0440\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u0435. \u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0438 \u043c\u043e\u0440\u0444\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0430\u043d\u0430\u043b\u0438\u0437, \u0447\u0442\u043e\u0431\u044b \u0432 \u0438\u0442\u043e\u0433\u0435 \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u0441\u043b\u043e\u0432\u043e \u043a \u0435\u0433\u043e \u043a\u0430\u043d\u043e\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0444\u043e\u0440\u043c\u0435 \u2013 \u043b\u0435\u043c\u043c\u0435.</p> </li> </ul>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html#stemmer","title":"Stemmer\u00b6","text":"<p>\u041a\u0430\u043a \u043f\u0440\u0438\u043c\u0435\u0440 \u0441\u043b\u0443\u0448\u0430\u0435\u043c, \u0441\u043b\u0443\u0448\u0430\u0442\u044c, \u0441\u043b\u0443\u0449\u0430\u043b\u0438 \u0438\u043c\u0435\u044e\u0442 \u0442\u043e\u0442 \u0436\u0435 \u0441\u0430\u043c\u044b\u0439 \u0431\u043e\u0437\u043e\u0432\u044b\u0439 \u043a\u043e\u0440\u0435\u043d\u0442\u044c \u0441\u043b\u0443\u0448\u0430, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0432\u0435\u043d\u043d\u043e \u043f\u0440\u0438 \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u0438 \u0432\u0441\u0435 \u044d\u0442\u0438 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u0441\u043b\u043e\u0432 \u0431\u0443\u0434\u0443\u0442 \u0438\u043c\u0435\u0442\u044c \u043e\u0434\u043d\u0443 \u0444\u043e\u0440\u043c\u0443 <code>\u0441\u043b\u0443\u0448\u0430</code>, \u0447\u0442\u043e \u0441\u043d\u0438\u0437\u0438\u0442 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u043b\u043e\u0432 \u0432 \u0441\u043b\u043e\u0432\u0430\u0440\u0435</p>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html#pymorphy","title":"pymorphy (\u043c\u043e\u0440\u0444\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440)\u00b6","text":"<p>\u0412\u0438\u0434\u043d\u043e, \u0447\u0442\u043e \u0434\u043b\u044f \u0440\u0443\u0441\u0441\u043a\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u0445\u043e\u0440\u043e\u0448\u0438\u0439. \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0447\u0442\u043e-\u043d\u0438\u0431\u0443\u0434\u044c \u043f\u043e\u043b\u0443\u0447\u0448\u0435</p>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html#bow","title":"\u041c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 (BoW)\u00b6","text":""},{"location":"portfolio/course_nlp/khazah-news-sentiment.html","title":"\u041c\u0435\u0448\u043e\u043a \u0441\u043b\u043e\u0432\u00b6","text":"<p>Bag of Words \u0438\u043b\u0438 \u043c\u0435\u0448\u043e\u043a \u0441\u043b\u043e\u0432 \u2014 \u044d\u0442\u043e \u043c\u043e\u0434\u0435\u043b\u044c, \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0449\u0430\u044f \u0441\u043e\u0431\u043e\u0439 \u043d\u0435\u0443\u043f\u043e\u0440\u044f\u0434\u043e\u0447\u0435\u043d\u043d\u044b\u0439 \u043d\u0430\u0431\u043e\u0440 \u0441\u043b\u043e\u0432, \u0432\u0445\u043e\u0434\u044f\u0449\u0438\u0445 \u0432 \u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0435\u043c\u044b\u0439 \u0442\u0435\u043a\u0441\u0442.</p> <p>\u0427\u0430\u0441\u0442\u043e \u043c\u043e\u0434\u0435\u043b\u044c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u0432 \u0432\u0438\u0434\u0435 \u043c\u0430\u0442\u0440\u0438\u0446\u044b, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\u043c\u0443 \u0442\u0435\u043a\u0441\u0442\u0443, \u0430 \u0441\u0442\u043e\u043b\u0431\u0446\u044b \u2014 \u0432\u0445\u043e\u0434\u044f\u0449\u0438\u0435 \u0432 \u043d\u0435\u0433\u043e \u0441\u043b\u043e\u0432\u0430. \u042f\u0447\u0435\u0439\u043a\u0438 \u043d\u0430 \u043f\u0435\u0440\u0435\u0441\u0435\u0447\u0435\u043d\u0438\u0438 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0447\u0438\u0441\u043b\u043e\u043c \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430 \u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0439 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442. \u0414\u0430\u043d\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u0443\u0434\u043e\u0431\u043d\u0430 \u0442\u0435\u043c, \u0447\u0442\u043e \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u0442 \u0447\u0435\u043b\u043e\u0432\u0435\u0447\u0435\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u043a \u0441\u043b\u043e\u0432 \u0432 \u043f\u043e\u043d\u044f\u0442\u043d\u044b\u0439 \u0434\u043b\u044f \u043a\u043e\u043c\u043f\u044c\u0442\u0435\u0440\u0430 \u044f\u0437\u044b\u043a \u0446\u0438\u0444\u0440.</p> <p>\u041d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435 \u043c\u044b \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430 \u0432 \u043a\u043e\u0440\u043f\u0443\u0441\u0435</p>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html","title":"\u041f\u043e\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u043c \u043c\u0435\u0442\u043a\u0443\u00b6","text":"<p>sklearn \u043c\u043e\u0434\u0435\u043b\u0438 (\u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u044b) \u043c\u043e\u0433\u0443\u0442\u044c \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 \u0444\u043e\u0440\u043c\u0430\u0442\u043e\u0440\u043c <code>str</code> \u0432 \u043c\u0435\u0442\u043a\u0430\u0445, \u0432 \u043b\u044e\u0431\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u0434\u0430\u0432\u0430\u0439\u0442\u0435 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0438\u0445 \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435</p>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html","title":"\u041c\u0435\u0448\u043e\u043a \u0441\u043b\u043e\u0432 (\u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445)\u00b6","text":"<p>Bag of Words \u0438\u043b\u0438 \u043c\u0435\u0448\u043e\u043a \u0441\u043b\u043e\u0432 \u2014 \u044d\u0442\u043e \u043c\u043e\u0434\u0435\u043b\u044c, \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0449\u0430\u044f \u0441\u043e\u0431\u043e\u0439 \u043d\u0435\u0443\u043f\u043e\u0440\u044f\u0434\u043e\u0447\u0435\u043d\u043d\u044b\u0439 \u043d\u0430\u0431\u043e\u0440 \u0441\u043b\u043e\u0432, \u0432\u0445\u043e\u0434\u044f\u0449\u0438\u0445 \u0432 \u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0435\u043c\u044b\u0439 \u0442\u0435\u043a\u0441\u0442</p>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html","title":"\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u00b6","text":"<p>\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c Random Forest</p>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html","title":"\u041e\u0446\u0435\u043d\u043a\u0430 \u041a\u0430\u0447\u0435\u0441\u0442\u0432\u0430\u00b6","text":"<p>\u041a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430 \u0431\u0443\u0434\u0435\u043c \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0442\u044c \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0430\u043c accuracy \u0438 f1.</p>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html","title":"\u041f\u0435\u0440\u0435\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b\u00b6","text":""},{"location":"portfolio/course_nlp/khazah-news-sentiment.html#tf-idf","title":"\u041c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 (TF-IDF)\u00b6","text":""},{"location":"portfolio/course_nlp/khazah-news-sentiment.html#tf-idf-","title":"TF-IDF - \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u0447\u0443\u0442\u044c \u043f\u043e\u0443\u043c\u043d\u0435\u0435 (\u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445)\u00b6","text":"<p>TF-IDF (\u043e\u0442 \u0430\u043d\u0433\u043b. TF \u2014 term frequency, IDF \u2014 inverse document frequency) \u2014 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043c\u0435\u0440\u0430, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0430\u044f \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u0441\u043b\u043e\u0432\u0430 \u0432 \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442\u0435 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430, \u044f\u0432\u043b\u044f\u044e\u0449\u0435\u0433\u043e\u0441\u044f \u0447\u0430\u0441\u0442\u044c\u044e \u043a\u043e\u043b\u043b\u0435\u043a\u0446\u0438\u0438 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0438\u043b\u0438 \u043a\u043e\u0440\u043f\u0443\u0441\u0430. \u0412\u0435\u0441 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430 \u043f\u0440\u043e\u043f\u043e\u0440\u0446\u0438\u043e\u043d\u0430\u043b\u0435\u043d \u0447\u0430\u0441\u0442\u043e\u0442\u0435 \u0443\u043f\u043e\u0442\u0440\u0435\u0431\u043b\u0435\u043d\u0438\u044f \u044d\u0442\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430 \u0432 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0435 \u0438 \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u043f\u0440\u043e\u043f\u043e\u0440\u0446\u0438\u043e\u043d\u0430\u043b\u0435\u043d \u0447\u0430\u0441\u0442\u043e\u0442\u0435 \u0443\u043f\u043e\u0442\u0440\u0435\u0431\u043b\u0435\u043d\u0438\u044f \u0441\u043b\u043e\u0432\u0430 \u0432\u043e \u0432\u0441\u0435\u0445 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0445 \u043a\u043e\u043b\u043b\u0435\u043a\u0446\u0438\u0438.</p> <p>Term Frequency \u0447\u0438\u0441\u043b\u043e \u0440\u0430\u0437 \u0442\u0435\u0440\u043c $t$ \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u0442\u0441\u044f \u0432 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0435 $d$.</p> <p>$$ TF_{t,d} = term\\!\\!-\\!\\!frequency(t, d) $$</p> <p>Inverse Document Frequency \u043c\u0435\u0440\u0430 \u0442\u043e\u0433\u043e, \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043d\u0435\u0441\u0435\u0442 \u0434\u0430\u043d\u043d\u043e\u0435 \u0441\u043b\u043e\u0432\u043e. \u0418\u043d\u044b\u043c\u0438 \u0441\u043b\u043e\u0432\u0430\u043c\u0438, \u0447\u0430\u0441\u0442\u043e\u0442\u043d\u044b\u0435 \u0441\u043b\u043e\u0432\u0430, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u0435\u0441\u044f \u0432\u043e \u0432\u0441\u0435\u0445 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0445 \u043d\u0435\u0441\u0443\u0442 \u043c\u0430\u043b\u043e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438, \u0432 \u0442\u043e \u0432\u0440\u0435\u043c\u044f \u043a\u0430\u043a \u0441\u043b\u043e\u0432\u0430 \u0447\u0430\u0441\u0442\u043e\u0442\u043d\u044b\u0435 \u043b\u0438\u0448\u044c \u0432 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u043d\u043e\u043c \u0447\u0438\u0441\u043b\u0435 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 \u0431\u043e\u043b\u044c\u0448\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043e\u0431 \u044d\u0442\u0438\u0445 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0445. IDF - \u044d\u0442\u043e \u0438\u043d\u0432\u0435\u0440\u0441\u0438\u044f \u0447\u0430\u0441\u0442\u043e\u0442\u044b, \u0441 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u0441\u043b\u043e\u0432\u043e \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u0442\u0441\u044f \u0432 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0445 \u043a\u043e\u043b\u043b\u0435\u043a\u0446\u0438\u0438.</p> <p>$$ IDF_t = inverse\\!\\!-\\!\\!document\\!\\!-\\!\\!frequency(t) = \\log \\frac{N}{DF_t} $$</p> <p>$N$ - \u0447\u0438\u0441\u043b\u043e \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u043a\u043e\u0440\u043f\u0443\u0441\u0435.</p> <p>$DF_t$ - \u0447\u0438\u0441\u043b\u043e \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u0445 \u0441\u043b\u043e\u0432\u043e $t$.</p> <p>$$ TF\\!\\!-\\!\\!IDF_{t,d} = TF_{t,d} \\times IDF_t $$</p> <p>TF-IDF \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u0442 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u0441\u043b\u043e\u0432 \u0432 \u043a\u043e\u0440\u043f\u0443\u0441\u0435 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432.</p>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html#tf-idf","title":"\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 (TF-IDF)\u00b6","text":"<p>\u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438 \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u0435\u0435 \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445, \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435</p>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html","title":"\u041e\u0446\u0435\u043d\u043a\u0430 \u041a\u0430\u0447\u0435\u0441\u0442\u0432\u0430\u00b6","text":"<p>\u041a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430 \u0431\u0443\u0434\u0435\u043c \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0442\u044c \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0430\u043c accuracy \u0438 f1.</p>"},{"location":"portfolio/course_nlp/khazah-news-sentiment.html","title":"\u041f\u0435\u0440\u0435\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b\u00b6","text":""},{"location":"portfolio/course_nlp/preset_NER.html","title":"preset NER","text":"In\u00a0[64]: Copied! <pre># !python3 -m pip install natasha\n</pre> # !python3 -m pip install natasha In\u00a0[14]: Copied! <pre>import os\nimport re\nfrom tqdm import tqdm\n\n# \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0432\u0441\u0435 \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0430\u043c \u043f\u043e\u043d\u0430\u0434\u043e\u0431\u044f\u0442\u0441\u044f\nfrom natasha import (Segmenter,MorphVocab,\n                     NewsEmbedding,NewsMorphTagger,NewsSyntaxParser,NewsNERTagger,\n                     PER,NamesExtractor,DatesExtractor,MoneyExtractor,AddrExtractor,\n                     Doc\n                    )\n</pre> import os import re from tqdm import tqdm  # \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0432\u0441\u0435 \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0430\u043c \u043f\u043e\u043d\u0430\u0434\u043e\u0431\u044f\u0442\u0441\u044f from natasha import (Segmenter,MorphVocab,                      NewsEmbedding,NewsMorphTagger,NewsSyntaxParser,NewsNERTagger,                      PER,NamesExtractor,DatesExtractor,MoneyExtractor,AddrExtractor,                      Doc                     ) <p>\u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0432\u0441\u0435 \u043d\u0430\u0448\u0438 \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u044b</p> In\u00a0[15]: Copied! <pre>segmenter = Segmenter()\nmorph_vocab = MorphVocab()\n\nemb = NewsEmbedding()\nmorph_tagger = NewsMorphTagger(emb)\nsyntax_parser = NewsSyntaxParser(emb)\nner_tagger = NewsNERTagger(emb)\n\nnames_extractor = NamesExtractor(morph_vocab)\n# dates_extractor = DatesExtractor(morph_vocab)\n# money_extractor = MoneyExtractor(morph_vocab)\n# addr_extractor = AddrExtractor(morph_vocab)\n</pre> segmenter = Segmenter() morph_vocab = MorphVocab()  emb = NewsEmbedding() morph_tagger = NewsMorphTagger(emb) syntax_parser = NewsSyntaxParser(emb) ner_tagger = NewsNERTagger(emb)  names_extractor = NamesExtractor(morph_vocab) # dates_extractor = DatesExtractor(morph_vocab) # money_extractor = MoneyExtractor(morph_vocab) # addr_extractor = AddrExtractor(morph_vocab) In\u00a0[16]: Copied! <pre>with open('res.txt', 'r') as f:\n    text = f.read().strip()\n</pre> with open('res.txt', 'r') as f:     text = f.read().strip() In\u00a0[17]: Copied! <pre>text\n</pre> text Out[17]: <pre>'\u041f\u0440\u0435\u0434\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u043c\u044b \u043f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u0438 \u043e\u0442 \u043e\u0431\u0435\u0437\u044c\u044f\u043d\u044b \u0432 21 \u0432\u0435\u043a\u0435 \u0442\u0435\u0440\u043f\u0438\u0442 \u043a\u0440\u0430\u0445. \u0423\u0447\u0435\u043d\u044b\u0435 \u0437\u0430\u0448\u043b\u0438 \u0432 \u0442\u0443\u043f\u0438\u043a. \u0417\u0430 \u0433\u043e\u0434\u044b \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0439 \u0438\u043c \u0442\u0430\u043a \u0438 \u043d\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c \u0434\u043e\u043a\u0430\u0437\u0430\u0442\u044c \u0442\u0435\u043e\u0440\u0438\u044e \u044d\u0432\u043e\u043b\u044e\u0446\u0438\u0438 \u0438 \u043d\u0430\u0439\u0442\u0438 \u043e\u0431\u0449\u0438\u0445 \u043f\u0440\u0435\u0434\u043a\u043e\u0432 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 \u0438 \u043f\u0440\u0438\u043c\u0430\u0442\u043e\u0432. \u0418\u043d\u0430\u0447\u0435 \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0448\u0438\u043c\u043f\u0430\u043d\u0437\u0435 \u0438\u043b\u0438 \u0433\u043e\u0440\u0438\u043b\u043b\u044b \u0443\u0436\u0435 \u0434\u0430\u0432\u043d\u043e \u043f\u0440\u0435\u0432\u0440\u0430\u0442\u0438\u043b\u0438\u0441\u044c \u0431\u044b \u0432 \u043b\u044e\u0434\u0435\u0439. \u041c\u044b \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u043d\u0435 \u043f\u043e\u0445\u043e\u0436\u0438 \u0434\u0440\u0443\u0433 \u043d\u0430 \u0434\u0440\u0443\u0433\u0430, \u0447\u0442\u043e\u0431\u044b \u0431\u044b\u0442\u044c \u0440\u043e\u0434\u0441\u0442\u0432\u0435\u043d\u043d\u0438\u043a\u0430\u043c\u0438. \u0418 \u0434\u0430\u0436\u0435 \u0433\u0435\u043d\u0435\u0442\u0438\u043a\u0438 \u044d\u0442\u043e \u043f\u043e\u0434\u0442\u0432\u0435\u0440\u0436\u0434\u0430\u044e\u0442. \u0422\u0430\u043a \u0447\u0442\u043e \u0414\u0430\u0440\u0432\u0438\u043d \u0431\u044b\u043b \u043d\u0435\u043f\u0440\u0430\u0432. \u041d\u0438\u043a\u0430\u043a\u043e\u0433\u043e \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044f \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u043a \u043e\u0431\u0435\u0437\u044c\u044f\u043d\u0430\u043c \u043d\u0435 \u0438\u043c\u0435\u0435\u0442. \u0414\u043e \u0441\u0438\u0445 \u043f\u043e\u0440 \u0443\u0447\u0435\u043d\u044b\u0435, \u0431\u0438\u043e\u043b\u043e\u0433\u0438, \u043f\u0430\u043b\u0435\u043e\u043d\u0442\u043e\u043b\u043e\u0433\u0438, \u0430\u043d\u0442\u0440\u043e\u043f\u043e\u043b\u043e\u0433\u0438 \u0438 \u043f\u0440\u043e\u0447\u0438\u0435, \u0438 \u0442\u0438\u043f\u0430 \u043c\u0435\u043d\u044f \u043f\u0435\u0440\u0441\u043e\u043d\u0430\u0436\u0438, \u0432\u044b\u043d\u0443\u0436\u0434\u0435\u043d\u044b \u044d\u0442\u043e \u043e\u043f\u0440\u043e\u0432\u0435\u0440\u0433\u0430\u0442\u044c. \u0418 \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043d\u044b\u043d\u0447\u0435 \u044f, \u0421\u0442\u0430\u043d\u0438\u0441\u043b\u0430\u0432 \u0414\u043e\u0440\u0431\u044b\u0448\u0435\u0432\u0441\u043a\u0438\u0439, \u0431\u0443\u0434\u0443 \u0436\u0430\u0440\u0438\u0442\u044c \u044d\u0442\u043e\u0442 \u0434\u0443\u0440\u0430\u0446\u043a\u0438\u0439 \u043c\u0438\u0444 \u0432\u043c\u0435\u0441\u0442\u0435 \u0441 \u0420\u0431\u0430\u043a\u0430 \u0422\u0440\u0435\u043d\u0434\u0430\u043c\u0438. \u041d\u0430 \u043f\u043b\u0430\u043d\u0435\u0442\u0435 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442 \u043e\u0447\u0435\u043d\u044c \u043c\u043d\u043e\u0433\u043e \u043c\u043b\u0435\u043a\u043e\u043f\u0438\u0442\u0430\u044e\u0449\u0438\u0445. \u0418 \u044d\u0442\u0438 \u043c\u043b\u0435\u043a\u043e\u043f\u0438\u0442\u0430\u044e\u0449\u0438\u0435 \u043f\u043e-\u0440\u0430\u0437\u043d\u043e\u043c\u0443 \u0440\u043e\u0434\u0441\u0442\u0432\u0435\u043d\u043d\u044b \u0434\u0440\u0443\u0433 \u0434\u0440\u0443\u0433\u0443. \u0415\u0441\u0442\u044c \u043c\u0435\u0440\u0437\u0443\u043d\u044b, \u0440\u0443\u043a\u043e\u043a\u0440\u044b\u043b\u044b\u0435, \u0445\u0438\u0449\u043d\u044b\u0435, \u043a\u043e\u043f\u044b\u0442\u043d\u044b\u0435, \u043a\u0438\u0442\u043e\u043e\u0431\u0440\u0430\u0437\u043d\u044b\u0435, \u043f\u0430\u043d\u0433\u043e\u043b\u0438\u043d\u044b \u0438 \u0432\u0441\u044f\u043a\u0438\u0435 \u043f\u0440\u043e\u0447\u0438\u0435. \u0410 \u0435\u0441\u0442\u044c \u043f\u0440\u0438\u043c\u0430\u0442\u044b. \u041f\u0440\u0438\u043c\u0430\u0442\u043e\u0432 \u043c\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u043d\u044b\u0445. \u0418 \u0443\u0434\u0438\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u044d\u0442\u043e \u043f\u0440\u043e\u0441\u0442\u043e \u043e\u0434\u0438\u043d \u0438\u0437 \u0432\u0435\u043b\u0438\u043a\u043e\u0433\u043e \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u0430 \u044d\u0442\u0438\u0445 \u0441\u0430\u043c\u044b\u0445 \u043f\u0440\u0438\u043c\u0430\u0442\u043e\u0432. \u0418 \u044d\u0442\u043e \u0440\u043e\u0434\u0441\u0442\u0432\u043e, \u0435\u0441\u043b\u0438 \u0435\u0449\u0435 \u0432 18-19 \u0432\u0435\u043a\u0435 \u0431\u044b\u043b\u043e \u043d\u0435\u0434\u0430\u043b\u0435\u043a\u043e \u043e\u0447\u0435\u0432\u0438\u0434\u043d\u043e \u0438 \u043d\u0435 \u0434\u043e \u043a\u043e\u043d\u0446\u0430 \u043e\u0447\u0435\u0432\u0438\u0434\u043d\u043e, \u0442\u043e \u0437\u0430 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0435 \u0433\u0434\u0435-\u0442\u043e \u043b\u0435\u0442 \u0441\u0442\u043e \u0443\u0436\u0435 \u0431\u043e\u043b\u0435\u0435-\u043c\u0435\u043d\u0435\u0435 \u0440\u0430\u0437\u043e\u0431\u0440\u0430\u043b\u0438\u0441\u044c. \u0418 \u0433\u043b\u0430\u0432\u043d\u0430\u044f \u0433\u0440\u0443\u043f\u043f\u0430 \u0441\u0430\u043c\u044b\u0445 \u0436\u0438\u0432\u044b\u0445 \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u043c\u043e\u0432 \u0443\u0436\u0435 \u0434\u0430\u0432\u043d\u043e \u043d\u0430\u0432\u0435\u0434\u0435\u043d \u0432 \u043f\u043e\u043b\u043d\u044b\u0439 \u043f\u043e\u0440\u044f\u0434\u043e\u043a. \u041e\u0442\u0440\u044f\u0434 \u043f\u0440\u0438\u043c\u0430\u0442\u043e\u0432 \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0437\u0443\u0435\u0442\u0441\u044f \u0441\u044a\u0435\u0434\u043d\u043e\u0441\u0442\u044c\u044e, \u0434\u0440\u0435\u0432\u0435\u0441\u043d\u043e\u0441\u0442\u044c\u044e, \u043a\u0430\u0441\u0442\u0440\u0438\u0442\u0435\u0439\u043a\u0438 \u0440\u0430\u0437\u043c\u043d\u043e\u0436\u0435\u043d\u0438\u044f, \u043a\u043e\u0433\u0434\u0430 \u043c\u0430\u043b\u043e \u0434\u0435\u0442\u0435\u043d\u044b\u0448\u0435\u0439 \u0434\u043e\u043b\u0433\u043e \u0432\u044b\u0440\u0430\u0449\u0438\u0432\u0430\u044e\u0442\u0441\u044f. \u041d\u0443 \u0438 \u043d\u0430 \u0441\u0430\u043c\u043e\u043c \u0434\u0435\u043b\u0435 \u0433\u043b\u0430\u0432\u043d\u043e\u0435 \u0430\u043d\u0430\u0442\u043e\u043c\u0438\u044f. \u0422\u043e \u0435\u0441\u0442\u044c \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u044b\u043c \u043d\u0430\u0431\u043e\u0440\u043e\u043c \u0437\u0443\u0431\u043e\u0432, \u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435\u043c \u0437\u0443\u0431\u043e\u0432, \u0442\u043e\u0439 \u0436\u0435 \u0441\u0430\u043c\u043e\u0439 \u0433\u0435\u043d\u0435\u0442\u0438\u043a\u043e\u0439, \u0431\u0438\u043e\u0445\u0438\u043c\u0438\u0435\u0439 \u0438 \u0442\u0430\u043a \u0434\u0430\u043b\u0435\u0435. \u0418 \u043f\u043e \u0432\u0441\u0435\u043c \u044d\u0442\u0438\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u044d\u0442\u043e \u043f\u0440\u0438\u043c\u0430\u0442 \u043f\u0440\u043e\u0441\u0442\u043e \u043e\u0434\u043d\u0430 \u0438\u0437 \u043e\u0431\u0435\u0437\u044c\u044f\u043d. \u0423 \u043d\u0430\u0441 \u0442\u043e\u0442 \u0436\u0435 \u0441\u0430\u043c\u044b\u0439 \u043d\u0430\u0431\u043e\u0440 \u0437\u0443\u0431\u043e\u0432 \u0438 \u0431\u043e\u043b\u0435\u0435 \u0442\u043e\u0433\u043e \u043e\u0447\u0435\u043d\u044c \u043f\u0440\u0438\u043c\u0438\u0442\u0438\u0432\u043d\u044b\u0439, \u043e\u0447\u0435\u043d\u044c \u0442\u0430\u043a\u043e\u0439 \u043f\u0440\u043e\u0441\u0442\u043e\u0439 \u043d\u0435\u0437\u0430\u043c\u044b\u0441\u043b\u043e\u0432\u0430\u0442\u044b\u0439. \u041d\u0443 \u0431\u0430\u0437\u043e\u0432\u043e\u0435 \u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u043f\u043e\u0434\u043e\u0431\u043d\u043e\u0435 \u0431\u044b\u043b\u043e \u0443\u0436\u0435 40 \u043c\u0438\u043b\u043b\u0438\u043e\u043d\u043e\u0432 \u043b\u0435\u0442 \u043d\u0430\u0437\u0430\u0434 \u043d\u0430 \u0441\u0430\u043c\u043e\u043c \u0434\u0435\u043b\u0435. \u041d\u0443 \u0430 \u043f\u043e\u0440\u044f\u0434\u043a\u0430 26 \u043c\u0438\u043b\u043b\u0438\u043e\u043d\u043e\u0432 \u043b\u0435\u0442 \u043d\u0430\u0437\u0430\u0434 \u0441\u043b\u043e\u0436\u0438\u043b\u0441\u044f \u0442\u0430\u043c \u044d\u0442\u043e\u0442 \u0441\u0430\u043c\u044b\u0439 \u0443\u0437\u043e\u0440 \u0434\u0440\u0438\u0430\u043f\u0438-\u0442\u0435\u0433\u0430, \u0442\u0430\u043a \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u043c\u044b\u0439 \u043d\u0430 \u0437\u0443\u0431\u0430\u0445. \u041d\u0443 \u0442\u0430\u043a\u043e\u0439 \u0433\u043e\u043c\u0438\u043d\u043e\u0438\u0434\u043d\u044b\u0439, \u0433\u043e\u043c\u0438\u043d\u0438\u0434\u043d\u044b\u0439 \u0434\u0430\u0436\u0435. \u0418 \u0432 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u0435 \u0441 \u0442\u0435\u0445 \u043f\u043e\u0440 \u0437\u0443\u0431\u044b \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u0438\u0430\u043b\u044c\u043d\u043e \u043d\u0435 \u043c\u0435\u043d\u044f\u043b\u0438\u0441\u044c. \u041d\u0435\u043c\u043d\u043e\u0436\u043a\u043e \u0438\u0437\u043c\u0435\u043d\u044f\u043b\u0438\u0441\u044c \u043f\u0440\u043e\u043f\u043e\u0440\u0446\u0438\u0438, \u0440\u0430\u0437\u043c\u0435\u0440\u044b, \u0442\u0430\u043c \u043a\u043b\u044b\u043a\u0438 \u0441\u0442\u0430\u043b\u0438 \u0447\u0443\u0442\u044c \u043f\u043e\u043c\u0435\u043d\u044c\u0448\u0435 \u0443 \u043d\u0430\u0441. \u0418 \u043d\u0430\u0448\u0438 \u0437\u0443\u0431\u044b, \u043f\u043e \u0441\u0443\u0442\u0438 \u044d\u0442\u043e \u0447\u0443\u0442\u044c \u0442\u0430\u043c \u043a\u0430\u043a \u0431\u044b \u043f\u043e\u043a\u0440\u0443\u043f\u043d\u0435\u0435 \u0437\u0443\u0431\u044b \u043f\u0440\u043e\u043a\u043e\u043d\u0441\u0443\u043b\u0430. \u0421\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u043d\u0430\u0448\u0438\u0445 \u0440\u0443\u0447\u0435\u043a, \u043d\u043e\u0436\u0435\u043a \u0442\u043e\u0436\u0435 \u0432\u043e\u043e\u0431\u0449\u0435 \u0441\u043e\u0432\u0441\u0435\u043c \u0431\u0430\u0437\u043e\u0432\u043e\u0435. \u0422\u043e \u0435\u0441\u0442\u044c \u0432\u0435\u0441\u044c \u043d\u0430\u0431\u043e\u0440 \u043a\u043e\u0441\u0442\u0435\u0439 \u0443 \u043d\u0430\u0441 \u0434\u0430\u0436\u0435 \u043d\u0435 \u0441\u0440\u043e\u0441\u043b\u0430\u0441\u044c, \u043c\u0430\u043b\u0430\u044f \u0431\u0435\u0440\u0446\u043e\u0432\u0430\u044f, \u0441 \u0431\u043e\u043b\u044c\u0448\u043e\u0439, \u043a\u0430\u043a \u044d\u0442\u043e \u0443 \u0434\u043e\u043b\u0433\u043e\u043f\u044f\u0442\u043e\u0432 \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u043f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u043e. \u0423 \u043d\u0430\u0441 \u0432\u0441\u0435 \u0442\u043e \u0436\u0435 \u0441\u0430\u043c\u043e\u0435, \u0437\u0430\u043f\u044f\u0441\u0442\u044c\u044f, \u043f\u044f\u0441\u0442\u044c\u044f, \u0444\u0430\u043b\u0430\u043d\u0433\u0438 \u043f\u0430\u043b\u044c\u0446\u0435\u0432, \u043d\u043e\u0433\u0442\u0438, \u0432\u0441\u0435 \u043a\u0430\u043a \u0443 \u043e\u0431\u0435\u0437\u044c\u044f\u043d. \u0418 \u0435\u0441\u043b\u0438 \u043c\u044b \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0438\u0441\u0442\u044c \u0442\u043e\u0433\u043e \u0436\u0435 \u0441\u0430\u043c\u043e\u0433\u043e \u043f\u0440\u043e\u043a\u043e\u043d\u0441\u0443\u043b\u0430 \u0438\u043b\u0438 \u0442\u0430\u043c \u0435\u0449\u0435 \u043a\u043e\u0433\u043e-\u0442\u043e \u0431\u043e\u043b\u0435\u0435 \u0434\u0440\u0435\u0432\u043d\u0435\u0433\u043e \u0434\u0430\u0436\u0435, \u043d\u0443 \u0443\u0436 \u0442\u0435\u043c \u0431\u043e\u043b\u0435\u0435 \u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u0437\u0434\u043d\u0435\u0433\u043e, \u0438 \u0447\u0435\u043b\u043e\u0432\u0435\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0430\u0437\u043d\u0438\u0446\u044b \u0442\u0430\u043c \u0431\u0443\u0434\u0435\u0442 \u043d\u0435\u043c\u043d\u043e\u0433\u043e. \u0414\u0430 \u0443 \u043d\u0430\u0441 \u043d\u0430 \u043f\u043e\u0437\u0434\u043d\u0435\u043c \u044d\u0442\u0430\u043f\u0435, \u0434\u0432\u0430 \u0441 \u043f\u043e\u043b\u043e\u0432\u0438\u043d\u043e\u0439 \u043c\u0438\u043b\u043b\u0438\u043e\u043d\u0430 \u043b\u0435\u0442 \u043d\u0430\u0437\u0430\u0434 \u0438 \u0434\u0430\u0436\u0435 \u043f\u043e\u0437\u0436\u0435 \u0442\u0430\u043c \u0434\u043e \u043f\u043e\u043b\u0442\u043e\u0440\u0430 \u043c\u0438\u043b\u043b\u0438\u043e\u043d\u043e\u0432 \u043b\u0435\u0442 \u0434\u043e\u0431\u0430\u0432\u0438\u043b\u0441\u044f \u0442\u0440\u0443\u0434\u043e\u0432\u043e\u0439 \u043a\u043e\u043c\u043f\u043b\u0435\u043a\u0441 \u043a\u0438\u0441\u0442\u044c, \u043d\u043e \u043d\u0430 \u0444\u043e\u043d\u0435 \u0442\u043e\u0433\u043e, \u0447\u0442\u043e \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442 \u0434\u0440\u0443\u0433\u0438\u0445 \u043f\u0440\u0438\u043c\u0430\u0442\u0442\u043e\u0432 \u044d\u0442\u043e \u043d\u0438 \u043e \u0447\u0435\u043c. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u0435\u0441\u043b\u0438 \u0432\u0437\u044f\u0442\u044c \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u043d\u0430\u0448\u0438\u0445 \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0438\u0445 \u0440\u043e\u0434\u0441\u0442\u0432\u0435\u043d\u043d\u0438\u043a\u043e\u0432 \u0433\u043e\u0440\u0438\u043b\u043b\u044b \u0448\u0438\u043c\u043f\u0430\u043d\u0437\u0435, \u0442\u043e \u043c\u044b \u0443\u0432\u0438\u0434\u0438\u043c, \u0447\u0442\u043e \u0443 \u043d\u0438\u0445 \u0435\u0441\u0442\u044c \u043a\u043e\u043c\u043f\u043b\u0435\u043a\u0441 \u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u043d\u0430 \u0441\u043e\u0433\u043d\u0443\u0442\u044b\u0445 \u043f\u0430\u043b\u044c\u0446\u0430\u0445. \u041a\u043e\u0441\u0442\u044f\u0448\u043a\u043e\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u044d\u0442\u043e\u0442 \u043a\u043e\u043c\u043f\u043b\u0435\u043a\u0441 \u0433\u043e\u0440\u0430\u0437\u0434\u043e \u0431\u043e\u043b\u0435\u0435 \u0441\u043f\u0435\u0446\u0438\u0444\u0438\u0447\u0435\u043d, \u0447\u0435\u043c \u043d\u0430\u0448 \u0442\u0440\u0443\u0434\u043e\u0432\u043e\u0439 \u043d\u0430 \u0441\u0430\u043c\u043e\u043c \u0434\u0435\u043b\u0435. \u041d\u0443 \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u043e\u043d \u0438\u0441\u043f\u044b\u0442\u044b\u0432\u0430\u0435\u0442 \u0431\u043e\u043b\u044c\u0448\u0438\u0435 \u043d\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0434\u043e \u0441\u0430\u043c\u043e\u0439 \u043a\u043e\u043c\u043f\u043b\u0435\u043a\u0441\u0430, \u0441\u0430\u043c\u043e\u0439 \u044d\u0442\u0438 \u043a\u0438\u0441\u0442\u0438 \u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u0433\u043e\u0440\u0430\u0437\u0434\u043e \u0431\u043e\u043b\u0435\u0435 \u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439. \u0410 \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0436\u0438\u0442\u044c \u0441 \u0442\u0440\u0443\u0434\u043e\u0432\u044b\u043c \u043a\u043e\u043c\u043f\u043b\u0435\u043a\u0441\u043e\u043c, \u0430 \u043c\u043e\u0436\u0435\u043c \u0432 \u0447\u0435\u043c-\u0442\u043e \u0438 \u0431\u0435\u0437. \u0422\u043e \u0435\u0441\u0442\u044c \u043d\u0430\u0448\u0430 \u043a\u0438\u0441\u0442\u044c \u043e\u0447\u0435\u043d\u044c \u043f\u0440\u043e\u0441\u0442\u0430, \u043e\u0447\u0435\u043d\u044c \u0437\u0430\u043c\u0430\u0441\u043b\u043e\u0432\u0430\u0442\u0430, \u0430 \u0435\u0436\u0435\u043b\u0438 \u0431\u0440\u0430\u0442\u044c \u043a\u0430\u043a\u0443\u044e-\u0442\u043e \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u0443\u044e \u043a\u043e\u0441\u0442\u044c \u0438\u0437 \u043d\u0435\u0435, \u043d\u0443 \u0434\u043e\u043f\u0443\u0441\u0442\u0438\u043c \u043f\u043b\u0435\u0447\u0435\u0432\u0443\u044e, \u0442\u043e \u043e\u0442\u043b\u0438\u0447\u0438\u0442\u044c \u0447\u0435\u043b\u043e\u0432\u0435\u0447\u0435\u0441\u043a\u0443\u044e \u043e\u0442 \u0448\u0438\u043c\u043f\u0430\u043d\u0437\u0438\u043d\u043e\u0439 \u043a\u0440\u0430\u0439\u043d\u0435 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0430\u0442\u0438\u0447\u043d\u043e. \u0418 \u0435\u0441\u043b\u0438 \u044d\u0442\u043e \u0438\u0441\u043a\u043e\u043f\u0430\u0435\u043c\u044b\u0439 \u043a\u0442\u043e-\u043d\u0438\u0431\u0443\u0434\u044c, \u043a\u0430\u043a\u0430\u044f-\u043d\u0438\u0431\u0443\u0434\u044c \u0442\u0430\u043c \u0430\u0432\u0441\u0442\u0440\u0430\u043b\u043e\u043f\u0438\u0442\u0435\u043a, \u0442\u043e \u0431\u044b\u0432\u0430\u0435\u0442 \u043e\u0447\u0435\u043d\u044c \u0442\u0440\u0443\u0434\u043d\u043e \u043f\u043e\u043d\u044f\u0442\u044c \u0447\u044c\u044f \u043a\u043e\u0441\u0442\u044c. \u0411\u043e\u043b\u0435\u0435 \u0442\u043e\u0433\u043e, \u0432 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044f\u0445 \u0441\u0432\u043e\u0435\u0439 \u0441\u043a\u0435\u043b\u0435\u0442\u043d\u043e\u0439 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u043c\u044b \u043e\u0447\u0435\u043d\u044c \u043f\u0440\u0438\u043c\u0438\u0442\u0438\u0432\u043d\u044b. \u041d\u0443, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0443 \u043d\u0430\u0441 \u0432 \u043a\u043e\u043f\u0447\u0438\u043a\u043e\u0432\u043e\u043c \u043e\u0442\u0434\u0435\u043b\u0435 \u0447\u0435\u0442\u044b\u0440\u0435 \u043a\u043e\u043f\u0447\u0438\u043a\u043e\u0432\u044b\u0445 \u043f\u043e\u0437\u0432\u043e\u043d\u043a\u0430, \u0430 \u0443 \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0433\u043e \u0448\u0438\u043c\u043f\u0430\u043d\u0437\u0435 \u0438 \u0434\u0430\u0436\u0435 \u0443 \u043e\u0440\u043d\u0435\u0433\u0443\u0442\u0430\u043d\u0430 \u0432 \u0441\u0440\u0435\u0434\u043d\u0435\u043c \u0442\u0440\u0438 \u043a\u043e\u043f\u0447\u0438\u043a\u043e\u0432\u044b\u0445 \u043f\u043e\u0437\u0432\u043e\u043d\u043a\u0430. \u0422\u043e \u0435\u0441\u0442\u044c \u043c\u044b \u0445\u0432\u043e\u0441\u0442\u0430\u0442\u0435\u0435, \u0447\u0435\u043c \u0448\u0438\u043c\u043f\u0430\u043d\u0437\u0435, \u043c\u044b \u043e\u0442\u0441\u0442\u0430\u043b\u0438 \u043e\u0442 \u043d\u0438\u0445, \u043c\u044b \u043f\u0440\u0438\u043c\u0438\u0442\u0438\u0432\u043d\u044b\u0435. \u0418 \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043d\u0430\u0448\u0430 \u0431\u0435\u0441\u0445\u0432\u043e\u0441\u0442\u043e\u0441\u0442\u044c, \u044d\u0442\u043e \u043d\u0435 \u0442\u043e \u0447\u0442\u043e \u0447\u0435\u043b\u043e\u0432\u0435\u0447\u0435\u0441\u043a\u0430\u044f \u0447\u0435\u0440\u0442\u0430, \u044d\u0442\u043e \u0447\u0435\u043b\u043e\u0432\u0435\u0447\u0435\u0441\u043a\u0430\u044f \u0447\u0435\u0440\u0442\u0430, \u0434\u0430, \u043d\u043e \u043e\u043d\u0430 \u043f\u0440\u0438\u043c\u0438\u0442\u0438\u0432\u043d\u0430\u044f \u0441\u0440\u0435\u0434\u0438 \u0434\u0430\u0436\u0435 \u043d\u0430\u0448\u0438\u0445 \u0432\u043e\u0442 \u0440\u043e\u0434\u0441\u0442\u0432\u0435\u043d\u043d\u0438\u043a\u043e\u0432 \u0430\u043d\u0442\u0440\u043e\u043f\u043e\u0438\u0434\u043e\u0432. \u041d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u0437 \u043d\u044e\u0430\u043d\u0441\u043e\u0432 \u043d\u0430\u0448\u0435\u0439 \u044d\u0432\u043e\u043b\u044e\u0446\u0438\u0438 \u0434\u043e \u0441\u0438\u0445 \u043f\u043e\u0440 \u0435\u0449\u0435 \u043d\u0430\u0445\u043e\u0434\u044f\u0442 \u043a\u0430\u043a\u0438\u0435-\u0442\u043e \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u043f\u043e\u0434\u0442\u0432\u0435\u0440\u0436\u0434\u0435\u043d\u0438\u044f, \u0432\u044b\u0441\u043b\u0435\u0436\u0438\u0432\u0430\u043d\u0438\u0435, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043f\u043e \u0433\u0435\u043d\u0435\u0442\u0438\u043a\u0435. \u041d\u0443, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0432\u043e\u0442 \u0441\u043e\u0432\u0441\u0435\u043c-\u0442\u0430\u043a\u0438 \u043d\u0435\u0434\u0430\u0432\u043d\u043e, \u0432 2021, \u043a\u0430\u0436\u0435\u0442\u0441\u044f, \u0433\u043e\u0434\u0443, \u0431\u044b\u043b\u0430 \u0447\u0443\u0434\u0435\u0441\u043d\u0430\u044f \u0441\u0442\u0430\u0442\u044c\u044f \u043f\u0440\u043e \u0433\u0435\u043d\u0435\u0442\u0438\u043a\u0443 \u0438\u0441\u0447\u0435\u0437\u043d\u043e\u0432\u0435\u043d\u0438\u044f \u043d\u0430\u0448\u0435\u0433\u043e \u0445\u0432\u043e\u0441\u0442\u0430. \u0414\u043e\u0432\u043e\u043b\u044c\u043d\u043e \u0434\u043e\u043b\u0433\u043e \u0441\u0447\u0438\u0442\u0430\u043b\u043e\u0441\u044c, \u0447\u0442\u043e \u0445\u0432\u043e\u0441\u0442 \u0438\u0441\u0447\u0435\u0437\u0430\u043b \u043a\u0430\u043a \u0431\u044b \u043f\u043e\u0441\u0442\u0435\u043f\u0435\u043d\u043d\u043e \u043f\u043e \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0443, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0432 \u043d\u0430\u0447\u0430\u043b\u0435 \u0431\u044b\u043b \u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u0442\u0430\u043a\u043e\u0439 \u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440, \u043f\u043e\u0442\u043e\u043c \u0441\u0442\u0430\u043b \u043f\u043e\u043a\u043e\u0440\u043e\u0447\u0435-\u043f\u043e\u043a\u043e\u0440\u043e\u0447\u0435-\u043f\u043e\u043a\u043e\u0440\u043e\u0447\u0435 \u0438 \u0441\u0442\u0430\u043b \u0441\u043e\u0432\u0441\u0435\u043c \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u0439. \u041d\u043e \u0431\u044b\u043b\u043e \u043f\u043e\u043a\u0430\u0437\u0430\u043d\u043e, \u0447\u0442\u043e \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442 \u043e\u0447\u0435\u043d\u044c \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0435 \u043d\u0430 \u0441\u0430\u043c\u043e\u043c \u0434\u0435\u043b\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u0443\u0442\u0430\u0446\u0438\u0439 \u0438 \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0439\u0448\u0435 \u043e\u043f\u0438\u0441\u0430\u043d\u043e, \u043a\u0430\u043a \u0432\u0441\u0435 \u044d\u0442\u043e \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u043b\u043e. \u041a\u043e\u0440\u043e\u0447\u0435 \u0433\u043e\u0432\u043e\u0440\u044f, \u0442\u0430\u043c \u0431\u0443\u043a\u0432\u0430\u043b\u044c\u043d\u043e \u0432\u0441\u0435\u0433\u043e \u043f\u0430\u0440\u043e\u0447\u043a\u0430 \u043c\u0443\u0442\u0430\u0446\u0438\u0439, \u043a\u0440\u043e\u043c\u0435 \u0442\u043e\u0433\u043e, \u0447\u0442\u043e \u0445\u0432\u043e\u0441\u0442 \u0431\u0430\u0446 \u0438 \u0438\u0441\u0447\u0435\u0437. \u0422\u043e \u0435\u0441\u0442\u044c \u0432\u043e\u0442 \u043e\u043d\u0438 \u0442\u043e\u043b\u044c\u043a\u043e \u0436\u0435 \u0443\u0431\u0435\u0433\u0430\u043b\u0438 \u0441 \u0431\u043e\u043b\u044c\u0448\u0438\u043c\u0438, \u0430 \u0442\u0443\u0442 \u0431\u0430\u0446 \u0443\u0436\u0435 \u0438 \u0441 \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u043c\u0438, \u0432\u043e\u043e\u0431\u0449\u0435 \u0443\u0436\u0435 \u0431\u0435\u0437 \u0445\u0432\u043e\u0441\u0442\u0430. \u0422\u043e \u0435\u0441\u0442\u044c \u043d\u0430 \u0441\u0430\u043c\u043e\u043c \u0434\u0435\u043b\u0435 \u044d\u0432\u043e\u043b\u044e\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u0438\u043d\u043e\u0433\u0434\u0430 \u043c\u043e\u0436\u0435\u0442 \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442\u044c \u0440\u0435\u0430\u043b\u044c\u043d\u043e \u043e\u0447\u0435\u043d\u044c \u0431\u044b\u0441\u0442\u0440\u043e \u0432 \u0442\u0435\u0447\u0435\u043d\u0438\u0435 \u0431\u0443\u043a\u0432\u0430\u043b\u044c\u043d\u043e \u0442\u0430\u043a\u0438\u0445 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u043c\u0443\u0442\u0430\u0446\u0438\u0439. \u041d\u0443, \u0435\u0441\u043b\u0438 \u044d\u0442\u0438 \u043c\u0443\u0442\u0430\u0446\u0438\u0438 \u0434\u0430\u044e\u0442 \u043a\u0430\u043a\u043e\u0435-\u0442\u043e \u043f\u0440\u0435\u0438\u043c\u0443\u0449\u0435\u0441\u0442\u0432\u043e \u0438\u043b\u0438 \u043a\u0430\u043a \u043c\u0438\u043d\u0438\u043c\u0443\u043c \u043d\u0435 \u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0442\u0441\u044f \u0432\u0440\u0435\u0434\u043d\u044b\u043c\u0438, \u0430 \u0434\u043b\u044f \u043f\u0435\u0440\u0432\u044b\u0445 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u043e\u043e\u0431\u0440\u0430\u0437\u043d\u044b\u0445 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u0445\u0432\u043e\u0441\u0442\u0430 \u043d\u0435 \u0431\u044b\u043b\u043e \u0432\u0440\u0435\u0434\u043d\u044b\u043c, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u043e\u043d\u0438 \u0443\u0436\u0435 \u043d\u0435 \u043f\u0440\u044b\u0433\u0430\u043b\u0438 \u0442\u0430\u043a \u0434\u0430\u043b\u0435\u043a\u043e, \u043a\u0430\u043a \u0438\u0445 \u043f\u0440\u0435\u0434\u043a\u0438 \u043f\u043e \u0432\u0435\u0442\u0432\u044f\u043c, \u0438 \u0445\u0432\u043e\u0441\u0442 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u0430 \u0441\u043c\u044b\u0441\u043b \u043f\u043e\u0442\u0435\u0440\u044f\u043b. \u0418 \u043f\u043e\u043a\u0430\u0437\u0430\u043d\u043e, \u0447\u0442\u043e \u043f\u043e\u0434\u043e\u0431\u043d\u044b\u0435 \u0436\u0435 \u043c\u0443\u0442\u0430\u0446\u0438\u0438 \u0431\u044b\u0432\u0430\u044e\u0442, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0443 \u043c\u044b\u0448\u0435\u0439. \u0412\u043e\u0442, \u043d\u043e \u043c\u044b\u0448\u0430\u043c \u0445\u0432\u043e\u0441\u0442\u0438\u043a \u0432\u0441\u0435-\u0442\u0430\u043a\u0438 \u043d\u0443\u0436\u0435\u043d, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043c\u044b\u0448\u0438 \u043e\u0431\u044b\u0447\u043d\u043e \u0442\u0430\u043a\u0438 \u0441 \u0445\u0432\u043e\u0441\u0442\u0430\u043c\u0438. \u041d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u0435 \u043c\u0443\u0442\u0430\u0446\u0438\u0438 \u0432\u044b\u0440\u0430\u0436\u0430\u044e\u0442\u0441\u044f \u0432 \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u0432\u043d\u0435\u0448\u043d\u0438\u0445 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f\u0445. \u041d\u0443, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0440\u0435\u0434\u0443\u043a\u0446\u0438\u044f \u0448\u0435\u0440\u0441\u0442\u0438, \u043a\u0430\u043a \u0431\u044b \u043e\u0447\u0435\u0432\u0438\u0434\u043d\u043e, \u0447\u0442\u043e \u043e\u0431\u0435\u0437\u044c\u044f\u043d\u0430 \u0432\u043e\u043b\u043e\u0441\u0430\u0442\u0430\u044f, \u0430 \u043c\u044b \u0442\u0438\u043f\u0430 \u043b\u044b\u0441\u044b\u0435. \u041e\u0434\u043d\u0430\u043a\u043e \u0436\u0435, \u0435\u0441\u043b\u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u043e\u043b\u043e\u0441\u044f\u043d\u044b\u0445 \u043b\u0443\u043a\u043e\u0432\u0438\u0446 \u043d\u0430 \u043a\u043e\u0436\u0435, \u0442\u043e \u0443 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 \u0448\u0438\u043c\u043f\u0430\u043d\u0437\u0435 \u043e\u043d\u043e \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u043e\u0435. \u041f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043d\u0430 \u043f\u0435\u0440\u0432\u044b\u0439 \u0432\u0437\u0433\u043b\u044f\u0434 \u043a\u0430\u0436\u0435\u0442\u0441\u044f, \u0447\u0442\u043e \u0443 \u043d\u0430\u0441 \u043b\u043e\u0431 \u0441\u043e\u0432\u0441\u0435\u043c \u043b\u044b\u0441\u044b\u0439, \u0430 \u0435\u0441\u043b\u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u043e\u043b\u043e\u0441\u044f\u043d\u044b\u0445 \u043b\u0443\u043a\u043e\u0432\u0438\u0446, \u0442\u0430\u043c \u0441\u0442\u043e\u043b\u044c\u043a\u043e \u0436\u0435, \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043d\u0430 \u0432\u043e\u043b\u043e\u0441\u0438\u0446\u0435 \u0447\u0430\u0441\u0442\u0438 \u0433\u043e\u043b\u043e\u0432\u044b. \u0418 \u043b\u043e\u0431-\u0442\u043e \u0443 \u043d\u0430\u0441 \u0441 \u043c\u043e\u0445\u043d\u0430\u0442\u043e\u0439, \u043d\u0430 \u0441\u0430\u043c\u043e\u043c \u0434\u0435\u043b\u0435, \u044d\u0442\u043e \u0434\u0430\u0436\u0435 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0442\u0440\u043e\u0433\u0430\u0442\u044c \u0438 \u043f\u0440\u0438\u043e\u0449\u0443\u0442\u0438\u0442\u044c, \u0435\u0441\u043b\u0438 \u0430\u043a\u043a\u0443\u0440\u0430\u0442\u043d\u0435\u043d\u044c\u043a\u043e-\u0430\u043a\u043a\u0443\u0440\u0430\u0442\u043d\u0435\u043d\u044c\u043a\u043e \u043f\u043e\u0442\u0440\u043e\u0433\u0430\u0442\u044c \u0441\u0432\u043e\u0439 \u043b\u043e\u0431\u043e\u0448\u043d\u0438\u043a. \u041d\u0443, \u0430, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0432\u043e\u043b\u043e\u0441\u0430\u0442\u043e\u0441\u0442\u044c \u0433\u0440\u0443\u0434\u0438 \u0443 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 \u0431\u043e\u043b\u044c\u0448\u0435, \u0447\u0435\u043c \u0443 \u0433\u043e\u0440\u0438\u043b\u043b\u044b. \u0423 \u0433\u043e\u0440\u0438\u043b\u043b\u044b \u0442\u0430\u043c \u0432\u043e\u043e\u0431\u0449\u0435 \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043d\u0435\u0442 \u0432\u043e\u043b\u043e\u0441, \u0430 \u0443 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 \u043e\u0447\u0435\u043d\u044c \u0434\u0430\u0436\u0435 \u0435\u0441\u0442\u044c. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u043c\u044b \u0442\u043e\u0436\u0435 \u0432\u043e\u043b\u043e\u0441\u044f\u0442\u044b, \u043d\u0430 \u0441\u0430\u043c\u043e\u043c-\u0442\u043e \u0434\u0435\u043b\u0435. \u041f\u0440\u043e\u0441\u0442\u043e \u0435\u0441\u0442\u044c \u0433\u0435\u043d\u044b, \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u044e\u0449\u0438\u0435 \u043a\u0430\u043a \u0431\u044b \u0441\u0430\u043c \u0431\u0435\u043b\u043e\u043a, \u043d\u0443, \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0440\u0443\u044e\u0449\u0438\u0435 \u0441\u0430\u043c \u0431\u0435\u043b\u043e\u043a, \u043a\u0435\u0440\u0430\u0442\u0438\u043d, \u0430 \u0435\u0441\u0442\u044c \u0433\u0435\u043d\u044b, \u0440\u0435\u0433\u0443\u043b\u0438\u0440\u0443\u044e\u0449\u0438\u0435, \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u044d\u0442\u043e\u0433\u043e \u043a\u0435\u0440\u0430\u0442\u0438\u043d\u0430 \u043d\u0430\u0440\u0430\u0441\u0442\u0435\u0442. \u0418 \u0432\u043e\u0442 \u043a\u0430\u043a\u043e\u0439 \u043a\u0435\u0440\u0430\u0442\u0438\u043d \u2013 \u044d\u0442\u043e \u043e\u0434\u043d\u043e \u0438 \u0442\u043e \u0436\u0435, \u0447\u0442\u043e \u0443 \u043d\u0430\u0441 \u0448\u0438\u043c\u043f\u0430\u043d\u0437\u0435, \u0430 \u0441\u043a\u043e\u043b\u044c\u043a\u043e? \u0423 \u0448\u0438\u043c\u043f\u0430\u043d\u0437\u0435 \u043c\u043d\u043e\u0433\u043e \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f, \u0443 \u043d\u0430\u0441 \u044d\u0442\u043e\u0442 \u0433\u0435\u043d \u043f\u043e\u043b\u043e\u043c\u0430\u043d\u043d\u044b\u0439, \u043e\u043d \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e \u043d\u0435 \u0440\u0435\u0433\u0443\u043b\u0438\u0440\u0443\u0435\u0442 \u0438 \u043c\u0430\u043b\u043e \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f. \u041d\u043e \u0438 \u0443 \u043d\u0430\u0441, \u0438 \u0443 \u0448\u0438\u043c\u043f\u0430\u043d\u0437\u0435, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043d\u0435\u0442 \u043f\u043e\u0434\u0448\u043e\u0440\u0441\u0442\u043a\u0430, \u0438 \u0435\u0433\u043e \u043d\u0435\u0442 \u0432\u043e\u043e\u0431\u0449\u0435 \u0443 \u0432\u0441\u0435\u0445 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u043e\u043e\u0431\u0440\u0430\u0437\u043d\u044b\u0445, \u043e\u0447\u0435\u043d\u044c-\u043e\u0447\u0435\u043d\u044c \u0440\u0435\u0434\u043a\u043e \u0431\u044b\u0432\u0430\u0435\u0442 \u0443 \u043c\u0430\u0440\u0442\u044b\u0448\u043a\u043e\u043e\u0431\u0440\u0430\u0437\u043d\u044b\u0445, \u043d\u0443 \u0438 \u0442\u0430\u043c \u0447\u0443\u0442\u044c \u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u043e, \u043d\u043e \u0442\u043e\u0436\u0435 \u0440\u0435\u0434\u043a\u043e, \u0443\u0448\u0438, \u0440\u0430\u043a\u043e\u043d\u043e\u0441\u043e\u043a \u043a\u0430\u043a\u0438\u0445-\u0442\u043e \u043e\u0431\u0435\u0437\u044c\u044f\u043d. \u0413\u0435\u043d\u0435\u0442\u0438\u043a\u0430 \u2013 \u044d\u0442\u043e \u043e\u0441\u043d\u043e\u0432\u0430 \u043d\u0430\u0448\u0435\u0439 \u043d\u0430\u0441\u043b\u0435\u0434\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0441\u0442\u0438, \u0434\u0430, \u044d\u0442\u043e \u0432\u043e\u0442 \u0410\u0411\u0412\u0413\u0414, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0437\u0430\u043f\u0438\u0441\u0430\u043d\u043e \u0438 \u0431\u0438\u043e\u0445\u0438\u043c\u0438\u044f, \u0438 \u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435, \u0438 \u0430\u043d\u0430\u0442\u043e\u043c\u0438\u044f, \u0438 \u0434\u0430\u0436\u0435 \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0435. \u0418 \u043c\u044b \u0432\u0438\u0434\u0438\u043c, \u0447\u0442\u043e \u044d\u0442\u0430 \u0441\u0430\u043c\u0430\u044f \u0433\u0435\u043d\u0435\u0442\u0438\u043a\u0430 \u043d\u0430\u0441 \u0438 \u0448\u0438\u043c\u043f\u0430\u043d\u0437\u0435 \u0440\u0430\u0441\u0445\u043e\u0434\u044f\u0442\u0441\u044f \u043d\u0430 \u043a\u0430\u043a\u0438\u0435-\u0442\u043e \u0436\u0430\u043b\u043e\u043a\u0438, \u0442\u0430\u043c 2,4%, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0432\u0430\u0436\u043d\u043e \u043d\u0435 \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0437\u0430\u043c\u0435\u043d \u043d\u0443\u043a\u043b\u0435\u043e\u0442\u0438\u0434\u043e\u0432, \u0430 \u0447\u0442\u043e \u0437\u043d\u0430\u0447\u0430\u0442 \u044d\u0442\u0438 \u0437\u0430\u043c\u0435\u043d\u044b. \u0423 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u043c\u0430\u0441\u0441\u0430 \u043d\u0435\u0437\u043d\u0430\u0447\u0438\u043c\u044b\u0445 \u0437\u0430\u043c\u0435\u043d, \u0430 \u0435\u0441\u0442\u044c \u043e\u0447\u0435\u043d\u044c \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u0437\u043d\u0430\u0447\u0438\u043c\u044b\u0445, \u043d\u043e \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u0441\u044e \u0440\u0430\u0437\u043d\u0438\u0446\u0443 \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0441\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442. \u0418 \u043e\u0440\u0438\u0435\u043d\u0442\u0438\u0440\u0443\u044f\u0441\u044c \u043d\u0430 \u0433\u0435\u043d\u0435\u0442\u0438\u043a\u0443, \u043c\u044b \u043e\u043f\u044f\u0442\u044c \u0436\u0435 \u0432\u0438\u0434\u0438\u043c, \u0447\u0442\u043e \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 \u0448\u0438\u043c\u043f\u0430\u043d\u0437\u0435 \u2013 \u044d\u0442\u043e \u043f\u043e\u0447\u0442\u0438 \u043e\u0434\u043d\u043e \u0438 \u0442\u043e \u0436\u0435, \u0442\u043e \u0435\u0441\u0442\u044c \u043c\u0435\u0436\u0434\u0443, \u0441\u043a\u0430\u0436\u0435\u043c, \u0438\u043d\u0434\u0438\u0439\u0441\u043a\u0438\u043c \u0438 \u0430\u0444\u0440\u0438\u043a\u0430\u043d\u0441\u043a\u0438\u043c \u0441\u043b\u043e\u043d\u043e\u0432\u0430\u043c\u0438 \u0440\u0430\u0437\u043d\u0438\u0446\u0430 \u0433\u043e\u0440\u0430\u0437\u0434\u043e \u0431\u043e\u043b\u044c\u0448\u0435 \u0432\u043e\u043e\u0431\u0449\u0435-\u0442\u043e, \u0438 \u0432\u043e\u0442 \u043f\u043e\u0438\u0441\u043a \u0433\u0435\u043d\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0440\u0430\u0437\u043d\u0438\u0446 \u043c\u0435\u0436\u0434\u0443 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u043e\u043c \u0438 \u043e\u0431\u0435\u0437\u044c\u044f\u043d\u043e\u0439 \u2013 \u044d\u0442\u043e \u0442\u0430\u043a\u0430\u044f, \u0441\u043a\u0430\u0436\u0435\u043c, \u043d\u0435\u043f\u0440\u043e\u0441\u0442\u0430\u044f \u0437\u0430\u0434\u0430\u0447\u0430. \u041d\u0443 \u0438 \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0438\u0432\u0448\u0438\u0441\u044c, \u0447\u0442\u043e \u0442\u0430\u043c \u0435\u0441\u0442\u044c \u043a\u0430\u043a\u0438\u0435-\u0442\u043e \u0433\u0435\u043d\u044b, \u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439 \u0437\u0440\u0435\u0447\u044c, \u0440\u0430\u0437\u0432\u0438\u0442\u0438\u0435 \u043c\u043e\u0437\u0433\u0430. \u0411\u0443\u043a\u0432\u0430\u043b\u044c\u043d\u043e \u0442\u0430\u043a\u0438 \u0432 2022 \u0433\u043e\u0434\u0443 \u043d\u0435\u0434\u0430\u0432\u043d\u043e \u0431\u044b\u043b\u0430 \u0441\u0442\u0430\u0442\u044c\u044f \u043f\u0440\u043e \u0442\u043e, \u043a\u0430\u043a \u0442\u0430\u043c \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u0447\u0435\u043d\u044c \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u0435 \u043c\u0443\u0442\u0430\u0446\u0438\u0438 \u043f\u0440\u0438\u0432\u0435\u043b\u0438 \u043a \u0442\u043e\u043c\u0443, \u0447\u0442\u043e \u043d\u0430\u0448\u0438 \u043c\u043e\u0437\u0433\u0438 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0431\u0443\u0440\u043d\u043e \u0440\u0430\u0441\u0442\u0438 \u0438 \u0440\u0430\u0441\u043f\u0443\u0445\u0430\u0442\u044c, \u0438 \u043e\u0447\u0435\u043d\u044c \u043d\u0435\u043c\u043d\u043e\u0433\u043e\u0447\u0438\u0441\u043b\u0435\u043d\u043d\u044b\u0435 \u043e\u0434\u043d\u043e\u0440\u0430\u0437\u043e\u0432\u044b\u0435, \u043c\u043e\u0436\u043d\u043e \u0441\u043a\u0430\u0437\u0430\u0442\u044c, \u0442\u0430\u043a\u0438\u0435 \u043c\u0443\u0442\u0430\u0446\u0438\u0438 \u043c\u043e\u0433\u043b\u0438 \u0438\u043c\u0435\u0442\u044c \u043e\u0447\u0435\u043d\u044c \u0434\u0430\u043b\u0435\u043a\u0438\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u0441\u0442\u0432\u0438\u044f \u0438 \u043f\u0440\u0438\u0432\u0435\u043b\u0438 \u043d\u0430\u0441 \u043d\u0430 \u0432\u0435\u0440\u0448\u0438\u043d\u0443 \u044d\u0432\u043e\u043b\u044e\u0446\u0438\u0438, \u043a\u0430\u043a \u043d\u0430\u043c \u043a\u0430\u0436\u0435\u0442\u0441\u044f, \u043f\u043e \u043a\u0440\u0430\u0439\u043d\u0435\u0439 \u043c\u0435\u0440\u0435, \u043c\u044b \u043d\u0430 \u0432\u0435\u0440\u0448\u0438\u043d\u0435, \u0438 \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043d\u0430\u0441 \u0443\u043c\u043d\u044b\u043c\u0438, \u0433\u043e\u0432\u043e\u0440\u044f\u0449\u0438\u043c\u0438 \u0438 \u0432\u0441\u0435 \u0442\u0430\u043a\u043e\u0435 \u043f\u0440\u043e\u0447\u0435\u0435. \u041f\u043e\u0435\u0434\u0435\u043d\u0438\u0435 \u2013 \u0442\u043e\u0436\u0435 \u0440\u0430\u0437\u043d\u0438\u0446\u0430 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 \u0438 \u043d\u0435\u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430, \u043d\u0443 \u043a\u0430\u0437\u0430\u043b\u043e\u0441\u044c \u0431\u044b \u043e\u0447\u0435\u0432\u0438\u0434\u043d\u043e, \u043c\u044b \u0442\u0430\u043a\u0438\u0435 \u0448\u0438\u0431\u043a\u043e \u0443\u043c\u043d\u044b\u0435, \u0433\u043e\u0432\u043e\u0440\u044f\u0449\u0438\u0435, \u043c\u0435\u0433\u0430\u043e\u0431\u0449\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435, \u0430 \u043d\u0430 \u043f\u0440\u0430\u043a\u0442\u0438\u043a\u0435 \u0441\u0432\u043e\u0434\u0438\u0442\u0441\u044f \u043a \u043e\u0447\u0435\u043d\u044c \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u043c\u0443 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0443 \u0441\u0432\u043e\u0439\u0441\u0442\u0432. \u0420\u0430\u0437\u043d\u0438\u0446\u044b, \u043a\u043e\u043d\u0435\u0447\u043d\u043e, \u0435\u0441\u0442\u044c. \u0427\u0435\u043b\u043e\u0432\u0435\u043a \u0433\u043e\u0440\u0430\u0437\u0434\u043e \u0431\u043e\u043b\u0435\u0435 \u0441\u043f\u043e\u043a\u043e\u0439\u043d\u044b\u0439, \u043e\u043d \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u0443\u0435\u0442 \u0441\u0432\u043e\u0438 \u043a\u0430\u043a\u0438\u0435-\u0442\u043e \u0432\u043e\u0442 \u044d\u0442\u0438 \u043f\u043e\u0437\u044b\u0432\u044b \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0438\u0435, \u0442\u043e, \u0447\u0442\u043e \u0443 \u043d\u0435\u0433\u043e \u0432\u0441\u0435-\u0442\u0430\u043a\u0438 \u043c\u044b\u0448\u043b\u0435\u043d\u0438\u0435 \u0440\u0430\u0437\u0432\u0438\u0442\u043e \u0447\u0443\u0442\u044c \u043f\u043e\u043b\u0443\u0447\u0448\u0435, \u043e\u043d \u043d\u0430\u043c\u043d\u043e\u0433\u043e \u0431\u043e\u043b\u0435\u0435 \u0437\u0430\u043d\u0443\u0434\u043d\u044b\u0439, \u043e\u043d \u043e\u0447\u0435\u043d\u044c \u0441\u043a\u043b\u043e\u043d\u0435\u043d \u0443\u0447\u0438\u0442\u044c \u0438 \u0443\u0447\u0438\u0442\u044c\u0441\u044f, \u0438 \u0441\u043a\u043b\u043e\u043d\u0435\u043d \u0431\u043e\u043b\u044c\u0448\u0435 \u0434\u043e\u0433\u043e\u0432\u0430\u0440\u0438\u0432\u0430\u0442\u044c\u0441\u044f, \u043d\u0435\u0436\u0435\u043b\u0438 \u0431\u0438\u0442\u044c \u0434\u0440\u0443\u0433 \u0434\u0440\u0443\u0433\u0443 \u043f\u043e \u043c\u043e\u0440\u0434\u0430\u0441\u0430\u043c, \u0430 \u0448\u0430\u043c\u043f\u0430\u043d\u0437\u0435, \u043d\u0443 \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u0432\u0441\u0435 \u0442\u043e \u0436\u0435 \u0441\u0430\u043c\u043e\u0435, \u043d\u043e \u0432 \u043e\u0431\u0440\u0430\u0442\u043d\u0443\u044e \u0441\u0442\u043e\u0440\u043e\u043d\u0443. \u0422\u043e \u0435\u0441\u0442\u044c \u043e\u043d\u0430 \u0430\u0433\u0440\u0435\u0441\u0441\u0438\u0432\u043d\u0435\u0435, \u043e\u043d\u0430 \u043c\u0435\u043d\u0435\u0435 \u0441\u043f\u043e\u043a\u043e\u0439\u043d\u0430\u044f, \u043e\u043d\u0430 \u0431\u044b\u0441\u0442\u0440\u043e \u043f\u0435\u0440\u0435\u043a\u043b\u044e\u0447\u0430\u0435\u0442\u0441\u044f, \u043e\u043d\u0430 \u0443\u0447\u0438\u0442\u0441\u044f \u043f\u0440\u043e\u0441\u0442\u043e \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u0435\u043c, \u043d\u043e \u043d\u0435 \u0446\u0435\u043b\u0435\u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u043e\u0439, \u043d\u0435 \u0441\u043a\u043b\u043e\u043d\u043d\u0430 \u0443\u0447\u0438\u0442\u044c \u043a\u043e\u0433\u043e-\u0442\u043e, \u043d\u0443 \u0437\u0430\u0442\u043e \u043d\u0430 \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u043c \u043e\u043f\u044b\u0442\u0435, \u0430 \u0442\u0430\u043a \u043f\u043e \u0431\u043e\u043b\u044c\u0448\u043e\u043c\u0443 \u0441\u0447\u0435\u0442\u0443 \u0442\u043e \u0436\u0435 \u0441\u0430\u043c\u043e\u0435. \u0418 \u0435\u0441\u043b\u0438, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043d\u0430\u0443\u0447\u0438\u0442\u044c \u043e\u0431\u0435\u0437\u044c\u044f\u043d \u0433\u043e\u0432\u043e\u0440\u0438\u0442\u044c, \u043d\u0443 \u043d\u0430 \u044f\u0437\u044b\u043a\u0435 \u0436\u0435\u0441\u0442\u043e\u0432, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0442\u043e \u043e\u043d\u0438 \u0433\u043e\u0432\u043e\u0440\u044f\u0442 \u043d\u0435 \u0442\u043e, \u0447\u0442\u043e \u043d\u0435 \u0445\u0443\u0436\u0435 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430, \u043d\u043e \u043d\u0430 \u0443\u0440\u043e\u0432\u043d\u0435 \u0434\u0432\u0443\u0445\u043b\u0435\u0442\u043d\u0435\u0433\u043e \u0440\u0435\u0431\u0435\u043d\u043a\u0430 \u0432\u043f\u043e\u043b\u043d\u0435 \u0441\u0435\u0431\u0435, \u043d\u0443 \u0442\u0430\u043a \u0443 \u043d\u0438\u0445 \u0438 \u043c\u043e\u0437\u043a\u043e\u0432 \u0442\u0440\u0438 \u0440\u0430\u0437\u0430 \u043c\u0435\u043d\u044c\u0448\u0435, \u0432 \u043e\u0431\u0449\u0435\u043c-\u0442\u043e. \u0422\u0430\u043a \u0447\u0442\u043e \u0440\u0430\u0437\u043b\u0438\u0447\u0438\u044f \u043f\u043e\u043b\u0443\u0447\u0430\u044e\u0442\u0441\u044f \u043d\u0435 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435, \u0430 \u0441\u043a\u043e\u0440\u0435\u0435 \u0442\u0430\u043a\u0438\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435, \u0438 \u0432\u0441\u0435 \u0433\u043b\u0430\u0432\u043d\u044b\u0435 \u0441\u0432\u043e\u0439\u0441\u0442\u0432\u0430, \u043d\u0430\u0448\u0438 \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u0441\u043a\u043b\u043e\u043d\u043d\u044b \u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0441\u0443\u0433\u0443\u0431\u043e \u0447\u0435\u043b\u043e\u0432\u0435\u0447\u0435\u0441\u043a\u0438\u043c\u0438, \u0434\u043e\u0431\u0440\u043e\u0442\u0430, \u044e\u043c\u043e\u0440, \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u043a \u043e\u0431\u043c\u0430\u043d\u0443, \u0440\u0443\u0433\u0430\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u043e \u0434\u0430\u0436\u0435, \u0434\u0430, \u0438 \u043f\u0440\u043e\u0447\u0435\u0435-\u043f\u0440\u043e\u0447\u0435\u0435, \u0443 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 \u0438 \u0443 \u043e\u0431\u0435\u0437\u044c\u044f\u043d \u0441\u0442\u0440\u043e\u0433\u043e \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b\u0435, \u0438 \u0443\u0436 \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e \u0437\u0434\u043e\u0440\u043e\u0432\u043e, \u044d\u0442\u043e \u0432\u0438\u0434\u043d\u043e \u043f\u043e \u043f\u0430\u043b\u0435\u043e\u043d\u0442\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u043c, \u043f\u043e\u0442\u043e\u043c\u0443 \u043a\u0430\u043a \u0443\u0436\u0435 200 \u043b\u0435\u0442 \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u043f\u0430\u043b\u0435\u043e\u043d\u0442\u043e\u043b\u043e\u0433\u0438 \u0432\u044b\u043a\u0430\u043f\u044b\u0432\u0430\u044e\u0442 \u0432\u0441\u044f\u0447\u0435\u0441\u043a\u0438\u0435 \u0437\u0430\u043f\u0447\u0430\u0441\u0442\u0438 \u0438\u0437 \u0437\u0435\u043c\u043b\u0438, \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u043d\u044b\u0445 \u043c\u0435\u0436\u0434\u0443 \u0441\u043e\u0431\u043e\u0439, \u0438 \u043d\u044b\u043d\u0447\u0435 \u0443 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u043f\u043e\u043b\u043d\u0430\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u043e\u0442 \u041f\u0443\u0440\u0433\u0430\u0442\u043e\u0440\u0438\u0443\u0441\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0436\u0438\u043b 66 \u043c\u0438\u043b\u043b\u0438\u043e\u043d\u043e\u0432 \u043b\u0435\u0442 \u043d\u0430\u0437\u0430\u0434, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u044b\u043b \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c \u0441 \u043a\u0440\u044b\u0441\u0443, \u043d\u0443 \u0434\u0430\u0436\u0435 \u043f\u043e\u043c\u0435\u043d\u044c\u0448\u0435, \u0442\u0430\u043c \u0433\u0440\u0430\u043c\u043c 20 \u043e\u043d \u0442\u0430\u043c \u0432\u0435\u0441\u0438\u043b, \u043d\u0443 \u0442\u0430\u043c \u0434\u043e 100 \u0433\u0440\u0430\u043c\u043c, \u043c\u043e\u0436\u0435\u0442 \u0441 \u043c\u0430\u043a\u0441\u0438\u043c\u0443\u043c, \u0438 \u0441 \u0432\u0438\u0434\u043e\u043c \u0437\u0435\u043c\u043b\u0435\u0440\u043e\u0439\u043a\u0438, \u0438 \u0432\u043e\u0442 \u043e\u0442 \u044d\u0442\u043e\u0433\u043e \u0441\u0430\u043c\u043e\u0433\u043e \u041f\u0443\u0440\u0433\u0430\u0442\u043e\u0440\u0438\u0443\u0441\u0430, \u0438 \u0434\u043e \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u0432\u0441\u0435 \u0433\u043b\u0430\u0432\u043d\u044b\u0435 \u043f\u0440\u043e\u043c\u0435\u0436\u0443\u0442\u043e\u0447\u043d\u044b\u0435 \u0444\u043e\u0440\u043c\u044b. \u0413\u0434\u0435-\u0442\u043e \u0447\u0442\u043e-\u0442\u043e \u0443 \u043d\u0430\u0441 \u043d\u0435 \u0434\u043e \u043a\u043e\u043d\u0446\u0430 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u043e\u0435, \u0432 \u043a\u0430\u043a\u0438\u0445-\u0442\u043e \u043e\u0442\u0440\u0435\u0437\u043a\u0430\u0445 \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0443 \u043d\u0430\u0441, \u0441\u043a\u0430\u0436\u0435\u043c, \u0442\u043e\u043b\u044c\u043a\u043e \u0437\u0443\u0431\u044b \u043f\u043e\u043b\u043d\u043e \u0441\u043a\u0435\u043b\u0435\u0442\u043e\u0432 \u043d\u0435\u0442, \u0434\u0430, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u043e\u0431\u0435\u0437\u044c\u044f\u043d\u044b \u043f\u0440\u044b\u0433\u0430\u043b\u0438 \u043f\u043e \u0434\u0435\u0440\u0435\u0432\u044c\u044f\u043c, \u0438 \u0432 \u043f\u0430\u043b\u0435\u043e\u043d\u0442\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043a\u0435\u043b\u0435\u0442\u043a\u0438 \u043f\u043e\u043f\u0430\u0434\u0430\u043b\u0438, \u043a \u0441\u043e\u0436\u0430\u043b\u0435\u043d\u0438\u044e, \u0434\u0430\u043b\u0435\u043a\u043e \u043d\u0435 \u0432\u0441\u0435\u0433\u0434\u0430, \u043d\u0443 \u0442\u0435\u043c \u0431\u043e\u043b\u0435\u0435, \u0447\u0442\u043e \u0431\u043e\u043b\u044c\u0448\u0430\u044f \u0447\u0430\u0441\u0442\u044c \u0438\u0441\u0442\u043e\u0440\u0438\u0438 \u0438\u0445 \u0432\u0430\u0440\u0438\u043b\u0430\u0441\u044c \u0433\u0434\u0435-\u0442\u043e \u0432 \u0442\u0440\u043e\u043f\u0438\u043a\u0430\u0445, \u0433\u0434\u0435 \u043f\u0440\u043e\u0441\u0442\u043e \u043f\u0430\u043b\u0435\u043e\u043d\u0442\u043e\u043b\u043e\u0433\u0438 \u043d\u0435 \u0441\u0438\u043b\u044c\u043d\u043e \u043a\u043e\u043f\u0430\u043b\u0438, \u0432\u043e\u0442, \u043d\u043e \u0434\u0430\u0436\u0435 \u043f\u0440\u0438 \u0432\u0441\u0435\u043c \u044d\u0442\u043e\u043c \u0443 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u0431\u0435\u0437\u0443\u043c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0430\u043b\u0435\u043e\u043d\u0442\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445, \u043c\u044b \u0436\u0438\u0432\u0435\u043c \u0432 \u0447\u0443\u0434\u0435\u0441\u043d\u043e\u0435 \u0432\u0440\u0435\u043c\u044f, \u043a\u043e\u0433\u0434\u0430 \u043c\u044b \u043d\u0435 \u043f\u0440\u043e\u0441\u0442\u043e \u0434\u043e\u0433\u0430\u0434\u044b\u0432\u0430\u0435\u043c\u0441\u044f, \u0447\u0442\u043e \u0431\u044b\u043b\u0438 \u043a\u0430\u043a\u0438\u0435-\u0442\u043e \u0442\u0430\u043c \u043f\u0440\u043e\u043c\u0435\u0436\u0443\u0442\u043e\u0447\u043d\u044b\u0435, \u0442\u0430\u043c, \u043d\u0435\u0434\u043e\u0441\u0442\u0430\u044e\u0449\u0438\u0435 \u0437\u0432\u0435\u043d\u044c\u044f, \u043a\u043e\u0433\u0434\u0430 \u043c\u044b \u0438\u0445 \u0432\u0438\u0434\u0438\u043c, \u043f\u0440\u043e\u0441\u0442\u043e \u0432 \u0440\u0443\u0447\u043a\u0430\u0445 \u043c\u043e\u0436\u0435\u043c \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0430\u0442\u044c \u0438 \u043f\u043e\u043d\u044f\u0442\u044c \u0441 \u043a\u0430\u043a\u043e\u0439 \u0441\u0442\u0430\u0442\u0438 \u043e\u043d\u0438 \u0432\u044b\u043b\u0435\u0437\u0430\u043b\u0438 \u043d\u0430 \u0434\u0435\u0440\u0435\u0432\u044c\u044f, \u043a\u0430\u043a \u043e\u043d\u0438 \u0441\u043b\u0438\u0437\u0430\u043b\u0438\u0441\u044c \u0441 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432, \u043f\u043e\u0447\u0435\u043c\u0443 \u043e\u043d\u0438 \u043c\u0435\u043d\u044f\u043b\u0438 \u0441\u0432\u043e\u044e \u0434\u0438\u0435\u0442\u0443, \u0431\u0440\u0430\u043b\u0438 \u0432 \u0440\u0443\u043a\u0438 \u0431\u0443\u043b\u044b\u0436\u043d\u0438\u043a\u0438, \u043d\u0430\u0447\u0438\u043d\u0430\u043b\u0438 \u0440\u0430\u0441\u0441\u0435\u043b\u044f\u0442\u044c\u0441\u044f \u0442\u0430\u043c \u0438\u0437 \u0442\u0440\u043e\u043f\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0437\u043e\u043d \u043a\u0443\u0434\u0430-\u0442\u043e, \u0438 \u0442\u0430\u043a\u0438\u0435 \u0432\u043e\u0442 \u0437\u0430\u043c\u0435\u0447\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u043d\u0430\u0445\u043e\u0434\u043a\u0438 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0442 \u0432\u043e\u0442 \u044d\u0442\u0438 \u0441\u0430\u043c\u044b\u0435 \u043f\u043b\u0430\u043d\u043d\u0435\u043d\u043a\u0438 \u043f\u0435\u0440\u0435\u0445\u043e\u0434\u044b. \u041e\u0434\u0438\u043d \u0438\u0437 \u0447\u0430\u0441\u0442\u0435\u0439\u0448\u0438\u0445 \u0442\u0430\u043a\u0438\u0445 \u0430\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0442\u0435\u0445, \u043a\u0442\u043e \u043d\u0435 \u0432\u0435\u0440\u0438\u0442 \u0432 \u043f\u0440\u0435\u0434\u043e\u0441\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430 \u043e\u0442 \u043e\u0431\u0435\u0437\u044c\u044f\u043d\u044b, \u0447\u0442\u043e, \u0434\u0435\u0441\u043a\u0430\u0442\u044c, \u0430 \u043f\u043e\u0447\u0435\u043c\u0443 \u0436\u0435 \u0442\u043e\u0433\u0434\u0430 \u0441\u0435\u0439\u0447\u0430\u0441 \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043e\u0431\u0435\u0437\u044c\u044f\u043d\u044b \u043d\u0435 \u043f\u0440\u0435\u0432\u0440\u0430\u0449\u0430\u044e\u0442\u0441\u044f \u0432 \u043b\u044e\u0434\u0435\u0439? \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442 \u0434\u0435\u0431\u0438\u043b\u044c\u043d\u044b\u0439, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442 \u043c\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u043d\u044b\u0445 \u044d\u043a\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043d\u0438\u0448 \u0438 \u043c\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u043d\u044b\u0445 \u044d\u0432\u043e\u043b\u044e\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u043f\u0443\u0442\u0435\u0439, \u0438 \u0434\u0430\u0436\u0435 \u043a \u043e\u0434\u043d\u043e\u0439 \u044d\u043a\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043d\u0438\u0448\u0435 \u043c\u043e\u0436\u043d\u043e \u043f\u0440\u0438\u0441\u043f\u043e\u0441\u043e\u0431\u0438\u0442\u044c\u0441\u044f \u043f\u043e-\u0440\u0430\u0437\u043d\u043e\u043c\u0443, \u0447\u0442\u043e \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u043a\u0430\u043a\u0438\u0445-\u0442\u043e \u0441\u0438\u044e\u043c\u0438\u043d\u0443\u0442\u043d\u044b\u0445 \u043d\u044e\u0430\u043d\u0441\u043e\u0432, \u043e\u0442 \u043d\u0430\u043b\u0438\u0447\u0438\u044f \u043c\u0443\u0442\u0430\u0446\u0438\u0439 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0445 \u0438 \u043e\u0442 \u0438\u0441\u0442\u043e\u0440\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0438\u0441\u0442\u043e\u0440\u0438\u0438, \u0432\u043e\u0442 \u044d\u0442\u043e\u0439 \u0432\u0441\u0435\u0439 \u044d\u0432\u043e\u043b\u044e\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0438\u0441\u0442\u043e\u0440\u0438\u0438, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0431\u044b\u043b\u0430 \u0434\u043e, \u0438 \u043d\u0435\u0442\u0443 \u0432\u043e\u043e\u0431\u0449\u0435 \u043d\u0438\u043a\u0430\u043a\u043e\u0439 \u044d\u0432\u043e\u043b\u044e\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0441\u0438\u043b\u044b, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0431\u044b \u0434\u0432\u0438\u0433\u0430\u043b\u0430 \u043a\u043e\u0433\u043e-\u0442\u043e \u0432 \u0441\u0442\u043e\u0440\u043e\u043d\u0443 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430, \u0443 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0436\u0438\u0432\u043e\u0433\u043e \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043e\u0432\u0430\u043d\u0438\u044f \u0435\u0441\u0442\u044c \u0441\u0432\u043e\u0439 \u044d\u0432\u043e\u043b\u044e\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u043f\u0443\u0442\u044c, \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043c\u0430\u0440\u0442\u044b\u0448\u043a\u0438 \u044d\u0432\u043e\u043b\u044e\u0446\u0438 Statue \u044d\u0432\u043e\u043b\u044e\u0446\u0438\u043e\u043d\u0438\u0440\u043e\u0432\u0430\u043b\u0438 \u0432 \u043c\u0430\u0440\u0442\u044b\u0448\u0435\u043a, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0438\u0445 \u043f\u0440\u0435\u0434\u0430\u043a\u0438 \u043d\u0435 \u0441\u043e\u0432\u0441\u0435\u043c \u0442\u0430\u043a, \u043a\u0430\u043a \u043c\u044b, \u0443 \u043d\u0438\u0445 \u0431\u044b\u043b\u0438 \u0434\u0440\u0443\u0433\u0438\u0435 \u043f\u0440\u0435\u0434\u043f\u043e\u0441\u044b\u043b\u043a\u0438, \u0443 \u043d\u0438\u0445 \u0431\u044b\u043b\u0438 \u0434\u0440\u0443\u0433\u0438\u0435 \u0442\u0430\u043c \u043d\u044e\u0430\u043d\u0441\u044b, \u0443 \u043d\u0438\u0445 \u0431\u044b\u043b\u043e \u0434\u0440\u0443\u0433\u043e\u0435 \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0435, \u0443 \u043d\u0438\u0445 \u0431\u044b\u043b\u0438 \u0441\u0432\u043e\u0438 \u043c\u0443\u0442\u0430\u0446\u0438\u0438, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0443 \u043d\u0438\u0445 \u0431\u044b\u043b\u0438 \u0434\u0440\u0443\u0433\u0438\u0435 \u0437\u0430\u0434\u0430\u0447\u0438, \u043e\u043d\u0438 \u043f\u043e-\u0434\u0440\u0443\u0433\u043e\u043c\u0443 \u043f\u0438\u0442\u0430\u043b\u0438\u0441\u044c, \u043e\u043d\u0438 \u0434\u0440\u0443\u0433\u043e\u0435\u904b\u0430\u043b\u0438\u0441\u044c, \u043e\u043d\u0438 \u0431\u044b\u043b\u0438 \u0434\u0440\u0443\u0433\u043e\u0433\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0430, \u043e\u043d\u0438 \u0441\u0442\u0430\u043b\u043a\u0438\u0432\u0430\u043b\u0438\u0441\u044c \u0441 \u0434\u0440\u0443\u0433\u0438\u043c\u0438 \u043a\u043e\u043d\u043a\u0443\u0440\u0435\u043d\u0442\u0430\u043c\u0438, \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u0437 \u043d\u0438\u0445 \u044d\u0432\u043e\u043b\u044e\u0446\u0438\u043e\u043d\u0438\u0440\u043e\u0432\u0430\u043b\u0438 \u0432\u043e\u043e\u0431\u0449\u0435, \u043d\u0430 \u0434\u0440\u0443\u0433\u0438\u0445 \u043a\u0430\u043a\u0438\u0445-\u0442\u043e \u0437\u0435\u043c\u043b\u044f\u0445, \u043d\u0443 \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440,iskari, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0433\u0434\u0435 \u0443\u0441\u043b\u043e\u0432\u0438\u044f \u0432\u0441\u0435-\u0442\u0430\u043a\u0438 \u0441\u043e\u0432\u0441\u0435\u043c \u043d\u0435 \u0442\u0430\u043a\u0438\u0435, \u043a\u0430\u043a \u0443 \u043d\u0430\u0441. \u0412\u043e\u0442, \u0430 \u0433\u0434\u0435-\u0442\u043e \u0442\u0430\u043c \u043a\u0442\u043e-\u0442\u043e \u0432 \u044e\u0433\u043e-\u0432\u043e\u0441\u0442\u043e\u0447\u043d\u043e\u0439 \u0430\u0437\u0438\u043a\u0435, \u0433\u0438\u0431\u043e\u043d\u044b, \u0433\u0434\u0435 \u043b\u0435\u0441\u0430 \u043f\u0440\u043e\u0441\u0442\u043e \u0433\u0443\u0449\u0438\u0435, \u0430 \u043a\u0442\u043e-\u0442\u043e \u0432 \u0410\u0444\u0440\u0438\u043a\u0435, \u0433\u0434\u0435 \u0441\u0443\u0432\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u0448\u043b\u0430 \u0432 \u043f\u043e\u043b\u043d\u043e\u043c \u0440\u043e\u0441\u0442 \u0438, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e, \u0438\u043c \u0442\u0430\u043c \u043d\u0430 \u0437\u0435\u043c\u043b\u044e \u043d\u0430\u0434\u043e \u0431\u044b\u043b\u043e \u0441\u043f\u0443\u0441\u043a\u0430\u0442\u044c\u0441\u044f. \u041d\u043e \u0442\u0435 \u0436\u0435 \u0433\u0438\u0431\u043e\u043d\u044b \u043e\u0442 \u0438\u0441\u0445\u043e\u0434\u043d\u0438\u043a\u0430 \u043e\u0442\u043b\u0438\u0447\u0430\u044e\u0442\u0441\u044f \u0441\u0438\u043b\u044c\u043d\u043e \u0431\u043e\u043b\u044c\u0448\u0435, \u0447\u0435\u043c \u043c\u044b. \u041d\u0430 \u0441\u0430\u043c\u043e\u043c \u0434\u0435\u043b\u0435, \u043e\u043d\u0438 \u0433\u043e\u0440\u0430\u0437\u0434\u043e \u0431\u043e\u043b\u0435\u0435 \u044d\u0432\u043e\u043b\u044e\u0446\u0438\u043e\u043d\u043d\u043e \u043f\u0440\u043e\u0434\u0432\u0438\u043d\u0443\u0442\u044b. \u041f\u043e\u0447\u0435\u043c\u0443 \u0432\u044b \u0434\u043e \u0441\u0438\u0445 \u043f\u043e\u0440, \u0441\u043f\u0440\u0430\u0448\u0438\u0432\u0430\u044e\u0449\u0438\u0435 \u043c\u0435\u043d\u044f \u043f\u0440\u043e \u043e\u0431\u0435\u0437\u044c\u044f\u043d, \u043d\u0435 \u0432\u0443\u043b\u044c\u0441\u0438\u043d\u0438\u0440\u043e\u0432\u0430\u043b\u0438 \u0432 \u0433\u0438\u0431\u043e\u043d\u044b? \u041d\u0435\u043f\u043e\u0440\u044f\u0434\u043e\u043a. \u0414\u0430\u043b\u0435\u043a\u043e \u0435\u0449\u0435 \u0432\u0430\u043c \u0434\u043e \u0433\u0438\u0431\u043e\u043d\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0432\u0435\u0440\u0448\u0438\u043d\u043e\u0439 \u044d\u0432\u043e\u043b\u044e\u0446\u0438\u0438, \u0443 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u0440\u0443\u043a\u0438 \u0432\u043e\u0442 \u0442\u0430\u043a\u043e\u0439 \u0434\u043b\u0438\u043d\u044b, \u043c\u043e\u0437\u0433\u0438 \u0432\u043e\u0442 \u0442\u0430\u043a\u0438\u0435, \u043e\u043d\u0438 \u043e\u0447\u0435\u043d\u044c \u0434\u043e\u0431\u0440\u044b\u0435, \u043c\u0435\u0436\u0434\u0443 \u043f\u0440\u043e\u0447\u0438\u043c, \u0438 \u043f\u0435\u0442\u044c \u0443\u043c\u0435\u044e\u0442 \u043e\u0447\u0435\u043d\u044c \u0445\u043e\u0440\u043e\u0448\u043e. \u0421\u0430\u043c \u043f\u043e \u0441\u0435\u0431\u0435 \u0432\u043e\u0442 \u044d\u0442\u043e\u0442 \u043f\u043e\u0434\u0445\u043e\u0434, \u0447\u0442\u043e \u043f\u043e\u0447\u0435\u043c\u0443 \u043e\u0431\u0435\u0437\u044c\u044f\u043d\u0430 \u043d\u0435 \u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u0441\u044f \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u043e\u043c, \u043e\u0431\u0435\u0437\u044c\u044f\u043d \u043c\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u043d\u044b\u0445. \u0415\u0441\u0442\u044c \u0440\u0443\u043a\u043e\u043d\u043e\u0436\u043a\u0438, \u0435\u0441\u0442\u044c \u043a\u043e\u043b\u044c\u0446\u0435\u0445\u0432\u043e\u0441\u0442\u043d\u044b\u0435 \u043b\u0435\u043c\u0443\u0440\u044b, \u0435\u0441\u0442\u044c \u043a\u0430\u043a\u0438\u0435-\u043d\u0438\u0431\u0443\u0434\u044c \u0442\u043e\u043b\u0441\u0442\u044b\u0435 \u043b\u043e\u0440\u0438, \u0430 \u0435\u0441\u0442\u044c \u0442\u043e\u043d\u043a\u0438\u0435 \u043b\u043e\u0440\u0438, \u0430 \u0435\u0441\u0442\u044c \u043c\u043d\u043e\u0433\u043e \u043f\u044f\u0442\u044b\u0435, \u0430 \u0435\u0441\u0442\u044c \u043c\u0430\u043a\u0430\u0439\u043a\u0438 \u0442\u0430\u043a\u0438\u0435, \u0435\u0441\u0442\u044c \u0441\u0438\u043a\u0438, \u0435\u0441\u0442\u044c \u043f\u0430\u0432\u0438\u0430\u043d\u044b, \u0435\u0441\u0442\u044c \u0433\u0438\u0431\u043e\u043d\u044b, \u043e\u0440\u0430\u043d\u0433\u0443\u0442\u0430\u043d\u044b, \u0433\u043e\u0440\u0438\u043b\u043b\u0430, \u0448\u0438\u043c\u043f\u0430\u043d\u0437\u0435. \u0418 \u043a\u0430\u0436\u0434\u0430\u044f \u0438\u0437 \u044d\u0442\u0438\u0445 \u0442\u0432\u0430\u0440\u0435\u0439, \u043e\u043d\u0430 \u043f\u0440\u043e\u0448\u043b\u0430 \u0441\u0432\u043e\u0439 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u043f\u0443\u0442\u044c. \u042f \u0443\u0436 \u043f\u043e\u043c\u043e\u043b\u0447\u0443 \u0435\u0449\u0435 \u043f\u0440\u043e \u0441\u043e\u043d \u0438\u0441\u043a\u043e\u043f\u0430\u0435\u043c\u044b\u0445 \u043f\u0440\u0438\u043c\u0430\u0442\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0432\u043e\u043e\u0431\u0449\u0435 \u043d\u0435\u043c\u0435\u0440\u0435\u043d\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0448\u043b\u0438 \u0435\u0449\u0435 \u0441\u0432\u043e\u0438\u043c\u0438 \u043a\u0430\u043a\u0438\u043c\u0438-\u0442\u043e \u043f\u0443\u0442\u044f\u043c\u0438. \u0414\u0430 \u0434\u0430\u0436\u0435 \u043b\u044e\u0434\u0438 \u044d\u0432\u043e\u043b\u044e\u0446\u0438\u043e\u043d\u0438\u0440\u043e\u0432\u0430\u043b\u0438 \u0441\u0438\u043b\u044c\u043d\u043e \u0438 \u043d\u0435 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u043e \u043d\u0430 \u0441\u0430\u043c\u043e\u043c \u0434\u0435\u043b\u0435, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u043e\u043a\u0440\u043e\u043c\u0435 \u043d\u0430\u0441 \u0431\u044b\u043b\u0438 \u0435\u0449\u0435 \u043d\u044f\u043d\u0434\u0435\u0440\u0442\u0430\u043b\u044c\u0446\u044b \u043f\u0430\u0440\u0430\u043b\u043b\u0435\u043b\u044c\u043d\u043e, \u0434\u0438\u043d\u043d\u0438\u0441\u043e\u0432\u0446\u044b, \u0445\u043e\u0431\u0431\u0438\u0442\u044b \u0444\u043b\u043e\u0440\u0435\u0446\u043a\u0438\u0435, \u0445\u043e\u0431\u0431\u0438\u0442\u044b \u043b\u0443\u0441\u043e\u043d\u0441\u043a\u0438\u0435, \u043a\u0430\u043a\u0438\u0435-\u0442\u043e \u0442\u0430\u043c \u0437\u0430\u043f\u0430\u0434\u043d\u043e\u0430\u0444\u0440\u0438\u043a\u0430\u043d\u0441\u043a\u0438\u0435, \u044d\u0442\u0438 \u043d\u0435\u043f\u043e\u043d\u044f\u0442\u043d\u044b\u0435 \u0442\u0430\u043c, \u0438\u0432\u044d\u043b\u044d\u0440\u0443\u044d\u0439\u0434\u0438\u043d\u0441\u0438\u0441, \u0438 \u0442\u0430\u043c, \u0431\u043e\u0433 \u0437\u043d\u0430\u0435\u0442, \u043a\u0442\u043e \u0435\u0449\u0435, \u043d\u0443 \u0434\u0430\u0436\u0435 \u0431\u0430\u043d\u0430\u043b\u044c\u043d\u043e, \u044f \u043d\u0435 \u0437\u043d\u0430\u044e, \u0434\u0430\u0436\u0435 \u0441\u0440\u0435\u0434\u0438 \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0433\u043e \u044d\u0442\u043e \u0447\u0435\u043b\u043e\u0432\u0435\u0447\u0435\u0441\u0442\u0432\u043e \u0438 \u0442\u043e, \u0440\u0430\u0437\u043d\u043e\u043e\u0431\u0440\u0430\u0437\u0438\u0435 \u0432\u0435\u043b\u0438\u043a\u043e. \u0412\u043e\u0442 \u043f\u043e\u0447\u0435\u043c\u0443 \u0432\u0441\u0435 \u043d\u0435 \u044d\u0432\u043e\u043b\u044e\u0446\u0438\u043e\u043d\u0438\u0440\u0443\u044e\u0442 \u0432 \u0430\u0432\u0441\u0442\u0440\u0430\u043b\u0438\u0439\u0441\u043a\u0438\u0445 \u0430\u0431\u043e\u0440\u0438\u0433\u0435\u043d\u043e\u0432? \u041d\u0443 \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u043d\u0435 \u0432\u0441\u0435 \u0436\u0438\u0432\u0443\u0442 \u0432 \u0410\u0432\u0441\u0442\u0440\u0430\u043b\u0438\u0438 \u0438 \u0436\u0438\u043b\u0438 \u0432 \u043f\u0440\u043e\u0448\u043b\u043e\u043c, \u0434\u0430? \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u0435\u0441\u043b\u0438 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u043f\u043e\u0434\u0445\u043e\u0434\u0438\u0442\u044c \u0447\u0438\u0441\u0442\u043e \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438, \u0434\u0430, \u043a\u0430\u043a \u0432\u043e\u0442 \u0432 \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0441\u0442\u0438, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u044e\u0440\u0438\u0441\u043f\u0443\u043d\u0434\u0435\u043d\u0446\u0438\u0438 \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u0440\u043e\u0434\u0441\u0442\u0432\u043e \u043b\u044e\u0434\u0435\u0439, \u043e\u0442\u0446\u043e\u0432\u0441\u0442\u0432\u043e, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0434\u0430, \u043a\u0442\u043e \u043f\u0430\u043f\u0430, \u043a\u0442\u043e \u043c\u0430\u043c\u0430, \u043a\u0442\u043e \u0434\u0435\u0434\u0443\u0448\u043a\u0430, \u0431\u0430\u0431\u0443\u0448\u043a\u0430, \u0442\u043e \u043e\u043a\u0430\u0436\u0435\u0442\u0441\u044f, \u0447\u0442\u043e \u0434\u0430, \u044d\u0442\u043e \u0442\u0430\u043a\u0438\u0435 \u0447\u0435\u043b\u043e\u0432\u0435\u043a-\u043e\u0431\u0435\u0437\u044c\u044f\u043d\u044b. \u041c\u043e\u0436\u043d\u043e, \u043a\u043e\u043d\u0435\u0447\u043d\u043e, \u0438\u0433\u043d\u043e\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c, \u044d\u0442\u043e \u043b\u0435\u0433\u043a\u043e, \u0432\u043e\u0442, \u043d\u043e \u043c\u043e\u0436\u043d\u043e \u043d\u0435 \u0437\u043d\u0430\u0442\u044c, \u043d\u0443 \u0435\u0441\u043b\u0438 \u043d\u0435 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0432\u043e\u043e\u0431\u0449\u0435, \u043d\u043e \u0435\u0441\u043b\u0438 \u0438\u043c\u0435\u0442\u044c \u0445\u043e\u0442\u044c \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0436\u0435\u043b\u0430\u043d\u0438\u0435, \u044d\u0442\u043e \u0442\u0430\u043a\u0430\u044f \u043f\u0440\u043e\u0440\u0432\u0430 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438, \u0447\u0442\u043e \u0432 \u043d\u0435\u043c \u043c\u043e\u0436\u043d\u043e \u0431\u0443\u043b\u044c\u043a\u043d\u0443\u0442\u044c\u0441\u044f, \u043f\u043e\u0433\u0440\u0443\u0437\u0438\u0442\u044c\u0441\u044f \u0441 \u043d\u043e\u0433\u0430\u043c\u0438, \u0441 \u0433\u043e\u043b\u043e\u0432\u043e\u0439, \u0441 \u0443\u0448\u0430\u043c\u0438 \u0438 \u0432\u0441\u044e \u0436\u0438\u0437\u043d\u044c \u0442\u0430\u043c \u043a\u0443\u0432\u044b\u0440\u043a\u0430\u0442\u044c\u0441\u044f \u0432 \u044d\u0442\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438. \u0427\u0435\u043c \u044f, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0437\u0430\u043d\u0438\u043c\u0430\u044e\u0441\u044c \u0438 \u0441\u043f\u043e\u0440\u0438\u0442\u044c \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u043d\u0435 \u043f\u0440\u043e\u0438\u0437\u043e\u0448\u0435\u043b \u043e\u0442 \u043e\u0431\u0435\u0437\u044c\u044f\u043d\u044b, \u043d\u0443 \u043a\u0430\u043a-\u0442\u043e \u0434\u0430\u0436\u0435 \u043d\u0430\u0438\u0432\u043d\u043e. \u0415\u0441\u043b\u0438 \u0443\u0436 \u043c\u044b \u0435\u0441\u0442\u044c \u043e\u0431\u0435\u0437\u044c\u044f\u043d\u044b, \u0442\u043e \u043a\u0443\u0434\u0430 \u0434\u0435\u0432\u0430\u0442\u044c\u0441\u044f? \u0412\u043e\u0442 \u043c\u044b \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u044b\u0435 \u043e\u0431\u0435\u0437\u044c\u044f\u043d\u044b, \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0434\u0443\u043c\u0430\u0442\u044c \u043e \u0434\u0440\u0443\u0433\u0438\u0445 \u043e\u0431\u0435\u0437\u044c\u044f\u043d\u0430\u0445, \u0438\u0437\u0443\u0447\u0430\u0442\u044c \u0434\u0440\u0443\u0433\u0438\u0445 \u043e\u0431\u0435\u0437\u044c\u044f\u043d \u0438 \u043f\u043b\u0430\u043d\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441\u0432\u043e\u0435 \u043e\u0431\u0435\u0437\u044c\u044f\u043d\u044c\u0435 \u0431\u0443\u0434\u0443\u0449\u0435\u0435, \u0447\u0435\u043c \u043c\u044b \u0437\u0430\u043c\u0435\u0447\u0430\u0442\u0435\u043b\u044c\u043d\u044b. \u0422\u0430\u043a \u0447\u0442\u043e \u0437\u0430\u043d\u0438\u043c\u0430\u0439\u0442\u0435\u0441\u044c \u043b\u0443\u0447\u0448\u0435 \u044d\u0442\u0438\u043c, \u0430 \u043d\u0435 \u0431\u0440\u0435\u0434\u043e\u043c, \u0430 \u043d\u0435 \u0432\u0441\u044f\u043a\u0438\u043c\u0438 \u043c\u0438\u0444\u0430\u043c\u0438.'</pre> In\u00a0[18]: Copied! <pre>len(text)\n</pre> len(text) Out[18]: <pre>13899</pre> In\u00a0[19]: Copied! <pre>doc = Doc(text)\ndoc\n</pre> doc = Doc(text) doc Out[19]: <pre>Doc(text='\u041f\u0440\u0435\u0434\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u043c\u044b \u043f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u0438 \u043e\u0442 \u043e\u0431\u0435\u0437\u044c\u044f\u043d\u044b...)</pre> In\u00a0[20]: Copied! <pre>[a for a in dir(doc) if not a.startswith('_')]\n</pre> [a for a in dir(doc) if not a.startswith('_')] Out[20]: <pre>['as_json',\n 'clear_envelopes',\n 'envelop_sent_spans',\n 'envelop_sent_tokens',\n 'envelop_span_tokens',\n 'from_json',\n 'morph',\n 'ner',\n 'parse_syntax',\n 'segment',\n 'sents',\n 'spans',\n 'syntax',\n 'tag_morph',\n 'tag_ner',\n 'text',\n 'tokens']</pre> In\u00a0[21]: Copied! <pre># vars(doc)\n</pre> # vars(doc) In\u00a0[22]: Copied! <pre># \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0432 doc tokens, sents\ndoc.segment(segmenter)\n\ndisplay(doc) # \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\ndisplay('sentences',doc.sents[:3]) # \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f\ndisplay('tokens',doc.tokens[:5])  # \u0442\u043e\u043a\u0435\u043d\u044b\n</pre> # \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0432 doc tokens, sents doc.segment(segmenter)  display(doc) # \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442 display('sentences',doc.sents[:3]) # \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f display('tokens',doc.tokens[:5])  # \u0442\u043e\u043a\u0435\u043d\u044b <pre>Doc(text='\u041f\u0440\u0435\u0434\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u043c\u044b \u043f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u0438 \u043e\u0442 \u043e\u0431\u0435\u0437\u044c\u044f\u043d\u044b..., tokens=[...], sents=[...])</pre> <pre>'sentences'</pre> <pre>[DocSent(stop=72, text='\u041f\u0440\u0435\u0434\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u043c\u044b \u043f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u0438 \u043e\u0442 \u043e\u0431\u0435\u0437\u044c\u044f\u043d\u044b..., tokens=[...]),\n DocSent(start=73, stop=94, text='\u0423\u0447\u0435\u043d\u044b\u0435 \u0437\u0430\u0448\u043b\u0438 \u0432 \u0442\u0443\u043f\u0438\u043a.', tokens=[...]),\n DocSent(start=95, stop=203, text='\u0417\u0430 \u0433\u043e\u0434\u044b \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0439 \u0438\u043c \u0442\u0430\u043a \u0438 \u043d\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c \u0434\u043e\u043a\u0430\u0437\u0430\u0442\u044c..., tokens=[...])]</pre> <pre>'tokens'</pre> <pre>[DocToken(stop=13, text='\u041f\u0440\u0435\u0434\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0435'),\n DocToken(start=14, stop=15, text='\u043e'),\n DocToken(start=16, stop=19, text='\u0442\u043e\u043c'),\n DocToken(start=19, stop=20, text=','),\n DocToken(start=21, stop=24, text='\u0447\u0442\u043e')]</pre> In\u00a0[23]: Copied! <pre># \u0432\u044b\u0434\u0435\u043b\u044f\u0435\u043c \u0447\u0430\u0441\u0442\u0438 \u0440\u0435\u0447\u0438\ndoc.tag_morph(morph_tagger)\ndoc.parse_syntax(syntax_parser)\ndisplay(doc.tokens[:15])\n</pre> # \u0432\u044b\u0434\u0435\u043b\u044f\u0435\u043c \u0447\u0430\u0441\u0442\u0438 \u0440\u0435\u0447\u0438 doc.tag_morph(morph_tagger) doc.parse_syntax(syntax_parser) display(doc.tokens[:15]) <pre>[DocToken(stop=13, text='\u041f\u0440\u0435\u0434\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0435', id='1_1', head_id='1_13', rel='nsubj', pos='NOUN', feats=&lt;Inan,Acc,Neut,Sing&gt;),\n DocToken(start=14, stop=15, text='\u043e', id='1_2', head_id='1_3', rel='case', pos='ADP'),\n DocToken(start=16, stop=19, text='\u0442\u043e\u043c', id='1_3', head_id='1_1', rel='nmod', pos='PRON', feats=&lt;Inan,Loc,Neut,Sing&gt;),\n DocToken(start=19, stop=20, text=',', id='1_4', head_id='1_7', rel='punct', pos='PUNCT'),\n DocToken(start=21, stop=24, text='\u0447\u0442\u043e', id='1_5', head_id='1_7', rel='mark', pos='SCONJ'),\n DocToken(start=25, stop=27, text='\u043c\u044b', id='1_6', head_id='1_7', rel='nsubj', pos='PRON', feats=&lt;Nom,Plur,1&gt;),\n DocToken(start=28, stop=37, text='\u043f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u0438', id='1_7', head_id='1_3', rel='acl', pos='VERB', feats=&lt;Perf,Ind,Plur,Past,Fin,Act&gt;),\n DocToken(start=38, stop=40, text='\u043e\u0442', id='1_8', head_id='1_9', rel='case', pos='ADP'),\n DocToken(start=41, stop=49, text='\u043e\u0431\u0435\u0437\u044c\u044f\u043d\u044b', id='1_9', head_id='1_7', rel='obl', pos='NOUN', feats=&lt;Anim,Gen,Fem,Sing&gt;),\n DocToken(start=50, stop=51, text='\u0432', id='1_10', head_id='1_12', rel='case', pos='ADP'),\n DocToken(start=52, stop=54, text='21', id='1_11', head_id='1_12', rel='amod', pos='ADJ'),\n DocToken(start=55, stop=59, text='\u0432\u0435\u043a\u0435', id='1_12', head_id='1_13', rel='obl', pos='NOUN', feats=&lt;Inan,Loc,Masc,Sing&gt;),\n DocToken(start=60, stop=66, text='\u0442\u0435\u0440\u043f\u0438\u0442', id='1_13', head_id='1_3', rel='acl', pos='VERB', feats=&lt;Imp,Ind,Sing,3,Pres,Fin,Act&gt;),\n DocToken(start=67, stop=71, text='\u043a\u0440\u0430\u0445', id='1_14', head_id='1_13', rel='obj', pos='NOUN', feats=&lt;Inan,Acc,Masc,Sing&gt;),\n DocToken(start=71, stop=72, text='.', id='1_15', head_id='1_13', rel='punct', pos='PUNCT')]</pre> In\u00a0[24]: Copied! <pre>doc.tag_ner(ner_tagger)\nlen(doc.spans)\n</pre> doc.tag_ner(ner_tagger) len(doc.spans) Out[24]: <pre>8</pre> In\u00a0[25]: Copied! <pre>doc.spans\n</pre> doc.spans Out[25]: <pre>[DocSpan(start=383, stop=389, type='PER', text='\u0414\u0430\u0440\u0432\u0438\u043d', tokens=[...]),\n DocSpan(start=584, stop=606, type='PER', text='\u0421\u0442\u0430\u043d\u0438\u0441\u043b\u0430\u0432 \u0414\u043e\u0440\u0431\u044b\u0448\u0435\u0432\u0441\u043a\u0438\u0439', tokens=[...]),\n DocSpan(start=647, stop=661, type='PER', text='\u0420\u0431\u0430\u043a\u0430 \u0422\u0440\u0435\u043d\u0434\u0430\u043c\u0438', tokens=[...]),\n DocSpan(start=7655, stop=7663, type='PER', text='\u041f\u043e\u0435\u0434\u0435\u043d\u0438\u0435', tokens=[...]),\n DocSpan(start=9141, stop=9153, type='PER', text='\u041f\u0443\u0440\u0433\u0430\u0442\u043e\u0440\u0438\u0443\u0441\u0430', tokens=[...]),\n DocSpan(start=9349, stop=9361, type='PER', text='\u041f\u0443\u0440\u0433\u0430\u0442\u043e\u0440\u0438\u0443\u0441\u0430', tokens=[...]),\n DocSpan(start=11547, stop=11553, type='LOC', text='\u0410\u0444\u0440\u0438\u043a\u0435', tokens=[...]),\n DocSpan(start=12972, stop=12981, type='LOC', text='\u0410\u0432\u0441\u0442\u0440\u0430\u043b\u0438\u0438', tokens=[...])]</pre> In\u00a0[26]: Copied! <pre>set([s.text for s in doc.spans])\n</pre> set([s.text for s in doc.spans]) Out[26]: <pre>{'\u0410\u0432\u0441\u0442\u0440\u0430\u043b\u0438\u0438',\n '\u0410\u0444\u0440\u0438\u043a\u0435',\n '\u0414\u0430\u0440\u0432\u0438\u043d',\n '\u041f\u043e\u0435\u0434\u0435\u043d\u0438\u0435',\n '\u041f\u0443\u0440\u0433\u0430\u0442\u043e\u0440\u0438\u0443\u0441\u0430',\n '\u0420\u0431\u0430\u043a\u0430 \u0422\u0440\u0435\u043d\u0434\u0430\u043c\u0438',\n '\u0421\u0442\u0430\u043d\u0438\u0441\u043b\u0430\u0432 \u0414\u043e\u0440\u0431\u044b\u0448\u0435\u0432\u0441\u043a\u0438\u0439'}</pre> In\u00a0[27]: Copied! <pre>for span in doc.spans:\n    print(span.text,span.type)\n</pre> for span in doc.spans:     print(span.text,span.type) <pre>\u0414\u0430\u0440\u0432\u0438\u043d PER\n\u0421\u0442\u0430\u043d\u0438\u0441\u043b\u0430\u0432 \u0414\u043e\u0440\u0431\u044b\u0448\u0435\u0432\u0441\u043a\u0438\u0439 PER\n\u0420\u0431\u0430\u043a\u0430 \u0422\u0440\u0435\u043d\u0434\u0430\u043c\u0438 PER\n\u041f\u043e\u0435\u0434\u0435\u043d\u0438\u0435 PER\n\u041f\u0443\u0440\u0433\u0430\u0442\u043e\u0440\u0438\u0443\u0441\u0430 PER\n\u041f\u0443\u0440\u0433\u0430\u0442\u043e\u0440\u0438\u0443\u0441\u0430 PER\n\u0410\u0444\u0440\u0438\u043a\u0435 LOC\n\u0410\u0432\u0441\u0442\u0440\u0430\u043b\u0438\u0438 LOC\n</pre> <p>\u041c\u043e\u0436\u043d\u043e \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f <code>.normalize</code>, \u0434\u0430\u043d\u043d\u044b\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u0432 <code>span.normal</code></p> In\u00a0[28]: Copied! <pre># \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u043d\u044b\u0435 \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u0438\nfor span in doc.spans:\n    span.normalize(morph_vocab)\n\n{s.text: s.normal for s in doc.spans}\n</pre> # \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u043d\u044b\u0435 \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u0438 for span in doc.spans:     span.normalize(morph_vocab)  {s.text: s.normal for s in doc.spans} Out[28]: <pre>{'\u0414\u0430\u0440\u0432\u0438\u043d': '\u0414\u0430\u0440\u0432\u0438\u043d',\n '\u0421\u0442\u0430\u043d\u0438\u0441\u043b\u0430\u0432 \u0414\u043e\u0440\u0431\u044b\u0448\u0435\u0432\u0441\u043a\u0438\u0439': '\u0421\u0442\u0430\u043d\u0438\u0441\u043b\u0430\u0432 \u0414\u043e\u0440\u0431\u044b\u0448\u0435\u0432\u0441\u043a\u0438\u0439',\n '\u0420\u0431\u0430\u043a\u0430 \u0422\u0440\u0435\u043d\u0434\u0430\u043c\u0438': '\u0420\u0431\u0430\u043a\u0430 \u0422\u0440\u0435\u043d\u0434\u044b',\n '\u041f\u043e\u0435\u0434\u0435\u043d\u0438\u0435': '\u041f\u043e\u0435\u0434\u0435\u043d\u0438\u0435',\n '\u041f\u0443\u0440\u0433\u0430\u0442\u043e\u0440\u0438\u0443\u0441\u0430': '\u041f\u0443\u0440\u0433\u0430\u0442\u043e\u0440\u0438\u0443\u0441',\n '\u0410\u0444\u0440\u0438\u043a\u0435': '\u0410\u0444\u0440\u0438\u043a\u0430',\n '\u0410\u0432\u0441\u0442\u0440\u0430\u043b\u0438\u0438': '\u0410\u0432\u0441\u0442\u0440\u0430\u043b\u0438\u044f'}</pre> In\u00a0[29]: Copied! <pre># \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043c \u043f\u0435\u0440\u0441\u043e\u043d\nfor span in doc.spans:\n    if(span.type == PER):\n        span.extract_fact(names_extractor)\n\n{s.normal: s.fact.as_dict for s in doc.spans if s.fact}\n</pre> # \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043c \u043f\u0435\u0440\u0441\u043e\u043d for span in doc.spans:     if(span.type == PER):         span.extract_fact(names_extractor)  {s.normal: s.fact.as_dict for s in doc.spans if s.fact} Out[29]: <pre>{'\u0414\u0430\u0440\u0432\u0438\u043d': {'last': '\u0414\u0430\u0440\u0432\u0438\u043d'},\n '\u0421\u0442\u0430\u043d\u0438\u0441\u043b\u0430\u0432 \u0414\u043e\u0440\u0431\u044b\u0448\u0435\u0432\u0441\u043a\u0438\u0439': {'first': '\u0421\u0442\u0430\u043d\u0438\u0441\u043b\u0430\u0432', 'last': '\u0414\u043e\u0440\u0431\u044b\u0448\u0435\u0432\u0441\u043a\u0438\u0439'},\n '\u0420\u0431\u0430\u043a\u0430 \u0422\u0440\u0435\u043d\u0434\u044b': {'last': '\u0420\u0431\u0430\u043a\u0430'}}</pre> In\u00a0[30]: Copied! <pre># \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f\nimport pymorphy2\nmorph = pymorphy2.MorphAnalyzer()\nfrom functools import lru_cache\n</pre> # \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f import pymorphy2 morph = pymorphy2.MorphAnalyzer() from functools import lru_cache In\u00a0[31]: Copied! <pre>dataset_folder = '.'\nfiles = os.listdir(dataset_folder)\nfiles = [i for i in files if '.txt' in i]\nfiles\n</pre> dataset_folder = '.' files = os.listdir(dataset_folder) files = [i for i in files if '.txt' in i] files Out[31]: <pre>['res.txt', 'res2.txt']</pre> In\u00a0[32]: Copied! <pre>texts = []\nfor f in files:\n    with open(os.path.join(dataset_folder, f), 'r') as fo:\n        texts.append(fo.read())\n</pre> texts = [] for f in files:     with open(os.path.join(dataset_folder, f), 'r') as fo:         texts.append(fo.read()) In\u00a0[33]: Copied! <pre>def get_ner_natasha(text):\n    text = re.sub(r'[^s\\d\\w\\-:,\\.\\?\\!]', ' ', text) # \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u0443\u043d\u043a\u0442\u0443\u0430\u0446\u0438\u044f\n    doc = Doc(text)                                 # \u0441\u043e\u0437\u0434\u0430\u0435\u043c natasha \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\n    doc.segment(segmenter)                          # tokenise, sentences\n    doc.tag_ner(ner_tagger)                         # tag NER\n\n    # normalise using morpher\n    for span in doc.spans:\n        span.normalize(morph_vocab)\n\n    res = set((s.normal for s in doc.spans))\n    return res\n</pre> def get_ner_natasha(text):     text = re.sub(r'[^s\\d\\w\\-:,\\.\\?\\!]', ' ', text) # \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u0443\u043d\u043a\u0442\u0443\u0430\u0446\u0438\u044f     doc = Doc(text)                                 # \u0441\u043e\u0437\u0434\u0430\u0435\u043c natasha \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442     doc.segment(segmenter)                          # tokenise, sentences     doc.tag_ner(ner_tagger)                         # tag NER      # normalise using morpher     for span in doc.spans:         span.normalize(morph_vocab)      res = set((s.normal for s in doc.spans))     return res <p>(2) NER \u0441 <code>re</code></p> <p>\u0412\u043e\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f NER \u0441 <code>re</code>, \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043a\u0430\u043c\u0438 \u0438 \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0442\u043e\u0440\u0430 \u0438\u0437 <code>pymorphy2</code></p> In\u00a0[34]: Copied! <pre>@lru_cache(10000)\ndef lemmatize(s):\n    s = str(s).lower()\n    return morph.parse(s)[0].normal_form.capitalize()\n</pre> @lru_cache(10000) def lemmatize(s):     s = str(s).lower()     return morph.parse(s)[0].normal_form.capitalize() In\u00a0[35]: Copied! <pre>reg1 = re.compile(r'[^s\\d\\w\\-:,\\.\\?\\!]')\nreg2 = re.compile(r'([\\.\\?!])')\n\ndef get_ner_regex(s):\n    s = reg1.sub(' ', s)\n    s = reg2.sub(r'\\g&lt;1&gt;&lt;sep&gt;', s)\n\n    # \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u043d\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f\n    sent1 = [sent.strip() for sent in s.split('&lt;sep&gt;')]\n    sent2 = [' '.join(ss.split()[1:]) for ss in sent1]\n\n    res = []\n    for ss in sent2:\n        res.extend([e.strip() for e in re.findall(r'(?:[A-Z\u0410-\u042f\u0401][A-Z\u0410-\u042f\u0401\u0430-\u044f\u0451a-z\\d-]+\\s*)+', ss)])\n\n    return set((lemmatize(s) for s in res))\n</pre> reg1 = re.compile(r'[^s\\d\\w\\-:,\\.\\?\\!]') reg2 = re.compile(r'([\\.\\?!])')  def get_ner_regex(s):     s = reg1.sub(' ', s)     s = reg2.sub(r'\\g&lt;1&gt;', s)      # \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u043d\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f     sent1 = [sent.strip() for sent in s.split('')]     sent2 = [' '.join(ss.split()[1:]) for ss in sent1]      res = []     for ss in sent2:         res.extend([e.strip() for e in re.findall(r'(?:[A-Z\u0410-\u042f\u0401][A-Z\u0410-\u042f\u0401\u0430-\u044f\u0451a-z\\d-]+\\s*)+', ss)])      return set((lemmatize(s) for s in res)) <p>(3) \u041e\u0431\u044a\u0435\u0434\u0438\u043d\u0438\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0438\u0437 <code>natasha</code> \u0441 <code>re</code></p> In\u00a0[36]: Copied! <pre>def get_ner(text):\n    return get_ner_regex(text).union(get_ner_natasha(text))\n</pre> def get_ner(text):     return get_ner_regex(text).union(get_ner_natasha(text)) In\u00a0[37]: Copied! <pre>text = texts[1]\nprint(text)\n</pre> text = texts[1] print(text) <pre>\u0420\u0430\u043a\u0435\u0442\u0430-\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c \u00ab\u0421\u043e\u044e\u0437-\u0421\u0422-\u0410\u00bb \u0443\u0441\u043f\u0435\u0448\u043d\u043e \u0437\u0430\u043f\u0443\u0449\u0435\u043d\u0430 \u0438\u0437 \u0413\u0432\u0438\u0430\u043d\u0441\u043a\u043e\u0433\u043e \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0446\u0435\u043d\u0442\u0440\u0430 26 \u0430\u043f\u0440\u0435\u043b\u044f \u0432 0:02 \u043f\u043e \u043c\u043e\u0441\u043a\u043e\u0432\u0441\u043a\u043e\u043c\u0443 \u0432\u0440\u0435\u043c\u0435\u043d\u0438. \u041d\u0430 \u0431\u043e\u0440\u0442\u0443 \u043d\u0430\u0445\u043e\u0434\u0438\u043b\u0438\u0441\u044c \u0441\u043f\u0443\u0442\u043d\u0438\u043a\u0438 Sentinel-1B, Microscope \u0438 FYS, \u0441\u043e\u043e\u0431\u0449\u0430\u0435\u0442 \u0420\u043e\u0441\u043a\u043e\u0441\u043c\u043e\u0441.\nSentnel-1B \u043f\u0440\u0435\u0434\u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d \u0434\u043b\u044f \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f \u0437\u0430 \u0441\u0443\u0448\u0435\u0439 \u0438 \u043e\u043a\u0435\u0430\u043d\u0430\u043c\u0438 \u0432 \u0440\u0430\u0434\u0438\u043e\u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0435. \u0421\u043f\u0443\u0442\u043d\u0438\u043a \u0438\u0434\u0435\u043d\u0442\u0438\u0447\u0435\u043d \u0437\u0430\u043f\u0443\u0449\u0435\u043d\u043d\u043e\u043c\u0443 \u0434\u0432\u0443\u043c\u044f \u0433\u043e\u0434\u0430\u043c\u0438 \u0440\u0430\u043d\u0435\u0435 Sentinel-1A, \u043e\u0431\u0430 \u0430\u043f\u043f\u0430\u0440\u0430\u0442\u0430 Sentinel-1 \u0431\u0443\u0434\u0443\u0442 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0432 \u043f\u0430\u0440\u0435, \u0441\u043e\u0431\u0438\u0440\u0430\u044f \u0434\u0430\u043d\u043d\u044b\u0435 \u0441 \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u043f\u043e\u043b\u043e\u0436\u043d\u044b\u0445 \u0442\u043e\u0447\u0435\u043a \u043e\u0440\u0431\u0438\u0442\u044b. Sentinel-1 \u043f\u043e\u043c\u043e\u0433\u0430\u044e\u0442 \u0441\u043b\u0435\u0434\u0438\u0442\u044c \u0437\u0430 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435\u043c \u043e\u043a\u0435\u0430\u043d\u043e\u0432, \u043b\u0435\u0434\u043d\u0438\u043a\u043e\u0432 \u0438 \u043b\u0435\u0441\u043e\u0432. \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c\u044b\u0435 \u0441 \u043e\u0440\u0431\u0438\u0442\u044b \u0434\u0430\u043d\u043d\u044b\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u0434\u043b\u044f \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u044f \u0430\u0439\u0441\u0431\u0435\u0440\u0433\u043e\u0432 \u0438 \u043d\u0435\u0444\u0442\u044f\u043d\u044b\u0445 \u0440\u0430\u0437\u043b\u0438\u0432\u043e\u0432, \u0430 \u0442\u0430\u043a\u0436\u0435 \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0430\u043a\u0442\u0443\u0430\u043b\u044c\u043d\u044b\u0445 \u043a\u0430\u0440\u0442\u043e\u0433\u0440\u0430\u0444\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u0440\u0438 \u0447\u0440\u0435\u0437\u0432\u044b\u0447\u0430\u0439\u043d\u044b\u0445 \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044f\u0445. \u041a\u0440\u043e\u043c\u0435 \u0442\u043e\u0433\u043e, \u0434\u0430\u043d\u043d\u044b\u0435 \u0441\u043e \u0441\u043f\u0443\u0442\u043d\u0438\u043a\u043e\u0432 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u0432 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u00ab\u043d\u0435\u043f\u0440\u043e\u0444\u0438\u043b\u044c\u043d\u044b\u0445\u00bb \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u044f\u0445. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0433\u0440\u0443\u043f\u043f\u0430 \u043d\u0435\u043c\u0435\u0446\u043a\u0438\u0445 \u0443\u0447\u0435\u043d\u044b\u0445 \u043e\u043f\u0443\u0431\u043b\u0438\u043a\u043e\u0432\u0430\u043b\u0430 \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0435 \u043e \u043f\u0440\u0435\u0434\u043f\u043e\u043b\u0430\u0433\u0430\u0435\u043c\u043e\u043c \u043c\u0435\u0441\u0442\u0435 \u0438\u0441\u043f\u044b\u0442\u0430\u043d\u0438\u0439 \u0441\u0435\u0432\u0435\u0440\u043e\u043a\u043e\u0440\u0435\u0439\u0441\u043a\u043e\u0433\u043e \u044f\u0434\u0435\u0440\u043d\u043e\u0433\u043e \u0438 \u0442\u0435\u0440\u043c\u043e\u044f\u0434\u0435\u0440\u043d\u043e\u0433\u043e \u043e\u0440\u0443\u0436\u0438\u044f, \u0432 \u0440\u0430\u0431\u043e\u0442\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u0441\u044f \u0438 \u0438\u043d\u0442\u0435\u0440\u0444\u0435\u0440\u043e\u043c\u0435\u0442\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0441\u043d\u0438\u043c\u043e\u043a \u043f\u0440\u0435\u0434\u043f\u043e\u043b\u0430\u0433\u0430\u0435\u043c\u043e\u0433\u043e \u043c\u0435\u0441\u0442\u0430, \u0441\u0434\u0435\u043b\u0430\u043d\u043d\u044b\u0439 Sentinel-1A.\n\u0417\u0430\u043f\u0443\u0441\u043a \u0441\u043f\u0443\u0442\u043d\u0438\u043a\u043e\u0432 \u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u0438 Sentinel \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0441\u044f \u0432 \u0440\u0430\u043c\u043a\u0430\u0445 \u043f\u0440\u043e\u0435\u043a\u0442\u0430 \u043c\u043e\u043d\u0438\u0442\u043e\u0440\u0438\u043d\u0433\u0430 \u043e\u043a\u0440\u0443\u0436\u0430\u044e\u0449\u0435\u0439 \u0441\u0440\u0435\u0434\u044b \u00ab\u041a\u043e\u043f\u0435\u0440\u043d\u0438\u043a\u00bb, \u0441\u043e\u0437\u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0415\u0432\u0440\u043e\u043f\u0435\u0439\u0441\u043a\u043e\u0439 \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u0435\u0439 \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u043d\u043e \u0441 \u0415\u0432\u0440\u043e\u043f\u0435\u0439\u0441\u043a\u0438\u043c \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u043c \u0430\u0433\u0435\u043d\u0442\u0441\u0442\u0432\u043e\u043c. \u041d\u0430 \u043f\u0440\u043e\u0442\u044f\u0436\u0435\u043d\u0438\u0438 \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0438\u0445 \u0434\u043d\u0435\u0439 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u044b \u0441\u043f\u0443\u0442\u043d\u0438\u043a\u0430 \u0431\u0443\u0434\u0443\u0442 \u043e\u0442\u043a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u0430\u043d\u044b \u0438 \u0437\u043e\u043d\u0434 \u043f\u0440\u0438\u0441\u0442\u0443\u043f\u0438\u0442 \u043a \u0440\u0430\u0431\u043e\u0442\u0435.\n\u041f\u0435\u0440\u0432\u044b\u0439 \u0441\u043f\u0443\u0442\u043d\u0438\u043a \u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u0438 Sentinel \u0431\u044b\u043b \u0432\u044b\u0432\u0435\u0434\u0435\u043d \u043d\u0430 \u043e\u0440\u0431\u0438\u0442\u0443 3 \u0430\u043f\u0440\u0435\u043b\u044f 2014, \u0432\u0441\u0435\u0433\u043e \u043a \u0437\u0430\u043f\u0443\u0441\u043a\u0443 \u043f\u043b\u0430\u043d\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u0448\u0435\u0441\u0442\u044c \u0440\u0430\u0437\u043d\u044b\u0445 \u043c\u043e\u0434\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0439 Sentinel \u0441 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u043e\u0439 \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0435\u0439. \u041f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0439 \u0441\u043f\u0443\u0442\u043d\u0438\u043a Sentinel-3A \u0431\u044b\u043b \u0432\u044b\u0432\u0435\u0434\u0435\u043d \u043d\u0430 \u043e\u0440\u0431\u0438\u0442\u0443 \u0432 \u0444\u0435\u0432\u0440\u0430\u043b\u0435 2016 \u0433\u043e\u0434\u0430.\n\u041d\u0438\u043a\u043e\u043b\u0430\u0439 \u0412\u043e\u0440\u043e\u043d\u0446\u043e\u0432\n</pre> In\u00a0[38]: Copied! <pre># NER \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043d\u0430\u0448\u0435\u043b natasha\nget_ner_natasha(text)\n</pre> # NER \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043d\u0430\u0448\u0435\u043b natasha get_ner_natasha(text) Out[38]: <pre>{'FYS',\n '\u0413\u0432\u0438\u0430\u043d\u0441\u043a\u043e\u0433\u043e \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0446\u0435\u043d\u0442\u0440\u0430',\n '\u0415\u0432\u0440\u043e\u043f\u0435\u0439\u0441\u043a\u0438\u043c \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u043c \u0430\u0433\u0435\u043d\u0442\u0441\u0442\u0432\u043e\u043c',\n '\u0415\u0432\u0440\u043e\u043f\u0435\u0439\u0441\u043a\u043e\u0439 \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u0435\u0439',\n '\u041a\u043e\u043f\u0435\u0440\u043d\u0438\u043a',\n '\u041d\u0438\u043a\u043e\u043b\u0430\u0439 \u0412\u043e\u0440\u043e\u043d\u0446\u043e\u0432',\n '\u0420\u043e\u0441\u043a\u043e\u0441\u043c\u043e\u0441'}</pre> In\u00a0[39]: Copied! <pre># NER \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043d\u0430\u0448\u0435\u043b re\nget_ner_regex(text)\n</pre> # NER \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043d\u0430\u0448\u0435\u043b re get_ner_regex(text) Out[39]: <pre>{'Fys',\n 'Microscope',\n 'Sentinel',\n 'Sentinel-1',\n 'Sentinel-1a',\n 'Sentinel-1b',\n 'Sentinel-3a',\n '\u0412\u043e\u0440\u043e\u043d\u0446\u043e\u0432',\n '\u0413\u0432\u0438\u0430\u043d\u0441\u043a\u0438\u0439',\n '\u0415\u0432\u0440\u043e\u043f\u0435\u0439\u0441\u043a\u0438\u0439',\n '\u041a\u043e\u043f\u0435\u0440\u043d\u0438\u043a',\n '\u0420\u043e\u0441\u043a\u043e\u0441\u043c\u043e\u0441',\n '\u0421\u043e\u044e\u0437-\u0441\u0442-'}</pre> In\u00a0[40]: Copied! <pre># \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0438\u043c \u043f\u043e\u0434\u0445\u043e\u0434\u044b\nget_ner(text)\n</pre> # \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0438\u043c \u043f\u043e\u0434\u0445\u043e\u0434\u044b get_ner(text) Out[40]: <pre>{'FYS',\n 'Fys',\n 'Microscope',\n 'Sentinel',\n 'Sentinel-1',\n 'Sentinel-1a',\n 'Sentinel-1b',\n 'Sentinel-3a',\n '\u0412\u043e\u0440\u043e\u043d\u0446\u043e\u0432',\n '\u0413\u0432\u0438\u0430\u043d\u0441\u043a\u0438\u0439',\n '\u0413\u0432\u0438\u0430\u043d\u0441\u043a\u043e\u0433\u043e \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0446\u0435\u043d\u0442\u0440\u0430',\n '\u0415\u0432\u0440\u043e\u043f\u0435\u0439\u0441\u043a\u0438\u0439',\n '\u0415\u0432\u0440\u043e\u043f\u0435\u0439\u0441\u043a\u0438\u043c \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u043c \u0430\u0433\u0435\u043d\u0442\u0441\u0442\u0432\u043e\u043c',\n '\u0415\u0432\u0440\u043e\u043f\u0435\u0439\u0441\u043a\u043e\u0439 \u043a\u043e\u043c\u0438\u0441\u0441\u0438\u0435\u0439',\n '\u041a\u043e\u043f\u0435\u0440\u043d\u0438\u043a',\n '\u041d\u0438\u043a\u043e\u043b\u0430\u0439 \u0412\u043e\u0440\u043e\u043d\u0446\u043e\u0432',\n '\u0420\u043e\u0441\u043a\u043e\u0441\u043c\u043e\u0441',\n '\u0421\u043e\u044e\u0437-\u0441\u0442-'}</pre> In\u00a0[41]: Copied! <pre>import pandas as pd\n\ndf_review = pd.read_csv('review_cleaned.csv',usecols=['user','review_cleaned'])\nuser_review = list(df_review['user'].values)\nactual_review = list(df_review['review_cleaned'].values)\n\nuser_review = user_review[:50]\nactual_review = actual_review[:50]\n</pre> import pandas as pd  df_review = pd.read_csv('review_cleaned.csv',usecols=['user','review_cleaned']) user_review = list(df_review['user'].values) actual_review = list(df_review['review_cleaned'].values)  user_review = user_review[:50] actual_review = actual_review[:50] In\u00a0[56]: Copied! <pre>res_ners = []\nfor text in tqdm(actual_review):\n    res_ners.append(get_ner(text))\n</pre> res_ners = [] for text in tqdm(actual_review):     res_ners.append(get_ner(text)) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [00:03&lt;00:00, 16.43it/s]\n</pre> <p>\u041f\u0440\u0438\u043c\u0435\u0440 NER \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0433\u043e \u043e\u0442\u0437\u044b\u0432\u0430</p> In\u00a0[62]: Copied! <pre>ner_tagged = pd.DataFrame({'user':user_review,'review':actual_review,'ner_tags':res_ners})\nner_tagged.head()\n</pre> ner_tagged = pd.DataFrame({'user':user_review,'review':actual_review,'ner_tags':res_ners}) ner_tagged.head() Out[62]: user review ner_tags 0 dncmail \u041f\u043e\u0434\u0435\u043b\u044e\u0441\u044c \u0441 \u0432\u0430\u043c\u0438 \u0438\u0441\u0442\u043e\u0440\u0438\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u0430 \u0441\u043e... {\u0412\u044b, \u0421\u0431\u0435\u0440, MasterCard Standard, \u0415\u0432\u0433\u0435\u043d\u0438\u0438, Maste... 1 fomicevaa851 \u0421\u0430\u043c\u0430 \u043d\u0435\u0434\u0430\u0432\u043d\u043e \u0443\u0437\u043d\u0430\u043b\u0430, \u0447\u0442\u043e \u0432 \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0435 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u043b... {\u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a, \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430, \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0435, \u0410\u043a\u0442\u0438\u0432\u043d\u044b\u0439} 2 AlexStulov \u0421\u0431\u0435\u0440 \u043f\u043e\u0442\u0435\u0440\u044f\u043b \u043c\u043e\u0439 \u043c\u0438\u043b\u043b\u0438\u043e\u043d. \u0412 \u0430\u043f\u0440\u0435\u043b\u0435 \u0431\u0440\u0430\u043b \u0438\u043f\u043e\u0442\u0435\u043a... {\u0418\u0432\u0430\u043d\u043e\u0432\u0441\u043a\u0438\u0439, \u0421\u0431\u0435\u0440, \u0421\u0431\u0440, \u0421\u0431\u0435\u0440\u0435, \u0418\u0432\u0430\u043d\u043e\u0432\u0441\u043a\u0430\u044f \u043e\u0431\u043b,... 3 Zakharkot \u0414\u043e\u0431\u0440\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0441\u0443\u0442\u043e\u043a \u0432\u0441\u0435\u043c, \u044f \u043e\u0442\u043a\u0440\u044b\u043b \u0432 \u0421\u0431\u0435\u0440\u0435 \u0432... {\u0421\u0431\u0435\u0440, \u0421\u0431\u0435\u0440\u0435} 4 sanaan \u0416\u0438\u0432\u0443 \u0441 \u043c\u0430\u043c\u043e\u0439, \u043e\u043f\u043b\u0430\u0442\u043e\u0439 \u043a\u043e\u043c\u043c\u0443\u043d\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u043b\u0430\u0442\u0435\u0436\u0435\u0439 \u0434\u043e... {\u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a\u0430, \u0421\u0431\u0435\u0440\u0431\u0430\u043d\u043a, Qr-\u043a\u043e\u0434} In\u00a0[63]: Copied! <pre># \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c NER \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 CSV\nner_tagged.to_csv('ner_tags.csv',index=False)\n</pre> # \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c NER \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 CSV ner_tagged.to_csv('ner_tags.csv',index=False)"},{"location":"portfolio/course_nlp/preset_NER.html#named-entity-recognition-ner","title":"Named Entity Recognition (NER)\u00b6","text":"<p>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0433\u043e\u0442\u043e\u0432\u044b\u043c\u0438 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u0430\u043c\u0438 \u0434\u043b\u044f \u0432\u044b\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0438\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u0435\u0439</p> <p>\u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0431\u0430\u0437\u043e\u0432\u044b\u0439 \u043f\u0430\u0439\u043f\u043b\u0430\u0438\u043d \u0434\u043b\u044f NER</p> <ul> <li>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044f \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u043e\u0439 <code>natasha</code> \u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u044b\u0435 \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u0435 <code>re</code></li> </ul>"},{"location":"portfolio/course_nlp/preset_NER.html#natasha","title":"\u0411\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0430 natasha\u00b6","text":""},{"location":"portfolio/course_nlp/preset_NER.html","title":"\u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0443\u043b\u044c\u00b6","text":"<ul> <li>\u041e\u0434\u043d\u0430 \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u0432\u044b\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0438\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u0435\u0439 \u044d\u0442\u043e \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0430 <code>natasha</code></li> <li><code>natasha</code> \u044d\u0442\u043e \u0433\u043e\u0442\u043e\u0432\u044b\u0439 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u0434\u043b\u044f <code>NER</code>, \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u0434\u043b\u044f \u0440\u0443\u0441\u0441\u043a\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430</li> <li>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u043a \u044d\u0442\u043e\u0439 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u043e\u0439 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0434\u043b\u044f \u0432\u044b\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0438\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u0435\u0439</li> </ul>"},{"location":"portfolio/course_nlp/preset_NER.html","title":"\u0427\u0442\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<p>\u0427\u0438\u0442\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435</p>"},{"location":"portfolio/course_nlp/preset_NER.html#natasha-document","title":"<code>natasha</code> document\u00b6","text":"<p>\u0418\u043d\u0438\u0446\u0438\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043a\u043b\u0430\u0441\u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u0443\u0434\u0435\u0442 \u0445\u0440\u0430\u043d\u0438\u0442\u044c \u0432\u0441\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043e \u0442\u0435\u043a\u0441\u0442\u0435, \u0432 \u043e\u0442\u043b\u0438\u0447\u0438\u0435 \u043e\u0442 <code>spacy</code>, doc \u044d\u0442\u043e \u043d\u0435 \u0433\u043e\u0442\u043e\u0432\u0430\u044f pipeline</p> <p>\u041d\u0430\u043c \u043d\u0430\u0434\u043e \u0431\u0443\u0434\u0435\u0442 \u0432\u044b\u0437\u044b\u0432\u0430\u0442\u044c \u043e\u0442\u0434\u0435\u043b\u043d\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u0437\u0430\u043f\u043e\u043b\u043d\u0438\u0442\u044c \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u0435 \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u043d\u0430\u0441 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u0443\u0435\u0442:</p> <ul> <li><code>.segmenter</code> - \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f, \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u043d\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435</li> <li><code>.tag_morph</code> - \u043c\u043e\u0440\u0444\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0440\u0430\u0437\u0431\u043e\u0440 \u0442\u043e\u043a\u0435\u043d\u043e\u0432 (e.g. NOUN)</li> <li><code>.tag_ner</code> - \u0432\u044b\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0438\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u0435\u0439</li> </ul>"},{"location":"portfolio/course_nlp/preset_NER.html","title":"\u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442 \u043d\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u0438 \u0442\u043e\u043a\u0435\u043d\u044b\u00b6","text":"<p>\u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0442\u0435\u043a\u0441\u0442\u0430 \u043d\u0430 \u0442\u043e\u043a\u0435\u043d\u044b (<code>tokens</code>) \u0438 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f (<code>sents</code>), \u0434\u0430\u043d\u043d\u044b\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u0432 <code>doc</code></p>"},{"location":"portfolio/course_nlp/preset_NER.html","title":"\u0412\u044b\u0434\u0435\u043b\u044f\u0435\u043c \u0447\u0430\u0441\u0442\u044c \u0440\u0435\u0447\u0438\u00b6","text":"<p>\u0412\u044b\u0437\u043e\u0432\u0438\u043c \u043c\u0435\u0442\u043e\u0434 <code>tag_morph</code> \u0438 \u043f\u0435\u0440\u0435\u0434\u0430\u0434\u0438\u043c \u043e\u0431\u044a\u0435\u043a\u0442 <code>morph_tagger</code></p> <ul> <li>\u0421\u0434\u0435\u043b\u0430\u0435\u043c \u043c\u043e\u0440\u0444\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0440\u0430\u0437\u0431\u043e\u0440 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430</li> <li>\"POS\" - \u0447\u0430\u0441\u0442\u044c \u0440\u0435\u0447\u0438</li> </ul>"},{"location":"portfolio/course_nlp/preset_NER.html","title":"\u0412\u044b\u0434\u0435\u043b\u044f\u0435\u043c \u0438\u043c\u0435\u043d\u043d\u043e\u0432\u0430\u043d\u044b\u0435 \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u0438\u00b6","text":"<ul> <li>\u0414\u043b\u044f \u0432\u044b\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0438\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u0435\u0439 \u0432 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0435 \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u043c <code>tag_ner</code></li> <li>\u0418\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u0432 <code>.spans</code><ul> <li>\u0412 <code>spans.type</code> \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442\u0441\u044f \u043a\u043b\u0430\u0441\u0441 \u0438\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u0438</li> <li>\u0412 <code>spans.text</code> \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442\u0441\u044f \u0432\u044b\u0434\u0435\u043b\u0435\u043d\u043d\u0430\u044f \u0438\u043c\u0435\u043d\u043d\u043e\u0432\u0430\u043d\u0430\u044f \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u044c</li> </ul> </li> </ul>"},{"location":"portfolio/course_nlp/preset_NER.html#ner","title":"\u041e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u043d\u044b\u0439 \u043c\u0435\u0442\u043e\u0434 NER\u00b6","text":""},{"location":"portfolio/course_nlp/preset_NER.html","title":"\u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<p><code>natasha</code> \u043c\u043e\u0436\u0435\u0442 \u0443\u043f\u0443\u0441\u043a\u0430\u0442\u044c \u0438\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u0438, \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0438\u0437 <code>re</code></p>"},{"location":"portfolio/course_nlp/preset_NER.html","title":"\u0412\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438\u00b6","text":"<p>(1) NER \u0441 <code>natasha</code></p> <p>\u0412\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f NER \u0441 <code>natasha</code></p>"},{"location":"portfolio/course_nlp/preset_NER.html","title":"\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043f\u043e\u0434\u0445\u043e\u0434\u0430\u00b6","text":"<p>\u041f\u043e\u0434\u0442\u0432\u0435\u0440\u0434\u0438\u043c \u0447\u0442\u043e \u0432\u0441\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u043f\u043e \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438</p> <ul> <li><code>get_ner_natasha</code> \u043d\u0430\u043c \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u044b\u0434\u0435\u043b\u0438\u043b\u0430 <code>natasha</code></li> <li><code>get_ner_regex</code> \u043d\u0430\u043c \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u044b\u0434\u0435\u043b\u0438\u043b <code>re</code></li> <li><code>get_ner</code> \u043d\u0430\u043c \u0432\u043e\u0437\u0432\u0430\u0449\u0430\u0435\u0442 \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u0435 \u0434\u0432\u0443\u0445 \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u0432</li> </ul>"},{"location":"portfolio/course_nlp/preset_NER.html","title":"\u0420\u0430\u0431\u043e\u0442\u0430 \u043d\u0430 \u0440\u0435\u0430\u043b\u044c\u043d\u043e\u043c \u043f\u0440\u0438\u043c\u0435\u0440\u0435\u00b6","text":"<p>\u0412\u0441\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442, \u0442\u0435\u043f\u0435\u0440\u044c \u0432\u043e\u0437\u043c\u0435\u043c \u043d\u0430\u0448 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u0439 \u043a\u043e\u0440\u043f\u0443\u0441 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 (\u043e\u0442\u0437\u044b\u0432\u044b \u043e \u0431\u0430\u043d\u043a\u0435)</p>"},{"location":"portfolio/course_nlp/text_embeddings.html","title":"Text embeddings","text":"In\u00a0[1]: Copied! <pre>pip install pymystem3\n</pre> pip install pymystem3 <pre>Collecting pymystem3\r\n  Downloading pymystem3-0.2.0-py3-none-any.whl (10 kB)\r\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from pymystem3) (2.31.0)\r\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/conda/lib/python3.10/site-packages (from requests-&gt;pymystem3) (3.1.0)\r\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.10/site-packages (from requests-&gt;pymystem3) (3.4)\r\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests-&gt;pymystem3) (1.26.15)\r\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests-&gt;pymystem3) (2023.5.7)\r\nInstalling collected packages: pymystem3\r\nSuccessfully installed pymystem3-0.2.0\r\nNote: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[2]: Copied! <pre>import pandas as pd\nimport random\nimport numpy as np\nfrom sklearn.metrics import *\nfrom sklearn.feature_extraction.text import *\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter, defaultdict\nimport warnings\nfrom pymystem3 import Mystem\nimport re\n\nwarnings.filterwarnings('ignore')\nrandom.seed(1228)\n</pre> import pandas as pd import random import numpy as np from sklearn.metrics import * from sklearn.feature_extraction.text import * from sklearn.model_selection import train_test_split from collections import Counter, defaultdict import warnings from pymystem3 import Mystem import re  warnings.filterwarnings('ignore') random.seed(1228) <pre>/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}\"\n</pre> In\u00a0[3]: Copied! <pre>df_pos = pd.read_csv(\"/kaggle/input/twitter/positive.csv\", sep=';', header = None)\ndf_pos.tail()\n</pre> df_pos = pd.read_csv(\"/kaggle/input/twitter/positive.csv\", sep=';', header = None) df_pos.tail() Out[3]: 0 1 2 3 4 5 6 7 8 9 10 11 114906 411368729235054592 1386912922 diminlisenok \u0421\u043f\u0430\u043b\u0430 \u0432 \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u0441\u043a\u043e\u043c \u0434\u043e\u043c\u0435, \u043d\u0430 \u0441\u0432\u043e\u0435\u0439 \u043a\u0440\u043e\u0432\u0430\u0442\u0438...... 1 0 0 0 1497 56 34 2 114907 411368729424187392 1386912922 qilepocagotu RT @jebesilofyt: \u042d\u0445... \u041c\u044b \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u0440\u0435\u0448\u0438\u043b\u0438 \u0441\u043e\u043a\u0440\u0430... 1 0 1 0 692 225 210 0 114908 411368796537257984 1386912938 DennyChooo \u0427\u0442\u043e \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442 \u0441\u043e \u043c\u043d\u043e\u0439, \u043a\u043e\u0433\u0434\u0430 \u0432 \u044d\u0444\u0438\u0440\u0435 #proacti... 1 0 0 0 4905 448 193 13 114909 411368797447417856 1386912938 bedowabymir \"\u041b\u044e\u0431\u0438\u043c\u0430\u044f,\u044f \u043f\u043e\u0434\u0430\u0440\u044e \u0442\u0435\u0431\u0435 \u044d\u0442\u0443 \u0437\u0432\u0435\u0437\u0434\u0443...\" \u0418\u043c\u044f \u043a\u0430\u043a\u043e... 1 0 0 0 989 254 251 0 114910 411368857035898880 1386912953 Prituljak_Sibir @Ma_che_rie \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438 #\u043d\u0435\u043f\u044b\u0442\u0430\u0439\u0442\u0435\u0441\u044c\u043f\u043e\u043a\u0438\u043d\u0443\u0442\u044c\u043e\u043c\u0441\u043a ... 1 0 0 0 1005 221 178 6 In\u00a0[4]: Copied! <pre>df_neg = pd.read_csv(\"/kaggle/input/twitter/negative.csv\", sep=';', header = None)\ndf_neg.head()\n</pre> df_neg = pd.read_csv(\"/kaggle/input/twitter/negative.csv\", sep=';', header = None) df_neg.head() Out[4]: 0 1 2 3 4 5 6 7 8 9 10 11 0 408906762813579264 1386325944 dugarchikbellko \u043d\u0430 \u0440\u0430\u0431\u043e\u0442\u0435 \u0431\u044b\u043b \u043f\u043e\u043b\u043d\u044b\u0439 \u043f\u0438\u0434\u0434\u0435\u0441 :| \u0438 \u0442\u0430\u043a \u043a\u0430\u0436\u0434\u043e\u0435 \u0437\u0430... -1 0 0 0 8064 111 94 2 1 408906818262687744 1386325957 nugemycejela \u041a\u043e\u043b\u043b\u0435\u0433\u0438 \u0441\u0438\u0434\u044f\u0442 \u0440\u0443\u0431\u044f\u0442\u0441\u044f \u0432 Urban terror, \u0430 \u044f \u0438\u0437-\u0437... -1 0 0 0 26 42 39 0 2 408906858515398656 1386325966 4post21 @elina_4post \u043a\u0430\u043a \u0433\u043e\u0432\u043e\u0440\u044f\u0442 \u043e\u0431\u0435\u0449\u0430\u043d\u043e\u0433\u043e \u0442\u0440\u0438 \u0433\u043e\u0434\u0430 \u0436\u0434... -1 0 0 0 718 49 249 0 3 408906914437685248 1386325980 Poliwake \u0416\u0435\u043b\u0430\u044e \u0445\u043e\u0440\u043e\u0448\u0435\u0433\u043e \u043f\u043e\u043b\u0451\u0442\u0430 \u0438 \u0443\u0434\u0430\u0447\u043d\u043e\u0439 \u043f\u043e\u0441\u0430\u0434\u043a\u0438,\u044f \u0431\u0443\u0434\u0443... -1 0 0 0 10628 207 200 0 4 408906914723295232 1386325980 capyvixowe \u041e\u0431\u043d\u043e\u0432\u0438\u043b \u0437\u0430 \u043a\u0430\u043a\u0438\u043c-\u0442\u043e \u043b\u0435\u0448\u0438\u043c surf, \u0442\u0435\u043f\u0435\u0440\u044c \u043d\u0435 \u0440\u0430\u0431\u043e... -1 0 0 0 35 17 34 0 In\u00a0[5]: Copied! <pre># \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0441 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043a\u0430\u043c\u0438\ndef words_only(text):\n    try:\n        return \" \".join(re.findall(\"[\u0410-\u042f\u0430-\u044f:=!\\)\\()A-z\\_\\%/|]+\",text))\n    except:\n        return \"\"\n\n# \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u0438 \u0441 pymystem\nlemmatiser = Mystem()\ndef lemmatize(text):\n    try:\n        return \"\".join(lemmatiser.lemmatize(text)).strip()  \n    except:\n        return \" \"\n    \nwords_only('g;iuhoikl 7.kjh 87h \u043e\u0434\u043b\u0436\u0434 :))')\n</pre> # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0441 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043a\u0430\u043c\u0438 def words_only(text):     try:         return \" \".join(re.findall(\"[\u0410-\u042f\u0430-\u044f:=!\\)\\()A-z\\_\\%/|]+\",text))     except:         return \"\"  # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u0438 \u0441 pymystem lemmatiser = Mystem() def lemmatize(text):     try:         return \"\".join(lemmatiser.lemmatize(text)).strip()       except:         return \" \"      words_only('g;iuhoikl 7.kjh 87h \u043e\u0434\u043b\u0436\u0434 :))') <pre>Installing mystem to /root/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n</pre> Out[5]: <pre>'g iuhoikl kjh h \u043e\u0434\u043b\u0436\u0434 :))'</pre> In\u00a0[6]: Copied! <pre>lemmatize('\u0432\u0435\u0447\u0435\u0440\u043e\u043c \u0438\u043b\u0438 \u043c\u0435\u0434\u043b\u0435\u043d\u043d\u044b\u0439')\n</pre> lemmatize('\u0432\u0435\u0447\u0435\u0440\u043e\u043c \u0438\u043b\u0438 \u043c\u0435\u0434\u043b\u0435\u043d\u043d\u044b\u0439') Out[6]: <pre>'\u0432\u0435\u0447\u0435\u0440 \u0438\u043b\u0438 \u043c\u0435\u0434\u043b\u0435\u043d\u043d\u044b\u0439'</pre> In\u00a0[7]: Copied! <pre>df_neg = pd.read_csv(\"/kaggle/input/twitter/negative.csv\", sep=';', header = None, usecols = [3])\ndf_pos = pd.read_csv(\"/kaggle/input/twitter/positive.csv\", sep=';', header = None, usecols = [3])\ndf_neg['sent'] = 'neg'\ndf_pos['sent'] = 'pos'\ndf_pos['text'] = df_pos[3]\ndf_neg['text'] = df_neg[3]\ndf = pd.concat([df_neg, df_pos])\ndf = df[['text', 'sent']]\n\n%time df.text = df.text.apply(words_only)\n#%time df.text = df.text.apply(lemmatize)\n</pre> df_neg = pd.read_csv(\"/kaggle/input/twitter/negative.csv\", sep=';', header = None, usecols = [3]) df_pos = pd.read_csv(\"/kaggle/input/twitter/positive.csv\", sep=';', header = None, usecols = [3]) df_neg['sent'] = 'neg' df_pos['sent'] = 'pos' df_pos['text'] = df_pos[3] df_neg['text'] = df_neg[3] df = pd.concat([df_neg, df_pos]) df = df[['text', 'sent']]  %time df.text = df.text.apply(words_only) #%time df.text = df.text.apply(lemmatize) <pre>CPU times: user 1.36 s, sys: 40.2 ms, total: 1.4 s\nWall time: 1.4 s\n</pre> In\u00a0[8]: Copied! <pre>print(df.shape)\n#df.head()\n</pre> print(df.shape) #df.head() <pre>(226834, 2)\n</pre> <p><code>Word2Vec</code> \u0442\u0440\u0435\u0431\u0443\u0435\u0442 \u043d\u0430 \u0432\u0445\u043e\u0434\u0435 \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0435 \u0441\u043f\u0438\u0441\u043a\u043e\u0432</p> In\u00a0[9]: Copied! <pre>texts = [df.text.iloc[i].split() for i in range(len(df))]\nprint(len(texts))\n</pre> texts = [df.text.iloc[i].split() for i in range(len(df))] print(len(texts)) <pre>226834\n</pre> In\u00a0[10]: Copied! <pre>%%time\nfrom gensim.models import Word2Vec\n\n# \u0422\u0440\u0435\u043d\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\nmodel = Word2Vec(texts, \n                 window=5, \n                 min_count=5, \n                 workers=4)\n\n# \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\nmodel.save(\"word2v.model\")\n\n# \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\n#model = Word2Vec.load(\"word2v.model\") \n</pre> %%time from gensim.models import Word2Vec  # \u0422\u0440\u0435\u043d\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c model = Word2Vec(texts,                   window=5,                   min_count=5,                   workers=4)  # \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c model.save(\"word2v.model\")  # \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c #model = Word2Vec.load(\"word2v.model\")  <pre>CPU times: user 36.8 s, sys: 366 ms, total: 37.1 s\nWall time: 13.2 s\n</pre> In\u00a0[11]: Copied! <pre>model.wv['\u0440\u0430\u0431\u043e\u0442\u0430']\n</pre> model.wv['\u0440\u0430\u0431\u043e\u0442\u0430'] Out[11]: <pre>array([-0.10470013,  0.03513598,  0.41875052,  0.5153433 ,  0.9402133 ,\n       -0.2930955 ,  0.5164637 ,  0.7244828 , -0.24704719, -0.27431324,\n       -0.21855737,  0.01049238, -0.12207789, -0.257858  ,  0.3303831 ,\n       -0.5363448 ,  0.16048016, -0.00357761, -0.02529725, -0.896249  ,\n        0.00231601, -0.09299509,  0.50959855, -0.41379848, -0.58466417,\n       -0.59111625,  0.24317443, -0.27386442, -0.09665257,  0.02590754,\n        0.36794418, -0.578324  , -0.00952124,  0.15676403,  0.14984065,\n        0.28297207,  0.1306792 ,  0.45648193, -0.4614138 , -0.00394126,\n       -0.18601353,  0.28170818, -0.34839335,  0.46978518,  0.1506986 ,\n        0.18715191, -0.05226476, -0.3970961 , -0.09965609,  0.16863827,\n        0.14389691, -0.1783841 , -0.37686712,  0.22169474, -0.38901794,\n        0.35773337, -0.08235621,  0.2484024 , -0.31825948,  0.02456559,\n       -0.04911847,  0.01784658,  0.9772008 , -0.03402596, -0.7810176 ,\n        0.17957872,  0.5262285 ,  0.41534203, -0.5283286 , -0.20104289,\n       -0.22328393, -0.07975829,  0.24007888,  0.47399127,  0.2318192 ,\n        0.04374537,  0.43821317,  0.78923124, -0.7456475 ,  0.543595  ,\n       -0.1420232 , -0.61270565, -0.18691902, -0.40484717, -0.2623664 ,\n        0.24123536,  0.17079079,  0.29155582,  0.66039234, -0.15984882,\n        0.35916597, -0.24354032, -0.10610883,  1.0515662 ,  0.50824046,\n        0.69234437,  0.4339073 , -0.12926975, -0.37009227, -0.17433347],\n      dtype=float32)</pre> In\u00a0[12]: Copied! <pre>model.wv.most_similar(\"\u0440\u0430\u0431\u043e\u0442\u0430\")\n</pre> model.wv.most_similar(\"\u0440\u0430\u0431\u043e\u0442\u0430\") Out[12]: <pre>[('\u0448\u043a\u043e\u043b\u0430', 0.8297739624977112),\n ('\u0443\u0447\u0435\u0431\u0430', 0.8025556802749634),\n ('\u0444\u0438\u0437\u0438\u043a\u0430', 0.7936404347419739),\n ('\u0443\u0436\u0430\u0441\u043d\u0430\u044f', 0.7904353737831116),\n ('\u043f\u043e\u0433\u043e\u0434\u0430', 0.785150408744812),\n ('\u043d\u0430\u0441\u0442\u043e\u044f\u0449\u0430\u044f', 0.7794501781463623),\n ('\u0441\u0435\u0441\u0441\u0438\u044f', 0.766066312789917),\n ('\u043f\u0430\u0440\u0430', 0.7567639946937561),\n ('\u0435\u043b\u043a\u0430', 0.75386643409729),\n ('\u0430\u043b\u0433\u0435\u0431\u0440\u0430', 0.7523488402366638)]</pre> In\u00a0[13]: Copied! <pre>vec = (model.wv['\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442'] - model.wv['\u0441\u0442\u0443\u0434\u0435\u043d\u0442'] + model.wv['\u0448\u043a\u043e\u043b\u044c\u043d\u0438\u043a'])/3\nmodel.wv.similar_by_vector(vec)\n</pre> vec = (model.wv['\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442'] - model.wv['\u0441\u0442\u0443\u0434\u0435\u043d\u0442'] + model.wv['\u0448\u043a\u043e\u043b\u044c\u043d\u0438\u043a'])/3 model.wv.similar_by_vector(vec) Out[13]: <pre>[('\u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442', 0.9683309197425842),\n ('\u043f\u043e\u0441\u0442\u0443\u043f\u043b\u0435\u043d\u0438\u0438', 0.9346603751182556),\n ('\u043f\u043e\u043f\u0440\u043e\u0441\u0438\u043b\u0438', 0.9012249708175659),\n ('\u041a\u043e\u043d\u0433\u0435\u043d\u0438\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c!!!', 0.835422158241272),\n ('\u041f\u0440\u0438', 0.821401059627533),\n ('\u043f\u0440\u0438\u043d\u0435\u0441\u0442\u0438', 0.821234405040741),\n ('\u0440\u0443\u043a\u0430\u0432', 0.7973737716674805),\n ('\u041c\u0438\u0441\u0441', 0.7918796539306641),\n ('\u0438\u0441\u043f\u043e\u043b\u043d\u0438\u0442\u044c', 0.784400224685669),\n ('\u0441\u0442\u043e\u0440\u043e\u043d\u0443)', 0.761738657951355)]</pre> In\u00a0[14]: Copied! <pre>model.wv.doesnt_match(\"\u0446\u0432\u0435\u0442\u043e\u043a \u0434\u0435\u0440\u0435\u0432\u043e \u043a\u0430\u043a\u0442\u0443\u0441 \u0435\u0434\u0430\".split())\n</pre> model.wv.doesnt_match(\"\u0446\u0432\u0435\u0442\u043e\u043a \u0434\u0435\u0440\u0435\u0432\u043e \u043a\u0430\u043a\u0442\u0443\u0441 \u0435\u0434\u0430\".split()) Out[14]: <pre>'\u0446\u0432\u0435\u0442\u043e\u043a'</pre> In\u00a0[15]: Copied! <pre>from collections import Counter\n\ncounter = Counter()\ntop_words = []\nfor text in texts:\n    counter.update(text)\n    \nfor i in counter.most_common(500):\n    top_words.append(i[0])\nprint(top_words[:10])\n</pre> from collections import Counter  counter = Counter() top_words = [] for text in texts:     counter.update(text)      for i in counter.most_common(500):     top_words.append(i[0]) print(top_words[:10]) <pre>['\u043d\u0435', '\u0438', '\u0432', '\u044f', 'RT', '\u043d\u0430', '\u0447\u0442\u043e', 'http://t', '\u0430', '\u0441']\n</pre> <p>\u0412\u044b\u0431\u0435\u0440\u0435\u043c \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0432\u044b\u0431\u0440\u0430\u043d\u043d\u044b\u0445 \u0441\u043b\u043e\u0432</p> In\u00a0[16]: Copied! <pre>top_words_vec = model.wv[top_words]\nprint(top_words_vec.shape)\n</pre> top_words_vec = model.wv[top_words] print(top_words_vec.shape) <pre>(500, 100)\n</pre> <p>\u041c\u044b \u043c\u043e\u0436\u0435\u043c \u0435\u0449\u0435 \u043a\u043b\u0430\u0441\u0442\u0438\u0440\u0438\u0437\u0438\u0432\u0430\u0442\u044c \u0441\u043b\u043e\u0432\u0430 \u043d\u0430 \u043f\u043e\u0434\u0433\u0440\u0443\u043f\u043f\u044b \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0430\u0433\u043b\u043e\u043c\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u043e\u0439 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0438\u0437 <code>sklearn</code></p> In\u00a0[17]: Copied! <pre>from sklearn.manifold import TSNE\nfrom sklearn.cluster import AgglomerativeClustering\n\n# TSNE manifold learning \ntsne = TSNE(n_components=2, \n            random_state=0)\n\n# Agglomerative Clusterisation\nhtop_words_tsne = tsne.fit_transform(top_words_vec)\nhtop_words_tsne[:10,:]\n</pre> from sklearn.manifold import TSNE from sklearn.cluster import AgglomerativeClustering  # TSNE manifold learning  tsne = TSNE(n_components=2,              random_state=0)  # Agglomerative Clusterisation htop_words_tsne = tsne.fit_transform(top_words_vec) htop_words_tsne[:10,:] Out[17]: <pre>array([[-17.201815  , -10.581679  ],\n       [ -5.498273  ,   1.1501819 ],\n       [ 24.924892  ,   5.29755   ],\n       [-22.047146  , -12.7216425 ],\n       [  4.926873  ,   7.4602714 ],\n       [ 25.276384  ,  12.435881  ],\n       [ -1.6100953 , -23.253643  ],\n       [-16.26932   ,  -0.80900466],\n       [ 25.187656  ,  -2.175251  ],\n       [ -2.679728  ,  15.2672615 ]], dtype=float32)</pre> In\u00a0[18]: Copied! <pre>model = AgglomerativeClustering(n_clusters=5)\nclusters = model.fit_predict(htop_words_tsne)\nclusters = list(clusters)\nclusters = list(map(str, clusters))\n\n# combine data\ndata = pd.DataFrame({'d1':htop_words_tsne[:,0],\n                     'd2':htop_words_tsne[:,1],\n                     'cluster':clusters,\n                     'names':top_words})\n</pre> model = AgglomerativeClustering(n_clusters=5) clusters = model.fit_predict(htop_words_tsne) clusters = list(clusters) clusters = list(map(str, clusters))  # combine data data = pd.DataFrame({'d1':htop_words_tsne[:,0],                      'd2':htop_words_tsne[:,1],                      'cluster':clusters,                      'names':top_words}) In\u00a0[19]: Copied! <pre>from bokeh.models import ColumnDataSource, LabelSet\nfrom bokeh.models import LinearColorMapper, ColorBar\nfrom bokeh.plotting import figure, show, output_file\nfrom bokeh.transform import transform\nfrom bokeh.palettes import GnBu3, OrRd3\nfrom bokeh.io import output_notebook\nfrom bokeh.transform import factor_cmap\nimport colorcet as cc\n\noutput_notebook()\np = figure(tools=\"pan,wheel_zoom,reset,save\",\n           toolbar_location=\"above\",\n           title=\"word2vec T-SNE for most common words\")\n\ncolor = LinearColorMapper(palette = 'Viridis256')\n\np.scatter(x=\"d1\", y=\"d2\",\n          fill_color = transform('cluster', color),\n          line_color='black',\n          size=8 ,\n          source=data)\n\nlabels = LabelSet(x=\"d1\", y=\"d2\", text=\"names\", y_offset=6,\n                   text_font_size=\"8pt\", text_color=\"#555555\",\n                   source=ColumnDataSource(data), text_align='center')\np.add_layout(labels)\nshow(p)\n</pre> from bokeh.models import ColumnDataSource, LabelSet from bokeh.models import LinearColorMapper, ColorBar from bokeh.plotting import figure, show, output_file from bokeh.transform import transform from bokeh.palettes import GnBu3, OrRd3 from bokeh.io import output_notebook from bokeh.transform import factor_cmap import colorcet as cc  output_notebook() p = figure(tools=\"pan,wheel_zoom,reset,save\",            toolbar_location=\"above\",            title=\"word2vec T-SNE for most common words\")  color = LinearColorMapper(palette = 'Viridis256')  p.scatter(x=\"d1\", y=\"d2\",           fill_color = transform('cluster', color),           line_color='black',           size=8 ,           source=data)  labels = LabelSet(x=\"d1\", y=\"d2\", text=\"names\", y_offset=6,                    text_font_size=\"8pt\", text_color=\"#555555\",                    source=ColumnDataSource(data), text_align='center') p.add_layout(labels) show(p) Loading BokehJS ... In\u00a0[20]: Copied! <pre>import gensim\nimport urllib.request\nimport zipfile\n\n# \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0438 URL\nwe_models = {\"geowac_lemmas_none_fasttextskipgram_300_5_2020\": \"http://vectors.nlpl.eu/repository/20/213.zip\",}\n</pre> import gensim import urllib.request import zipfile  # \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0438 URL we_models = {\"geowac_lemmas_none_fasttextskipgram_300_5_2020\": \"http://vectors.nlpl.eu/repository/20/213.zip\",} In\u00a0[21]: Copied! <pre># \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\ndef get_models(model_url, model_name, path_to_save=\"/kaggle/working/\"):\n    model_path = path_to_save + model_name + \".zip\"\n    urllib.request.urlretrieve(model_url, model_path)\n\nfor model_name, model_url in we_models.items():\n    get_models(model_url, model_name)\n</pre> # \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c def get_models(model_url, model_name, path_to_save=\"/kaggle/working/\"):     model_path = path_to_save + model_name + \".zip\"     urllib.request.urlretrieve(model_url, model_path)  for model_name, model_url in we_models.items():     get_models(model_url, model_name) In\u00a0[22]: Copied! <pre># \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0447\u0442\u0435\u043d\u0438\u044f word2vec / FastText\n\ndef open_model(model_name,model_path, is_fasttext = True):\n    \n    # word2vec (model.bin)\n    if is_fasttext == False:\n        model_file = model_path + model_name + \".zip\"\n        with zipfile.ZipFile(model_file, 'r') as archive:\n            stream = archive.open('model.bin')\n            model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)\n            \n    # fasttext (model.model)\n    else:\n        model_file = model_path + model_name\n        model = gensim.models.KeyedVectors.load(model_file + \"/model.model\")\n    return model\n\nwith zipfile.ZipFile(\"/kaggle/working/geowac_lemmas_none_fasttextskipgram_300_5_2020.zip\", 'r') as zip_ref: \n    zip_ref.extractall(\"/kaggle/working/geowac_lemmas_none_fasttextskipgram_300_5_2020\")\n</pre> # \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0447\u0442\u0435\u043d\u0438\u044f word2vec / FastText  def open_model(model_name,model_path, is_fasttext = True):          # word2vec (model.bin)     if is_fasttext == False:         model_file = model_path + model_name + \".zip\"         with zipfile.ZipFile(model_file, 'r') as archive:             stream = archive.open('model.bin')             model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)                  # fasttext (model.model)     else:         model_file = model_path + model_name         model = gensim.models.KeyedVectors.load(model_file + \"/model.model\")     return model  with zipfile.ZipFile(\"/kaggle/working/geowac_lemmas_none_fasttextskipgram_300_5_2020.zip\", 'r') as zip_ref:      zip_ref.extractall(\"/kaggle/working/geowac_lemmas_none_fasttextskipgram_300_5_2020\") In\u00a0[23]: Copied! <pre># \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c KeyedVectors \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432\u044b\u0439 \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \ngeowac_model = open_model('geowac_lemmas_none_fasttextskipgram_300_5_2020','/kaggle/working/')\n</pre> # \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c KeyedVectors \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432\u044b\u0439 \u0432\u0435\u043a\u0442\u043e\u0440\u0430  geowac_model = open_model('geowac_lemmas_none_fasttextskipgram_300_5_2020','/kaggle/working/') In\u00a0[24]: Copied! <pre>top_words_vec = geowac_model[top_words]\nprint(top_words_vec.shape)\n</pre> top_words_vec = geowac_model[top_words] print(top_words_vec.shape) <pre>(500, 300)\n</pre> In\u00a0[25]: Copied! <pre>from sklearn.manifold import TSNE\nfrom sklearn.cluster import AgglomerativeClustering\nfrom bokeh.models import ColumnDataSource, LabelSet\nfrom bokeh.models import LinearColorMapper\nfrom bokeh.plotting import figure, show, output_file\nfrom bokeh.transform import transform\nfrom bokeh.io import output_notebook\n\ndef visualise_clusters(data):\n\n    '''\n\n    Vector Dimension Reduction\n\n    '''\n\n    # TSNE manifold learning \n    tsne = TSNE(n_components=2, \n                random_state=0)\n\n    # Agglomerative Clusterisation\n    htop_words_tsne = tsne.fit_transform(data)\n    htop_words_tsne[:10,:]\n\n    '''\n\n    Clusterise \n\n    '''\n\n\n    model = AgglomerativeClustering(n_clusters=10)\n    clusters = model.fit_predict(htop_words_tsne)\n    clusters = list(clusters)\n    clusters = list(map(str, clusters))\n\n    # combine data\n    data = pd.DataFrame({'d1':htop_words_tsne[:,0],\n                         'd2':htop_words_tsne[:,1],\n                         'cluster':clusters,\n                         'names':top_words})\n\n    '''\n\n    Plot Cluster \n\n    '''\n\n\n    output_notebook()\n    p = figure(tools=\"pan,wheel_zoom,reset,save\",\n               toolbar_location=\"above\",\n               title=\"word2vec T-SNE for most common words\")\n\n    color = LinearColorMapper(palette = 'Viridis256')\n\n    p.scatter(x=\"d1\", y=\"d2\",\n              fill_color = transform('cluster', color),\n              line_color='black',\n              size=8 ,\n              source=data)\n\n    labels = LabelSet(x=\"d1\", y=\"d2\", text=\"names\", y_offset=6,\n                       text_font_size=\"8pt\", text_color=\"#555555\",\n                       source=ColumnDataSource(data), text_align='center')\n    p.add_layout(labels)\n    show(p)\n</pre> from sklearn.manifold import TSNE from sklearn.cluster import AgglomerativeClustering from bokeh.models import ColumnDataSource, LabelSet from bokeh.models import LinearColorMapper from bokeh.plotting import figure, show, output_file from bokeh.transform import transform from bokeh.io import output_notebook  def visualise_clusters(data):      '''      Vector Dimension Reduction      '''      # TSNE manifold learning      tsne = TSNE(n_components=2,                  random_state=0)      # Agglomerative Clusterisation     htop_words_tsne = tsne.fit_transform(data)     htop_words_tsne[:10,:]      '''      Clusterise       '''       model = AgglomerativeClustering(n_clusters=10)     clusters = model.fit_predict(htop_words_tsne)     clusters = list(clusters)     clusters = list(map(str, clusters))      # combine data     data = pd.DataFrame({'d1':htop_words_tsne[:,0],                          'd2':htop_words_tsne[:,1],                          'cluster':clusters,                          'names':top_words})      '''      Plot Cluster       '''       output_notebook()     p = figure(tools=\"pan,wheel_zoom,reset,save\",                toolbar_location=\"above\",                title=\"word2vec T-SNE for most common words\")      color = LinearColorMapper(palette = 'Viridis256')      p.scatter(x=\"d1\", y=\"d2\",               fill_color = transform('cluster', color),               line_color='black',               size=8 ,               source=data)      labels = LabelSet(x=\"d1\", y=\"d2\", text=\"names\", y_offset=6,                        text_font_size=\"8pt\", text_color=\"#555555\",                        source=ColumnDataSource(data), text_align='center')     p.add_layout(labels)     show(p) In\u00a0[26]: Copied! <pre>visualise_clusters(top_words_vec)\n</pre> visualise_clusters(top_words_vec) Loading BokehJS ..."},{"location":"portfolio/course_nlp/text_embeddings.html","title":"\u0412\u0435\u043a\u0442\u043e\u0440\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043b\u043e\u0432 \u0438 \u0440\u0430\u0431\u043e\u0442\u0430 \u0441 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u043c\u0438 \u044d\u043c\u0431\u0435\u0434\u0438\u043d\u0433\u0430\u043c\u0438\u00b6","text":""},{"location":"portfolio/course_nlp/text_embeddings.html","title":"\u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/course_nlp/text_embeddings.html","title":"\u0420\u0430\u0431\u043e\u0442\u0430 \u0441 \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u044b\u043c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435\u043c \u0441\u043b\u043e\u0432\u00b6","text":"<ul> <li>\u0412 \u043f\u0440\u043e\u0448\u043b\u044b\u0439 \u0440\u0430\u0437 \u043c\u044b \u0437\u0430\u0442\u0440\u043e\u043d\u0443\u043b\u0438 \u0442\u0435\u043c\u0443 \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0445 \u0441\u043b\u043e\u0432 \u0432 \u0434\u043e\u043a\u0443\u043c\u0435\u0442\u043d\u0435 (<code>BoW</code>,<code>TF-IDF</code>)</li> <li>\u0421\u0435\u0433\u043e\u0434\u043d\u044f \u043c\u044b \u043f\u043e\u0434\u0430\u0431\u043e\u0442\u0430\u0435\u043c \u0441 \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u044b\u043c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435\u043c \u0441\u043b\u043e\u0432 <code>word2vec</code> \u0438 <code>fasttext</code> (\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443 <code>gensim</code>)</li> <li>\u0421 <code>BoW</code>,<code>TF-IDF</code> \u043d\u0430\u043c \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043b\u043e\u0441\u044c \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0438\u0432\u0430\u0442\u044c \u043f\u0440\u043e\u0441\u0442\u0430\u043d\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0442\u0430\u043a \u043a\u0430\u043a \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432 \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u0434\u043b\u0438\u043d\u044b \u0441\u043b\u043e\u0432\u0430\u0440\u044f, \u0447\u0442\u043e \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u0442 \u043a \u043e\u0447\u0435\u043d\u044c \u0431\u043e\u043b\u044c\u0448\u0438\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0430\u043c</li> <li>\u0412 <code>word2vec</code> \u0438 <code>fasttext</code> \u043c\u044b \u0441\u043e\u043f\u0430\u0441\u0442\u043e\u0432\u043b\u044f\u0435\u043c \u0441\u043b\u043e\u0432\u0443 \u0438\u043b\u0438 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044e \u043d\u0435\u043a\u043e\u0435 \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043b\u044e\u0431\u043e\u0433\u043e \u043f\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0430</li> </ul>"},{"location":"portfolio/course_nlp/text_embeddings.html","title":"\u0412\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u0441\u043b\u043e\u0432 \u0438\u0437 \u0422\u0432\u0438\u0442\u0442\u0435\u0440\u0430\u00b6","text":"<ul> <li>\u0412 \u0434\u0430\u043d\u043d\u043e\u043c \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435 \u043c\u044b \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438\u0437 \u0442\u0432\u0438\u0442\u0442\u0435\u0440\u0430, \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u043d\u044b\u0435 \u0432 <code>csv</code></li> </ul>"},{"location":"portfolio/course_nlp/text_embeddings.html","title":"\u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438\u00b6","text":""},{"location":"portfolio/course_nlp/text_embeddings.html","title":"\u0414\u0430\u043d\u043d\u044b\u0435\u00b6","text":""},{"location":"portfolio/course_nlp/text_embeddings.html","title":"\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\u00b6","text":"<p>\u041f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u044b\u0435 \u0438 \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u0442\u0432\u0438\u0442\u044b \u043d\u0430\u0445\u043e\u0434\u044f\u0442\u0441\u044f \u0432 \u0440\u0430\u0437\u043d\u044b\u0445 \u0444\u0430\u0439\u043b\u0430\u0445</p>"},{"location":"portfolio/course_nlp/text_embeddings.html","title":"\u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<p>\u0414\u0432\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u043e\u0447\u0438\u0441\u0442\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0437 \u0441\u043e\u0446\u0441\u0435\u0442\u0435\u0439</p> <ul> <li><code>words_only</code> \u041e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u0441\u043b\u043e\u0432\u0430 \u0438 \u0441\u0438\u043c\u0432\u043e\u043b\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u043f\u043e\u043b\u0435\u0437\u043d\u044b \u0434\u043b\u044f \u0441\u043c\u0430\u0439\u043b\u0438\u043a\u043e\u0432</li> <li><code>lemmatize</code> \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0442\u0435\u043a\u0441\u0442 (\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f pymystem3)</li> </ul>"},{"location":"portfolio/course_nlp/text_embeddings.html#word2vec","title":"\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c Word2Vec\u00b6","text":"<p>\u042d\u043c\u0431\u0435\u0434\u0438\u043d\u0433\u043e\u0432\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043e\u0431\u0443\u0447\u0438\u0432 \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u043d\u0430\u0448\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0437 \u0442\u0432\u0438\u0442\u0435\u0440\u0430</p> <ul> <li>\u041d\u0430 \u0432\u0445\u043e\u0434\u0435 \u043f\u043e\u0434\u0430\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0442\u043e\u043a\u0435\u043d\u043e\u0432 \u0432\u0441\u0435\u0445 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u043a\u043e\u0440\u043f\u0443\u0441\u0435 <code>text</code></li> <li>\u041a\u043e\u043d\u0442\u0435\u043a\u0441\u0442\u043d\u043e\u0435 \u043e\u043a\u043d\u043e \u0431\u0443\u0434\u0435\u0442 +/- 5 \u0441\u043b\u043e\u0432</li> <li>\u041c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0447\u0430\u0441\u0442\u043e\u0442\u0430 \u0441\u043b\u043e\u0432 5 (\u0432\u0441\u0435 \u0447\u0442\u043e \u043c\u0435\u043d\u044c\u0448\u0435 \u0438\u0433\u043d\u043e\u0440\u0438\u0440\u0443\u0435\u0442\u0441\u044f)</li> </ul>"},{"location":"portfolio/course_nlp/text_embeddings.html","title":"\u041e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 \u0441 \u0432\u0435\u043a\u0442\u043e\u0440\u0430\u043c\u0438\u00b6","text":"<p>\u041f\u043e\u043b\u0435\u0437\u043d\u044b\u0435 <code>word2vec</code> \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438:</p> <ul> <li>\u0418\u0437 \u043c\u043e\u0434\u0435\u043b\u0438 \u043c\u043e\u0436\u043d\u043e \u0432\u044b\u0433\u0440\u0443\u0437\u0438\u0442\u044c \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043b\u043e\u0432 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e <code>wv</code></li> <li>\u041c\u044b \u043c\u043e\u0436\u0435\u043c \u043d\u0430\u0439\u0442\u0438 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0431\u043b\u0438\u0437\u043a\u0438\u0435 \u043a \u0441\u043b\u043e\u0432\u0443 \u0441\u043b\u043e\u0432\u0430 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f <code>wv.most_similar</code></li> <li>\u0421 \u044d\u0442\u0438\u043c\u0438 \u0432\u0435\u043a\u0442\u043e\u0440\u0430\u043c\u0438 \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0434\u0435\u043b\u0430\u0442\u044c \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u0438 \u0441\u0443\u043c\u043c\u0438\u043e\u0432\u0430\u043d\u0438\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f <code>wv.similar_by_vector</code></li> <li>\u041c\u043e\u0436\u0435\u043c \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c \u0441\u043b\u043e\u0432\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u0432\u043b\u0438\u0432\u0430\u044e\u0442\u0441\u044f \u0432 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f <code>wv.doesnt_match</code></li> </ul>"},{"location":"portfolio/course_nlp/text_embeddings.html","title":"\u041f\u043e\u043d\u0438\u0436\u0435\u043d\u0438\u0435 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432\u00b6","text":"<p>\u0414\u043b\u044f \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u043c\u043d\u043e\u0433\u043e\u043c\u0435\u0440\u043d\u043e\u0433\u043e \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u0432 \u0434\u0432\u0443\u0445\u043c\u0435\u0440\u043d\u043e\u043c \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0435</p> <ul> <li>\u041d\u0430\u0439\u0434\u0435\u043c \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u043e \u0432\u0441\u0440\u0435\u0447\u0430\u0435\u043c\u044b\u0435 \u0441\u043b\u043e\u0432\u0430 \u0432 \u043a\u043e\u0440\u043f\u0443\u0441\u0435, \u0431\u0443\u0434\u0435\u043c \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0430\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u0447\u0430\u0441\u0442\u044c \u0441\u043b\u043e\u0432 (\u0442\u043e\u043f 500)</li> <li>\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0435 \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u043e \u0441\u043b\u043e\u0432 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u043e\u0434\u0430 \u043f\u043e\u043d\u0438\u0436\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432 <code>TSNE</code> (manifold learning)</li> </ul>"},{"location":"portfolio/course_nlp/text_embeddings.html","title":"\u041a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u0438 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0441\u043b\u043e\u0432\u00b6","text":"<ul> <li>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u043c\u0435\u0442\u043e\u0434\u043e\u043c <code>AgglomerativeClustering</code> \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 <code>sklearn</code> \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u0441\u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u043e\u0445\u043e\u0436\u0438\u0435 \u0441\u043b\u043e\u0432\u0430 \u0432 \u0433\u0440\u0443\u043f\u043f\u044b, \u0432 \u044d\u0442\u043e\u043c \u043f\u043e\u0434\u0445\u043e\u0434\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u043c\u044b \u0441\u0430\u043c\u0438 \u0434\u043e\u043b\u0436\u043d\u044b \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c \u043d\u0430 \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u043c\u044b \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435</li> <li>\u041f\u043e\u043b\u0443\u0447\u0438\u0432 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043d\u043e\u0435 \u043f\u0440\u0435\u043d\u043e\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u044c \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u043e, \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 <code>bokeh</code></li> </ul>"},{"location":"portfolio/course_nlp/text_embeddings.html","title":"\u041f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u044d\u043c\u0431\u0435\u0434\u0438\u043d\u0433\u0438\u00b6","text":""},{"location":"portfolio/course_nlp/text_embeddings.html","title":"\u0414\u043e\u043f\u043e\u0434\u043d\u0435\u043d\u0438\u0435\u00b6","text":"<ul> <li>(1) \u0417\u0430\u0439\u0434\u0438\u0442\u0435 \u043d\u0430 \u0441\u0430\u0439\u0442 RusVectores \u0438 \u0441\u043a\u0430\u0447\u0430\u0439\u0442\u0435 \u043e\u0434\u043d\u0443 \u0438\u0437 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 gensim</li> <li>(2) \u041f\u0440\u043e\u0432\u0435\u0434\u0438\u0442\u0435 \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u044b\u0435 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u044b \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0441\u043a\u0430\u0447\u0430\u043d\u043d\u044b\u0445 \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u044b\u0445 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0439</li> </ul> <p>\u0414\u043b\u044f \u043d\u0430\u0448\u0435\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 \u0432\u043e\u0437\u044c\u043c\u0435\u043c \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 (\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u0431\u0435\u0437 \u0442\u0430\u0433\u0441\u0435\u0442)</p> <ul> <li>geowac_lemmas_none_fasttextskipgram_300_5_2020</li> <li>\u0422\u0430\u0433\u0441\u0435\u0442 \u044d\u0442\u043e \u0434\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435 \u043a \u0441\u043b\u043e\u0432\u0443 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 NOUN_\u0434\u0435\u0440\u0435\u0432\u043e)</li> </ul>"},{"location":"portfolio/course_nlp/text_embeddings.html","title":"\u0423\u0442\u0438\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0412\u0435\u043a\u0442\u043e\u0440\u043e\u0432\u00b6","text":"<ul> <li>\u0414\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u044d\u043c\u0431\u0435\u0434\u0438\u043d\u0433\u0438, \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u043e\u043c \u0434\u0438\u0441\u043a\u0435</li> <li>\u0414\u0430\u043d\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u0431\u044b\u043b\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0430 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u043e\u0434\u0430 <code>fasttext</code> \u043d\u0430 \u0431\u043e\u043b\u044c\u0448\u043e\u043c \u0440\u0443\u0441\u0441\u043a\u043e\u044f\u0437\u044b\u0447\u043d\u044b\u043c \u043a\u043e\u0440\u043f\u0443\u0441\u0435</li> <li>\u0412\u0435\u043a\u0442\u043e\u0440\u0430 \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0432 \u0440\u0430\u0437\u043d\u044b\u0445 \u0446\u0435\u043b\u044f\u0445; \u0432 \u0434\u0430\u043d\u043d\u043e\u043e\u043c \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435 \u043c\u044b \u043f\u0440\u043e\u0441\u0442\u043e \u0438\u0445 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c</li> </ul>"},{"location":"portfolio/course_nlp/text_embeddings.html","title":"\u0420\u0430\u0437\u0430\u0440\u0445\u0438\u0432\u0438\u0440\u0443\u0435\u043c \u041c\u043e\u0434\u0435\u043b\u044c\u00b6","text":"<ul> <li>\u0420\u0430\u0437\u0430\u0440\u0445\u0438\u0432\u0438\u0440\u0443\u0435\u043c \u0441\u043a\u0430\u0447\u0435\u043d\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0438 \u0432 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 \u0444\u043e\u0440\u043c\u0430\u0442\u0430 \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043b\u0438\u0431\u043e <code>Word2Vec</code> \u043b\u0438\u0431\u043e <code>Fasttext</code></li> <li>\u0417\u0430\u0433\u0440\u0443\u0436\u0435\u043d\u043d\u043e\u0439 \u043d\u0430\u043c\u0438 \u043c\u043e\u0434\u0435\u043b\u044c \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c 300</li> <li>\u041f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0435\u043b\u044c\u0437\u044f \u0434\u043e\u043e\u0431\u0443\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432 \u0445\u0440\u0430\u043d\u044f\u0442\u0441\u044f \u0432 <code>KeyedVectors</code></li> </ul>"},{"location":"portfolio/course_nlp/text_embeddings.html","title":"\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435\u00b6","text":"<p>\u041a\u0430\u043a \u0438 \u0432 \u043f\u0440\u043e\u0448\u043b\u043e\u043c \u0440\u0430\u0437\u0434\u0435\u043b\u0435 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0441\u0430\u043c\u044b \u0447\u0430\u0441\u0442\u043e \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0448\u0438\u0445\u0441\u044f \u0441\u043b\u043e\u0432 \u0438 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0443\u0435\u043c \u0438\u0445 \u043d\u0430 \u0433\u0440\u0443\u043f\u043f\u044b</p>"},{"location":"portfolio/course_recsys/index.html","title":"Recsys","text":""},{"location":"portfolio/course_recsys/index.html#_1","title":"\u041f\u0435\u0440\u0441\u043e\u043d\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f","text":"<ul> <li> <p>\u0421\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u041f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439</p> <p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0437\u0430\u0434\u0430\u0447\u0443 \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439, \u0440\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 \u0447\u0430\u0441\u0442\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u044d\u0432\u0440\u0435\u0441\u0442\u0438\u043a\u0443, RFM, \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044e \u0438 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e. </p> </li> <li> <p>Look-a-like \u0421\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u041f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439</p> <p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0437\u0430\u0434\u0430\u0447\u0443 look-a-like \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u043c\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0431\u0438\u043d\u0430\u0440\u043d\u0443\u044e \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044e \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043f\u043e\u0434\u043e\u0431\u043d\u044b\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439. \u041d\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u043e\u0442\u0442\u043e\u043a\u0430 \u043a\u043b\u0438\u0435\u0442\u043d\u043e\u0432 \u0438\u0437 \u0431\u0430\u043d\u043a\u0430, \u0438\u043c\u0435\u044f \u0438\u0441\u0442\u043e\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043e \u043a\u043b\u0438\u0435\u043d\u0430\u0445 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0443\u0448\u043b\u0438 \u0438 \u0442\u0435 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u0443\u0448\u043b\u0438 \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0443\u0447\u0438\u0442\u0441\u044f \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u044c \u043f\u043e\u0445\u043e\u0436\u0438\u0445 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432</p> </li> <li> <p>Next Best Action \u041c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435</p> <p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0437\u0430\u0434\u0430\u0447\u0443 \u043f\u0435\u0440\u0441\u043e\u043d\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438  \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0438, Next Best Action \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0438\u0439 ML. \u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c NBA \u043a\u0430\u043a \u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u044e \u0431\u0438\u043d\u0430\u0440\u043d\u044b\u0439 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043e\u0442\u043a\u043b\u0438\u043a\u0430 \u0438 \u0441\u0440\u0430\u0432\u043d\u0438\u043c \u043b\u0443\u0447\u0448\u0435\u0435 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439 \u0441 \u043b\u0443\u0447\u0448\u0438\u043c \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435\u043c \u0441 \u0443\u0447\u0435\u0442\u043e\u043c NPV \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430. \u042d\u0442\u043e \u043d\u0443\u0436\u043d\u043e \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c \u0447\u0442\u043e \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0438\u0442\u044c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044e</p> </li> <li> <p>Uplift \u041c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f</p> <p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043c\u0435\u0442\u043e\u0434 (Uplift \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435) \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043e\u0446\u0435\u043d\u0438\u0442\u044c \u044d\u0444\u0444\u0435\u043a\u0442 \u043e\u0442 \u0432\u043e\u0437\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0439 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043d\u0430 \u0446\u0435\u043b\u0435\u0432\u0443\u044e \u0430\u0443\u0434\u0438\u0442\u043e\u0440\u0438\u044e. \u041e\u043d \u043f\u043e\u043c\u043e\u0433\u0430\u0435\u0442 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c, \u043a\u0430\u043a\u0438\u0435 \u043a\u043b\u0438\u0435\u043d\u0442\u044b \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0432\u044b\u0438\u0433\u0440\u0430\u044e\u0442 \u043e\u0442 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438, \u0430 \u043d\u0435 \u043f\u0440\u043e\u0441\u0442\u043e \u043e\u0442\u0440\u0435\u0430\u0433\u0438\u0440\u0443\u044e\u0442 \u043d\u0430 \u043d\u0435\u0433\u043e, \u0447\u0442\u043e \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0431\u043e\u043b\u0435\u0435 \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0442\u044c \u0440\u0435\u0441\u0443\u0440\u0441\u044b \u0438 \u043f\u043e\u0432\u044b\u0448\u0430\u0442\u044c \u0440\u0435\u043d\u0442\u0430\u0431\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0438\u043d\u0432\u0435\u0441\u0442\u0438\u0446\u0438\u0439</p> </li> </ul>"},{"location":"portfolio/course_recsys/index.html#_2","title":"\u041a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439","text":"<p><code>\u042d\u0432\u0440\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435</code>, <code>apriori</code> \u0438 \u043f\u043e\u0434\u0445\u043e\u0434\u044b <code>\u043c\u0430\u0442\u0440\u0438\u0447\u043d\u043e\u0439 \u0444\u0430\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u0438</code></p> <ul> <li>\u042d\u0432\u0440\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u041c\u043e\u0434\u0435\u043b\u0438,\u041a\u043e\u043b\u043b\u0430\u0431\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u0430\u044f \u0424\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u044f</li> <li>\u041c\u043e\u0434\u0435\u043b\u0438 \u041c\u0430\u0442\u0440\u0438\u0447\u043d\u043e\u0439 \u0424\u0430\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u0438</li> <li>\u041c\u043e\u0434\u0435\u043b\u0438 \u041c\u0430\u0442\u0440\u0438\u0447\u043d\u043e\u0439 \u0424\u0430\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430</li> <li>A/B - \u0442\u0435\u0441\u0442\u044b \u0432 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0441\u0438\u0441\u0442\u0435\u043c\u0430\u0445</li> </ul>"},{"location":"portfolio/course_recsys/index.html#_3","title":"\u041a\u043e\u043d\u0442\u0435\u043d\u0442\u043d\u044b\u0435 \u0438 \u0433\u0438\u0431\u0440\u0438\u0434\u043d\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439","text":"<p>\u041f\u043e\u0434\u0445\u043e\u0434\u044b \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 <code>LightFM</code> \u0438 \u043c\u043d\u043e\u0433\u043e \u044d\u0442\u0430\u043f\u043d\u044b\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044b \u0434\u043b\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u0437\u0430\u0434\u0430\u0447 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438  </p> <ul> <li>\u041a\u043e\u043d\u0442\u0435\u043d\u0442\u043d\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439</li> <li>\u0420\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u043f\u043e \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u043c\u0443 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u044e</li> <li>\u0420\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043a\u0430\u0442\u0430\u043b\u043e\u0433\u0430 \u0442\u043e\u0432\u0430\u0440\u043e\u0432 I</li> <li>\u0420\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043a\u0430\u0442\u0430\u043b\u043e\u0433\u0430 \u0442\u043e\u0432\u0430\u0440\u043e\u0432 II</li> <li>\u041f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0437\u0430\u043d\u044f\u0442\u0438\u0435 \u043f\u043e \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u043c \u0441\u0438\u0441\u0442\u0435\u043c\u0430\u043c. \u0414\u0432\u0443\u0445\u0443\u0440\u043e\u0432\u043d\u0435\u0432\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c</li> </ul>"},{"location":"portfolio/course_recsys/index.html#_4","title":"\u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439","text":"<p>\u041d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0435\u0432\u044b\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044b (<code>NeuMF</code>,<code>DSSM</code>), \u043c\u043d\u043e\u0433\u043e \u0440\u0443\u043a\u0438\u0435 \u0431\u0430\u043d\u0434\u0438\u0442\u044b</p> <ul> <li>\u041d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0435\u0432\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439 (DSSM) \u041f\u0440\u043e\u0441\u0442\u043e\u0439</li> <li>\u041d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0435\u0432\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439 (DSSM) \u041f\u0440\u043e\u0434\u0432\u0438\u043d\u0443\u0442\u044b\u0439</li> <li>\u041c\u043d\u043e\u0433\u043e\u0440\u0443\u043a\u0438\u0435 \u0431\u0430\u043d\u0434\u0438\u0442\u044b \u0434\u043b\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 A/B - \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f</li> <li>\u041c\u043d\u043e\u0433\u043e\u0440\u0443\u043a\u0438\u0435 \u0431\u0430\u043d\u0434\u0438\u0442\u044b \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u0447\u0438 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438</li> </ul>"},{"location":"portfolio/course_recsys/index.html#_5","title":"\u041f\u0440\u043e\u0435\u043a\u0442\u044b","text":"<ul> <li> <p>X-Learner Uplift</p> <p>\u0426\u0435\u043b\u044c: \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0441\u0430\u043c\u043e\u043f\u0438\u0441\u043d\u044b\u0439 \u043a\u043e\u0434 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 X-learner. </p> </li> <li> <p>\u041c\u043e\u0434\u0435\u043b\u0438 \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u043e\u0439 \u0444\u0430\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u0438</p> <p>\u0426\u0435\u043b\u044c: \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0438 \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f \u0438\u0445 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0442\u0440\u0438 \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0438; \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 (<code>hitrate</code>), \u0440\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f (<code>MRR</code>,<code>NDCG</code>) \u0438 \u0440\u0430\u0437\u043d\u043e\u043e\u0431\u0440\u0430\u0437\u0438\u044f (<code>coverage</code>)</p> </li> <li> <p>\u041a\u043e\u043d\u0442\u0435\u043d\u0442\u043d\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439</p> </li> <li> <p>\u0413\u0438\u0431\u0440\u0438\u0434\u043d\u044b\u0435 \u043f\u043e\u0434\u0445\u043e\u0434 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439</p> </li> <li> <p>\u041d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0435\u0432\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439 (NeuMF)</p> </li> </ul>"},{"location":"portfolio/course_recsys/2stagerecsys.html","title":"2stagerecsys","text":"In\u00a0[1]: Copied! <pre>!pip install implicit -qqq\n!pip install catboost -qqq\n</pre> !pip install implicit -qqq !pip install catboost -qqq <pre>   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8.9/8.9 MB 66.1 MB/s eta 0:00:00\r\n</pre> In\u00a0[2]: Copied! <pre>import datetime\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport scipy.sparse as sparse\nfrom catboost import CatBoostClassifier\nimport implicit\nfrom implicit.bpr import BayesianPersonalizedRanking as BPR\nimport warnings; warnings.filterwarnings('ignore')\n</pre> import datetime import numpy as np import pandas as pd from tqdm.auto import tqdm import matplotlib.pyplot as plt import seaborn as sns from sklearn.utils import shuffle from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score import scipy.sparse as sparse from catboost import CatBoostClassifier import implicit from implicit.bpr import BayesianPersonalizedRanking as BPR import warnings; warnings.filterwarnings('ignore') In\u00a0[3]: Copied! <pre>def recall(df: pd.DataFrame, pred_col='preds', true_col='item_id', k=30) -&gt; float:\n    recall_values = []\n    for _, row in df.iterrows():\n      num_relevant = len(set(row[true_col]) &amp; set(row[pred_col][:k]))\n      num_true = len(row[true_col])\n      recall_values.append(num_relevant / num_true)\n    return np.mean(recall_values)\n\ndef precision(df: pd.DataFrame, pred_col='preds', true_col='item_id', k=30) -&gt; float:\n    precision_values = []\n    for _, row in df.iterrows():\n      num_relevant = len(set(row[true_col]) &amp; set(row[pred_col][:k]))\n      num_true = min(k, len(row[true_col]))\n      precision_values.append(num_relevant / num_true)\n    return np.mean(precision_values)\n\ndef mrr(df: pd.DataFrame, pred_col='preds', true_col='item_id', k=30) -&gt; float:\n    mrr_values = []\n    for _, row in df.iterrows():\n      intersection = set(row[true_col]) &amp; set(row[pred_col][:k])\n      user_mrr = 0\n      if len(intersection) &gt; 0:\n          for item in intersection:\n              user_mrr = max(user_mrr, 1 / (row[pred_col].index(item) + 1))\n      mrr_values.append(user_mrr)\n    return np.mean(mrr_values)\n</pre> def recall(df: pd.DataFrame, pred_col='preds', true_col='item_id', k=30) -&gt; float:     recall_values = []     for _, row in df.iterrows():       num_relevant = len(set(row[true_col]) &amp; set(row[pred_col][:k]))       num_true = len(row[true_col])       recall_values.append(num_relevant / num_true)     return np.mean(recall_values)  def precision(df: pd.DataFrame, pred_col='preds', true_col='item_id', k=30) -&gt; float:     precision_values = []     for _, row in df.iterrows():       num_relevant = len(set(row[true_col]) &amp; set(row[pred_col][:k]))       num_true = min(k, len(row[true_col]))       precision_values.append(num_relevant / num_true)     return np.mean(precision_values)  def mrr(df: pd.DataFrame, pred_col='preds', true_col='item_id', k=30) -&gt; float:     mrr_values = []     for _, row in df.iterrows():       intersection = set(row[true_col]) &amp; set(row[pred_col][:k])       user_mrr = 0       if len(intersection) &gt; 0:           for item in intersection:               user_mrr = max(user_mrr, 1 / (row[pred_col].index(item) + 1))       mrr_values.append(user_mrr)     return np.mean(mrr_values) In\u00a0[4]: Copied! <pre>import os; os.listdir('/kaggle/input/kion-dataset/')\n</pre> import os; os.listdir('/kaggle/input/kion-dataset/') Out[4]: <pre>['items.csv', 'users.csv', 'interactions.csv']</pre> In\u00a0[5]: Copied! <pre>interactions = pd.read_csv(\"/kaggle/input/kion-dataset/interactions.csv\")\nitems = pd.read_csv(\"/kaggle/input/kion-dataset/items.csv\")\nusers = pd.read_csv(\"/kaggle/input/kion-dataset/users.csv\")\n</pre> interactions = pd.read_csv(\"/kaggle/input/kion-dataset/interactions.csv\") items = pd.read_csv(\"/kaggle/input/kion-dataset/items.csv\") users = pd.read_csv(\"/kaggle/input/kion-dataset/users.csv\") In\u00a0[6]: Copied! <pre># convert the column [last_watch_dt] into datetime\ninteractions['last_watch_dt'] = pd.to_datetime(interactions['last_watch_dt']).map(lambda x: x.date())\n\nprint(f\"\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u044e\u0437\u0435\u0440\u043e\u0432 \u0432 interactions: {interactions['user_id'].nunique()}\")\nprint(f\"\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0430\u0439\u0442\u0435\u043c\u043e\u0432 \u0432 interactions: {interactions['item_id'].nunique()}\")\n</pre> # convert the column [last_watch_dt] into datetime interactions['last_watch_dt'] = pd.to_datetime(interactions['last_watch_dt']).map(lambda x: x.date())  print(f\"\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u044e\u0437\u0435\u0440\u043e\u0432 \u0432 interactions: {interactions['user_id'].nunique()}\") print(f\"\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0430\u0439\u0442\u0435\u043c\u043e\u0432 \u0432 interactions: {interactions['item_id'].nunique()}\") <pre>\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u044e\u0437\u0435\u0440\u043e\u0432 \u0432 interactions: 962179\n\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0430\u0439\u0442\u0435\u043c\u043e\u0432 \u0432 interactions: 15706\n</pre> In\u00a0[7]: Copied! <pre>interactions.head()\n</pre> interactions.head() Out[7]: user_id item_id last_watch_dt total_dur watched_pct 0 176549 9506 2021-05-11 4250 72.0 1 699317 1659 2021-05-29 8317 100.0 2 656683 7107 2021-05-09 10 0.0 3 864613 7638 2021-07-05 14483 100.0 4 964868 9506 2021-04-30 6725 100.0 Item Information <p>Information about the movies/serials <code>item_id</code></p> <ul> <li><code>content_type</code> - Type of item</li> <li><code>title</code> - Title of item</li> <li><code>title_orig</code> - Original title name</li> <li><code>release_year</code> - Date of release</li> <li><code>countries</code> - Countries</li> <li><code>for_kids</code> - For kids</li> <li><code>age_rating</code>- Age rating</li> <li><code>studios</code> - film studio</li> <li><code>directors</code> - Directors</li> <li><code>actors</code>- Actors</li> <li><code>keywords</code> - Keywords</li> <li><code>description</code> - Description</li> </ul> In\u00a0[8]: Copied! <pre>users.head(2)\n</pre> users.head(2) Out[8]: user_id age income sex kids_flg 0 973171 age_25_34 income_60_90 \u041c 1 1 962099 age_18_24 income_20_40 \u041c 0 In\u00a0[9]: Copied! <pre>items.head(2)\n</pre> items.head(2) Out[9]: item_id content_type title title_orig release_year genres countries for_kids age_rating studios directors actors description keywords 0 10711 film \u041f\u043e\u0433\u043e\u0432\u043e\u0440\u0438 \u0441 \u043d\u0435\u0439 Hable con ella 2002.0 \u0434\u0440\u0430\u043c\u044b, \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435, \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0432\u044b, \u043c\u0435\u043b\u043e\u0434\u0440\u0430\u043c\u044b \u0418\u0441\u043f\u0430\u043d\u0438\u044f NaN 16.0 NaN \u041f\u0435\u0434\u0440\u043e \u0410\u043b\u044c\u043c\u043e\u0434\u043e\u0432\u0430\u0440 \u0410\u0434\u043e\u043b\u044c\u0444\u043e \u0424\u0435\u0440\u043d\u0430\u043d\u0434\u0435\u0441, \u0410\u043d\u0430 \u0424\u0435\u0440\u043d\u0430\u043d\u0434\u0435\u0441, \u0414\u0430\u0440\u0438\u043e \u0413\u0440\u0430\u043d\u0434\u0438... \u041c\u0435\u043b\u043e\u0434\u0440\u0430\u043c\u0430 \u043b\u0435\u0433\u0435\u043d\u0434\u0430\u0440\u043d\u043e\u0433\u043e \u041f\u0435\u0434\u0440\u043e \u0410\u043b\u044c\u043c\u043e\u0434\u043e\u0432\u0430\u0440\u0430 \u00ab\u041f\u043e\u0433\u043e... \u041f\u043e\u0433\u043e\u0432\u043e\u0440\u0438, \u043d\u0435\u0439, 2002, \u0418\u0441\u043f\u0430\u043d\u0438\u044f, \u0434\u0440\u0443\u0437\u044c\u044f, \u043b\u044e\u0431\u043e\u0432\u044c, ... 1 2508 film \u0413\u043e\u043b\u044b\u0435 \u043f\u0435\u0440\u0446\u044b Search Party 2014.0 \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435, \u043f\u0440\u0438\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f, \u043a\u043e\u043c\u0435\u0434\u0438\u0438 \u0421\u0428\u0410 NaN 16.0 NaN \u0421\u043a\u043e\u0442 \u0410\u0440\u043c\u0441\u0442\u0440\u043e\u043d\u0433 \u0410\u0434\u0430\u043c \u041f\u0430\u043b\u043b\u0438, \u0411\u0440\u0430\u0439\u0430\u043d \u0425\u0430\u0441\u043a\u0438, \u0414\u0436.\u0411. \u0421\u043c\u0443\u0432, \u0414\u0436\u0435\u0439\u0441\u043e\u043d ... \u0423\u043c\u043e\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u043a\u043e\u043c\u0435\u0434\u0438\u044f \u043d\u0430 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u0443\u044e... \u0413\u043e\u043b\u044b\u0435, \u043f\u0435\u0440\u0446\u044b, 2014, \u0421\u0428\u0410, \u0434\u0440\u0443\u0437\u044c\u044f, \u0441\u0432\u0430\u0434\u044c\u0431\u044b, \u043f\u0440\u0435\u043e... In\u00a0[10]: Copied! <pre>interactions = interactions[interactions['total_dur'] &gt;= 300]\n</pre> interactions = interactions[interactions['total_dur'] &gt;= 300] In\u00a0[11]: Copied! <pre>user_interactions_count = interactions.groupby('user_id')[['item_id']].count().reset_index()\nfiltered_users = user_interactions_count[user_interactions_count['item_id'] &gt;= 10][['user_id']]\ninteractions = filtered_users.merge(interactions, how='left')\n</pre> user_interactions_count = interactions.groupby('user_id')[['item_id']].count().reset_index() filtered_users = user_interactions_count[user_interactions_count['item_id'] &gt;= 10][['user_id']] interactions = filtered_users.merge(interactions, how='left') In\u00a0[12]: Copied! <pre>item_interactions_count = interactions.groupby('item_id')[['user_id']].count().reset_index()\nfiltered_items = item_interactions_count[item_interactions_count['user_id'] &gt;= 10][['item_id']]\ninteractions = filtered_items.merge(interactions, how='left')\n</pre> item_interactions_count = interactions.groupby('item_id')[['user_id']].count().reset_index() filtered_items = item_interactions_count[item_interactions_count['user_id'] &gt;= 10][['item_id']] interactions = filtered_items.merge(interactions, how='left') In\u00a0[13]: Copied! <pre>max_date = interactions['last_watch_dt'].max()\nmin_date = interactions['last_watch_dt'].min()\n\nprint(f\"min \u0434\u0430\u0442\u0430 \u0432 interactions: {min_date}\")\nprint(f\"max \u0434\u0430\u0442\u0430 \u0432 interactions: {max_date}\")\n</pre> max_date = interactions['last_watch_dt'].max() min_date = interactions['last_watch_dt'].min()  print(f\"min \u0434\u0430\u0442\u0430 \u0432 interactions: {min_date}\") print(f\"max \u0434\u0430\u0442\u0430 \u0432 interactions: {max_date}\") <pre>min \u0434\u0430\u0442\u0430 \u0432 interactions: 2021-03-13\nmax \u0434\u0430\u0442\u0430 \u0432 interactions: 2021-08-22\n</pre> <p>Split information:</p> <ul> <li><code>[test]</code> : Contains the last 7 days of interactions</li> <li><code>[train_val]</code> : train &amp; validation dataset</li> <li><code>[train_val]</code><ul> <li><code>[train]</code> up to last 60 days of interactions (to test start date)</li> <li><code>[val]</code> last 60 days of interactions (to test start date)</li> </ul> </li> </ul> In\u00a0[14]: Copied! <pre># global test dataset starting time (7 days)\ntest_threshold = max_date - pd.Timedelta(days=7)\n\n# validation dataset starting time (2 months)\nval_threshold = test_threshold - pd.Timedelta(days=60) \n\ntest = interactions[(interactions['last_watch_dt'] &gt;= test_threshold)]\ntrain_val = interactions[(interactions['last_watch_dt'] &lt; test_threshold)]\nval = train_val[(train_val['last_watch_dt'] &gt;= val_threshold)]\ntrain = train_val[(train_val['last_watch_dt'] &lt; val_threshold)]\n\nprint(f\"train: {train.shape}\")\nprint(f\"val: {val.shape}\")\nprint(f\"test: {test.shape}\")\n</pre> # global test dataset starting time (7 days) test_threshold = max_date - pd.Timedelta(days=7)  # validation dataset starting time (2 months) val_threshold = test_threshold - pd.Timedelta(days=60)   test = interactions[(interactions['last_watch_dt'] &gt;= test_threshold)] train_val = interactions[(interactions['last_watch_dt'] &lt; test_threshold)] val = train_val[(train_val['last_watch_dt'] &gt;= val_threshold)] train = train_val[(train_val['last_watch_dt'] &lt; val_threshold)]  print(f\"train: {train.shape}\") print(f\"val: {val.shape}\") print(f\"test: {test.shape}\") <pre>train: (881660, 5)\nval: (1246263, 5)\ntest: (172593, 5)\n</pre> In\u00a0[15]: Copied! <pre># train model on [train]\nusers_id = list(np.sort(train.user_id.unique()))\nitems_train = list(train.item_id.unique())\nratings_train = list(train.watched_pct)\n\nrows_train = train.user_id.astype('category').cat.codes\ncols_train = train.item_id.astype('category').cat.codes\n\n# create rating matrix (watched percentage [watched_pct])\ntrain_sparse = sparse.csr_matrix((ratings_train, (rows_train, cols_train)), \n                                 shape=(len(users_id), len(items_train)))\n\nalgo = BPR(factors=50, \n            regularization=0.01, \n            iterations=50, \n            use_gpu=False)\nalgo.fit((train_sparse).astype('double'))\n\n# Output [1] from first model; \n# user and item factorisation matrices\nuser_vecs = algo.user_factors\nitem_vecs = algo.item_factors\n\n\n# BPR implicit prediction \ndef predict(user_vecs, item_vecs, k=10):\n    \n    \"\"\"\n    \n    Helper function for matrix factorisation prediction\n    \n    \"\"\"\n    \n    id2user = dict(zip(rows_train, train.user_id))\n    id2item = dict(zip(cols_train, train.item_id))\n    scores = user_vecs.dot(item_vecs.T)\n\n    ind_part = np.argpartition(scores, -k + 1)[:, -k:].copy()\n    scores_not_sorted = np.take_along_axis(scores, ind_part, axis=1)\n    ind_sorted = np.argsort(scores_not_sorted, axis=1)\n    indices = np.take_along_axis(ind_part, ind_sorted, axis=1)\n    indices = np.flip(indices, 1)\n    preds = pd.DataFrame({\n        'user_id': range(user_vecs.shape[0]),\n        'preds': indices.tolist(),\n        })\n    preds['user_id'] = preds['user_id'].map(id2user)\n    preds['preds'] = preds['preds'].map(lambda inds: [id2item[i] for i in inds])\n    return preds\n\n\nk=30\n\n# films watched in [val] dataset\nval_user_history = val.groupby('user_id')[['item_id']].agg(lambda x: list(x))\n\n# films recommended from algo (on train)\npred_bpr = predict(user_vecs, item_vecs, k)\npred_bpr = val_user_history.merge(pred_bpr, how='left', on='user_id')\npred_bpr = pred_bpr.dropna(subset=['preds'])\npred_bpr.head()\n</pre> # train model on [train] users_id = list(np.sort(train.user_id.unique())) items_train = list(train.item_id.unique()) ratings_train = list(train.watched_pct)  rows_train = train.user_id.astype('category').cat.codes cols_train = train.item_id.astype('category').cat.codes  # create rating matrix (watched percentage [watched_pct]) train_sparse = sparse.csr_matrix((ratings_train, (rows_train, cols_train)),                                   shape=(len(users_id), len(items_train)))  algo = BPR(factors=50,              regularization=0.01,              iterations=50,              use_gpu=False) algo.fit((train_sparse).astype('double'))  # Output [1] from first model;  # user and item factorisation matrices user_vecs = algo.user_factors item_vecs = algo.item_factors   # BPR implicit prediction  def predict(user_vecs, item_vecs, k=10):          \"\"\"          Helper function for matrix factorisation prediction          \"\"\"          id2user = dict(zip(rows_train, train.user_id))     id2item = dict(zip(cols_train, train.item_id))     scores = user_vecs.dot(item_vecs.T)      ind_part = np.argpartition(scores, -k + 1)[:, -k:].copy()     scores_not_sorted = np.take_along_axis(scores, ind_part, axis=1)     ind_sorted = np.argsort(scores_not_sorted, axis=1)     indices = np.take_along_axis(ind_part, ind_sorted, axis=1)     indices = np.flip(indices, 1)     preds = pd.DataFrame({         'user_id': range(user_vecs.shape[0]),         'preds': indices.tolist(),         })     preds['user_id'] = preds['user_id'].map(id2user)     preds['preds'] = preds['preds'].map(lambda inds: [id2item[i] for i in inds])     return preds   k=30  # films watched in [val] dataset val_user_history = val.groupby('user_id')[['item_id']].agg(lambda x: list(x))  # films recommended from algo (on train) pred_bpr = predict(user_vecs, item_vecs, k) pred_bpr = val_user_history.merge(pred_bpr, how='left', on='user_id') pred_bpr = pred_bpr.dropna(subset=['preds']) pred_bpr.head() Out[15]: user_id item_id preds 0 2 [242, 3628, 5819, 7106, 7921, 8482, 9164, 1077... [3166, 9164, 12965, 12299, 11919, 8482, 4072, ... 2 21 [308, 3784, 4495, 5077, 6384, 7102, 7571, 8251... [849, 1053, 11237, 826, 4382, 11661, 24, 14703... 3 30 [1107, 2346, 2743, 3031, 7250, 9728, 9842, 112... [10464, 10440, 16447, 7946, 2100, 2303, 9728, ... 4 46 [10440] [142, 4880, 9996, 6809, 11640, 10440, 2498, 86... 6 60 [1179, 1343, 1590, 3550, 6044, 6606, 8612, 972... [4880, 13865, 4151, 1083, 7107, 1449, 7571, 11... <p>Check the metrics to understand how good the model recommends relevant item_id to users</p> In\u00a0[16]: Copied! <pre># metrics for trained bpr prediction &amp; [val] dataset overlap\nprint('recall',round(recall(pred_bpr),3))\nprint('precision',round(precision(pred_bpr),3))\nprint('mrr',round(mrr(pred_bpr),3))\n</pre>  # metrics for trained bpr prediction &amp; [val] dataset overlap print('recall',round(recall(pred_bpr),3)) print('precision',round(precision(pred_bpr),3)) print('mrr',round(mrr(pred_bpr),3)) <pre>recall 0.116\nprecision 0.117\nmrr 0.132\n</pre> <p>Prepare <code>CatBoost</code> model dataset:</p> <ul> <li>Prepare the dataset for the 2nd stage model, by exploding the <code>item_id</code> and adding the order ranking</li> <li>Each rating is partitions for each user, starting from a value of 1</li> </ul> In\u00a0[17]: Copied! <pre>candidates = pred_bpr[['user_id', 'preds']]\ncandidates = candidates.explode('preds').rename(columns={'preds': 'item_id'})\ncandidates['rank'] = candidates.groupby('user_id').cumcount() + 1\ncandidates.head()\n</pre>  candidates = pred_bpr[['user_id', 'preds']] candidates = candidates.explode('preds').rename(columns={'preds': 'item_id'}) candidates['rank'] = candidates.groupby('user_id').cumcount() + 1 candidates.head() Out[17]: user_id item_id rank 0 2 3166 1 0 2 9164 2 0 2 12965 3 0 2 12299 4 0 2 11919 5 In\u00a0[18]: Copied! <pre>pos = candidates.merge(val,\n                       on=['user_id', 'item_id'],\n                       how='inner')\npos['target'] = 1\nprint('number of positive samples',pos.shape)\npos.head()\n</pre> pos = candidates.merge(val,                        on=['user_id', 'item_id'],                        how='inner') pos['target'] = 1 print('number of positive samples',pos.shape) pos.head() <pre>number of positive samples (64588, 7)\n</pre> Out[18]: user_id item_id rank last_watch_dt total_dur watched_pct target 0 2 9164 2 2021-06-23 6650 100.0 1 1 2 8482 6 2021-06-18 5886 100.0 1 2 30 9728 7 2021-06-21 8436 100.0 1 3 46 10440 6 2021-07-05 7449 20.0 1 4 60 9728 22 2021-06-23 8066 100.0 1 <p>Prepare <code>negative</code> data samples</p> <ul> <li>There will be much more negative samples</li> <li>The validation dataset is added to the predictions using left join, so we will have some negative candidates (user/item) combinations that don't exist in the <code>val</code> dataset</li> <li>Well keep the negative to positive sample ration at roughly 2:1</li> </ul> In\u00a0[19]: Copied! <pre># \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u043e\u043d\u0438 \u043d\u0435 \u0441\u043c\u043e\u0442\u0440\u0435\u043b\u0438 \u0432 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\n# defaults to left join\nneg = candidates.set_index(['user_id', 'item_id'])\\\n        .join(val.set_index(['user_id', 'item_id']))\n\nneg = neg[neg['watched_pct'].isnull()].reset_index()\nprint(neg.shape)\nneg = neg.sample(frac=0.07)\nprint(neg.shape)\nneg['target'] = 0\nneg.head()\n</pre> # \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u043e\u043d\u0438 \u043d\u0435 \u0441\u043c\u043e\u0442\u0440\u0435\u043b\u0438 \u0432 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 # defaults to left join neg = candidates.set_index(['user_id', 'item_id'])\\         .join(val.set_index(['user_id', 'item_id']))  neg = neg[neg['watched_pct'].isnull()].reset_index() print(neg.shape) neg = neg.sample(frac=0.07) print(neg.shape) neg['target'] = 0 neg.head() <pre>(1814012, 6)\n(126981, 6)\n</pre> Out[19]: user_id item_id rank last_watch_dt total_dur watched_pct target 130488 80687 6588 10 NaN NaN NaN 0 1450066 875976 16356 21 NaN NaN NaN 0 1135561 685640 9157 26 NaN NaN NaN 0 736980 445205 10770 14 NaN NaN NaN 0 1766269 1069796 6006 13 NaN NaN NaN 0 <p>Split the <code>val</code> users</p> <ul> <li>Split the <code>val</code> unique users into train (<code>ctb_train_users</code>), validation (<code>ctb_eval_users</code>) and test (<code>ctb_test_users</code>) subsets, used only for creating subsets for <code>catboost</code> 2nd stage model</li> </ul> In\u00a0[20]: Copied! <pre># divide the users into 3 subgroups\n\nctb_train_users, ctb_test_users = train_test_split(val['user_id'].unique(),\n                                                  random_state=1,\n                                                  test_size=0.2)\n\nctb_train_users, ctb_eval_users = train_test_split(ctb_train_users,\n                                                  random_state=1,\n                                                  test_size=0.1)\n\nprint('number of users in ctb train',ctb_train_users)\nprint('number of users in ctb eval',ctb_eval_users)\nprint('number of users in ctb test',ctb_test_users)\n</pre> # divide the users into 3 subgroups  ctb_train_users, ctb_test_users = train_test_split(val['user_id'].unique(),                                                   random_state=1,                                                   test_size=0.2)  ctb_train_users, ctb_eval_users = train_test_split(ctb_train_users,                                                   random_state=1,                                                   test_size=0.1)  print('number of users in ctb train',ctb_train_users) print('number of users in ctb eval',ctb_eval_users) print('number of users in ctb test',ctb_test_users) <pre>number of users in ctb train [790260 678092  74663 ... 239212 914265 167935]\nnumber of users in ctb eval [633541 825546 531429 ... 102166 686044  55175]\nnumber of users in ctb test [1069931  871670  834200 ...  178739  463296 1091209]\n</pre> <p>Define the subset groups for the user groups we just defined</p> <ul> <li><code>cbt_train</code> : used for training</li> <li><code>cbt_eval</code> : used for evaluation during training</li> <li><code>cbt_test</code> : used for test evaluation on unseen data</li> </ul> In\u00a0[21]: Copied! <pre>select_col = ['user_id', 'item_id', 'rank', 'target']\n\n# Training dataset\nctb_train = shuffle(\n    pd.concat([\n        pos[pos['user_id'].isin(ctb_train_users)],\n        neg[neg['user_id'].isin(ctb_train_users)]\n])[select_col]\n)\ndisplay(ctb_train.head())\n\n\n# Test subset (used to check metrics on unseen users)\nctb_test = shuffle(\n    pd.concat([\n        pos[pos['user_id'].isin(ctb_test_users)],\n        neg[neg['user_id'].isin(ctb_test_users)]\n])[select_col]\n)\n\n# Evaluation train subset\nctb_eval = shuffle(\n    pd.concat([\n        pos[pos['user_id'].isin(ctb_eval_users)],\n        neg[neg['user_id'].isin(ctb_eval_users)]\n])[select_col]\n)\n</pre> select_col = ['user_id', 'item_id', 'rank', 'target']  # Training dataset ctb_train = shuffle(     pd.concat([         pos[pos['user_id'].isin(ctb_train_users)],         neg[neg['user_id'].isin(ctb_train_users)] ])[select_col] ) display(ctb_train.head())   # Test subset (used to check metrics on unseen users) ctb_test = shuffle(     pd.concat([         pos[pos['user_id'].isin(ctb_test_users)],         neg[neg['user_id'].isin(ctb_test_users)] ])[select_col] )  # Evaluation train subset ctb_eval = shuffle(     pd.concat([         pos[pos['user_id'].isin(ctb_eval_users)],         neg[neg['user_id'].isin(ctb_eval_users)] ])[select_col] ) user_id item_id rank target 1755444 1063468 2498 1 0 13617 232693 10440 29 1 682599 412208 8101 15 0 1008013 609311 11275 12 0 1588295 960986 15695 24 0 <p>Add <code>user_id</code> and <code>item_id</code> features to the train and evaluation subsets</p> In\u00a0[22]: Copied! <pre>user_col = ['user_id', 'age', 'income', 'sex', 'kids_flg']\nitem_col = ['item_id', 'content_type', 'countries', 'for_kids', 'age_rating', 'studios']\n\n# train   \ntrain_feat = (ctb_train\n              .merge(users[user_col], on=['user_id'], how='left')\n              .merge(items[item_col], on=['item_id'], how='left'))\n\n# evaluation dataset with train for early stopping\neval_feat = (ctb_eval\n             .merge(users[user_col], on=['user_id'], how='left')\n             .merge(items[item_col], on=['item_id'], how='left'))\n\ntrain_feat.head()\n</pre> user_col = ['user_id', 'age', 'income', 'sex', 'kids_flg'] item_col = ['item_id', 'content_type', 'countries', 'for_kids', 'age_rating', 'studios']  # train    train_feat = (ctb_train               .merge(users[user_col], on=['user_id'], how='left')               .merge(items[item_col], on=['item_id'], how='left'))  # evaluation dataset with train for early stopping eval_feat = (ctb_eval              .merge(users[user_col], on=['user_id'], how='left')              .merge(items[item_col], on=['item_id'], how='left'))  train_feat.head()  Out[22]: user_id item_id rank target age income sex kids_flg content_type countries for_kids age_rating studios 0 1063468 2498 1 0 age_45_54 income_20_40 \u041c 0.0 film \u0420\u043e\u0441\u0441\u0438\u044f, \u0410\u0440\u043c\u0435\u043d\u0438\u044f NaN 16.0 NaN 1 232693 10440 29 1 age_25_34 income_40_60 \u0416 0.0 series \u0420\u043e\u0441\u0441\u0438\u044f NaN 18.0 NaN 2 412208 8101 15 0 age_25_34 income_20_40 \u0416 0.0 series \u0420\u043e\u0441\u0441\u0438\u044f NaN 0.0 NaN 3 609311 11275 12 0 age_35_44 income_20_40 \u041c 1.0 film \u0420\u043e\u0441\u0441\u0438\u044f NaN 16.0 NaN 4 960986 15695 24 0 age_35_44 income_60_90 \u0416 1.0 series \u0421\u0428\u0410 NaN 12.0 NaN <p>Split the <code>target</code> from the dataset and set the features for training and evaluation subsets</p> In\u00a0[23]: Copied! <pre>'''\n\nDefine column information for model \n\n- drop columns [user_id], [item_id]\n- target column: [target]\n- categorical columns: [age] [income] [sex] [content_type] [countries] [studios]\n\n'''\n\n# drop pointless columns and separate target\ndrop_col = ['user_id', 'item_id']\ntarget_col = ['target']\n\n# we will define the categorical columns in catboost\ncat_col = ['age', 'income', 'sex', 'content_type', 'countries', 'studios']\n\nX_train, y_train = train_feat.drop(drop_col + target_col, axis=1), train_feat[target_col]\nX_val, y_val = eval_feat.drop(drop_col + target_col, axis=1), eval_feat[target_col]\nX_train.shape, y_train.shape, X_val.shape, y_val.shape\nX_train.head()\n</pre> '''  Define column information for model   - drop columns [user_id], [item_id] - target column: [target] - categorical columns: [age] [income] [sex] [content_type] [countries] [studios]  '''  # drop pointless columns and separate target drop_col = ['user_id', 'item_id'] target_col = ['target']  # we will define the categorical columns in catboost cat_col = ['age', 'income', 'sex', 'content_type', 'countries', 'studios']  X_train, y_train = train_feat.drop(drop_col + target_col, axis=1), train_feat[target_col] X_val, y_val = eval_feat.drop(drop_col + target_col, axis=1), eval_feat[target_col] X_train.shape, y_train.shape, X_val.shape, y_val.shape X_train.head() Out[23]: rank age income sex kids_flg content_type countries for_kids age_rating studios 0 1 age_45_54 income_20_40 \u041c 0.0 film \u0420\u043e\u0441\u0441\u0438\u044f, \u0410\u0440\u043c\u0435\u043d\u0438\u044f NaN 16.0 NaN 1 29 age_25_34 income_40_60 \u0416 0.0 series \u0420\u043e\u0441\u0441\u0438\u044f NaN 18.0 NaN 2 15 age_25_34 income_20_40 \u0416 0.0 series \u0420\u043e\u0441\u0441\u0438\u044f NaN 0.0 NaN 3 12 age_35_44 income_20_40 \u041c 1.0 film \u0420\u043e\u0441\u0441\u0438\u044f NaN 16.0 NaN 4 24 age_35_44 income_60_90 \u0416 1.0 series \u0421\u0428\u0410 NaN 12.0 NaN In\u00a0[24]: Copied! <pre>X_train.isna().sum()\n</pre> X_train.isna().sum() Out[24]: <pre>rank                 0\nage              25220\nincome           25080\nsex              25294\nkids_flg         24161\ncontent_type         0\ncountries            0\nfor_kids        134693\nage_rating           0\nstudios         137203\ndtype: int64</pre> <p>Process missing values with the most frequent result</p> In\u00a0[25]: Copied! <pre># fillna for catboost with the most frequent value\nX_train = X_train.fillna(X_train.mode().iloc[0])\n\n# fillna for catboost with the most frequent value\nX_val = X_val.fillna(X_train.mode().iloc[0])\n</pre> # fillna for catboost with the most frequent value X_train = X_train.fillna(X_train.mode().iloc[0])  # fillna for catboost with the most frequent value X_val = X_val.fillna(X_train.mode().iloc[0]) In\u00a0[26]: Copied! <pre>'''\n\nDefine Hyperparameters for Classifier\n\n'''\n\n# model hyperparameters\nest_params = {\n  'subsample': 0.9,\n  'max_depth': 5,\n  'n_estimators': 2000,\n  'learning_rate': 0.01,\n  'thread_count': 20,\n  'random_state': 42,\n  'verbose': 200,\n}\n\nctb_model = CatBoostClassifier(**est_params)\n\nimport warnings; warnings.filterwarnings('ignore')\nctb_model.fit(X_train,\n              y_train,\n              eval_set=(X_val, y_val),\n              early_stopping_rounds=100,\n              cat_features=cat_col)\n</pre> '''  Define Hyperparameters for Classifier  '''  # model hyperparameters est_params = {   'subsample': 0.9,   'max_depth': 5,   'n_estimators': 2000,   'learning_rate': 0.01,   'thread_count': 20,   'random_state': 42,   'verbose': 200, }  ctb_model = CatBoostClassifier(**est_params)  import warnings; warnings.filterwarnings('ignore') ctb_model.fit(X_train,               y_train,               eval_set=(X_val, y_val),               early_stopping_rounds=100,               cat_features=cat_col) <pre>0:\tlearn: 0.6902088\ttest: 0.6903005\tbest: 0.6903005 (0)\ttotal: 150ms\tremaining: 4m 59s\n200:\tlearn: 0.5318929\ttest: 0.5413494\tbest: 0.5413494 (200)\ttotal: 13.4s\tremaining: 1m 59s\n400:\tlearn: 0.5237664\ttest: 0.5347690\tbest: 0.5347690 (400)\ttotal: 28.2s\tremaining: 1m 52s\n600:\tlearn: 0.5206398\ttest: 0.5321545\tbest: 0.5321545 (600)\ttotal: 42.8s\tremaining: 1m 39s\n800:\tlearn: 0.5188275\ttest: 0.5307083\tbest: 0.5307083 (800)\ttotal: 57.8s\tremaining: 1m 26s\n1000:\tlearn: 0.5178101\ttest: 0.5299766\tbest: 0.5299766 (1000)\ttotal: 1m 11s\tremaining: 1m 11s\n1200:\tlearn: 0.5167082\ttest: 0.5291069\tbest: 0.5291069 (1200)\ttotal: 1m 26s\tremaining: 57.6s\n1400:\tlearn: 0.5157984\ttest: 0.5284334\tbest: 0.5284334 (1400)\ttotal: 1m 40s\tremaining: 43.1s\n1600:\tlearn: 0.5151791\ttest: 0.5280492\tbest: 0.5280479 (1598)\ttotal: 1m 54s\tremaining: 28.6s\n1800:\tlearn: 0.5147084\ttest: 0.5277376\tbest: 0.5277376 (1800)\ttotal: 2m 8s\tremaining: 14.2s\n1999:\tlearn: 0.5141424\ttest: 0.5273592\tbest: 0.5273592 (1999)\ttotal: 2m 22s\tremaining: 0us\n\nbestTest = 0.5273591755\nbestIteration = 1999\n\n</pre> Out[26]: <pre>&lt;catboost.core.CatBoostClassifier at 0x78e1015b6980&gt;</pre> <p>Evaluation of <code>metrics:</code></p> <ul> <li>Test model on training set and evaluate the <code>ROCAUC</code> metric</li> </ul> In\u00a0[27]: Copied! <pre># prediction on train subset of users\ny_pred = ctb_model.predict_proba(X_train)\nf\"ROC AUC score = {roc_auc_score(y_train, y_pred[:, 1]):.2f}\"\n</pre> # prediction on train subset of users y_pred = ctb_model.predict_proba(X_train) f\"ROC AUC score = {roc_auc_score(y_train, y_pred[:, 1]):.2f}\" Out[27]: <pre>'ROC AUC score = 0.78'</pre> <ul> <li>Test model on unseen test data and evaluate the <code>ROCAUC</code> metric</li> </ul> In\u00a0[28]: Copied! <pre>'''\n\nPrepare Test Set\n\n'''\n\ntest_feat = (ctb_test\n             .merge(users[user_col], on=['user_id'], how='left')\n             .merge(items[item_col], on=['item_id'], how='left'))\n\n# fillna for catboost with the most frequent value\ntest_feat = test_feat.fillna(X_train.mode().iloc[0])\n\nX_test, y_test = test_feat.drop(drop_col + target_col, axis=1), test_feat['target']\n\n'''\n\nMake prediction on test set\n\n'''\n\ny_pred = ctb_model.predict_proba(X_test)\nf\"ROC AUC score = {roc_auc_score(y_test, y_pred[:, 1]):.2f}\"\n</pre> '''  Prepare Test Set  '''  test_feat = (ctb_test              .merge(users[user_col], on=['user_id'], how='left')              .merge(items[item_col], on=['item_id'], how='left'))  # fillna for catboost with the most frequent value test_feat = test_feat.fillna(X_train.mode().iloc[0])  X_test, y_test = test_feat.drop(drop_col + target_col, axis=1), test_feat['target']  '''  Make prediction on test set  '''  y_pred = ctb_model.predict_proba(X_test) f\"ROC AUC score = {roc_auc_score(y_test, y_pred[:, 1]):.2f}\" Out[28]: <pre>'ROC AUC score = 0.77'</pre> In\u00a0[29]: Copied! <pre># group [item_id] for each [user_id] in test (main test)\ntest = test[test['user_id'].isin(val['user_id'].unique())] # test user_id must contain val user_id\ntest_user_history = test.groupby('user_id')[['item_id']].agg(lambda x: list(x))\ndisplay(test_user_history.head())\n</pre> # group [item_id] for each [user_id] in test (main test) test = test[test['user_id'].isin(val['user_id'].unique())] # test user_id must contain val user_id test_user_history = test.groupby('user_id')[['item_id']].agg(lambda x: list(x)) display(test_user_history.head()) item_id user_id 3 [47, 965, 2025, 2722, 9438, 10240] 21 [13787, 14488] 30 [4181, 8584, 8636] 53 [1445, 15629, 15810, 16426] 98 [89, 512] <p>Now, define 100 candidates from the 1st stage model</p> <ul> <li>Also lets evaluate the overlapping metrics so we know how well the first stage model alone performs</li> </ul> In\u00a0[30]: Copied! <pre># first model prediction for k=100\npred_bpr = predict(user_vecs, item_vecs, k=100)\npred_bpr = test_user_history.merge(pred_bpr, how='left', on='user_id')\npred_bpr = pred_bpr.dropna(subset=['preds'])\ndisplay(pred_bpr.head()) # overlap b/w [train] (user_vect/item_vecs) prediction and test set (item_id)\n\n# determine overlapping metrics b/w test and train\nprint('recall',round(recall(pred_bpr, k=20),3))\nprint('precision',round(precision(pred_bpr, k=20),3))\nprint('mrr',round(mrr(pred_bpr, k=20),3))\n</pre> # first model prediction for k=100 pred_bpr = predict(user_vecs, item_vecs, k=100) pred_bpr = test_user_history.merge(pred_bpr, how='left', on='user_id') pred_bpr = pred_bpr.dropna(subset=['preds']) display(pred_bpr.head()) # overlap b/w [train] (user_vect/item_vecs) prediction and test set (item_id)  # determine overlapping metrics b/w test and train print('recall',round(recall(pred_bpr, k=20),3)) print('precision',round(precision(pred_bpr, k=20),3)) print('mrr',round(mrr(pred_bpr, k=20),3)) user_id item_id preds 1 21 [13787, 14488] [849, 1053, 11237, 826, 4382, 11661, 24, 14703... 2 30 [4181, 8584, 8636] [10464, 10440, 16447, 7946, 2100, 2303, 9728, ... 4 98 [89, 512] [15410, 9713, 14378, 14053, 12604, 11402, 1049... 5 106 [337, 1439, 2808, 2836, 5411, 6267, 10544, 128... [16166, 3182, 9506, 15224, 4718, 16270, 10732,... 8 241 [6162, 8986, 10440, 12138] [13915, 5894, 6588, 10083, 13935, 13913, 16166... <pre>recall 0.044\nprecision 0.044\nmrr 0.021\n</pre> <p>Now lets rearrange them into two colums and define their ranking order, as before.</p> In\u00a0[31]: Copied! <pre>pred_bpr = pred_bpr[['user_id', 'preds']] \npred_bpr = pred_bpr.explode('preds').rename(columns={'preds': 'item_id'})\npred_bpr['rank'] = pred_bpr.groupby('user_id').cumcount() + 1 # give rank to each item_id order\npred_bpr.head()\n</pre> pred_bpr = pred_bpr[['user_id', 'preds']]  pred_bpr = pred_bpr.explode('preds').rename(columns={'preds': 'item_id'}) pred_bpr['rank'] = pred_bpr.groupby('user_id').cumcount() + 1 # give rank to each item_id order pred_bpr.head() Out[31]: user_id item_id rank 1 21 849 1 1 21 1053 2 1 21 11237 3 1 21 826 4 1 21 4382 5 <p>Add <code>user_id</code> and <code>item_id</code> features to the dataset</p> <p>We prepared the dataset on which we will make a prediction using our 2nd model classifier</p> In\u00a0[32]: Copied! <pre>pred_bpr_ctb = pred_bpr.copy()\n\n# \u0444\u0438\u0447\u0438 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u0430\nscore_feat = (pred_bpr_ctb\n              .merge(users[user_col], on=['user_id'], how='left')\n              .merge(items[item_col], on=['item_id'], how='left'))\n\n# fillna for catboost with the most frequent value\nscore_feat = score_feat.fillna(X_train.mode().iloc[0])\nscore_feat.head()\n</pre> pred_bpr_ctb = pred_bpr.copy()  # \u0444\u0438\u0447\u0438 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u0430 score_feat = (pred_bpr_ctb               .merge(users[user_col], on=['user_id'], how='left')               .merge(items[item_col], on=['item_id'], how='left'))  # fillna for catboost with the most frequent value score_feat = score_feat.fillna(X_train.mode().iloc[0]) score_feat.head()  Out[32]: user_id item_id rank age income sex kids_flg content_type countries for_kids age_rating studios 0 21 849 1 age_45_54 income_20_40 \u0416 0.0 film \u0421\u0428\u0410 0.0 18.0 BBC 1 21 1053 2 age_45_54 income_20_40 \u0416 0.0 film \u0421\u0428\u0410 0.0 18.0 BBC 2 21 11237 3 age_45_54 income_20_40 \u0416 0.0 film \u0420\u043e\u0441\u0441\u0438\u044f 0.0 16.0 BBC 3 21 826 4 age_45_54 income_20_40 \u0416 0.0 film \u0412\u0435\u043b\u0438\u043a\u043e\u0431\u0440\u0438\u0442\u0430\u043d\u0438\u044f 0.0 16.0 BBC 4 21 4382 5 age_45_54 income_20_40 \u0416 0.0 film \u0421\u0428\u0410 0.0 16.0 BBC <p>Using the 2nd stage model, we create a new sorting order <code>rank_ctb</code>, which should improve upon the 1 stage model metrics</p> In\u00a0[33]: Copied! <pre># prediction and sort by predict proba weak values\nctb_prediction = ctb_model.predict_proba(score_feat.drop(drop_col, axis=1, errors='ignore'))\n\npred_bpr_ctb['ctb_pred'] = ctb_prediction[:, 1] # prob for positive class\n\npred_bpr_ctb = pred_bpr_ctb.sort_values(\n                                        by=['user_id', 'ctb_pred'], \n                                        ascending=[True, False])\npred_bpr_ctb['rank_ctb'] = pred_bpr_ctb.groupby('user_id').cumcount() + 1\npred_bpr_ctb.head()\n</pre> # prediction and sort by predict proba weak values ctb_prediction = ctb_model.predict_proba(score_feat.drop(drop_col, axis=1, errors='ignore'))  pred_bpr_ctb['ctb_pred'] = ctb_prediction[:, 1] # prob for positive class  pred_bpr_ctb = pred_bpr_ctb.sort_values(                                         by=['user_id', 'ctb_pred'],                                          ascending=[True, False]) pred_bpr_ctb['rank_ctb'] = pred_bpr_ctb.groupby('user_id').cumcount() + 1 pred_bpr_ctb.head() Out[33]: user_id item_id rank ctb_pred rank_ctb 1 21 11237 3 0.509732 1 1 21 11661 6 0.453971 2 1 21 15464 13 0.420558 3 1 21 10824 19 0.322406 4 1 21 12659 12 0.302753 5 In\u00a0[34]: Copied! <pre>true_items = test.groupby('user_id').agg(lambda x: list(x))[['item_id']].reset_index()\npred_items = pred_bpr_ctb.groupby('user_id').agg(lambda x: list(x))[['item_id']].reset_index().rename(columns={'item_id': 'preds'})\ntrue_pred_items = true_items.merge(pred_items, how='left')\ntrue_pred_items = true_pred_items.dropna(subset=['preds'])\ntrue_pred_items.head()\n</pre> true_items = test.groupby('user_id').agg(lambda x: list(x))[['item_id']].reset_index() pred_items = pred_bpr_ctb.groupby('user_id').agg(lambda x: list(x))[['item_id']].reset_index().rename(columns={'item_id': 'preds'}) true_pred_items = true_items.merge(pred_items, how='left') true_pred_items = true_pred_items.dropna(subset=['preds']) true_pred_items.head() Out[34]: user_id item_id preds 1 21 [13787, 14488] [11237, 11661, 15464, 10824, 12659, 4382, 8447... 2 30 [4181, 8584, 8636] [10440, 1465, 15297, 9728, 676, 12346, 12995, ... 4 98 [89, 512] [1204, 9728, 12346, 15410, 14378, 6447, 9653, ... 5 106 [337, 1439, 2808, 2836, 5411, 6267, 10544, 128... [3182, 16166, 5894, 9506, 11919, 4718, 5411, 1... 8 241 [6162, 8986, 10440, 12138] [5894, 13915, 13913, 16166, 3182, 13018, 10761... <p>Confirm how well the catboost model made user recommendations</p> In\u00a0[35]: Copied! <pre># evaluate metrics \nprint('recall',round(recall(true_pred_items, k=20),3))\nprint('precision',round(precision(true_pred_items, k=20),3))\nprint('mrr',round(mrr(true_pred_items, k=20),3))\n</pre> # evaluate metrics  print('recall',round(recall(true_pred_items, k=20),3)) print('precision',round(precision(true_pred_items, k=20),3)) print('mrr',round(mrr(true_pred_items, k=20),3)) <pre>recall 0.056\nprecision 0.056\nmrr 0.034\n</pre>"},{"location":"portfolio/course_recsys/2stagerecsys.html#1-background","title":"1 \u2759  Background\u00b6","text":"<p>In this notebook we will be doing the following:</p> <ul> <li>Split time interaction data into 3 \"global parts\" (train,val,test). We treat <code>train</code> and <code>val</code> as subsets for which we have recommendations. And <code>test</code> is used for making the user recommendations. The existing interactions are only used as reference to understand how well the models perform together</li> <li>Train 1st stage BRP model to generate candidates (ranked features) to be used in <code>val</code> &amp; <code>test</code> classifier model</li> <li>Train 2nd stage classifier based on positive and negative samples (from <code>candidate</code> and <code>val</code> missing combination) with their item ranking in order of candidate priority from 1st stage model prediction</li> <li>The 2nd stage model is trained on a train subset of users, validated on the validation subset of users and then we check how well it performs on the unseen subset of user data, all within the <code>val</code> global subset</li> <li>Once we have both models, we create recommendations on the <code>test</code> global set (last 7 days)</li> <li>Using the first stage model, we create candidates like on <code>val</code>, but this time we group them with <code>test</code> and define the new rank order, and we evaluate the metrics</li> <li>Lastly we use the 2nd stage model, trained on validation set <code>val</code> to get probability of positive class (<code>ctb_pred</code>) using the previous step data, and sort based on this new order <code>rank_ctb</code> and reevaluate the new metrics</li> </ul>"},{"location":"portfolio/course_recsys/2stagerecsys.html#2-read-dataset-film-interactions","title":"2 \u2759  Read Dataset : Film interactions\u00b6","text":"<p>KION Movie/Serial Dataset:</p> <ul> <li><code>interactions</code> contain user/item interaction information</li> <li><code>users</code> contains information about the user (user_id)</li> <li><code>items</code> contains information about the item (item_id)</li> </ul> <p>The dataset contains quite the typical recommendation contents of user/item interaction data, some information about the user and item (which in this dataset are movies/serials)</p> <p>Lets start off by reading the datasets and exploring them later</p>"},{"location":"portfolio/course_recsys/2stagerecsys.html#useritem-interactions","title":"User/Item Interactions \u00b6","text":"<p>Standard user/item interaction features:</p> <ul> <li><code>user_id</code> : users</li> <li><code>item_id</code> : film/serial</li> <li><code>last_watch_dt</code> : The last watched data of movie/serial</li> <li><code>total_dur</code> : Watched duration (implicit interaction)</li> <li><code>watched_pct</code> : Watched percentage (implicit interaction)</li> </ul> <p>We have a dataset which contains implicit user/item interactions. We'll be using <code>watched_pct</code> as our interaction column.</p>"},{"location":"portfolio/course_recsys/2stagerecsys.html#user-features","title":"User Features\u00b6","text":"<p>Features that tell us about the <code>user_id</code></p> <ul> <li><code>age</code> : Age group</li> <li><code>income</code> : User income</li> <li><code>sex</code> : User gender</li> <li><code>kids_flg</code> : Kid flag identifier</li> </ul>"},{"location":"portfolio/course_recsys/2stagerecsys.html#3-preprocessing-dataset","title":"3 \u2759  Preprocessing Dataset\u00b6","text":"<p>Preprocessing stages:</p> <ul> <li><p>We should probably do some form of preprocessing in order for the model to have sufficient data to extract relations in data when training</p> </li> <li><p>Filter from [interactions] views with less than 300 second views</p> </li> <li><p>Filter [users] (user_id) who have less than 10 film views</p> </li> <li><p>Filter out [movies] (item_id) which have less than 10 film views</p> </li> </ul>"},{"location":"portfolio/course_recsys/2stagerecsys.html#1-filter-accidental-views","title":"1) Filter accidental views\u00b6","text":"<p>Interaction dataset contains column <code>total_dur</code>, which tells us how many seconds the user has watched the <code>item_id</code>. Lets filter out items which were for whatever reason started and not watched further, setting the threshold to 300 seconds</p>"},{"location":"portfolio/course_recsys/2stagerecsys.html#2-user-filtration","title":"2) User Filtration\u00b6","text":""},{"location":"portfolio/course_recsys/2stagerecsys.html#3-film-filtration","title":"3) Film Filtration\u00b6","text":""},{"location":"portfolio/course_recsys/2stagerecsys.html#4-create-subsets","title":"4 \u2759  Create subsets\u00b6","text":""},{"location":"portfolio/course_recsys/2stagerecsys.html#timeseries-splitting","title":"Timeseries splitting\u00b6","text":"<ul> <li>We will be splitting the interactions dataset into different parts, based on the datetime columns <code>last_watch_dt</code>, which is the only time related column in the dataset</li> <li>Different parts of the interactions datasaet will be used for different purposes in the two stage process</li> <li>In this notebook, we assume that the global training and validation sets are those that we have, beyond this data we want to make predictions, but since we also have this data, well monitor the recommendation metrics</li> </ul>"},{"location":"portfolio/course_recsys/2stagerecsys.html#5-item-candidate-selection","title":"5 \u2759  Item candidate selection\u00b6","text":"<p>Model 1 purpose:</p> <ul> <li>The purpose of the first stage model is to generate item candidates for the second stage model</li> <li>The BPR based model from library implicit will be used, which uses a loss function suitable for sorting.</li> <li>The model outputs will be ranked based on their recommendation order</li> </ul> <p>The end product of this section will be:</p> <ul> <li><code>positive samples</code> : correctly identified <code>item_id</code> between the model output (trained on <code>train</code>) and the <code>val</code> global subset. A limit of <code>30 candidates (k=30)</code> is set</li> <li><code>negative samples</code> : joining the prediction &amp; <code>val</code>, those which don't have any <code>watched_pct</code> data, ie. no interaction is found in <code>interactions</code></li> <li><code>rank</code> in the order that the model had predicted; this will be one of the features in the 2nd stage model</li> </ul>"},{"location":"portfolio/course_recsys/2stagerecsys.html#6-prepare-dataset-for-2nd-stage-model","title":"6 \u2759  Prepare dataset for 2nd stage model\u00b6","text":"<p>Prepare the <code>positive</code> data samples:</p> <ul> <li>Using an inner join between datasets <code>candidates</code> (1st model prediction) &amp; <code>val</code> (global validation dataset)</li> <li>Each row represents a found overlapping <code>user_id</code> / <code>item_id</code> combination that was present in the interactions dataset (<code>val</code>)</li> </ul>"},{"location":"portfolio/course_recsys/2stagerecsys.html#7-training-evaluation-of-metrics","title":"7 \u2759  Training &amp; Evaluation of metrics \u00b6","text":"<p>Model setting:</p> <ul> <li>We define some standard hyperparameters, which havent been optimised</li> <li>Well be comparing the model performance using the <code>ROCAUC</code> metric</li> </ul>"},{"location":"portfolio/course_recsys/2stagerecsys.html#8-recommendation-on-global-test","title":"8 \u2759  Recommendation on global test \u00b6","text":"<p>Time to make recommendations for period after <code>val</code></p> <ul> <li><p>Time to return to the global <code>test</code> dataset (ie. last 7 days of interaction data)</p> </li> <li><p>We use the last 7 days of interactions as a way to confirm how well the models will work on unseen data</p> </li> </ul> <p>Starting off by grouping <code>item_id</code> for each user and storing them in a list, as before.</p>"},{"location":"portfolio/course_recsys/8_2_feature_importance.html","title":"8 2 feature importance","text":"In\u00a0[\u00a0]: Copied! <pre># \u0423\u0431\u0435\u0440\u0435\u043c \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f, \u0447\u0442\u043e\u0431\u044b \u043e\u043d\u0438 \u043d\u0435 \u0437\u0430\u0433\u0440\u043e\u043c\u043e\u0436\u0434\u0430\u043b\u0438 \u0432\u044b\u0432\u043e\u0434\n\nimport warning|s \nwarnings.filterwarnings('ignore')\n</pre> # \u0423\u0431\u0435\u0440\u0435\u043c \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f, \u0447\u0442\u043e\u0431\u044b \u043e\u043d\u0438 \u043d\u0435 \u0437\u0430\u0433\u0440\u043e\u043c\u043e\u0436\u0434\u0430\u043b\u0438 \u0432\u044b\u0432\u043e\u0434  import warning|s  warnings.filterwarnings('ignore') In\u00a0[\u00a0]: Copied! <pre># \u0423\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043c \u0440\u0430\u0437\u043c\u0435\u0440\u044b \u0448\u0440\u0438\u0444\u0442\u043e\u0432 \u0434\u043b\u044f \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=16)          # \u0440\u0430\u0437\u043c\u0435\u0440\u044b \u0442\u0435\u043a\u0441\u0442\u0430 \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e\nplt.rc('axes', titlesize=20)     # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u0430 \u043e\u0441\u0435\u0439\nplt.rc('axes', labelsize=18)     # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u043e\u0441\u0435\u0439\nplt.rc('xtick', labelsize=18)    # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u043c\u0435\u0442\u043e\u043a \u043f\u043e \u043e\u0441\u0438 x\nplt.rc('ytick', labelsize=18)    # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u043c\u0435\u0442\u043e\u043a \u043f\u043e \u043e\u0441\u0438 y\nplt.rc('legend', fontsize=16)    # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u043b\u0435\u0433\u0435\u043d\u0434\u044b\nplt.rc('figure', titlesize=20)   # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u0430 \u0432\u0441\u0435\u0433\u043e \u0433\u0440\u0430\u0444\u0438\u043a\u0430\n</pre> # \u0423\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043c \u0440\u0430\u0437\u043c\u0435\u0440\u044b \u0448\u0440\u0438\u0444\u0442\u043e\u0432 \u0434\u043b\u044f \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432 import matplotlib.pyplot as plt  plt.rc('font', size=16)          # \u0440\u0430\u0437\u043c\u0435\u0440\u044b \u0442\u0435\u043a\u0441\u0442\u0430 \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e plt.rc('axes', titlesize=20)     # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u0430 \u043e\u0441\u0435\u0439 plt.rc('axes', labelsize=18)     # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u043e\u0441\u0435\u0439 plt.rc('xtick', labelsize=18)    # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u043c\u0435\u0442\u043e\u043a \u043f\u043e \u043e\u0441\u0438 x plt.rc('ytick', labelsize=18)    # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u043c\u0435\u0442\u043e\u043a \u043f\u043e \u043e\u0441\u0438 y plt.rc('legend', fontsize=16)    # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u043b\u0435\u0433\u0435\u043d\u0434\u044b plt.rc('figure', titlesize=20)   # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u0430 \u0432\u0441\u0435\u0433\u043e \u0433\u0440\u0430\u0444\u0438\u043a\u0430 In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\ndf = pd.read_csv(\"diabetes.csv\")\nprint(df.shape)\ndf.head()\n</pre> import pandas as pd  df = pd.read_csv(\"diabetes.csv\") print(df.shape) df.head() <pre>(768, 9)\n</pre> Out[\u00a0]: Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome 0 6 148 72 35 0 33.6 0.627 50 1 1 1 85 66 29 0 26.6 0.351 31 0 2 8 183 64 0 0 23.3 0.672 32 1 3 1 89 66 23 94 28.1 0.167 21 0 4 0 137 40 35 168 43.1 2.288 33 1 In\u00a0[\u00a0]: Copied! <pre>target = df[\"Outcome\"]\n\ndf = df.drop(\"Outcome\", axis=1)\ndf.head()\n</pre> target = df[\"Outcome\"]  df = df.drop(\"Outcome\", axis=1) df.head() Out[\u00a0]: Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age 0 6 148 72 35 0 33.6 0.627 50 1 1 85 66 29 0 26.6 0.351 31 2 8 183 64 0 0 23.3 0.672 32 3 1 89 66 23 94 28.1 0.167 21 4 0 137 40 35 168 43.1 2.288 33 <p>\u041a\u0430\u0436\u0435\u0442\u0441\u044f, \u0447\u0442\u043e \u0432\u0441\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0447\u0438\u0441\u043b\u0435\u043d\u043d\u044b\u0435. \u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u044d\u0442\u043e:</p> In\u00a0[\u00a0]: Copied! <pre>numerical_cols = df.select_dtypes(exclude=[\"object\"]).columns.tolist()\nlen(numerical_cols) == df.shape[1]\n</pre> numerical_cols = df.select_dtypes(exclude=[\"object\"]).columns.tolist() len(numerical_cols) == df.shape[1] Out[\u00a0]: <pre>True</pre> In\u00a0[\u00a0]: Copied! <pre>df.isna().sum()\n</pre> df.isna().sum() Out[\u00a0]: <pre>Pregnancies                 0\nGlucose                     0\nBloodPressure               0\nSkinThickness               0\nInsulin                     0\nBMI                         0\nDiabetesPedigreeFunction    0\nAge                         0\ndtype: int64</pre> <p>\u041f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0435\u0442, \u0432\u0441\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0447\u0438\u0441\u043b\u0435\u043d\u043d\u044b\u0435, \u0438 \u043c\u044b \u0441\u043e\u0431\u0438\u0440\u0430\u0435\u043c\u0441\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u0438 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043f\u0440\u043e\u0432\u043e\u0434\u0438\u0442\u044c \u043d\u0435 \u0431\u0443\u0434\u0435\u043c.</p> In\u00a0[\u00a0]: Copied! <pre>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    df,\n    target,\n    test_size=0.33,\n    random_state=42,\n    stratify=target, # \u043f\u043e\u0434\u0435\u043b\u0438\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u0442\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u0447\u0442\u043e\u0431\u044b \u0438 \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439, \u0438 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0431\u044b\u043b\u043e \u0442\u0430\u043a\u043e\u0435 \u0436\u0435 \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432, \u043a\u0430\u043a \u0438 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\n)\nX_train.shape, X_test.shape\n</pre> from sklearn.model_selection import train_test_split  X_train, X_test, y_train, y_test = train_test_split(     df,     target,     test_size=0.33,     random_state=42,     stratify=target, # \u043f\u043e\u0434\u0435\u043b\u0438\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u0442\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u0447\u0442\u043e\u0431\u044b \u0438 \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439, \u0438 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0431\u044b\u043b\u043e \u0442\u0430\u043a\u043e\u0435 \u0436\u0435 \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432, \u043a\u0430\u043a \u0438 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 ) X_train.shape, X_test.shape Out[\u00a0]: <pre>((514, 8), (254, 8))</pre> In\u00a0[\u00a0]: Copied! <pre># \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0431\u0430\u043b\u0430\u043d\u0441 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \n\nval_count_init = pd.Series(target).value_counts().to_dict()\nval_count_train = y_train.value_counts().to_dict()\nval_count_test = y_test.value_counts().to_dict()\n\nget_relation = lambda x: round(x[1] / x[0], 2)\n\nprint(\n    \"\"\"\n    \u0421\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435:  {}\\t(\u043a\u043b\u0430\u0441\u0441 1 / \u043a\u043b\u0430\u0441\u0441 0 = {})\n    \u0421\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435: {}\\t(\u043a\u043b\u0430\u0441\u0441 1 / \u043a\u043b\u0430\u0441\u0441 0 = {})\n    \u0421\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435:  {}\\t(\u043a\u043b\u0430\u0441\u0441 1 / \u043a\u043b\u0430\u0441\u0441 0 = {})\n    \"\"\".format(\n        val_count_init, get_relation(val_count_init),\n        val_count_train, get_relation(val_count_train),\n        val_count_test, get_relation(val_count_test),\n    )\n)\n</pre> # \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0431\u0430\u043b\u0430\u043d\u0441 \u043a\u043b\u0430\u0441\u0441\u043e\u0432   val_count_init = pd.Series(target).value_counts().to_dict() val_count_train = y_train.value_counts().to_dict() val_count_test = y_test.value_counts().to_dict()  get_relation = lambda x: round(x[1] / x[0], 2)  print(     \"\"\"     \u0421\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435:  {}\\t(\u043a\u043b\u0430\u0441\u0441 1 / \u043a\u043b\u0430\u0441\u0441 0 = {})     \u0421\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435: {}\\t(\u043a\u043b\u0430\u0441\u0441 1 / \u043a\u043b\u0430\u0441\u0441 0 = {})     \u0421\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435:  {}\\t(\u043a\u043b\u0430\u0441\u0441 1 / \u043a\u043b\u0430\u0441\u0441 0 = {})     \"\"\".format(         val_count_init, get_relation(val_count_init),         val_count_train, get_relation(val_count_train),         val_count_test, get_relation(val_count_test),     ) ) <pre>\n    \u0421\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435:  {0: 500, 1: 268}\t(\u043a\u043b\u0430\u0441\u0441 1 / \u043a\u043b\u0430\u0441\u0441 0 = 0.54)\n    \u0421\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435: {0: 335, 1: 179}\t(\u043a\u043b\u0430\u0441\u0441 1 / \u043a\u043b\u0430\u0441\u0441 0 = 0.53)\n    \u0421\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435:  {0: 165, 1: 89}\t(\u043a\u043b\u0430\u0441\u0441 1 / \u043a\u043b\u0430\u0441\u0441 0 = 0.54)\n    \n</pre> <p>\u041f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0443 \u043d\u0430\u0441 \u043d\u0435\u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435, \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c F1-\u043c\u0435\u0440\u0443 \u0438 PR AUC.</p> In\u00a0[\u00a0]: Copied! <pre>from sklearn.metrics import (\n    f1_score, \n    average_precision_score,\n)\n</pre> from sklearn.metrics import (     f1_score,      average_precision_score, ) <p>\u0414\u043b\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u0438 \u0432\u044b\u0432\u0435\u0434\u0435\u043d\u0438\u044f F1-\u043c\u0435\u0440\u044b \u0438 PR AUC \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0438\u0437 \u044f\u0447\u0435\u0439\u043a\u0438 \u043d\u0438\u0436\u0435:</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\ndef get_all_metrics(\n    true_labels, \n    predictions, \n    probabilities, \n    print_metrics=False,\n):\n    \"\"\"\u041f\u043e\u0434\u0441\u0447\u0451\u0442 \u0438 \u0432\u044b\u0432\u043e\u0434 \u0432\u0441\u0435\u0445 \u043c\u0435\u0442\u0440\u0438\u043a.\n\n    :param true_labels: \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043a\u0438 \u043a\u043b\u0430\u0441\u0441\u043e\u0432\n    :param predictions: \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f (\u043c\u0435\u0442\u043a\u0438 \u043a\u043b\u0430\u0441\u0441\u043e\u0432)\n    :param probabilities: \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f (\u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u043a \u043a\u0430\u0436\u0434\u043e\u043c\u0443 \u043a\u043b\u0430\u0441\u0441\u0443)\n    :param print_metrics: \u0435\u0441\u043b\u0438 True, \u0442\u043e \u043f\u0435\u0447\u0430\u0442\u0430\u0435\u0442 \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0435\u0441\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0438\n    :return: \u0441\u043b\u043e\u0432\u0430\u0440\u044c, \u043a\u043b\u044e\u0447\u0438 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e -- \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u043c\u0435\u0442\u0440\u0438\u043a, \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f -- \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0435\u0441\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0438   \n    \"\"\"\n    f1 = f1_score(true_labels, predictions)\n    pr_auc =  average_precision_score(true_labels, probabilities)\n    \n    if print_metrics:\n        print(\n            \"F1-\u043c\u0435\u0440\u0430 = {}\\nPR AUC = {}\".format(\n                round(f1, 3), round(pr_auc, 3)\n            )\n        )\n    return {\n        \"F1-score\": f1, \n        \"PR AUC\": pr_auc,\n    }\n</pre> import numpy as np  def get_all_metrics(     true_labels,      predictions,      probabilities,      print_metrics=False, ):     \"\"\"\u041f\u043e\u0434\u0441\u0447\u0451\u0442 \u0438 \u0432\u044b\u0432\u043e\u0434 \u0432\u0441\u0435\u0445 \u043c\u0435\u0442\u0440\u0438\u043a.      :param true_labels: \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043a\u0438 \u043a\u043b\u0430\u0441\u0441\u043e\u0432     :param predictions: \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f (\u043c\u0435\u0442\u043a\u0438 \u043a\u043b\u0430\u0441\u0441\u043e\u0432)     :param probabilities: \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f (\u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u043a \u043a\u0430\u0436\u0434\u043e\u043c\u0443 \u043a\u043b\u0430\u0441\u0441\u0443)     :param print_metrics: \u0435\u0441\u043b\u0438 True, \u0442\u043e \u043f\u0435\u0447\u0430\u0442\u0430\u0435\u0442 \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0435\u0441\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0438     :return: \u0441\u043b\u043e\u0432\u0430\u0440\u044c, \u043a\u043b\u044e\u0447\u0438 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e -- \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u043c\u0435\u0442\u0440\u0438\u043a, \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f -- \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0435\u0441\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0438        \"\"\"     f1 = f1_score(true_labels, predictions)     pr_auc =  average_precision_score(true_labels, probabilities)          if print_metrics:         print(             \"F1-\u043c\u0435\u0440\u0430 = {}\\nPR AUC = {}\".format(                 round(f1, 3), round(pr_auc, 3)             )         )     return {         \"F1-score\": f1,          \"PR AUC\": pr_auc,     } In\u00a0[\u00a0]: Copied! <pre>from xgboost import XGBClassifier                   # \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433 \nfrom sklearn.ensemble import (\n    AdaBoostClassifier,                             # \u0430\u0434\u0430\u043f\u0442\u0438\u0432\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433\n    RandomForestClassifier,                         # \u0431\u044d\u0433\u0433\u0438\u043d\u0433\n    StackingClassifier,                             # \u0441\u0442\u0435\u043a\u0438\u043d\u0433\n    VotingClassifier,                               # \u0432\u043e\u0442\u0438\u043d\u0433\n)\n\nfrom sklearn.tree import DecisionTreeClassifier\n</pre> from xgboost import XGBClassifier                   # \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433  from sklearn.ensemble import (     AdaBoostClassifier,                             # \u0430\u0434\u0430\u043f\u0442\u0438\u0432\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433     RandomForestClassifier,                         # \u0431\u044d\u0433\u0433\u0438\u043d\u0433     StackingClassifier,                             # \u0441\u0442\u0435\u043a\u0438\u043d\u0433     VotingClassifier,                               # \u0432\u043e\u0442\u0438\u043d\u0433 )  from sklearn.tree import DecisionTreeClassifier In\u00a0[\u00a0]: Copied! <pre>from sklearn.linear_model import LogisticRegression\n</pre> from sklearn.linear_model import LogisticRegression <p>\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0432 \u044f\u0447\u0435\u0439\u043a\u0435 \u043d\u0438\u0436\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u043c\u0435\u0442\u0440\u0438\u043a \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430.</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\ndef model_train_test(X_train, y_train, X_test, y_test, model):\n    \"\"\"\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e.\n\n    :param X_train: \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\n    :param y_train: \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\n    :param X_test: \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\n    :param y_test: \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\n    :param model: \u043c\u043e\u0434\u0435\u043b\u044c, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u0443\u0435\u0442\u0441\u044f\n    :return: \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f -- \u043a\u043b\u0430\u0441\u0441\u044b \u0438 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043a \u043a\u0430\u0436\u0434\u043e\u043c\u0443 \u043a\u043b\u0430\u0441\u0441\u0443, \u0432\u0441\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438  \n    \"\"\"\n    model.fit(X_train, y_train)\n\n    pred = model.predict(X_test)\n    proba = model.predict_proba(X_test)\n\n    print(str(model), end=\"\\n\\n\")\n\n    all_metrics = get_all_metrics(\n        y_test, \n        pred, \n        proba[:, 1], \n        print_metrics=True,\n    )\n    return model, pred, proba, all_metrics\n</pre> import matplotlib.pyplot as plt  def model_train_test(X_train, y_train, X_test, y_test, model):     \"\"\"\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e.      :param X_train: \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438     :param y_train: \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438     :param X_test: \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438     :param y_test: \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438     :param model: \u043c\u043e\u0434\u0435\u043b\u044c, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u0443\u0435\u0442\u0441\u044f     :return: \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f -- \u043a\u043b\u0430\u0441\u0441\u044b \u0438 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043a \u043a\u0430\u0436\u0434\u043e\u043c\u0443 \u043a\u043b\u0430\u0441\u0441\u0443, \u0432\u0441\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438       \"\"\"     model.fit(X_train, y_train)      pred = model.predict(X_test)     proba = model.predict_proba(X_test)      print(str(model), end=\"\\n\\n\")      all_metrics = get_all_metrics(         y_test,          pred,          proba[:, 1],          print_metrics=True,     )     return model, pred, proba, all_metrics <p>\u0411\u0443\u0434\u0435\u043c \u0441\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0442\u044c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u044b \u0441 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e (\u043f\u043e\u0434\u0431\u043e\u0440 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0434\u0435\u043b\u0430\u0442\u044c \u043d\u0435 \u0431\u0443\u0434\u0435\u043c).</p> In\u00a0[\u00a0]: Copied! <pre>model_logreg, pred_logreg, proba_logreg, all_metrics_logreg = model_train_test(\n    X_train, y_train,\n    X_test, y_test,\n    LogisticRegression(),\n)\n</pre> model_logreg, pred_logreg, proba_logreg, all_metrics_logreg = model_train_test(     X_train, y_train,     X_test, y_test,     LogisticRegression(), ) <pre>LogisticRegression()\n\nF1-\u043c\u0435\u0440\u0430 = 0.596\nPR AUC = 0.728\n</pre> In\u00a0[\u00a0]: Copied! <pre>model_xgb, pred_xgb, proba_xgb, all_metrics_xgb = model_train_test(\n    X_train, y_train,\n    X_test, y_test,\n    XGBClassifier(),\n)\n</pre> model_xgb, pred_xgb, proba_xgb, all_metrics_xgb = model_train_test(     X_train, y_train,     X_test, y_test,     XGBClassifier(), ) <pre>XGBClassifier()\n\nF1-\u043c\u0435\u0440\u0430 = 0.617\nPR AUC = 0.697\n</pre> In\u00a0[\u00a0]: Copied! <pre>model_adab, pred_adab, proba_adab, all_metrics_adab = model_train_test(\n    X_train, y_train,\n    X_test, y_test,\n    AdaBoostClassifier(\n        base_estimator=DecisionTreeClassifier()\n    ),\n)\n</pre> model_adab, pred_adab, proba_adab, all_metrics_adab = model_train_test(     X_train, y_train,     X_test, y_test,     AdaBoostClassifier(         base_estimator=DecisionTreeClassifier()     ), ) <pre>AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n\nF1-\u043c\u0435\u0440\u0430 = 0.473\nPR AUC = 0.418\n</pre> In\u00a0[\u00a0]: Copied! <pre>model_rf, pred_rf, proba_rf, all_metrics_rf = model_train_test(\n    X_train, y_train,\n    X_test, y_test,\n    RandomForestClassifier(),\n)\n</pre> model_rf, pred_rf, proba_rf, all_metrics_rf = model_train_test(     X_train, y_train,     X_test, y_test,     RandomForestClassifier(), ) <pre>RandomForestClassifier()\n\nF1-\u043c\u0435\u0440\u0430 = 0.625\nPR AUC = 0.698\n</pre> In\u00a0[\u00a0]: Copied! <pre>model_trees_stacking, pred_trees_stacking, proba_trees_stacking, all_metrics_trees_stacking = model_train_test(\n    X_train, y_train,\n    X_test, y_test,\n    StackingClassifier(\n        estimators=[\n            (\"decision_tree_{}\".format(i+1), DecisionTreeClassifier()) for i in range(10)\n        ],\n        final_estimator=DecisionTreeClassifier()\n    ),\n)\n</pre> model_trees_stacking, pred_trees_stacking, proba_trees_stacking, all_metrics_trees_stacking = model_train_test(     X_train, y_train,     X_test, y_test,     StackingClassifier(         estimators=[             (\"decision_tree_{}\".format(i+1), DecisionTreeClassifier()) for i in range(10)         ],         final_estimator=DecisionTreeClassifier()     ), ) <pre>StackingClassifier(estimators=[('decision_tree_1', DecisionTreeClassifier()),\n                               ('decision_tree_2', DecisionTreeClassifier()),\n                               ('decision_tree_3', DecisionTreeClassifier()),\n                               ('decision_tree_4', DecisionTreeClassifier()),\n                               ('decision_tree_5', DecisionTreeClassifier()),\n                               ('decision_tree_6', DecisionTreeClassifier()),\n                               ('decision_tree_7', DecisionTreeClassifier()),\n                               ('decision_tree_8', DecisionTreeClassifier()),\n                               ('decision_tree_9', DecisionTreeClassifier()),\n                               ('decision_tree_10', DecisionTreeClassifier())],\n                   final_estimator=DecisionTreeClassifier())\n\nF1-\u043c\u0435\u0440\u0430 = 0.527\nPR AUC = 0.445\n</pre> In\u00a0[\u00a0]: Copied! <pre>model_trees_voting, pred_trees_voting, proba_trees_voting, all_metrics_trees_voting = model_train_test(\n    X_train, y_train,\n    X_test, y_test,\n    VotingClassifier(\n        estimators=[\n            (\"decision_tree_{}\".format(i+1), DecisionTreeClassifier()) for i in range(10)\n        ],\n        voting=\"soft\"\n    ),\n)\n</pre> model_trees_voting, pred_trees_voting, proba_trees_voting, all_metrics_trees_voting = model_train_test(     X_train, y_train,     X_test, y_test,     VotingClassifier(         estimators=[             (\"decision_tree_{}\".format(i+1), DecisionTreeClassifier()) for i in range(10)         ],         voting=\"soft\"     ), ) <pre>VotingClassifier(estimators=[('decision_tree_1', DecisionTreeClassifier()),\n                             ('decision_tree_2', DecisionTreeClassifier()),\n                             ('decision_tree_3', DecisionTreeClassifier()),\n                             ('decision_tree_4', DecisionTreeClassifier()),\n                             ('decision_tree_5', DecisionTreeClassifier()),\n                             ('decision_tree_6', DecisionTreeClassifier()),\n                             ('decision_tree_7', DecisionTreeClassifier()),\n                             ('decision_tree_8', DecisionTreeClassifier()),\n                             ('decision_tree_9', DecisionTreeClassifier()),\n                             ('decision_tree_10', DecisionTreeClassifier())],\n                 voting='soft')\n\nF1-\u043c\u0435\u0440\u0430 = 0.529\nPR AUC = 0.495\n</pre> <p>\u0414\u043b\u044f \u0443\u0434\u043e\u0431\u0441\u0442\u0432\u0430 \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0445\u0441\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0441\u0430\u0433\u0440\u0435\u0433\u0438\u0440\u0443\u0435\u043c \u0438\u0445.</p> In\u00a0[\u00a0]: Copied! <pre>metrics_df = pd.DataFrame(\n    [all_metrics_logreg, all_metrics_adab, all_metrics_xgb, all_metrics_rf, all_metrics_trees_stacking, all_metrics_trees_voting], \n    index=[\"Logistic Regression\", \"AdaBoost\", \"XGBoost\", \"RandomForest\", \"Stacking\", \"Voting\"]\n)\nmetrics_df.sort_values(by=[\"F1-score\", \"PR AUC\"], ascending=False)\n</pre> metrics_df = pd.DataFrame(     [all_metrics_logreg, all_metrics_adab, all_metrics_xgb, all_metrics_rf, all_metrics_trees_stacking, all_metrics_trees_voting],      index=[\"Logistic Regression\", \"AdaBoost\", \"XGBoost\", \"RandomForest\", \"Stacking\", \"Voting\"] ) metrics_df.sort_values(by=[\"F1-score\", \"PR AUC\"], ascending=False) Out[\u00a0]: F1-score PR AUC RandomForest 0.625000 0.697630 XGBoost 0.617284 0.697381 Logistic Regression 0.596273 0.727906 Voting 0.529412 0.495224 Stacking 0.526946 0.445035 AdaBoost 0.473373 0.417632 <p>\u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u0441\u0443\u0434\u044f \u043f\u043e F1-\u043c\u0435\u0440\u0435 \u0438 PR AUC, \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u043c\u0438 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430\u043c\u0438 \u0432 \u0434\u0430\u043d\u043d\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441 \u0438 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f. \u041e\u0434\u043d\u0430\u043a\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438\u0441\u044c \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043d\u0435\u0432\u044b\u0441\u043e\u043a\u0438\u0435.</p> <p>\u0421\u043d\u0430\u0447\u0430\u043b\u0430 \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438. \u0414\u043b\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u043f\u0440\u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0435.</p> In\u00a0[\u00a0]: Copied! <pre>logreg_coefs = np.abs(model_logreg.coef_).squeeze()\n</pre> logreg_coefs = np.abs(model_logreg.coef_).squeeze() In\u00a0[\u00a0]: Copied! <pre>print(\n    \"\u041d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a: {}\\n\u041d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a: {}\".format(\n        X_train.columns[np.argmax(logreg_coefs)],\n        X_train.columns[np.argmin(logreg_coefs)]\n    )\n)\n</pre> print(     \"\u041d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a: {}\\n\u041d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a: {}\".format(         X_train.columns[np.argmax(logreg_coefs)],         X_train.columns[np.argmin(logreg_coefs)]     ) ) <pre>\u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a: DiabetesPedigreeFunction\n\u041d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a: Insulin\n</pre> <p>\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0432 \u043f\u043e\u0440\u044f\u0434\u043a\u0435 \u0438\u0445 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438.</p> In\u00a0[\u00a0]: Copied! <pre>sorted_idx = np.argsort(logreg_coefs)\nsorted_cols = X_train.columns[sorted_idx].tolist()\nsorted_fi = logreg_coefs[sorted_idx]\n</pre> sorted_idx = np.argsort(logreg_coefs) sorted_cols = X_train.columns[sorted_idx].tolist() sorted_fi = logreg_coefs[sorted_idx] In\u00a0[\u00a0]: Copied! <pre>plt.barh(sorted_cols, sorted_fi);\n</pre> plt.barh(sorted_cols, sorted_fi); <p>\u041f\u0435\u0440\u0435\u0439\u0434\u0435\u043c \u043a \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u044f\u043c:</p> In\u00a0[\u00a0]: Copied! <pre># \u0427\u0442\u043e\u0431\u044b \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u044c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443 shap, \u0440\u0430\u0441\u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0438\u0440\u0443\u0439\u0442\u0435 \u0441\u0442\u0440\u043e\u043a\u0443 \u043d\u0438\u0436\u0435\n\n# !pip install shap\n</pre> # \u0427\u0442\u043e\u0431\u044b \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u044c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443 shap, \u0440\u0430\u0441\u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0438\u0440\u0443\u0439\u0442\u0435 \u0441\u0442\u0440\u043e\u043a\u0443 \u043d\u0438\u0436\u0435  # !pip install shap In\u00a0[\u00a0]: Copied! <pre>import shap\n</pre> import shap <p>\u0414\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0438\u0437 \u044f\u0447\u0435\u0439\u043a\u0438 \u043d\u0438\u0436\u0435:</p> In\u00a0[\u00a0]: Copied! <pre>def get_model_name_fi(model, fi_type, X_train=None):\n    \"\"\"\u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432.\n\n    :param model: \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c\n    :param fi_type: \u0442\u0438\u043f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432: 'builded' -- \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u044b\u0439 \u0432 \u043a\u043b\u0430\u0441\u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u043b\u0438 'shap' -- \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e shap-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439\n    :param X_train: \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 (\u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u0430 \u043f\u0440\u0438 fi_type='shap')\n    :return: \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    \"\"\"\n    model_name = str(model)[:str(model).find(\"(\")]\n\n    if fi_type == \"builded\":\n        if \"xgbclassifier\" in model_name.lower() or \"randomforest\" in model_name.lower():\n            fi = model.feature_importances_\n        else:\n            # AdaBoostClassifier, StackingClassifier \u0438 VotingClassifier \u043d\u0435 \u0438\u043c\u0435\u044e\u0442 \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432,\n            # \u043d\u043e \u0442.\u043a. \u0443 \u043d\u0430\u0441 \u0432 \u043e\u0441\u043d\u0432\u043e\u0435 \u043b\u0435\u0436\u0430\u0442 \u0434\u0435\u0440\u0435\u0432\u044c\u044f, \u0442\u043e \u043c\u044b \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u043e \u0432\u0441\u0435\u043c \u0434\u0435\u0440\u0435\u0432\u044c\u044f\u043c\n            fi = np.concatenate(\n                [\n                    dt.feature_importances_.reshape((1, -1)) for dt in model.estimators_\n                ]\n            ).mean(axis=0)\n    elif fi_type == \"shap\":\n        explainer = shap.TreeExplainer(model)\n        fi = explainer.shap_values(X_train)\n    return model_name, fi\n</pre> def get_model_name_fi(model, fi_type, X_train=None):     \"\"\"\u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432.      :param model: \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c     :param fi_type: \u0442\u0438\u043f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432: 'builded' -- \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u044b\u0439 \u0432 \u043a\u043b\u0430\u0441\u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u043b\u0438 'shap' -- \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e shap-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439     :param X_train: \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 (\u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u0430 \u043f\u0440\u0438 fi_type='shap')     :return: \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432     \"\"\"     model_name = str(model)[:str(model).find(\"(\")]      if fi_type == \"builded\":         if \"xgbclassifier\" in model_name.lower() or \"randomforest\" in model_name.lower():             fi = model.feature_importances_         else:             # AdaBoostClassifier, StackingClassifier \u0438 VotingClassifier \u043d\u0435 \u0438\u043c\u0435\u044e\u0442 \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432,             # \u043d\u043e \u0442.\u043a. \u0443 \u043d\u0430\u0441 \u0432 \u043e\u0441\u043d\u0432\u043e\u0435 \u043b\u0435\u0436\u0430\u0442 \u0434\u0435\u0440\u0435\u0432\u044c\u044f, \u0442\u043e \u043c\u044b \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u043e \u0432\u0441\u0435\u043c \u0434\u0435\u0440\u0435\u0432\u044c\u044f\u043c             fi = np.concatenate(                 [                     dt.feature_importances_.reshape((1, -1)) for dt in model.estimators_                 ]             ).mean(axis=0)     elif fi_type == \"shap\":         explainer = shap.TreeExplainer(model)         fi = explainer.shap_values(X_train)     return model_name, fi <p>\u0422\u0430\u043a\u0436\u0435 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0434\u043b\u044f \u0440\u0430\u0437\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c:</p> In\u00a0[\u00a0]: Copied! <pre>def plot_several_models_feature_importances(models, features, fi_type, X_train=None):\n    \"\"\"\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u0442 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432. \n\n    :param models: \u0441\u043f\u0438\u0441\u043e\u043a \u043e\u0431\u0443\u0447\u043d\u0435\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \n    :param features: \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    :param fi_type: \u0442\u0438\u043f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432: 'builded' -- \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u044b\u0439 \u0432 \u043a\u043b\u0430\u0441\u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u043b\u0438 'shap' -- \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e shap-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439\n    :param X_train: \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 (\u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u0430 \u043f\u0440\u0438 fi_type='shap')\n    \"\"\"\n    N = len(models)\n    plt.figure(figsize=(10 * N, 8 * (N // 2 + 1)))\n\n    for i, model in enumerate(models):\n        model_name, fi = get_model_name_fi(model, fi_type, X_train=X_train)\n\n        if fi_type == \"builded\":\n            plt.subplot(2, N // 2 + 1, i + 1)\n            sorted_idx = np.argsort(models[0].feature_importances_)[::-1]\n            plt.barh(features[sorted_idx], fi[sorted_idx])\n            plt.title(model_name);\n        elif fi_type == \"shap\":\n            shap.summary_plot(fi, X_train, plot_type=\"bar\")\n    \n    if fi_type == \"builded\":\n        plt.subplot(2, N // 2 + 1, i + 2)\n        for i, model in enumerate(models):\n            model_name, fi = get_model_name_fi(model, fi_type, X_train=X_train)\n            plt.barh(features[sorted_idx], fi[sorted_idx], alpha=0.5, label=model_name)\n        plt.legend()\n        plt.title(\"All models\");\n\n    plt.show()\n</pre> def plot_several_models_feature_importances(models, features, fi_type, X_train=None):     \"\"\"\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u0442 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432.       :param models: \u0441\u043f\u0438\u0441\u043e\u043a \u043e\u0431\u0443\u0447\u043d\u0435\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439      :param features: \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432     :param fi_type: \u0442\u0438\u043f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432: 'builded' -- \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u044b\u0439 \u0432 \u043a\u043b\u0430\u0441\u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u043b\u0438 'shap' -- \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e shap-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439     :param X_train: \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 (\u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u0430 \u043f\u0440\u0438 fi_type='shap')     \"\"\"     N = len(models)     plt.figure(figsize=(10 * N, 8 * (N // 2 + 1)))      for i, model in enumerate(models):         model_name, fi = get_model_name_fi(model, fi_type, X_train=X_train)          if fi_type == \"builded\":             plt.subplot(2, N // 2 + 1, i + 1)             sorted_idx = np.argsort(models[0].feature_importances_)[::-1]             plt.barh(features[sorted_idx], fi[sorted_idx])             plt.title(model_name);         elif fi_type == \"shap\":             shap.summary_plot(fi, X_train, plot_type=\"bar\")          if fi_type == \"builded\":         plt.subplot(2, N // 2 + 1, i + 2)         for i, model in enumerate(models):             model_name, fi = get_model_name_fi(model, fi_type, X_train=X_train)             plt.barh(features[sorted_idx], fi[sorted_idx], alpha=0.5, label=model_name)         plt.legend()         plt.title(\"All models\");      plt.show() In\u00a0[\u00a0]: Copied! <pre>plot_several_models_feature_importances(\n    models=[\n        model_adab,\n        model_xgb,\n        model_rf,\n        model_trees_stacking,\n        model_trees_voting,\n    ], \n    features=X_train.columns,\n    fi_type=\"builded\",\n)\n</pre> plot_several_models_feature_importances(     models=[         model_adab,         model_xgb,         model_rf,         model_trees_stacking,         model_trees_voting,     ],      features=X_train.columns,     fi_type=\"builded\", ) <p>\u0412\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0434\u043b\u044f \u0440\u0430\u0437\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438\u0441\u044c \u0440\u0430\u0437\u043d\u044b\u0435, \u043d\u043e \u0441\u043b\u0435\u0434\u0443\u0435\u0442 \u0437\u0430\u043c\u0435\u0442\u0438\u0442\u044c, \u0447\u0442\u043e \u0432\u0441\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043b\u0438 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u043c \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0433\u043b\u044e\u043a\u043e\u0437\u044b <code>Glucose</code>.</p> In\u00a0[\u00a0]: Copied! <pre>plot_several_models_feature_importances(\n    models=[\n        model_xgb,\n        model_rf, \n        # \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0435 \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0435\u043c\u044b\u0435 \u043d\u0430\u043c\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0435 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u044e\u0442\u0441\u044f shap.TreeExplainer, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u044d\u0442\u0438 \u0434\u0432\u0435 \u043c\u043e\u0434\u0435\u043b\u0438\n    ], \n    features=X_train.columns,\n    fi_type=\"shap\",\n    X_train=X_train,\n)\n</pre> plot_several_models_feature_importances(     models=[         model_xgb,         model_rf,          # \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0435 \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0435\u043c\u044b\u0435 \u043d\u0430\u043c\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0435 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u044e\u0442\u0441\u044f shap.TreeExplainer, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u044d\u0442\u0438 \u0434\u0432\u0435 \u043c\u043e\u0434\u0435\u043b\u0438     ],      features=X_train.columns,     fi_type=\"shap\",     X_train=X_train, ) <p>\u041a\u0430\u043a \u0432\u0438\u0434\u043d\u043e \u0438\u0437 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432 \u0432\u044b\u0448\u0435, \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u0430\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043e\u0442\u043b\u0438\u0447\u0430\u0435\u0442\u0441\u044f \u043e\u0442 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e <code>shap</code>-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439.</p> <p>\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 <code>XGBoost</code>:</p> <ol> <li>\u0412\u043e\u0437\u044c\u043c\u0435\u043c \u0442\u043e\u043f-5 \u043b\u0443\u0447\u0448\u0438\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043f\u043e \u043c\u043d\u0435\u043d\u0438\u044e <code>xgboost</code> \u0438 \u043e\u0431\u0443\u0447\u0438\u043c \u043d\u0430 \u043d\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u044c</li> <li>\u0412\u043e\u0437\u044c\u043c\u0435\u043c \u0442\u043e\u043f-5 \u0445\u0443\u0434\u0448\u0438\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0438 \u043e\u0431\u0443\u0447\u0438\u043c \u043d\u0430 \u043d\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u044c</li> </ol> <p>\u041e\u0436\u0438\u0434\u0430\u0435\u0442\u0441\u044f, \u0447\u0442\u043e \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0432 \u043f\u0435\u0440\u0432\u043e\u043c \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0435 \u0431\u0443\u0434\u0435\u0442 \u0441\u0438\u043b\u044c\u043d\u043e \u0432\u044b\u0448\u0435, \u0447\u0435\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u043e \u0432\u0442\u043e\u0440\u043e\u043c \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0435. \u0410\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u044b\u0435 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u044b \u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043c \u0441 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c\u044e, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e <code>shap</code>-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0438 <code>forward-backward-\u043e\u0442\u0431\u043e\u0440\u0430</code>.</p> In\u00a0[\u00a0]: Copied! <pre>def train_test_n_most_worst_important(n, sorted_idx, columns, model, X_train, y_train, X_test, y_test):\n    \"\"\"\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 n \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0438 \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445.\n\n    :param n: \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    :param sorted_idx: \u0438\u043d\u0434\u0435\u043a\u0441\u044b \u043e\u0442\u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u043f\u043e \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0430\u043d\u0438\u044e \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0435\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \n    :param columns: \u043a\u043e\u043b\u043e\u043d\u043a\u0438 (\u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438) \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u043c \u043f\u043e\u0440\u044f\u0434\u043a\u0435 \n    :param model: \u043c\u043e\u0434\u0435\u043b\u044c\n    :param X_train: \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 (\u0431\u0435\u0437 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439)\n    :param y_train: \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\n    :param X_test: \u0442\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 (\u0431\u0435\u0437 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439)\n    :param y_test: \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\n    :return: \u043a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0445 \u043a\u043e\u0440\u0442\u0435\u0436\u0435\u0439: \n             1: \n                - n \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n                - \u043c\u043e\u0434\u0435\u043b\u044c, \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0430\u044f \u043d\u0430 n \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445\n                - \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 (\u043c\u0435\u0442\u043a\u0438 \u043a\u043b\u0430\u0441\u0441\u043e\u0432)\n                - \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 (\u0432\u0435\u0440\u043e\u0447\u0442\u043d\u043e\u044f\u0442\u0438 \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043a \u043a\u043b\u0430\u0441\u0441\u0430\u043c)\n                - \u043c\u0435\u0442\u0440\u0438\u043a\u0438: F1-\u043c\u0435\u0440\u0430 \u0438 PR AUC\n            2: \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u044b\u0439 \u043d\u0430\u0431\u043e\u0440, \u043d\u043e \u0434\u043b\u044f n \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    \"\"\"\n    # \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\n    top_n_features = columns[sorted_idx[:-n-1:-1]].tolist()\n    print(\n        \"{} \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\\n{}\".format(\n            n, top_n_features\n        )\n    )\n    model_top_n, pred_top_n, proba_top_n, all_metrics_top_n = model_train_test(\n        X_train[top_n_features], y_train,\n        X_test[top_n_features], y_test,\n        model,\n    )\n    print(\"-\"*100)\n    # \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\n    bottom_n_features = columns[sorted_idx[:n]].tolist()\n    print(\n        \"{} \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\\n{}\".format(\n            n, bottom_n_features\n        )\n    )\n    model_bottom_n, pred_bottom_n, proba_bottom_n, all_metrics_bottom_n = model_train_test(\n        X_train[bottom_n_features], y_train,\n        X_test[bottom_n_features], y_test,\n        model,\n    )\n    return (\n        (\n           top_n_features,\n           model_top_n,\n           pred_top_n,\n           proba_top_n,\n           all_metrics_top_n\n       ), (\n            bottom_n_features,\n            model_bottom_n,\n            pred_bottom_n,\n            proba_bottom_n,\n            all_metrics_bottom_n\n        )\n    )\n</pre> def train_test_n_most_worst_important(n, sorted_idx, columns, model, X_train, y_train, X_test, y_test):     \"\"\"\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 n \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0438 \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445.      :param n: \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432     :param sorted_idx: \u0438\u043d\u0434\u0435\u043a\u0441\u044b \u043e\u0442\u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u043f\u043e \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0430\u043d\u0438\u044e \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0435\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432      :param columns: \u043a\u043e\u043b\u043e\u043d\u043a\u0438 (\u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438) \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u043c \u043f\u043e\u0440\u044f\u0434\u043a\u0435      :param model: \u043c\u043e\u0434\u0435\u043b\u044c     :param X_train: \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 (\u0431\u0435\u0437 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439)     :param y_train: \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438     :param X_test: \u0442\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 (\u0431\u0435\u0437 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439)     :param y_test: \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438     :return: \u043a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0445 \u043a\u043e\u0440\u0442\u0435\u0436\u0435\u0439:               1:                  - n \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432                 - \u043c\u043e\u0434\u0435\u043b\u044c, \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0430\u044f \u043d\u0430 n \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445                 - \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 (\u043c\u0435\u0442\u043a\u0438 \u043a\u043b\u0430\u0441\u0441\u043e\u0432)                 - \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 (\u0432\u0435\u0440\u043e\u0447\u0442\u043d\u043e\u044f\u0442\u0438 \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043a \u043a\u043b\u0430\u0441\u0441\u0430\u043c)                 - \u043c\u0435\u0442\u0440\u0438\u043a\u0438: F1-\u043c\u0435\u0440\u0430 \u0438 PR AUC             2: \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u044b\u0439 \u043d\u0430\u0431\u043e\u0440, \u043d\u043e \u0434\u043b\u044f n \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432     \"\"\"     # \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438     top_n_features = columns[sorted_idx[:-n-1:-1]].tolist()     print(         \"{} \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\\n{}\".format(             n, top_n_features         )     )     model_top_n, pred_top_n, proba_top_n, all_metrics_top_n = model_train_test(         X_train[top_n_features], y_train,         X_test[top_n_features], y_test,         model,     )     print(\"-\"*100)     # \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438     bottom_n_features = columns[sorted_idx[:n]].tolist()     print(         \"{} \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\\n{}\".format(             n, bottom_n_features         )     )     model_bottom_n, pred_bottom_n, proba_bottom_n, all_metrics_bottom_n = model_train_test(         X_train[bottom_n_features], y_train,         X_test[bottom_n_features], y_test,         model,     )     return (         (            top_n_features,            model_top_n,            pred_top_n,            proba_top_n,            all_metrics_top_n        ), (             bottom_n_features,             model_bottom_n,             pred_bottom_n,             proba_bottom_n,             all_metrics_bottom_n         )     ) In\u00a0[\u00a0]: Copied! <pre># \u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0438\u0437 \u0430\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u0431\u0443\u0434\u0443\u0442 \u043c\u0435\u043d\u044f\u0442\u044c\u0441\u044f \u043f\u0440\u0438 \u0432\u044b\u0437\u043e\u0432\u0430\u0445 \n# \u0444\u0443\u043d\u043a\u0446\u0438\u0438 `train_test_n_most_worst_important`\n\nCONFIG = {\n    \"n\": 5,\n    \"columns\": X_train.columns, \n    \"model\": XGBClassifier(), \n    \"X_train\": X_train, \n    \"y_train\": y_train, \n    \"X_test\": X_test, \n    \"y_test\": y_test,\n}\n</pre> # \u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0438\u0437 \u0430\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u0431\u0443\u0434\u0443\u0442 \u043c\u0435\u043d\u044f\u0442\u044c\u0441\u044f \u043f\u0440\u0438 \u0432\u044b\u0437\u043e\u0432\u0430\u0445  # \u0444\u0443\u043d\u043a\u0446\u0438\u0438 `train_test_n_most_worst_important`  CONFIG = {     \"n\": 5,     \"columns\": X_train.columns,      \"model\": XGBClassifier(),      \"X_train\": X_train,      \"y_train\": y_train,      \"X_test\": X_test,      \"y_test\": y_test, } In\u00a0[\u00a0]: Copied! <pre>_, fi_xgb = get_model_name_fi(model_xgb, fi_type=\"builded\")\nsorted_idx_xgb = np.argsort(fi_xgb)\n\nbest_xgb, worst_xgb = train_test_n_most_worst_important(\n    sorted_idx=sorted_idx_xgb, \n    **CONFIG\n)\n</pre> _, fi_xgb = get_model_name_fi(model_xgb, fi_type=\"builded\") sorted_idx_xgb = np.argsort(fi_xgb)  best_xgb, worst_xgb = train_test_n_most_worst_important(     sorted_idx=sorted_idx_xgb,      **CONFIG ) <pre>5 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\n['Glucose', 'BMI', 'Age', 'Pregnancies', 'BloodPressure']\nXGBClassifier()\n\nF1-\u043c\u0435\u0440\u0430 = 0.643\nPR AUC = 0.68\n----------------------------------------------------------------------------------------------------\n5 \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\n['SkinThickness', 'Insulin', 'DiabetesPedigreeFunction', 'BloodPressure', 'Pregnancies']\nXGBClassifier()\n\nF1-\u043c\u0435\u0440\u0430 = 0.368\nPR AUC = 0.473\n</pre> In\u00a0[\u00a0]: Copied! <pre>_, fi_xgb_shap = get_model_name_fi(model_xgb, fi_type=\"shap\", X_train=X_train)\n\n# \u0412 `fi_xgb_shap` \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u0442.\u0435. \n# \u0440\u0430\u0437\u043c\u0435\u0440 `fi_xgb_shap`: &lt;\u043a\u043e\u043b-\u0432\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0435&gt; x &lt;\u043a\u043e\u043b-\u0432\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432&gt;. \n# \u041f\u0440\u0438\u0447\u0435\u043c \u0435\u0441\u043b\u0438 \u0447\u0430\u0441\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u043d\u0430\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \n# \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u043f\u043e \u043c\u043e\u0434\u0443\u043b\u044e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 -- \u044d\u0442\u043e \u0447\u0438\u0441\u043b\u043e \u0438 \u0431\u0443\u0434\u0435\u0442 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c\u044e \u0442\u043e\u0433\u043e \u0438\u043b\u0438 \u0438\u043d\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430. \n\nfi_xgb_shap = np.abs(fi_xgb_shap).mean(axis=0)\n\nsorted_idx_xgb_shap = np.argsort(fi_xgb_shap)\n\nbest_xgb_shap, worst_xgb_shap = train_test_n_most_worst_important(\n    sorted_idx=sorted_idx_xgb_shap, \n    **CONFIG\n)\n</pre> _, fi_xgb_shap = get_model_name_fi(model_xgb, fi_type=\"shap\", X_train=X_train)  # \u0412 `fi_xgb_shap` \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u0442.\u0435.  # \u0440\u0430\u0437\u043c\u0435\u0440 `fi_xgb_shap`: &lt;\u043a\u043e\u043b-\u0432\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0435&gt; x &lt;\u043a\u043e\u043b-\u0432\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432&gt;.  # \u041f\u0440\u0438\u0447\u0435\u043c \u0435\u0441\u043b\u0438 \u0447\u0430\u0441\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u043d\u0430\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430  # \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u043f\u043e \u043c\u043e\u0434\u0443\u043b\u044e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 -- \u044d\u0442\u043e \u0447\u0438\u0441\u043b\u043e \u0438 \u0431\u0443\u0434\u0435\u0442 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c\u044e \u0442\u043e\u0433\u043e \u0438\u043b\u0438 \u0438\u043d\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430.   fi_xgb_shap = np.abs(fi_xgb_shap).mean(axis=0)  sorted_idx_xgb_shap = np.argsort(fi_xgb_shap)  best_xgb_shap, worst_xgb_shap = train_test_n_most_worst_important(     sorted_idx=sorted_idx_xgb_shap,      **CONFIG ) <pre>5 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\n['Glucose', 'BMI', 'DiabetesPedigreeFunction', 'Pregnancies', 'Age']\nXGBClassifier()\n\nF1-\u043c\u0435\u0440\u0430 = 0.605\nPR AUC = 0.692\n----------------------------------------------------------------------------------------------------\n5 \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\n['SkinThickness', 'BloodPressure', 'Insulin', 'Age', 'Pregnancies']\nXGBClassifier()\n\nF1-\u043c\u0435\u0440\u0430 = 0.362\nPR AUC = 0.49\n</pre> In\u00a0[\u00a0]: Copied! <pre>from sklearn.feature_selection import SequentialFeatureSelector\n\nfbs = SequentialFeatureSelector(XGBClassifier(), n_features_to_select=5)\nfbs.fit(X_train, y_train)\ntop_5_features_xgb_fbs = X_train.columns[fbs.support_]\n\nprint(\"5 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\\n\", top_5_features_xgb_fbs)\n\nbest_xgb_fbs = model_train_test(\n    X_train[top_5_features_xgb_fbs], y_train,\n    X_test[top_5_features_xgb_fbs], y_test,\n    XGBClassifier(),\n)\n</pre> from sklearn.feature_selection import SequentialFeatureSelector  fbs = SequentialFeatureSelector(XGBClassifier(), n_features_to_select=5) fbs.fit(X_train, y_train) top_5_features_xgb_fbs = X_train.columns[fbs.support_]  print(\"5 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\\n\", top_5_features_xgb_fbs)  best_xgb_fbs = model_train_test(     X_train[top_5_features_xgb_fbs], y_train,     X_test[top_5_features_xgb_fbs], y_test,     XGBClassifier(), ) <pre>5 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\n Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'BMI'], dtype='object')\nXGBClassifier()\n\nF1-\u043c\u0435\u0440\u0430 = 0.593\nPR AUC = 0.668\n</pre> In\u00a0[\u00a0]: Copied! <pre># \u041f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 SequentialFeatureSelector \u043d\u0435 \u0432\u044b\u0434\u0430\u0435\u0442 \u0447\u0438\u0441\u043b\u0435\u043d\u043d\u0443\u044e \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c, \u0442\u043e \u0432\u044b\u0431\u0435\u0440\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \n# \u043d\u0435 \u043f\u043e\u043f\u0430\u0432\u0448\u0438\u0435 \u0432 \u0441\u043f\u0438\u0441\u043e\u043a \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043a\u0430\u043a \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0435\n\nbottom_5_features_xgb_fbs = X_train.columns[~fbs.support_] \n# \u0442\u0430\u043a\u0438\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0431\u0443\u0434\u0435\u0442 \u0432\u0441\u0435\u0433\u043e 3, \u043f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 5 \u0443\u0436\u0435 \u043e\u0442\u043e\u0431\u0440\u0430\u043d\u044b \u043a\u0430\u043a \u043b\u0443\u0447\u0448\u0438\u0435, \u0430 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0432\u0441\u0435\u0433\u043e 8 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n\nprint(\"3 \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430:\\n\", bottom_5_features_xgb_fbs)\n\nworst_xgb_fbs = model_train_test(\n    X_train[bottom_5_features_xgb_fbs], y_train,\n    X_test[bottom_5_features_xgb_fbs], y_test,\n    XGBClassifier(),\n)\n</pre> # \u041f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 SequentialFeatureSelector \u043d\u0435 \u0432\u044b\u0434\u0430\u0435\u0442 \u0447\u0438\u0441\u043b\u0435\u043d\u043d\u0443\u044e \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c, \u0442\u043e \u0432\u044b\u0431\u0435\u0440\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438,  # \u043d\u0435 \u043f\u043e\u043f\u0430\u0432\u0448\u0438\u0435 \u0432 \u0441\u043f\u0438\u0441\u043e\u043a \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043a\u0430\u043a \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0435  bottom_5_features_xgb_fbs = X_train.columns[~fbs.support_]  # \u0442\u0430\u043a\u0438\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0431\u0443\u0434\u0435\u0442 \u0432\u0441\u0435\u0433\u043e 3, \u043f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 5 \u0443\u0436\u0435 \u043e\u0442\u043e\u0431\u0440\u0430\u043d\u044b \u043a\u0430\u043a \u043b\u0443\u0447\u0448\u0438\u0435, \u0430 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0432\u0441\u0435\u0433\u043e 8 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432  print(\"3 \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430:\\n\", bottom_5_features_xgb_fbs)  worst_xgb_fbs = model_train_test(     X_train[bottom_5_features_xgb_fbs], y_train,     X_test[bottom_5_features_xgb_fbs], y_test,     XGBClassifier(), ) <pre>3 \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430:\n Index(['Insulin', 'DiabetesPedigreeFunction', 'Age'], dtype='object')\nXGBClassifier()\n\nF1-\u043c\u0435\u0440\u0430 = 0.493\nPR AUC = 0.591\n</pre> In\u00a0[\u00a0]: Copied! <pre>metrics_df = pd.DataFrame(\n    [\n        best_xgb[-1], worst_xgb[-1],\n        best_xgb_shap[-1], worst_xgb_shap[-1],\n        best_xgb_fbs[-1], worst_xgb_fbs[-1]\n    ], \n    index=[\n        \"XGBoost top 5 feats\", \"XGBoost bottom 5 feats\", \n        \"SHAP top 5 feats\", \"SHAP bottom 5 feats\", \n        \"FBS top 5 feats\", \"FBS bottom 5 feats\"\n    ]\n)\nmetrics_df \n</pre> metrics_df = pd.DataFrame(     [         best_xgb[-1], worst_xgb[-1],         best_xgb_shap[-1], worst_xgb_shap[-1],         best_xgb_fbs[-1], worst_xgb_fbs[-1]     ],      index=[         \"XGBoost top 5 feats\", \"XGBoost bottom 5 feats\",          \"SHAP top 5 feats\", \"SHAP bottom 5 feats\",          \"FBS top 5 feats\", \"FBS bottom 5 feats\"     ] ) metrics_df  Out[\u00a0]: F1-score PR AUC XGBoost top 5 feats 0.642857 0.679527 XGBoost bottom 5 feats 0.368421 0.473027 SHAP top 5 feats 0.604938 0.692427 SHAP bottom 5 feats 0.362416 0.490260 FBS top 5 feats 0.592593 0.668465 FBS bottom 5 feats 0.493333 0.590893 <p>\u041a\u0430\u043a \u0438 \u043e\u0436\u0438\u0434\u0430\u043b\u043e\u0441\u044c, \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043f\u0440\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0438 5 \u00ab\u0445\u0443\u0434\u0448\u0438\u0445\u00bb \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043d\u0438\u0436\u0435, \u0447\u0435\u043c \u043f\u0440\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0438 5 \u00ab\u043b\u0443\u0447\u0448\u0438\u0445\u00bb \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432. \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u00ab\u043b\u0443\u0447\u0448\u0438\u0435\u00bb \u0438 \u00ab\u0445\u0443\u0434\u0448\u0438\u0435\u00bb \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u043b\u044f \u0440\u0430\u0437\u043d\u044b\u0445 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u0438 \u0441\u0440\u0430\u0432\u043d\u0438\u0442\u044c, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u0435\u0440\u0435\u0441\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u044d\u0442\u0438 \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u0430.</p> In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(16, 8))\nplt.title(\"Top-5 features\")\nplt.hist(\n    [best_xgb[0], best_xgb_shap[0], top_5_features_xgb_fbs], \n    label=[\"Builded FI\", \"SHAP\", \"Forward-backward selection\"]\n)\nplt.xticks(rotation=45);\nplt.legend();\n</pre> plt.figure(figsize=(16, 8)) plt.title(\"Top-5 features\") plt.hist(     [best_xgb[0], best_xgb_shap[0], top_5_features_xgb_fbs],      label=[\"Builded FI\", \"SHAP\", \"Forward-backward selection\"] ) plt.xticks(rotation=45); plt.legend(); In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(16, 8))\nplt.title(\"Bottom-10 features\")\nplt.hist(\n    [worst_xgb[0], worst_xgb_shap[0], bottom_5_features_xgb_fbs],\n    label=[\"Builded FI\", \"SHAP\", \"Forward-backward selection\"]\n)\nplt.xticks(rotation=45);\nplt.legend();\n</pre> plt.figure(figsize=(16, 8)) plt.title(\"Bottom-10 features\") plt.hist(     [worst_xgb[0], worst_xgb_shap[0], bottom_5_features_xgb_fbs],     label=[\"Builded FI\", \"SHAP\", \"Forward-backward selection\"] ) plt.xticks(rotation=45); plt.legend(); <p>\u041a\u0430\u043a \u0432\u0438\u0434\u043d\u043e \u0438\u0437 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432 \u0432\u044b\u0448\u0435, \u043f\u0435\u0440\u0435\u0441\u0435\u0447\u0435\u043d\u0438\u044f \u0435\u0441\u0442\u044c, \u0438 \u0438\u0445 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043c\u043d\u043e\u0433\u043e.</p> In\u00a0[\u00a0]: Copied! <pre>metrics_df.sort_values(by=[\"F1-score\", \"PR AUC\"], ascending=False)\n</pre> metrics_df.sort_values(by=[\"F1-score\", \"PR AUC\"], ascending=False) Out[\u00a0]: F1-score PR AUC XGBoost top 5 feats 0.642857 0.679527 SHAP top 5 feats 0.604938 0.692427 FBS top 5 feats 0.592593 0.668465 FBS bottom 5 feats 0.493333 0.590893 XGBoost bottom 5 feats 0.368421 0.473027 SHAP bottom 5 feats 0.362416 0.490260 <p>\u0418\u0437 \u0442\u0430\u0431\u043b\u0438\u0446\u044b \u0432\u044b\u0448\u0435 \u0441\u043b\u0435\u0434\u0443\u0435\u0442, \u0447\u0442\u043e \u0432 \u043d\u0430\u0448\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u0435 \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u0430\u044e\u0442 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u044b\u0441\u043e\u043a\u0438\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438, \u043f\u043e\u0434\u043e\u0431\u0440\u0430\u043b\u0438 XGBoost  \u0441\u043e \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u043e\u0439 \u043e\u0446\u0435\u043d\u043a\u043e\u0439 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (\u0441\u0443\u0434\u044f \u043f\u043e F1-\u043c\u0435\u0440\u0435) \u0438 SHAP (\u0441\u0443\u0434\u044f \u043f\u043e PR AUC), \u043e\u0442\u0431\u043e\u0440 forward-backward \u043f\u043e\u043a\u0430\u0437\u0430\u043b \u0447\u0443\u0442\u044c \u043c\u0435\u043d\u0435\u0435 \u0445\u043e\u0440\u043e\u0448\u0438\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442. \u041e\u0434\u043d\u0430\u043a\u043e \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432 \u0432\u044b\u0431\u043e\u0440\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043f\u0440\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 top-5 \u0438\u043b\u0438 bottom-5  \u043f\u043e\u043a\u0430\u0437\u0430\u043b\u0438 \u043d\u0435 \u0441\u0438\u043b\u044c\u043d\u043e \u043e\u0442\u043b\u0438\u0447\u0430\u044e\u0449\u0438\u0435\u0441\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b.</p> <p>\u041f\u043e\u043c\u0438\u043c\u043e \u043f\u0440\u043e\u0447\u0435\u0433\u043e, \u0432 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0435 <code>shap</code> \u0435\u0441\u0442\u044c \u043c\u043d\u043e\u0433\u043e \u0445\u043e\u0440\u043e\u0448\u0438\u0445 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u043c\u043e\u0433\u0430\u044e\u0442 \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0432\u044b\u0432\u043e\u0434 \u043e \u043f\u0440\u0438\u0447\u0438\u043d\u0430\u0445 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0438 \u0442\u0435\u043c \u0441\u0430\u043c\u044b\u043c \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0438\u0440\u0443\u0435\u043c\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0435\u0439. \u041f\u0440\u0438\u0432\u0435\u0434\u0435\u043c \u043f\u0440\u0438\u043c\u0435\u0440\u044b \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0439:</p> In\u00a0[\u00a0]: Copied! <pre>explainer = shap.TreeExplainer(model_xgb)\nshap_values = explainer.shap_values(X_train)\nshap.summary_plot(shap_values, X_train, plot_type=\"bar\") # \u0440\u0430\u043d\u0435\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u044b\n</pre> explainer = shap.TreeExplainer(model_xgb) shap_values = explainer.shap_values(X_train) shap.summary_plot(shap_values, X_train, plot_type=\"bar\") # \u0440\u0430\u043d\u0435\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u044b <p>\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c, \u043a\u0430\u043a \u043a\u0430\u043a\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u0432\u043b\u0438\u044f\u044e\u0442 \u043d\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 (\u0433\u0440\u0430\u0444\u0438\u043a \u0432 \u044f\u0447\u0435\u0439\u043a\u0435 \u043d\u0438\u0436\u0435). \u041a\u0430\u0436\u0434\u0430\u044f \u0442\u043e\u0447\u043a\u0430 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0441\u043e\u0431\u043e\u0439 \u043e\u0434\u043d\u043e SHAP-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0438 \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430. \u041a\u0440\u0430\u0441\u043d\u044b\u0439 \u0446\u0432\u0435\u0442 \u0442\u043e\u0447\u043a\u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0432\u044b\u0441\u043e\u043a\u043e\u0439 \u0446\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043e\u0431\u044a\u0435\u043a\u0442\u0430, \u0441\u0438\u043d\u0438\u0439 \u2014 \u043d\u0438\u0437\u043a\u043e\u0439. \u041d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043a\u0440\u0430\u0441\u043d\u044b\u0445 \u0438 \u0441\u0438\u043d\u0438\u0445 \u0442\u043e\u0447\u0435\u043a \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043e \u0432\u043b\u0438\u044f\u043d\u0438\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430.</p> <p>\u0412 \u043d\u0430\u0448\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u044f\u0445 \u0447\u0435\u043c \u043d\u0438\u0436\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0438 \u0433\u043b\u044e\u043a\u043e\u0437\u044b, \u0442\u0435\u043c \u043d\u0438\u0436\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u0434\u0438\u0430\u0431\u0435\u0442\u0430, \u0438 \u0447\u0435\u043c \u0432\u044b\u0448\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044c \u0433\u043b\u044e\u043a\u043e\u0437\u044b, \u0442\u0435\u043c \u0432\u044b\u0448\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c, \u0447\u0442\u043e \u0443 \u043f\u0430\u0446\u0438\u0435\u043d\u0442\u0430 \u0434\u0438\u0430\u0431\u0435\u0442.</p> In\u00a0[\u00a0]: Copied! <pre>shap.summary_plot(shap_values, X_train)\n</pre> shap.summary_plot(shap_values, X_train) <p>\u0418\u0437 \u0433\u0440\u0430\u0444\u0438\u043a\u0430 \u0432\u044b\u0448\u0435 \u0442\u0430\u043a\u0436\u0435 \u0441\u043b\u0435\u0434\u0443\u0435\u0442, \u0447\u0442\u043e <code>Glucose</code> \u2014 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a.</p> <p>\u0422\u0430\u043a\u0436\u0435 \u043c\u043e\u0436\u043d\u043e \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c, \u043a\u0430\u043a \u0432\u044b\u0431\u0440\u0430\u043d\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0432\u043b\u0438\u044f\u044e\u0442 \u043d\u0430 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0428\u0435\u043f\u043b\u0438. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c, \u043a\u0430\u043a \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0433\u043b\u044e\u043a\u043e\u0437\u044b <code>Glucose</code> \u0432\u043b\u0438\u044f\u0435\u0442 \u043d\u0430 \u043d\u0430\u043b\u0438\u0447\u0438\u0435 \u0434\u0438\u0430\u0431\u0435\u0442\u0430. \u0414\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u0437\u0430\u0434\u0430\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a, \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0435 \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043c\u044b \u0445\u043e\u0442\u0438\u043c \u043f\u0440\u043e\u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u0443\u0436\u0435 \u0432\u044b\u0431\u0440\u0430\u043d\u043d\u043e\u0433\u043e \u043d\u0430\u043c\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 <code>Glucose</code>, \u043d\u0443\u0436\u043d\u043e \u043f\u0435\u0440\u0435\u0434\u0430\u0442\u044c \u0434\u0440\u0443\u0433\u043e\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430 <code>interaction_index</code> (\u0435\u0441\u043b\u0438 \u044d\u0442\u043e\u0442 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u043d\u0435 \u0431\u0443\u0434\u0435\u0442 \u0437\u0430\u0434\u0430\u043d, \u0442\u043e \u0434\u0440\u0443\u0433\u043e\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a \u0431\u0443\u0434\u0435\u0442 \u0432\u044b\u0431\u0440\u0430\u043d \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438).</p> <p>\u0413\u0440\u0430\u0444\u0438\u043a \u0438\u0437 \u044f\u0447\u0435\u0439\u043a\u0438 \u043d\u0438\u0436\u0435 \u0441\u0432\u0438\u0434\u0435\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u0443\u0435\u0442 \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0433\u043b\u044e\u043a\u043e\u0437\u044b \u0438\u043c\u0435\u0435\u0442 \u043c\u0435\u043d\u044c\u0448\u0435\u0435 \u0432\u043b\u0438\u044f\u043d\u0438\u0435 \u043d\u0430 \u0440\u0435\u0448\u0435\u043d\u0438\u0435 \u043e \u0434\u0438\u0430\u0431\u0435\u0442\u0435 \u043f\u0440\u0438 \u043c\u043e\u043b\u043e\u0434\u043e\u043c \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0435 (&lt; 30 \u043b\u0435\u0442).</p> In\u00a0[\u00a0]: Copied! <pre>shap.dependence_plot(\"Glucose\", shap_values, X_train, interaction_index=\"Age\")\n</pre> shap.dependence_plot(\"Glucose\", shap_values, X_train, interaction_index=\"Age\") In\u00a0[\u00a0]: Copied! <pre># \u0413\u0440\u0430\u0444\u0438\u043a\u0438 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c:\n\nfor feature in X_train.columns:\n    shap.dependence_plot(feature, shap_values, X_train, display_features=X_train) \n</pre> # \u0413\u0440\u0430\u0444\u0438\u043a\u0438 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c:  for feature in X_train.columns:     shap.dependence_plot(feature, shap_values, X_train, display_features=X_train)  <p>\u0414\u0430\u043d\u043d\u044b\u0439 \u0442\u0438\u043f \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043d\u0430 \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u044b\u0435 \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f: \u043a\u0430\u043a \u0440\u0430\u0437\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0432\u043b\u0438\u044f\u044e\u0442 \u043d\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438.</p> In\u00a0[\u00a0]: Copied! <pre># \u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f \u0432\u043e \u0432\u0441\u0435\u043c \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\n\nshap.initjs() # \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438\u043d\u0442\u0435\u0440\u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0433\u043e \u0433\u0440\u0430\u0444\u0438\u043a\u0430\nshap.force_plot(explainer.expected_value, shap_values, features=X_train) \n</pre> # \u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f \u0432\u043e \u0432\u0441\u0435\u043c \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445  shap.initjs() # \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438\u043d\u0442\u0435\u0440\u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0433\u043e \u0433\u0440\u0430\u0444\u0438\u043a\u0430 shap.force_plot(explainer.expected_value, shap_values, features=X_train)  Out[\u00a0]: Visualization omitted, Javascript library not loaded!   Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.  In\u00a0[\u00a0]: Copied! <pre># \u0414\u043b\u044f \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u0433\u043e \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f \u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445 \n# (\u0432 \u044d\u0442\u043e\u0439 \u044f\u0447\u0435\u0439\u043a\u0435 \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u043f\u0435\u0440\u0432\u043e\u0435 \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u0435)\n\nshap.initjs() # \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438\u043d\u0442\u0435\u0440\u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0433\u043e \u0433\u0440\u0430\u0444\u0438\u043a\u0430\nshap.force_plot(explainer.expected_value, shap_values[0, :], X_train.iloc[0])\n</pre> # \u0414\u043b\u044f \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u0433\u043e \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f \u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445  # (\u0432 \u044d\u0442\u043e\u0439 \u044f\u0447\u0435\u0439\u043a\u0435 \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u043f\u0435\u0440\u0432\u043e\u0435 \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u0435)  shap.initjs() # \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438\u043d\u0442\u0435\u0440\u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0433\u043e \u0433\u0440\u0430\u0444\u0438\u043a\u0430 shap.force_plot(explainer.expected_value, shap_values[0, :], X_train.iloc[0]) Out[\u00a0]: Visualization omitted, Javascript library not loaded!   Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.  <p>\u041d\u0430 \u0433\u0440\u0430\u0444\u0438\u043a\u0435 \u0432\u044b\u0448\u0435 <code>base_value</code> \u043f\u043e\u043c\u0435\u0447\u0435\u043d\u043e \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0435 \u043f\u0440\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438, \u0430 \u0436\u0438\u0440\u043d\u044b\u043c \u0432\u044b\u0434\u0435\u043b\u0435\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435.</p> In\u00a0[\u00a0]: Copied! <pre>shap.initjs() # \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438\u043d\u0442\u0435\u0440\u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0433\u043e \u0433\u0440\u0430\u0444\u0438\u043a\u0430\nshap.force_plot(explainer.expected_value, shap_values[1, :], X_train.iloc[1])\n</pre> shap.initjs() # \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438\u043d\u0442\u0435\u0440\u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0433\u043e \u0433\u0440\u0430\u0444\u0438\u043a\u0430 shap.force_plot(explainer.expected_value, shap_values[1, :], X_train.iloc[1]) Out[\u00a0]: Visualization omitted, Javascript library not loaded!   Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.  <p>\u041a\u043e\u043d\u0435\u0447\u043d\u043e, \u043d\u0438 \u043e\u0434\u0438\u043d \u0438\u0437 \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0435\u043c\u044b\u0445 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u043e\u0446\u0435\u043d\u043a\u0438 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043d\u0435 \u0438\u0434\u0435\u0430\u043b\u0435\u043d, \u043e\u0434\u043d\u0430\u043a\u043e, \u043a\u0430\u043a \u043f\u043e\u043a\u0430\u0437\u0430\u043b \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442, \u0432\u0441\u0435 \u044d\u0442\u0438 \u043c\u0435\u0442\u043e\u0434\u044b \u043f\u043e\u043c\u043e\u0433\u0430\u044e\u0442 \u0432\u044b\u0431\u0440\u0430\u0442\u044c \u0442\u0430\u043a\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u043f\u0440\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u044c \u0431\u0443\u0434\u0435\u0442 \u0445\u043e\u0440\u043e\u0448\u043e \u043e\u0431\u0443\u0447\u0430\u0442\u044c\u0441\u044f.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"portfolio/course_recsys/8_2_feature_importance.html","title":"\u0410\u043d\u0441\u0430\u043c\u0431\u043b\u0438 \u0440\u0435\u0448\u0430\u044e\u0449\u0438\u0445 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432: \u043e\u0446\u0435\u043d\u043a\u0430 \u0437\u043d\u0430\u0447\u0438\u043c\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\u00b6","text":""},{"location":"portfolio/course_recsys/8_2_feature_importance.html","title":"\u0422\u0435\u043e\u0440\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0447\u0430\u0441\u0442\u044c\u00b6","text":""},{"location":"portfolio/course_recsys/8_2_feature_importance.html","title":"\u041e\u0446\u0435\u043d\u043a\u0430 \u0437\u043d\u0430\u0447\u0438\u043c\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u0435\u0439 \u0440\u0435\u0448\u0430\u044e\u0449\u0438\u0445 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432\u00b6","text":"<p>\u041d\u0430 \u043f\u0440\u0430\u043a\u0442\u0438\u043a\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c\u0438 \u0441\u043f\u043e\u0441\u043e\u0431\u0430\u043c\u0438:</p> <ul> <li>\u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u044b\u0445 \u0444\u0443\u043d\u043a\u0446\u0438\u0439 \u0432 \u0442\u0443 \u0438\u043b\u0438 \u0438\u043d\u0443\u044e \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443. \u0412 \u0434\u0430\u043d\u043d\u043e\u043c \u0441\u0435\u043c\u0438\u043d\u0430\u0440\u0435 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0442\u044c \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u0438 \u0440\u0435\u0448\u0430\u044e\u0449\u0438\u0445 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a <code>xgboost</code> \u0438 <code>sklearn</code>, \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0443\u0436\u0435 \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u0430 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043f\u043e\u0441\u043b\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438</li> <li>\u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e SHAP-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439. \u041f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043e\u0437\u043d\u0430\u0447\u0430\u044e\u0442 \u00ab\u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435\u00bb \u0432\u043b\u0438\u044f\u043d\u0438\u0435 \u043d\u0430 \u043f\u0440\u043e\u0433\u043d\u043e\u0437, \u0442. \u0435. \u0447\u0442\u043e \u0434\u0430\u043d\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u0442 \u043c\u043e\u0434\u0435\u043b\u044c \u043a \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044e \u043a\u043b\u0430\u0441\u0441\u0430 $1$; \u043e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u2014 \u00ab\u043e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0435\u00bb \u0432\u043b\u0438\u044f\u043d\u0438\u0435 \u043d\u0430 \u043f\u0440\u043e\u0433\u043d\u043e\u0437 \u2014 \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u0442 \u043c\u043e\u0434\u0435\u043b\u044c \u043a \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044e \u043a\u043b\u0430\u0441\u0441\u0430 $0$. \u041f\u0440\u043e SHAP-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0431\u0443\u0434\u0435\u0442 \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u0430\u043d\u043e \u0434\u0430\u043b\u0435\u0435 \u0432 \u043a\u0443\u0440\u0441\u0435. \u0414\u043b\u044f \u0438\u0445 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443 <code>shap</code></li> <li>Forward, backward selection, forward-backward \u043e\u0442\u0431\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:<ul> <li>\u043f\u0440\u044f\u043c\u043e\u0439 \u043e\u0442\u0431\u043e\u0440 (Forward selection): \u043d\u0430\u0447\u0438\u043d\u0430\u0435\u043c \u0441 \u043f\u0443\u0441\u0442\u043e\u0433\u043e \u043d\u0430\u0431\u043e\u0440\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u0430 \u0437\u0430\u0442\u0435\u043c \u0438\u0442\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u043e \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u044e\u0449\u0438\u0435 \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u0439 \u043f\u0440\u0438\u0440\u043e\u0441\u0442 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043c\u043e\u0434\u0435\u043b\u0435\u0439</li> <li>\u043e\u0431\u0440\u0430\u0442\u043d\u044b\u0439 \u043e\u0442\u0431\u043e\u0440 (Backward selection): \u043d\u0430\u0447\u0438\u043d\u0430\u0435\u043c \u0441 \u043d\u0430\u0431\u043e\u0440\u0430, \u0441\u043e\u0441\u0442\u043e\u044f\u0449\u0435\u0433\u043e \u0438\u0437 \u0432\u0441\u0435\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u0434\u0430\u043b\u0435\u0435 \u043d\u0430 \u043a\u0430\u0436\u0434\u043e\u0439 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438 \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u00ab\u0445\u0443\u0434\u0448\u0438\u0439\u00bb \u043f\u0440\u0438\u0437\u043d\u0430\u043a</li> <li>Forward-backward selection: \u043e\u0441\u043d\u043e\u0432\u043d\u0430\u044f \u0438\u0434\u0435\u044f \u0437\u0430\u043a\u043b\u044e\u0447\u0430\u0435\u0442\u0441\u044f \u0432 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0438 \u0438\u043b\u0438 \u0443\u0434\u0430\u043b\u0435\u043d\u0438\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0438 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0438 \u0442\u043e\u0433\u043e, \u043a\u0430\u043a \u044d\u0442\u043e \u0443\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u0432\u043b\u0438\u044f\u0435\u0442 \u043d\u0430 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c \u0438\u043b\u0438 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0434\u0440\u0443\u0433\u0438\u0445 \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0435\u0432. \u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u043c\u0435\u0442\u043e\u0434\u0430 \u0432 <code>sklearn</code>. \u041f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u043e\u043d \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043b\u0438\u0448\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0432 \u043f\u043e\u0440\u044f\u0434\u043a\u0435 \u0438\u0445 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438, \u0442\u043e \u0431\u0443\u0434\u0435\u043c \u0441\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0442\u044c \u043e\u0442\u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0432 \u043f\u043e\u0440\u044f\u0434\u043a\u0435 \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0430\u043d\u0438\u044f/\u0443\u0431\u044b\u0432\u0430\u043d\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u0441\u043f\u0438\u0441\u043a\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0441 \u0434\u0430\u043d\u043d\u044b\u043c \u043c\u0435\u0442\u043e\u0434\u043e\u043c (\u0430 \u043f\u0435\u0440\u0432\u044b\u0435 \u0434\u0432\u0430 \u043c\u0435\u0442\u043e\u0434\u0430 \u043e\u0446\u0435\u043d\u043a\u0438 \u0437\u043d\u0430\u0447\u0438\u043c\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0431\u0443\u0434\u0435\u043c \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c)</li> </ul> </li> </ul>"},{"location":"portfolio/course_recsys/8_2_feature_importance.html","title":"\u041e\u0446\u0435\u043d\u043a\u0430 \u0437\u043d\u0430\u0447\u0438\u043c\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0432 \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u044f\u0445\u00b6","text":""},{"location":"portfolio/course_recsys/8_2_feature_importance.html","title":"\u041f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0447\u0430\u0441\u0442\u044c\u00b6","text":"<p>\u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043e\u0442\u0441\u044e\u0434\u0430. \u0414\u0430\u043d\u043d\u044b\u0435 \u0431\u044b\u043b\u0438 \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u041d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u043c \u0438\u043d\u0441\u0442\u0438\u0442\u0443\u0442\u043e\u043c \u0434\u0438\u0430\u0431\u0435\u0442\u0430, \u0431\u043e\u043b\u0435\u0437\u043d\u0435\u0439 \u043e\u0440\u0433\u0430\u043d\u043e\u0432 \u043f\u0438\u0449\u0435\u0432\u0430\u0440\u0435\u043d\u0438\u044f \u0438 \u043f\u043e\u0447\u0435\u043a. \u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c, \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0449\u0443\u044e, \u0435\u0441\u0442\u044c \u043b\u0438 \u0443 \u043f\u0430\u0446\u0438\u0435\u043d\u0442\u0430 \u0434\u0438\u0430\u0431\u0435\u0442, \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u044b\u0445 \u0434\u0438\u0430\u0433\u043d\u043e\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0438\u0437\u043c\u0435\u0440\u0435\u043d\u0438\u0439.</p>"},{"location":"portfolio/course_recsys/8_2_feature_importance.html","title":"\u0421\u043a\u0430\u0447\u0438\u0432\u0430\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":""},{"location":"portfolio/course_recsys/8_2_feature_importance.html","title":"\u0420\u0430\u0437\u0432\u0435\u0434\u043e\u0447\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u0438 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":""},{"location":"portfolio/course_recsys/8_2_feature_importance.html","title":"\u0414\u0435\u043b\u0435\u043d\u0438\u0435 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0443\u044e \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e\u00b6","text":""},{"location":"portfolio/course_recsys/8_2_feature_importance.html","title":"\u0412\u044b\u0431\u043e\u0440 \u043c\u0435\u0442\u0440\u0438\u043a \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430\u00b6","text":""},{"location":"portfolio/course_recsys/8_2_feature_importance.html","title":"\u0412\u044b\u0431\u043e\u0440, \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0435\u0439\u00b6","text":"<p>\u0411\u0443\u0434\u0435\u043c \u0441\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0442\u044c \u0432\u0441\u0435 \u0442\u0438\u043f\u044b \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u0435\u0439 \u043c\u043e\u0434\u0435\u043b\u0435\u0439, \u0432 \u043e\u0441\u043d\u043e\u0432\u0435 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043b\u0435\u0436\u0438\u0442 \u0440\u0435\u0448\u0430\u044e\u0449\u0435\u0435 \u0434\u0435\u0440\u0435\u0432\u043e.</p>"},{"location":"portfolio/course_recsys/8_2_feature_importance.html","title":"\u0412\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\u00b6","text":"<p>\u0418\u0437\u043d\u0430\u0447\u0430\u043b\u044c\u043d\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0428\u0435\u043f\u043b\u0438 (SHAP-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f, SHAP-values) \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u043b\u0438\u0441\u044c \u0432 \u0442\u0435\u043e\u0440\u0438\u0438 \u0438\u0433\u0440 \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u043d\u0438\u044f \u0432\u043a\u043b\u0430\u0434\u0430 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0437 \u0443\u0447\u0430\u0441\u0442\u043d\u0438\u043a\u043e\u0432 \u043a\u043e\u043e\u043f\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u043e\u0439 \u0438\u0433\u0440\u044b (\u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0438\u0433\u0440\u043e\u043a\u0438 \u0434\u0435\u0439\u0441\u0442\u0432\u0443\u044e\u0442 \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u043d\u043e).</p> <p>\u0411\u043e\u043b\u0435\u0435 \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u043e\u0435 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u043e \u0432 \u043b\u043e\u043d\u0433\u0440\u0438\u0434\u0435. \u0421 \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0442\u043e\u0447\u043a\u0438 \u0437\u0440\u0435\u043d\u0438\u044f \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0428\u0435\u043f\u043b\u0438 (\u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 <code>shap</code>) \u043c\u043e\u0433\u0443\u0442 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0442\u044c\u0441\u044f \u0434\u043b\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0442\u0430\u0431\u043b\u0438\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 (\u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0442, \u043a\u0430\u043a \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u0432\u043b\u0438\u044f\u044e\u0442 \u043d\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f), \u043f\u0440\u0438 \u0440\u0430\u0431\u043e\u0442\u0435 \u0441 \u0442\u0435\u043a\u0441\u0442\u0430\u043c\u0438 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043c\u043e\u0433\u0443\u0442 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0442\u044c\u0441\u044f \u0434\u043b\u044f \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u044f \u0432\u043b\u0438\u044f\u043d\u0438\u044f \u0441\u043b\u043e\u0432 \u043f\u0440\u0438 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432 \u043f\u043e \u044d\u043c\u043e\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u043c\u0443 \u043e\u043a\u0440\u0430\u0441\u0443) \u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c\u0438 (\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0428\u0435\u043f\u043b\u0438 \u0431\u0443\u0434\u0443\u0442 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c, \u043a\u0430\u043a \u043f\u043e\u0432\u043b\u0438\u044f\u043b \u043d\u0430 \u043f\u0440\u0438\u043d\u044f\u0442\u0438\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u044c\u044e \u0442\u043e\u0442 \u0438\u043b\u0438 \u0438\u043d\u043e\u0439 \u043f\u0438\u043a\u0441\u0435\u043b\u044c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f).</p>"},{"location":"portfolio/course_recsys/8_2_feature_importance.html#shap","title":"\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u0438\u0434\u044b \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432 \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 <code>shap</code>\u00b6","text":""},{"location":"portfolio/course_recsys/8_2_feature_importance.html#shapsummary_plot","title":"<code>shap.summary_plot</code>\u00b6","text":""},{"location":"portfolio/course_recsys/8_2_feature_importance.html#shapdependence_plot","title":"<code>shap.dependence_plot</code>\u00b6","text":""},{"location":"portfolio/course_recsys/8_2_feature_importance.html#shapforce_plot","title":"<code>shap.force_plot</code>\u00b6","text":""},{"location":"portfolio/course_recsys/AB_seminar.html","title":"AB seminar","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import ttest_ind  # independent student's t-test\nimport statsmodels.stats.power as smp\nimport seaborn as sns;sns.set(style='whitegrid')\nfrom sklearn.model_selection import StratifiedShuffleSplit\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from scipy.stats import ttest_ind  # independent student's t-test import statsmodels.stats.power as smp import seaborn as sns;sns.set(style='whitegrid') from sklearn.model_selection import StratifiedShuffleSplit In\u00a0[2]: Copied! <pre># \u0413\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0434\u0432\u0430 \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0440\u0430\u0437\u043d\u044b\u043c \u0441\u0440\u0435\u0434\u043d\u0438\u043c \u0438 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u043e\u0439 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0435\u0439\nnp.random.seed(42)\ngroup_a = np.random.normal(loc=25, scale=5, size=1000)\ngroup_b = np.random.normal(loc=26, scale=5, size=1000)\n</pre> # \u0413\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0434\u0432\u0430 \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0440\u0430\u0437\u043d\u044b\u043c \u0441\u0440\u0435\u0434\u043d\u0438\u043c \u0438 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u043e\u0439 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0435\u0439 np.random.seed(42) group_a = np.random.normal(loc=25, scale=5, size=1000) group_b = np.random.normal(loc=26, scale=5, size=1000) In\u00a0[3]: Copied! <pre>def plot_histogram_comparison(group_a,group_b):\n\n    # \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439\n    plt.figure(figsize=(5, 3))\n    sns.histplot(group_a, \n                 label='\u0433\u0440\u0443\u043f\u043f\u0430 A', \n                 kde=False,\n    #              edgecolor=None,\n                 color='blue',\n                 lw=1,\n                 fill=False,\n                 element='step')\n    ax = sns.histplot(group_b, \n                      label='\u0433\u0440\u0443\u043f\u043f\u0430 B', \n                      kde=False,\n    #                   edgecolor=None,\n                      color='red',\n                      lw=1,\n                      fill=False,\n                      element='step')\n    ax.grid(color='gray',linestyle='--',lw=1,alpha=0.2)\n    ax.tick_params(axis='both',labelsize=9)\n    plt.title('\u041f\u0440\u0438\u043c\u0435\u0440 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0433\u0440\u0443\u043f\u043f A \u0438 B')\n    plt.legend(fontsize=9)\n    sns.despine(left=True,top=True,right=True)\n    plt.show()\n    \nplot_histogram_comparison(group_a,group_b)\n</pre> def plot_histogram_comparison(group_a,group_b):      # \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439     plt.figure(figsize=(5, 3))     sns.histplot(group_a,                   label='\u0433\u0440\u0443\u043f\u043f\u0430 A',                   kde=False,     #              edgecolor=None,                  color='blue',                  lw=1,                  fill=False,                  element='step')     ax = sns.histplot(group_b,                        label='\u0433\u0440\u0443\u043f\u043f\u0430 B',                        kde=False,     #                   edgecolor=None,                       color='red',                       lw=1,                       fill=False,                       element='step')     ax.grid(color='gray',linestyle='--',lw=1,alpha=0.2)     ax.tick_params(axis='both',labelsize=9)     plt.title('\u041f\u0440\u0438\u043c\u0435\u0440 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0433\u0440\u0443\u043f\u043f A \u0438 B')     plt.legend(fontsize=9)     sns.despine(left=True,top=True,right=True)     plt.show()      plot_histogram_comparison(group_a,group_b) <p>\u0414\u043b\u044f <code>ttest_ind</code> \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e \u043d\u0443\u043b\u0435\u0432\u0430\u044f \u0433\u0438\u043f\u043e\u0442\u0438\u0437\u0430:</p> <ul> <li>\u041d\u0435\u0442 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0439 \u0440\u0430\u0437\u043d\u0438\u0446\u044b \u043c\u0435\u0436\u0434\u0443 \u0441\u0440\u0435\u0434\u043d\u0438\u043c\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0434\u0432\u0443\u0445 \u0433\u0440\u0443\u043f\u043f</li> <li>\u0415\u0441\u043b\u0438 <code>p&lt;0.05</code>; \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0435 \u0440\u0430\u0437\u043b\u0438\u0447\u0438\u0435 \u0435\u0441\u0442\u044c</li> </ul> In\u00a0[4]: Copied! <pre># \u041f\u0440\u043e\u0432\u043e\u0434\u0438\u043c independent student's t-test\nt_stat, p_value = ttest_ind(group_a, group_b)\nprint(f'T-test results: t-statistic = {round(t_stat,3)}, p-value = {round(p_value,6)}')\n</pre> # \u041f\u0440\u043e\u0432\u043e\u0434\u0438\u043c independent student's t-test t_stat, p_value = ttest_ind(group_a, group_b) print(f'T-test results: t-statistic = {round(t_stat,3)}, p-value = {round(p_value,6)}') <pre>T-test results: t-statistic = -5.69, p-value = 0.0\n</pre> In\u00a0[5]: Copied! <pre># \u0418\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\nalpha = 0.05\nif p_value &lt; alpha:\n    print('\u0420\u0430\u0437\u043b\u0438\u0447\u0438\u044f \u0441\u0442\u0430\u0442 \u0437\u043d\u0430\u0447\u0438\u043c\u044b')\nelse:\n    print('\u0420\u0430\u0437\u043b\u0438\u0447\u0438\u044f \u043d\u0435 \u0441\u0442\u0430\u0442 \u0437\u043d\u0430\u0447\u0438\u043c\u044b')\n</pre> # \u0418\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 alpha = 0.05 if p_value &lt; alpha:     print('\u0420\u0430\u0437\u043b\u0438\u0447\u0438\u044f \u0441\u0442\u0430\u0442 \u0437\u043d\u0430\u0447\u0438\u043c\u044b') else:     print('\u0420\u0430\u0437\u043b\u0438\u0447\u0438\u044f \u043d\u0435 \u0441\u0442\u0430\u0442 \u0437\u043d\u0430\u0447\u0438\u043c\u044b') <pre>\u0420\u0430\u0437\u043b\u0438\u0447\u0438\u044f \u0441\u0442\u0430\u0442 \u0437\u043d\u0430\u0447\u0438\u043c\u044b\n</pre> In\u00a0[6]: Copied! <pre>round(group_a.mean(),3), round(group_b.mean(),3)\n</pre> round(group_a.mean(),3), round(group_b.mean(),3) Out[6]: <pre>(25.097, 26.354)</pre> <p>\u041f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 group_b \u0432\u044b\u0448\u0435, \u0438 \u043c\u044b \u0437\u043d\u0430\u0435\u043c, \u0447\u0442\u043e \u0440\u0430\u0437\u043d\u0438\u0446\u0430 \u0441\u0442\u0430\u0442 \u0437\u043d\u0430\u0447\u0438\u043c\u0430, \u044d\u0442\u043e \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442, \u0447\u0442\u043e \u043c\u044b \u0441\u043c\u043e\u0433\u043b\u0438 \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0442\u044c \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0438\u043a\u043e\u0432.</p> In\u00a0[7]: Copied! <pre>np.random.seed(42)\nvalues = np.random.normal(loc=100, scale=20, size=1000)\n</pre> np.random.seed(42) values = np.random.normal(loc=100, scale=20, size=1000) In\u00a0[8]: Copied! <pre># \u041d\u0435\u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u0430\u044f \u0440\u0430\u043d\u0434\u043e\u043c\u0438\u0437\u0430\u0446\u0438\u044f\ngroup_a_indices = np.sort(values)[:500]\ngroup_b_indices = np.sort(values)[500:]\n</pre> # \u041d\u0435\u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u0430\u044f \u0440\u0430\u043d\u0434\u043e\u043c\u0438\u0437\u0430\u0446\u0438\u044f group_a_indices = np.sort(values)[:500] group_b_indices = np.sort(values)[500:] In\u00a0[9]: Copied! <pre>plot_histogram_comparison(group_a_indices,group_b_indices)\n</pre> plot_histogram_comparison(group_a_indices,group_b_indices) In\u00a0[10]: Copied! <pre># \u0414\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0430\ndata_a = pd.DataFrame({'conversion_rate': group_a_indices, 'group': 'A'})\ndata_b = pd.DataFrame({'conversion_rate': group_b_indices, 'group': 'B'})\ndata = pd.concat([data_a, data_b])\n</pre> # \u0414\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0430 data_a = pd.DataFrame({'conversion_rate': group_a_indices, 'group': 'A'}) data_b = pd.DataFrame({'conversion_rate': group_b_indices, 'group': 'B'}) data = pd.concat([data_a, data_b]) <p>\u041f\u0440\u043e\u0432\u043e\u0434\u0438\u043c <code>t-test</code> \u0434\u043b\u044f \u044d\u0442\u0438\u0445 \u0434\u0432\u0443\u0445 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043e\u043a \u043f\u043e\u043a\u0430\u0437\u0430\u043d\u044b \u0432\u044b\u0448\u0435</p> In\u00a0[11]: Copied! <pre># \u041f\u0440\u043e\u0432\u043e\u0434\u0438\u043c t-test\nt_stat, p_value = ttest_ind(data[data['group'] == 'A']['conversion_rate'],\n                            data[data['group'] == 'B']['conversion_rate'])\nprint(f'T-test results (incorrect randomization): t-statistic = {round(t_stat,3)}, p-value = {round(p_value,7)}')\n</pre> # \u041f\u0440\u043e\u0432\u043e\u0434\u0438\u043c t-test t_stat, p_value = ttest_ind(data[data['group'] == 'A']['conversion_rate'],                             data[data['group'] == 'B']['conversion_rate']) print(f'T-test results (incorrect randomization): t-statistic = {round(t_stat,3)}, p-value = {round(p_value,7)}')  <pre>T-test results (incorrect randomization): t-statistic = -41.565, p-value = 0.0\n</pre> In\u00a0[12]: Copied! <pre>np.random.seed(42)\n\n# \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u0430\u044f \u0440\u0430\u043d\u0434\u043e\u043c\u0438\u0437\u0430\u0446\u0438\u044f. \u0421\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0432 \u0433\u0440\u0443\u043f\u043f\u0443 A \u0438\u043b\u0438 \u0433\u0440\u0443\u043f\u043f\u0443 B\ndata['random_assignment'] = np.random.choice(['A', 'B'], size=len(data))\n</pre> np.random.seed(42)  # \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u0430\u044f \u0440\u0430\u043d\u0434\u043e\u043c\u0438\u0437\u0430\u0446\u0438\u044f. \u0421\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0432 \u0433\u0440\u0443\u043f\u043f\u0443 A \u0438\u043b\u0438 \u0433\u0440\u0443\u043f\u043f\u0443 B data['random_assignment'] = np.random.choice(['A', 'B'], size=len(data)) In\u00a0[13]: Copied! <pre>group_a_random = data[data['random_assignment'] == 'A'].conversion_rate.tolist()\ngroup_b_random = data[data['random_assignment'] == 'B'].conversion_rate.tolist()\nplot_histogram_comparison(group_a_random,group_b_random)\n</pre> group_a_random = data[data['random_assignment'] == 'A'].conversion_rate.tolist() group_b_random = data[data['random_assignment'] == 'B'].conversion_rate.tolist() plot_histogram_comparison(group_a_random,group_b_random) <p>\u0421\u043d\u043e\u0432\u0430 \u043f\u0440\u043e\u0432\u043e\u0434\u0438\u043c <code>t-test</code>, \u0434\u043b\u044f \u0432\u044b\u0448\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0445 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043e\u043a</p> In\u00a0[14]: Copied! <pre># \u041f\u0440\u043e\u0432\u043e\u0434\u0438\u043c t-test\nt_stat_correct, p_value_correct = ttest_ind(group_a_random,\n                                            group_b_random)\nprint(f'T-test results (correct randomization): t-statistic = {round(t_stat_correct,3)}, p-value = {round(p_value_correct,6)}')\n</pre> # \u041f\u0440\u043e\u0432\u043e\u0434\u0438\u043c t-test t_stat_correct, p_value_correct = ttest_ind(group_a_random,                                             group_b_random) print(f'T-test results (correct randomization): t-statistic = {round(t_stat_correct,3)}, p-value = {round(p_value_correct,6)}')  <pre>T-test results (correct randomization): t-statistic = -0.002, p-value = 0.99815\n</pre> <p>\u041f\u043e\u0441\u043b\u0435 \u0442\u043e\u0433\u043e, \u043a\u0430\u043a \u043c\u044b \u0432\u0437\u044f\u043b\u0438 \u0442\u043e\u0442 \u0436\u0435 \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0440\u0430\u043d\u0434\u043e\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043b\u0438 \u0435\u0433\u043e, <code>t-test</code> \u0437\u0430\u043c\u0435\u0442\u0438\u043b, \u0447\u0442\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u043e\u0435</p> In\u00a0[15]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom scipy.stats import shapiro, levene, ttest_ind, mannwhitneyu\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import warnings warnings.filterwarnings(\"ignore\") warnings.simplefilter(action='ignore', category=FutureWarning) from scipy.stats import shapiro, levene, ttest_ind, mannwhitneyu In\u00a0[16]: Copied! <pre>df = pd.read_csv('cookie_cats-261704-9894ec.csv')\ndf.head()\n</pre> df = pd.read_csv('cookie_cats-261704-9894ec.csv') df.head() Out[16]: userid version sum_gamerounds retention_1 retention_7 0 116 gate_30 3 False False 1 337 gate_30 38 True False 2 377 gate_40 165 True False 3 483 gate_40 1 False False 4 488 gate_40 179 True True <p>\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u043e \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u043e\u0431\u043e\u0438\u0445 \u0432\u0435\u0440\u0441\u0438\u0439</p> In\u00a0[17]: Copied! <pre>df['version'].value_counts()\n</pre> df['version'].value_counts() Out[17]: <pre>gate_40    45489\ngate_30    44700\nName: version, dtype: int64</pre> In\u00a0[18]: Copied! <pre>ax = df[df['version'] == 'gate_30']['sum_gamerounds'].hist(bins=30)\nax.grid(color='gray',linestyle='--',lw=1,alpha=0.2)\nax.tick_params(axis='both',labelsize=9)\nsns.despine(left=True,top=True,right=True)\nplt.show()\n</pre> ax = df[df['version'] == 'gate_30']['sum_gamerounds'].hist(bins=30) ax.grid(color='gray',linestyle='--',lw=1,alpha=0.2) ax.tick_params(axis='both',labelsize=9) sns.despine(left=True,top=True,right=True) plt.show() In\u00a0[19]: Copied! <pre># \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0438 \u0438\u0437 \u0431\u0443\u0434\u0435\u043c \u0441\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0442\u044c\ndf['sum_gamerounds'] = np.log(df['sum_gamerounds'] + 1)\n                  \n# \u043f\u0435\u0440\u0432\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430\nax = df[df['version'] == 'gate_30']['sum_gamerounds'].hist(bins=30)\nax.grid(color='gray',linestyle='--',lw=1,alpha=0.2)\nax.tick_params(axis='both',labelsize=9)\nsns.despine(left=True,top=True,right=True)\nplt.show()\n</pre> # \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0438 \u0438\u0437 \u0431\u0443\u0434\u0435\u043c \u0441\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0442\u044c df['sum_gamerounds'] = np.log(df['sum_gamerounds'] + 1)                    # \u043f\u0435\u0440\u0432\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 ax = df[df['version'] == 'gate_30']['sum_gamerounds'].hist(bins=30) ax.grid(color='gray',linestyle='--',lw=1,alpha=0.2) ax.tick_params(axis='both',labelsize=9) sns.despine(left=True,top=True,right=True) plt.show() In\u00a0[20]: Copied! <pre># \u0432\u0442\u043e\u0440\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 \nax = df[df['version'] == 'gate_40']['sum_gamerounds'].hist(bins=30)\nax.grid(color='gray',linestyle='--',lw=1,alpha=0.2)\nax.tick_params(axis='both',labelsize=9)\nsns.despine(left=True,top=True,right=True)\nplt.show()\n</pre> # \u0432\u0442\u043e\u0440\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430  ax = df[df['version'] == 'gate_40']['sum_gamerounds'].hist(bins=30) ax.grid(color='gray',linestyle='--',lw=1,alpha=0.2) ax.tick_params(axis='both',labelsize=9) sns.despine(left=True,top=True,right=True) plt.show() In\u00a0[24]: Copied! <pre>def AB_Test(dataframe, group, target, A, B):\n\n    # \u0420\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 A/B\n    groupA = dataframe[dataframe[group] == A][target]\n    groupB = dataframe[dataframe[group] == B][target]\n    \n    \"\"\"\n    \n    Shapiro-Wilk for normality\n    \n    false : normal (p &gt; 0.05)\n    \n    \"\"\"\n\n    # \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f\n    ntA = shapiro(groupA)[1] &lt; 0.05\n    ntB = shapiro(groupB)[1] &lt; 0.05\n    # H0: \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0435! - False\n    # H1: \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u0435\u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0435! - True\n\n    # \"H0: \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0435\"\n    if (ntA == False) &amp; (ntB == False): \n        \n        \"\"\"\n        \n        # \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0442\u0435\u0441\u0442\n        # \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0440\u0430\u0432\u0435\u043d\u0441\u0442\u0432\u0430 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0439\n        \n        \"\"\"\n        \n        leveneTest = levene(groupA, groupB)[1] &lt; 0.05\n        # H0: \u0414\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438 \u0440\u0430\u0432\u043d\u044b: False\n        # H1: \u0414\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438 \u043d\u0435\u0440\u0430\u0432\u043d\u044b: True\n\n        if leveneTest == False:\n            \n            \"\"\"\n            \n            t-test w/ [equal_var] (variances are equal)\n            \n            \"\"\"\n            \n            # \u0414\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438 \u0440\u0430\u0432\u043d\u044b\n            ttest = ttest_ind(groupA, groupB, equal_var=True)[1]\n            # H0: M1 == M2 - False\n            # H1: M1 != M2 - True\n        else:\n            \n            \"\"\"\n            \n            t-test w/o [qual_var] (variances aren't equal)\n            \n            \"\"\"\n            \n            # \u0414\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438 \u043d\u0435\u0440\u0430\u0432\u043d\u044b\n            ttest = ttest_ind(groupA, groupB, equal_var=False)[1]\n            # H0: M1 == M2 - False\n            # H1: M1 != M2 - True\n    else:\n        \n        \"\"\"\n        \n        Mann Whitney Test (for non normally distributed)\n        \n        \"\"\"\n        \n        # \u041d\u0435\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0442\u0435\u0441\u0442\n        ttest = mannwhitneyu(groupA, groupB)[1]\n        # H0: \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u04351 == \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u04352 - False\n        # H1: \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u04351 != \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u04352 - True\n\n    # \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b\n    temp = pd.DataFrame({\n        \"AB Hypothesis\":[ttest &lt; 0.05],\n        \"p-value\":[ttest]\n    })\n    \n    # if normal -&gt; parametric\n    temp[\"Test Type\"] = np.where((ntA == False) &amp; (ntB == False), \"Parametric\", \"Non-Parametric\")\n    \n    # parametric (student's t-test) /non-parametric (mannwhitney)\n    temp[\"AB Hypothesis\"] = np.where(temp[\"AB Hypothesis\"] == False, \"Fail to Reject H0\", \"Reject H0\")\n    temp[\"Comment\"] = np.where(temp[\"AB Hypothesis\"] == \"Fail to Reject H0\", \"A/B groups are similar!\", \"A/B groups are not similar!\")\n\n    # Normally distributed \n    if (ntA == False) &amp; (ntB == False):\n        temp[\"Homogeneity\"] = np.where(leveneTest == False, \"Yes\", \"No\")\n        temp = temp[[\"Test Type\",\n                     \"Homogeneity\",\n                     \"AB Hypothesis\", \n                     \"p-value\",\n                     \"Comment\"]]\n    else:\n        temp = temp[[\"Test Type\",\"AB Hypothesis\", \"p-value\", \"Comment\"]]\n\n    print(\"# A/B Testing Hypothesis\")\n    print(\"H0: A == B\")\n    print(\"H1: A != B\", \"\\n\")\n\n    return temp\n\n# Apply A/B Testing\nAB_Test(dataframe=df, group=\"version\", target=\"sum_gamerounds\", A='gate_30', B='gate_40')\n</pre> def AB_Test(dataframe, group, target, A, B):      # \u0420\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 A/B     groupA = dataframe[dataframe[group] == A][target]     groupB = dataframe[dataframe[group] == B][target]          \"\"\"          Shapiro-Wilk for normality          false : normal (p &gt; 0.05)          \"\"\"      # \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f     ntA = shapiro(groupA)[1] &lt; 0.05     ntB = shapiro(groupB)[1] &lt; 0.05     # H0: \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0435! - False     # H1: \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u0435\u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0435! - True      # \"H0: \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0435\"     if (ntA == False) &amp; (ntB == False):                   \"\"\"                  # \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0442\u0435\u0441\u0442         # \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0440\u0430\u0432\u0435\u043d\u0441\u0442\u0432\u0430 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0439                  \"\"\"                  leveneTest = levene(groupA, groupB)[1] &lt; 0.05         # H0: \u0414\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438 \u0440\u0430\u0432\u043d\u044b: False         # H1: \u0414\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438 \u043d\u0435\u0440\u0430\u0432\u043d\u044b: True          if leveneTest == False:                          \"\"\"                          t-test w/ [equal_var] (variances are equal)                          \"\"\"                          # \u0414\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438 \u0440\u0430\u0432\u043d\u044b             ttest = ttest_ind(groupA, groupB, equal_var=True)[1]             # H0: M1 == M2 - False             # H1: M1 != M2 - True         else:                          \"\"\"                          t-test w/o [qual_var] (variances aren't equal)                          \"\"\"                          # \u0414\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438 \u043d\u0435\u0440\u0430\u0432\u043d\u044b             ttest = ttest_ind(groupA, groupB, equal_var=False)[1]             # H0: M1 == M2 - False             # H1: M1 != M2 - True     else:                  \"\"\"                  Mann Whitney Test (for non normally distributed)                  \"\"\"                  # \u041d\u0435\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0442\u0435\u0441\u0442         ttest = mannwhitneyu(groupA, groupB)[1]         # H0: \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u04351 == \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u04352 - False         # H1: \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u04351 != \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u04352 - True      # \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b     temp = pd.DataFrame({         \"AB Hypothesis\":[ttest &lt; 0.05],         \"p-value\":[ttest]     })          # if normal -&gt; parametric     temp[\"Test Type\"] = np.where((ntA == False) &amp; (ntB == False), \"Parametric\", \"Non-Parametric\")          # parametric (student's t-test) /non-parametric (mannwhitney)     temp[\"AB Hypothesis\"] = np.where(temp[\"AB Hypothesis\"] == False, \"Fail to Reject H0\", \"Reject H0\")     temp[\"Comment\"] = np.where(temp[\"AB Hypothesis\"] == \"Fail to Reject H0\", \"A/B groups are similar!\", \"A/B groups are not similar!\")      # Normally distributed      if (ntA == False) &amp; (ntB == False):         temp[\"Homogeneity\"] = np.where(leveneTest == False, \"Yes\", \"No\")         temp = temp[[\"Test Type\",                      \"Homogeneity\",                      \"AB Hypothesis\",                       \"p-value\",                      \"Comment\"]]     else:         temp = temp[[\"Test Type\",\"AB Hypothesis\", \"p-value\", \"Comment\"]]      print(\"# A/B Testing Hypothesis\")     print(\"H0: A == B\")     print(\"H1: A != B\", \"\\n\")      return temp  # Apply A/B Testing AB_Test(dataframe=df, group=\"version\", target=\"sum_gamerounds\", A='gate_30', B='gate_40') <pre># A/B Testing Hypothesis\nH0: A == B\nH1: A != B \n\n</pre> Out[24]: Test Type AB Hypothesis p-value Comment 0 Non-Parametric Fail to Reject H0 0.050209 A/B groups are similar!"},{"location":"portfolio/course_recsys/AB_seminar.html#ab-testing-seminar","title":"A/B Testing Seminar\u00b6","text":""},{"location":"portfolio/course_recsys/AB_seminar.html","title":"\u0413\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u00b6","text":"<ul> <li>\u0413\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435; \u043e\u0434\u043d\u0443 \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u0441 \u0441\u0440\u0435\u0434\u043d\u0435\u043c \u0437\u043d\u0430\u0447\u043a\u043d\u0438\u0435\u043c 25 \u0438 \u0434\u0440\u0443\u0433\u0443\u044e \u0441 26</li> <li>\u0414\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u043f\u0440\u0438\u043c\u0435\u0440\u0430 \u044d\u0442\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0430\u044e\u0442 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0438\u043a\u043e\u0432</li> </ul>"},{"location":"portfolio/course_recsys/AB_seminar.html","title":"\u0412\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u0440\u0430\u043d\u0434\u043e\u043c\u0438\u0437\u0430\u0446\u0438\u0438\u00b6","text":""},{"location":"portfolio/course_recsys/AB_seminar.html#a","title":"(a) \u041f\u0440\u0438\u043c\u0435\u0440 \u043d\u0435\u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\u0439 \u0440\u0430\u043d\u0434\u043e\u043c\u0438\u0437\u0430\u0446\u0438\u0438\u00b6","text":"<ul> <li>\u041f\u0440\u0435\u0434\u043f\u043e\u043b\u043e\u0436\u0438\u043c, \u0432\u0435\u0431-\u0441\u0430\u0439\u0442 \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u043d\u043e\u0439 \u043a\u043e\u043c\u043c\u0435\u0440\u0446\u0438\u0438 \u0445\u043e\u0447\u0435\u0442 \u043f\u0440\u043e\u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0434\u0432\u0435 \u0440\u0430\u0437\u043d\u044b\u0435 \u0432\u0435\u0440\u0441\u0438\u0438 \u0441\u0432\u043e\u0435\u0439 \u0434\u043e\u043c\u0430\u0448\u043d\u0435\u0439 \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u044b, \u0447\u0442\u043e\u0431\u044b \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c, \u043a\u0430\u043a\u0430\u044f \u0438\u0437 \u043d\u0438\u0445 \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u0442 \u043a \u0431\u043e\u043b\u0435\u0435 \u0432\u044b\u0441\u043e\u043a\u043e\u043c\u0443 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u0443 \u043a\u043e\u043d\u0432\u0435\u0440\u0441\u0438\u0438.</li> <li>\u0411\u0435\u0437 \u043d\u0430\u0434\u043b\u0435\u0436\u0430\u0449\u0435\u0439 \u0440\u0430\u043d\u0434\u043e\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u043e\u043d\u0438 \u0440\u0435\u0448\u0430\u044e\u0442 \u043d\u0430\u0437\u043d\u0430\u0447\u0430\u0442\u044c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u043c\u0435\u0442\u043e\u043a \u0438\u0445 \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0430\u0446\u0438\u0438.</li> <li>\u041f\u0435\u0440\u0432\u044b\u0435 50 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043d\u0430\u0437\u043d\u0430\u0447\u0430\u044e\u0442\u0441\u044f \u0432\u0435\u0440\u0441\u0438\u0438 A, \u0430 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0435 50 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u2014 \u0432\u0435\u0440\u0441\u0438\u0438 B.</li> </ul> <p>\u041f\u0440\u043e\u0431\u043b\u0435\u043c\u044b \u0441 \u043d\u0435\u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\u0439 \u0440\u0430\u043d\u0434\u043e\u043c\u0438\u0437\u0430\u0446\u0438\u0435\u0439:</p> <ul> <li>\u041f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438, \u0437\u0430\u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u0432\u0448\u0438\u0435\u0441\u044f \u0440\u0430\u043d\u044c\u0448\u0435, \u043c\u043e\u0433\u0443\u0442 \u0438\u043c\u0435\u0442\u044c \u0434\u0440\u0443\u0433\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u044f, \u0447\u0435\u043c \u0442\u0435, \u043a\u0442\u043e \u0437\u0430\u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043b\u0441\u044f \u043f\u043e\u0437\u0436\u0435.</li> <li>\u0421\u0435\u0437\u043e\u043d\u043d\u044b\u0435 \u0442\u0435\u043d\u0434\u0435\u043d\u0446\u0438\u0438 \u0438\u043b\u0438 \u0432\u043d\u0435\u0448\u043d\u0438\u0435 \u0441\u043e\u0431\u044b\u0442\u0438\u044f \u043c\u043e\u0433\u0443\u0442 \u043f\u043e\u0432\u043b\u0438\u044f\u0442\u044c \u043d\u0430 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0430\u0446\u0438\u0438, \u0441\u043e\u0437\u0434\u0430\u0432\u0430\u044f \u043c\u0435\u0448\u0430\u044e\u0449\u0438\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435.</li> <li>\u0413\u0440\u0443\u043f\u043f\u044b \u043c\u043e\u0433\u0443\u0442 \u043d\u0435 \u0431\u044b\u0442\u044c \u0440\u0435\u043f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0442\u0438\u0432\u043d\u044b\u043c\u0438 \u0434\u043b\u044f \u043e\u0431\u0449\u0435\u0433\u043e \u0447\u0438\u0441\u043b\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439.</li> </ul>"},{"location":"portfolio/course_recsys/AB_seminar.html#b-sampling","title":"(b) \u041f\u0440\u0438\u043c\u0435\u0440 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\u0433\u043e sampling\u00b6","text":"<p>\u041c\u044b \u0432\u0438\u0434\u0438\u043c, \u0447\u0442\u043e \u0445\u043e\u0442\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438 \u0432\u0437\u044f\u0442\u044b \u0438\u0437 \u043e\u0434\u043d\u043e\u0433\u043e \u0438 \u0442\u043e\u0433\u043e \u0436\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f, \u0437\u0430 \u0441\u0447\u0435\u0442 \u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0438 \u0438\u0445 \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043d\u0430\u0441\u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0442\u043b\u0438\u0446\u0430\u044e\u0442\u0441\u044f, \u0447\u0442\u043e t-test \u0443\u0432\u0435\u0440\u0435\u043d, \u0447\u0442\u043e \u044d\u0442\u043e \u0440\u0430\u0437\u043d\u044b\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f.</p>"},{"location":"portfolio/course_recsys/AB_seminar.html#ab","title":"A/B \u0442\u0435\u0441\u0442\u044b \u043d\u0430 \u0440\u0435\u0430\u043b\u044c\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":""},{"location":"portfolio/course_recsys/AB_seminar.html","title":"\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043f\u0440\u0438\u043c\u0435\u0440 AB-\u0442\u0435\u0441\u0442\u043e\u0432 \u043d\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u0438\u0433\u0440\u044b.</p> <ul> <li>\u0414\u0430\u043d\u043d\u044b\u0435 \u0440\u0430\u0437\u043d\u044b\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u0438\u0445 \u043f\u0440\u043e\u0438\u0434\u0435\u043d\u043d\u044b\u0445 \u0440\u0430\u0443\u043d\u0434\u043e\u0432</li> <li>\u0414\u0430\u043d\u043d\u044b\u0435 \u0443 \u043d\u0430\u0441 \u0434\u043b\u044f \u0434\u0432\u0435 \u0432\u0435\u0440\u0441\u0438\u0439 \u0438\u0433\u0440\u044b <code>gate_30</code> \u0438 <code>gate_40</code></li> </ul>"},{"location":"portfolio/course_recsys/AB_seminar.html#gate_30","title":"\u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 <code>gate_30</code>\u00b6","text":"<p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432\u0435\u0440\u0441\u0438\u0439 <code>gate_30</code></p>"},{"location":"portfolio/course_recsys/AB_seminar.html#gate_40","title":"\u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 <code>gate_40</code>\u00b6","text":""},{"location":"portfolio/course_recsys/AB_seminar.html","title":"\u0421\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u0430\u044f \u0441\u0445\u0435\u043c\u0430 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0433\u0438\u043f\u043e\u0442\u0435\u0437\u00b6","text":"<p>\u0422\u0435\u0441\u0442 <code>\u0428\u0430\u043f\u0438\u0440\u043e</code> \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438 (<code>stats.shapiro</code>)</p> <ul> <li>H0 : \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0435 <code>p &gt; 0.05</code></li> </ul> <p>\u0422\u0435\u0441\u0442 <code>\u041b\u0435\u0432\u0435\u043d\u0430</code> \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u0440\u0430\u0432\u0435\u043d\u0441\u0442\u0432\u0430 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0439 (<code>stats.levene</code>)</p> <ul> <li>H0 : \u0414\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438 \u0440\u0430\u0432\u043d\u044b: <code>p &gt; 0.05</code></li> </ul> <ul> <li>\u0415\u0441\u043b\u0438 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0435 + \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438 \u0440\u0430\u0432\u043d\u044b -&gt; \u0442\u0435\u0441\u0442 <code>T-Test</code> (equal_var=True)</li> <li>\u0415\u0441\u043b\u0438 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0435 + \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438 \u043d\u0435 \u0440\u0430\u0432\u043d\u044b -&gt; \u0442\u0435\u0441\u0442 <code>T-Test</code> (equal_var=False)</li> <li>\u0415\u0441\u043b\u0438 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u0435\u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0435 -&gt; \u0442\u0435\u0441\u0442 <code>\u041c\u0430\u043d\u043d\u0430-\u0423\u0438\u0442\u043d\u0438</code> (<code>stats.mannwhitneyu</code>)</li> </ul>"},{"location":"portfolio/course_recsys/Look-a-like_Segmentation.html","title":"Look a like Segmentation","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport random\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder\nfrom sklearn.utils import shuffle\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# \u0423\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u043d\u0430\u0441\u0442\u0440\u043e\u0435\u043a \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0432\u0441\u0435\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u0438 \u0441\u0442\u0440\u043e\u043a \u043f\u0440\u0438 \u043f\u0435\u0447\u0430\u0442\u0438\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\n# \u0437\u0430\u0440\u0430\u043d\u0435\u0435 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043c \u0432 \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u0443 random_state\nrandom_state = 47\n\nsns.set(style=\"whitegrid\")\n</pre> import pandas as pd import numpy as np import seaborn as sns import random from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV  from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier  from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score from sklearn.preprocessing import StandardScaler, OrdinalEncoder from sklearn.utils import shuffle  import warnings warnings.filterwarnings(\"ignore\")  # \u0423\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u043d\u0430\u0441\u0442\u0440\u043e\u0435\u043a \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0432\u0441\u0435\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u0438 \u0441\u0442\u0440\u043e\u043a \u043f\u0440\u0438 \u043f\u0435\u0447\u0430\u0442\u0438 pd.set_option('display.max_columns', None) pd.set_option('display.max_rows', None)  # \u0437\u0430\u0440\u0430\u043d\u0435\u0435 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043c \u0432 \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u0443 random_state random_state = 47  sns.set(style=\"whitegrid\") In\u00a0[\u00a0]: Copied! <pre>churn = pd.read_csv('/content/Churn_Modelling.csv')\nprint(churn.shape)\nchurn.head()\n</pre> churn = pd.read_csv('/content/Churn_Modelling.csv') print(churn.shape) churn.head() <pre>(10000, 14)\n</pre> Out[\u00a0]: RowNumber CustomerId Surname CreditScore Geography Gender Age Tenure Balance NumOfProducts HasCrCard IsActiveMember EstimatedSalary Exited 0 1 15634602 Hargrave 619 France Female 42 2 0.00 1 1 1 101348.88 1 1 2 15647311 Hill 608 Spain Female 41 1 83807.86 1 0 1 112542.58 0 2 3 15619304 Onio 502 France Female 42 8 159660.80 3 1 0 113931.57 1 3 4 15701354 Boni 699 France Female 39 1 0.00 2 0 0 93826.63 0 4 5 15737888 Mitchell 850 Spain Female 43 2 125510.82 1 1 1 79084.10 0 <p>\u0421\u0440\u0430\u0437\u0443 \u0438\u0441\u043a\u043b\u044e\u0447\u0438\u043c \u043d\u0435\u043d\u0443\u0436\u043d\u044b\u0435 \u0441\u0442\u043e\u043b\u0431\u0446\u044b, \u0447\u0442\u043e\u0431\u044b \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0435 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0430\u043b\u0438\u0441\u044c \u043f\u043e\u0434 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439:</p> In\u00a0[\u00a0]: Copied! <pre>churn = churn.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\nchurn.head()\n</pre> churn = churn.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1) churn.head() Out[\u00a0]: CreditScore Geography Gender Age Tenure Balance NumOfProducts HasCrCard IsActiveMember EstimatedSalary Exited 0 619 France Female 42 2 0.00 1 1 1 101348.88 1 1 608 Spain Female 41 1 83807.86 1 0 1 112542.58 0 2 502 France Female 42 8 159660.80 3 1 0 113931.57 1 3 699 France Female 39 1 0.00 2 0 0 93826.63 0 4 850 Spain Female 43 2 125510.82 1 1 1 79084.10 0 In\u00a0[\u00a0]: Copied! <pre># \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043f\u043e\u0434\u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432\nfig, axes = plt.subplots(4, 2, figsize=(10, 13))\n\n# \u0413\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u043a\u0440\u0435\u0434\u0438\u0442\u043d\u044b\u0445 \u0431\u0430\u043b\u043b\u043e\u0432\nsns.histplot(data=churn, x='CreditScore', kde=True, ax=axes[0, 0])\naxes[0, 0].set_title('Distribution of Credit Scores')\n\n# \u0413\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u0431\u0430\u043b\u0430\u043d\u0441\u0430 \u043d\u0430 \u0441\u0447\u0451\u0442\u0435\nsns.histplot(data=churn, x='Balance', kde=True, ax=axes[0, 1])\naxes[0, 1].set_title('Distribution of Balance')\n\n# \u0413\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u0437\u0430\u0440\u043f\u043b\u0430\u0442\nsns.histplot(data=churn, x='EstimatedSalary', kde=True, ax=axes[1, 0])\naxes[1, 0].set_title('Distribution of EstimatedSalary')\n\n# \u0413\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0430\nsns.histplot(data=churn, x='Age', kde=True, ax=axes[1, 1])\naxes[1, 1].set_title('Distribution of Age')\n\n# \u0411\u043e\u043a\u0441\u043f\u043b\u043e\u0442 \u0431\u0430\u043b\u0430\u043d\u0441\u0430 \u043f\u043e \u0441\u0442\u0440\u0430\u043d\u0430\u043c\nsns.boxplot(x='Geography', y='Balance', data=churn, ax=axes[2, 0])\naxes[2, 0].set_title('Balance by Geography')\n\n# \u0411\u043e\u043a\u0441\u043f\u043b\u043e\u0442 \u0437\u0430\u0440\u043f\u043b\u0430\u0442\u044b \u043f\u043e \u043f\u043e\u043b\u0443\nsns.boxplot(x='Gender', y='EstimatedSalary', data=churn, ax=axes[2, 1])\naxes[2, 1].set_title('Estimated Salary by Gender')\n\n# \u041a\u0440\u0443\u0433\u043e\u0432\u0430\u044f \u0434\u0438\u0430\u0433\u0440\u0430\u043c\u043c\u0430 \u0434\u043b\u044f \u043a\u043e\u043b\u043e\u043d\u043a\u0438 Exited (\u043e\u0442\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432)\nexited_counts = churn['Exited'].value_counts()\naxes[3, 0].pie(exited_counts, labels=exited_counts.index, autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff'])\naxes[3, 0].set_title('Exited Distribution')\n\n# \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0430\u043a\u0442\u0438\u0432\u043d\u044b\u0445 \u0447\u043b\u0435\u043d\u043e\u0432 \u043f\u043e \u0441\u0442\u0440\u0430\u043d\u0430\u043c\nsns.countplot(x='Geography', hue='IsActiveMember', data=churn, ax=axes[3, 1])\naxes[3, 1].set_title('Active Members by Geography')\n\n# \u041f\u043e\u0434\u0433\u043e\u043d\u043a\u0430 \u0438 \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432\nplt.tight_layout()\nplt.show()\n</pre> # \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043f\u043e\u0434\u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432 fig, axes = plt.subplots(4, 2, figsize=(10, 13))  # \u0413\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u043a\u0440\u0435\u0434\u0438\u0442\u043d\u044b\u0445 \u0431\u0430\u043b\u043b\u043e\u0432 sns.histplot(data=churn, x='CreditScore', kde=True, ax=axes[0, 0]) axes[0, 0].set_title('Distribution of Credit Scores')  # \u0413\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u0431\u0430\u043b\u0430\u043d\u0441\u0430 \u043d\u0430 \u0441\u0447\u0451\u0442\u0435 sns.histplot(data=churn, x='Balance', kde=True, ax=axes[0, 1]) axes[0, 1].set_title('Distribution of Balance')  # \u0413\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u0437\u0430\u0440\u043f\u043b\u0430\u0442 sns.histplot(data=churn, x='EstimatedSalary', kde=True, ax=axes[1, 0]) axes[1, 0].set_title('Distribution of EstimatedSalary')  # \u0413\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0430 sns.histplot(data=churn, x='Age', kde=True, ax=axes[1, 1]) axes[1, 1].set_title('Distribution of Age')  # \u0411\u043e\u043a\u0441\u043f\u043b\u043e\u0442 \u0431\u0430\u043b\u0430\u043d\u0441\u0430 \u043f\u043e \u0441\u0442\u0440\u0430\u043d\u0430\u043c sns.boxplot(x='Geography', y='Balance', data=churn, ax=axes[2, 0]) axes[2, 0].set_title('Balance by Geography')  # \u0411\u043e\u043a\u0441\u043f\u043b\u043e\u0442 \u0437\u0430\u0440\u043f\u043b\u0430\u0442\u044b \u043f\u043e \u043f\u043e\u043b\u0443 sns.boxplot(x='Gender', y='EstimatedSalary', data=churn, ax=axes[2, 1]) axes[2, 1].set_title('Estimated Salary by Gender')  # \u041a\u0440\u0443\u0433\u043e\u0432\u0430\u044f \u0434\u0438\u0430\u0433\u0440\u0430\u043c\u043c\u0430 \u0434\u043b\u044f \u043a\u043e\u043b\u043e\u043d\u043a\u0438 Exited (\u043e\u0442\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432) exited_counts = churn['Exited'].value_counts() axes[3, 0].pie(exited_counts, labels=exited_counts.index, autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff']) axes[3, 0].set_title('Exited Distribution')  # \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0430\u043a\u0442\u0438\u0432\u043d\u044b\u0445 \u0447\u043b\u0435\u043d\u043e\u0432 \u043f\u043e \u0441\u0442\u0440\u0430\u043d\u0430\u043c sns.countplot(x='Geography', hue='IsActiveMember', data=churn, ax=axes[3, 1]) axes[3, 1].set_title('Active Members by Geography')  # \u041f\u043e\u0434\u0433\u043e\u043d\u043a\u0430 \u0438 \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432 plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># One-Hot \u0434\u043b\u044f \u043b\u043e\u0433\u0440\u0435\u0433\u0430 (\u0434\u043b\u044f \u0437\u0435\u043b\u0435\u043d\u0438 \u0442\u043e\u0436\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u0438\u0442)\nchurn = pd.get_dummies(churn, drop_first=True)\nprint(churn.shape)\nchurn.head()\n</pre> # One-Hot \u0434\u043b\u044f \u043b\u043e\u0433\u0440\u0435\u0433\u0430 (\u0434\u043b\u044f \u0437\u0435\u043b\u0435\u043d\u0438 \u0442\u043e\u0436\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u0438\u0442) churn = pd.get_dummies(churn, drop_first=True) print(churn.shape) churn.head() <pre>(10000, 12)\n</pre> Out[\u00a0]: CreditScore Age Tenure Balance NumOfProducts HasCrCard IsActiveMember EstimatedSalary Exited Geography_Germany Geography_Spain Gender_Male 0 619 42 2 0.00 1 1 1 101348.88 1 False False False 1 608 41 1 83807.86 1 0 1 112542.58 0 False True False 2 502 42 8 159660.80 3 1 0 113931.57 1 False False False 3 699 39 1 0.00 2 0 0 93826.63 0 False False False 4 850 43 2 125510.82 1 1 1 79084.10 0 False True False In\u00a0[\u00a0]: Copied! <pre>features = churn.drop(['Exited'], axis=1)\ntarget = churn['Exited']\n</pre> features = churn.drop(['Exited'], axis=1) target = churn['Exited'] In\u00a0[\u00a0]: Copied! <pre># \u043c\u043e\u0449\u043d\u043e\u0441\u0442\u044c \u043a\u043b\u0430\u0441\u0441\u043e\u0432\ntarget.value_counts()\n</pre> # \u043c\u043e\u0449\u043d\u043e\u0441\u0442\u044c \u043a\u043b\u0430\u0441\u0441\u043e\u0432 target.value_counts() Out[\u00a0]: count Exited 0 7963 1 2037 dtype: int64 In\u00a0[\u00a0]: Copied! <pre>target.mean()\n</pre> target.mean() Out[\u00a0]: <pre>0.2037</pre> In\u00a0[\u00a0]: Copied! <pre># \u043e\u0442\u0434\u0435\u043b\u044f\u0435\u043c 20% - \u043f\u044f\u0442\u0443\u044e \u0447\u0430\u0441\u0442\u044c \u0432\u0441\u0435\u0433\u043e - \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443\nX_train_valid, X_test, y_train_valid, y_test = train_test_split(features, target,\n                                                                            test_size=0.2,\n                                                                            random_state=random_state)\n# \u043e\u0442\u0434\u0435\u043b\u044f\u0435\u043c 25% - \u0447\u0435\u0442\u0432\u0435\u0440\u0442\u0443\u044e \u0447\u0430\u0441\u0442\u044c \u0442\u0440\u0435\u0439\u043d+\u0432\u0430\u043b\u0438\u0434 - \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u0443\u044e\u0449\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid,\n                                                                              test_size=0.25,\n                                                                              random_state=random_state)\n\ns1 = y_train.size\ns2 = y_valid.size\ns3 = y_test.size\nprint('\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 train:valid:test \u0432 \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0438 '\n      + str(round(s1/s3)) + ':' + str(round(s2/s3)) + ':' + str(round(s3/s3)))\n</pre> # \u043e\u0442\u0434\u0435\u043b\u044f\u0435\u043c 20% - \u043f\u044f\u0442\u0443\u044e \u0447\u0430\u0441\u0442\u044c \u0432\u0441\u0435\u0433\u043e - \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443 X_train_valid, X_test, y_train_valid, y_test = train_test_split(features, target,                                                                             test_size=0.2,                                                                             random_state=random_state) # \u043e\u0442\u0434\u0435\u043b\u044f\u0435\u043c 25% - \u0447\u0435\u0442\u0432\u0435\u0440\u0442\u0443\u044e \u0447\u0430\u0441\u0442\u044c \u0442\u0440\u0435\u0439\u043d+\u0432\u0430\u043b\u0438\u0434 - \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u0443\u044e\u0449\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443 X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid,                                                                               test_size=0.25,                                                                               random_state=random_state)  s1 = y_train.size s2 = y_valid.size s3 = y_test.size print('\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 train:valid:test \u0432 \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0438 '       + str(round(s1/s3)) + ':' + str(round(s2/s3)) + ':' + str(round(s3/s3))) <pre>\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 train:valid:test \u0432 \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0438 3:1:1\n</pre> In\u00a0[\u00a0]: Copied! <pre>targets = [y_train, y_train_valid, y_valid, y_test]\nnames = ['train:', 'train+valid:', 'valid:', 'test:']\nprint('\u0411\u0430\u043b\u0430\u043d\u0441 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u043d\u0430 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f\u0445:\\n')\ni = 0\nfor target in targets:\n    pc = target.mean()\n    print(names[i], pc.round(3))\n    i += 1\n</pre> targets = [y_train, y_train_valid, y_valid, y_test] names = ['train:', 'train+valid:', 'valid:', 'test:'] print('\u0411\u0430\u043b\u0430\u043d\u0441 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u043d\u0430 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f\u0445:\\n') i = 0 for target in targets:     pc = target.mean()     print(names[i], pc.round(3))     i += 1 <pre>\u0411\u0430\u043b\u0430\u043d\u0441 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u043d\u0430 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f\u0445:\n\ntrain: 0.201\ntrain+valid: 0.202\nvalid: 0.204\ntest: 0.212\n</pre> In\u00a0[\u00a0]: Copied! <pre># \u0412\u044b\u0434\u0435\u043b\u044f\u0435\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u043b\u044f \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u0430\u0446\u0438\u0438\nnumeric = ['CreditScore', 'Age', 'Tenure', 'Balance', 'EstimatedSalary']\n\n# \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u043d\u0430 \u0442\u0440\u0435\u0439\u043d\u0435\nscaler = StandardScaler()\nscaler.fit(X_train[numeric])\n\n# \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0432\u0441\u0435 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0432\u044b\u0448\u0435\nX_train[numeric] = scaler.transform(X_train[numeric])\nX_valid[numeric] = scaler.transform(X_valid[numeric])\nX_test[numeric] = scaler.transform(X_test[numeric])\n\nX_train_valid[numeric] = scaler.fit_transform(X_train_valid[numeric])\n</pre> # \u0412\u044b\u0434\u0435\u043b\u044f\u0435\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u043b\u044f \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u0430\u0446\u0438\u0438 numeric = ['CreditScore', 'Age', 'Tenure', 'Balance', 'EstimatedSalary']  # \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u043d\u0430 \u0442\u0440\u0435\u0439\u043d\u0435 scaler = StandardScaler() scaler.fit(X_train[numeric])  # \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0432\u0441\u0435 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0432\u044b\u0448\u0435 X_train[numeric] = scaler.transform(X_train[numeric]) X_valid[numeric] = scaler.transform(X_valid[numeric]) X_test[numeric] = scaler.transform(X_test[numeric])  X_train_valid[numeric] = scaler.fit_transform(X_train_valid[numeric]) In\u00a0[\u00a0]: Copied! <pre>X_train[numeric].describe().round(3)\n</pre> X_train[numeric].describe().round(3) Out[\u00a0]: CreditScore Age Tenure Balance EstimatedSalary count 6000.000 6000.000 6000.000 6000.000 6000.000 mean 0.000 0.000 0.000 -0.000 -0.000 std 1.000 1.000 1.000 1.000 1.000 min -3.136 -1.994 -1.730 -1.236 -1.753 25% -0.690 -0.663 -0.695 -1.236 -0.860 50% 0.007 -0.187 -0.004 0.335 0.009 75% 0.693 0.478 1.031 0.821 0.855 max 2.067 5.042 1.721 2.807 1.729 <ul> <li>\u0417\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u043e \u0432\u044b\u0431\u0440\u0430\u043d\u043d\u044b\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c \u0442\u0435\u043f\u0435\u0440\u044c \u0432\u044b\u0433\u043b\u044f\u0434\u044f\u0442 \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u043d\u0435\u0430\u0434\u0435\u043a\u0432\u0430\u0442\u043d\u043e,</li> <li>\u0437\u0430\u0442\u043e \u0441 \u043d\u0443\u043b\u0435\u0432\u044b\u043c \u0441\u0440\u0435\u0434\u043d\u0438\u043c \u0438 \u0441\u0440\u0435\u0434\u043d\u0435\u043a\u0432\u0430\u0434\u0440\u0430\u0442\u0438\u0447\u043d\u044b\u043c, \u0440\u0430\u0432\u043d\u044b\u043c 1.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438\ndef calc_metrics(model, X_train, y_train, X_valid, y_valid, X_test, y_test):\n\n    # \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\n    y_train_pred = model.predict(X_train)\n    y_train_proba = model.predict_proba(X_train)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_train)\n\n    # \u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f\n    y_valid_pred = model.predict(X_valid)\n    y_valid_proba = model.predict_proba(X_valid)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_valid)\n\n    # \u0422\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\n    y_test_pred = model.predict(X_test)\n    y_test_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test)\n\n    train_metrics = {\n        'precision': precision_score(y_train, y_train_pred),\n        'recall': recall_score(y_train, y_train_pred),\n        'f1': f1_score(y_train, y_train_pred),\n        'roc_auc': roc_auc_score(y_train, y_train_proba)\n    }\n\n    valid_metrics = {\n        'precision': precision_score(y_valid, y_valid_pred),\n        'recall': recall_score(y_valid, y_valid_pred),\n        'f1': f1_score(y_valid, y_valid_pred),\n        'roc_auc': roc_auc_score(y_valid, y_valid_proba)\n    }\n\n    test_metrics = {\n        'precision': precision_score(y_test, y_test_pred),\n        'recall': recall_score(y_test, y_test_pred),\n        'f1': f1_score(y_test, y_test_pred),\n        'roc_auc': roc_auc_score(y_test, y_test_proba)\n    }\n\n    return train_metrics, valid_metrics, test_metrics\n\ndef print_metrics(model, X_train, y_train, X_valid, y_valid, X_test, y_test):\n    res = calc_metrics(model, X_train, y_train, X_valid, y_valid, X_test, y_test)\n    metrics = pd.DataFrame(res, index=['train', 'valid', 'test']).round(3)\n    return metrics\n</pre> # \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 def calc_metrics(model, X_train, y_train, X_valid, y_valid, X_test, y_test):      # \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435     y_train_pred = model.predict(X_train)     y_train_proba = model.predict_proba(X_train)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_train)      # \u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f     y_valid_pred = model.predict(X_valid)     y_valid_proba = model.predict_proba(X_valid)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_valid)      # \u0422\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435     y_test_pred = model.predict(X_test)     y_test_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test)      train_metrics = {         'precision': precision_score(y_train, y_train_pred),         'recall': recall_score(y_train, y_train_pred),         'f1': f1_score(y_train, y_train_pred),         'roc_auc': roc_auc_score(y_train, y_train_proba)     }      valid_metrics = {         'precision': precision_score(y_valid, y_valid_pred),         'recall': recall_score(y_valid, y_valid_pred),         'f1': f1_score(y_valid, y_valid_pred),         'roc_auc': roc_auc_score(y_valid, y_valid_proba)     }      test_metrics = {         'precision': precision_score(y_test, y_test_pred),         'recall': recall_score(y_test, y_test_pred),         'f1': f1_score(y_test, y_test_pred),         'roc_auc': roc_auc_score(y_test, y_test_proba)     }      return train_metrics, valid_metrics, test_metrics  def print_metrics(model, X_train, y_train, X_valid, y_valid, X_test, y_test):     res = calc_metrics(model, X_train, y_train, X_valid, y_valid, X_test, y_test)     metrics = pd.DataFrame(res, index=['train', 'valid', 'test']).round(3)     return metrics In\u00a0[\u00a0]: Copied! <pre>%%time\n\n\"\"\"\n\n\u0420\u0443\u0447\u043d\u043e\u0439 \u043f\u0435\u0440\u0435\u0431\u043e\u0440 \u0432\u0441\u0435\u0445 \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432\n\n- \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0435 ROC-AUC\n\n\"\"\"\n\nparam_grid = {\n    'penalty': ['l2', 'l1', 'elasticnet'],      # \u0422\u0438\u043f \u0440\u0435\u0433\u0443\u0440\u0435\u043b\u0438\u0437\u0430\u0446\u0438\u0438\n    # 'solver': ['lbfgs', 'liblinear', 'saga'],\n    'C': np.linspace(0.001, 2, 50)  # \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043e\u0442\u0432\u0435\u0447\u0430\u0435\u0442 \u0437\u0430 \u0432\u0435\u0441 \u044d\u0442\u0438\u0445 \u043f\u043e\u043f\u0440\u0430\u0432\u043e\u043a\n}\n\ngrid_search = GridSearchCV(LogisticRegression(random_state=42),\n                           param_grid,\n                           cv=5,\n                           scoring='roc_auc',\n                           n_jobs=-1)\n\ngrid_search.fit(X_train_valid, y_train_valid)\n\nbest_logreg = grid_search.best_estimator_\nprint(best_logreg.get_params())\n</pre> %%time  \"\"\"  \u0420\u0443\u0447\u043d\u043e\u0439 \u043f\u0435\u0440\u0435\u0431\u043e\u0440 \u0432\u0441\u0435\u0445 \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432  - \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0435 ROC-AUC  \"\"\"  param_grid = {     'penalty': ['l2', 'l1', 'elasticnet'],      # \u0422\u0438\u043f \u0440\u0435\u0433\u0443\u0440\u0435\u043b\u0438\u0437\u0430\u0446\u0438\u0438     # 'solver': ['lbfgs', 'liblinear', 'saga'],     'C': np.linspace(0.001, 2, 50)  # \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043e\u0442\u0432\u0435\u0447\u0430\u0435\u0442 \u0437\u0430 \u0432\u0435\u0441 \u044d\u0442\u0438\u0445 \u043f\u043e\u043f\u0440\u0430\u0432\u043e\u043a }  grid_search = GridSearchCV(LogisticRegression(random_state=42),                            param_grid,                            cv=5,                            scoring='roc_auc',                            n_jobs=-1)  grid_search.fit(X_train_valid, y_train_valid)  best_logreg = grid_search.best_estimator_ print(best_logreg.get_params()) <pre>{'C': 0.04179591836734694, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\nCPU times: user 632 ms, sys: 143 ms, total: 774 ms\nWall time: 7.9 s\n</pre> In\u00a0[\u00a0]: Copied! <pre>res_logreg = print_metrics(best_logreg, X_train, y_train, X_valid, y_valid, X_test, y_test)\nres_logreg\n</pre> res_logreg = print_metrics(best_logreg, X_train, y_train, X_valid, y_valid, X_test, y_test) res_logreg Out[\u00a0]: precision recall f1 roc_auc train 0.615 0.201 0.303 0.774 valid 0.617 0.174 0.272 0.765 test 0.644 0.201 0.306 0.754 In\u00a0[\u00a0]: Copied! <pre># \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0438\ncoef = best_logreg.coef_[0]\n\n# \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 DataFrame \u0434\u043b\u044f \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u043e\u0432 \u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\ncoef_df = pd.DataFrame({\n    'Feature': X_train_valid.columns,\n    'Coefficient_logreg': coef.round(3)\n})\n\n# \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u0442\u043e\u043b\u0431\u0446\u0430 \u0441 \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u0435\u0439\ncoef_df['Interpretation_logreg'] = coef_df['Coefficient_logreg'].apply(lambda x: 'Positive' if x &gt; 0 else 'Negative')\n</pre> # \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0438 coef = best_logreg.coef_[0]  # \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 DataFrame \u0434\u043b\u044f \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u043e\u0432 \u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 coef_df = pd.DataFrame({     'Feature': X_train_valid.columns,     'Coefficient_logreg': coef.round(3) })  # \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u0442\u043e\u043b\u0431\u0446\u0430 \u0441 \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u0435\u0439 coef_df['Interpretation_logreg'] = coef_df['Coefficient_logreg'].apply(lambda x: 'Positive' if x &gt; 0 else 'Negative') <p>\u0418\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u044f \u0412\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u041f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432</p> <p>\u0414\u043b\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043c\u043e\u0436\u043d\u043e \u043e\u0446\u0435\u043d\u0438\u0442\u044c \u043f\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044e \u0432\u0435\u0441\u043e\u0432</p> <ul> <li>Positive: \u0415\u0441\u043b\u0438 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439, \u044d\u0442\u043e \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442, \u0447\u0442\u043e \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0432\u0430\u0435\u0442 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u0438\u0441\u0445\u043e\u0434\u0430.</li> <li>Negative: \u0415\u0441\u043b\u0438 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u043e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0439, \u044d\u0442\u043e \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442, \u0447\u0442\u043e \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u0443\u043c\u0435\u043d\u044c\u0448\u0430\u0435\u0442 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u0438\u0441\u0445\u043e\u0434\u0430.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>coef_df\n</pre> coef_df Out[\u00a0]: Feature Coefficient_logreg Interpretation_logreg 0 CreditScore -0.064 Negative 1 Age 0.738 Positive 2 Tenure -0.032 Negative 3 Balance 0.169 Positive 4 NumOfProducts -0.128 Negative 5 HasCrCard -0.019 Negative 6 IsActiveMember -0.975 Negative 7 EstimatedSalary 0.047 Positive 8 Geography_Germany 0.707 Positive 9 Geography_Spain 0.021 Positive 10 Gender_Male -0.505 Negative In\u00a0[\u00a0]: Copied! <pre>%%time\n\n\"\"\"\n\n- \u0411\u0443\u0434\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u0438\u0442\u044c \u0438\u0437 \u043f\u0440\u0435\u0434\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \u0447\u0442\u043e \u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u0435\u0441\u0442\u044c \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0441\u0442\u044c, \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0435 \u044f\u0434\u0440\u043e \u0438 \u043f\u043e\u043b\u0438\u043d\u043e\u043c\u0438\u0430\u043b\u044c\u043d\u043e\u0435\n- probability = True : \u041d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u043d\u043e\u0435 \u0440\u0430\u0441\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435\n\n'kernel': 'poly' \u0433\u043e\u0432\u043e\u0440\u0438\u0442 \u043d\u0430\u043c \u043e \u0442\u043e\u043c \u0447\u0442\u043e \u043f\u043e\u043b\u0438\u043d\u043e\u043c\u0438\u0430\u043b\u043d\u043e\u0435 \u044f\u0434\u043d\u043e \u0434\u0430\u0435\u0442 \u043b\u0443\u0447\u0448\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\n\u0418\u0437 \u044d\u0442\u043e\u0433\u043e \u043c\u044b \u043e\u0434\u043d\u043e\u0437\u043d\u0430\u0447\u043d\u043e \u0434\u0435\u043b\u0430\u0435\u043c \u0432\u044b\u0432\u043e\u0434 \u0447\u0442\u043e \u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u0435\u0441\u0442\u044c \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0441\u0442\u044c\n\n\n\"\"\"\n\nparam_grid = {\n    # 'C': np.linspace(0.001, 2, 50),\n    # 'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n    'kernel': ['linear', 'poly']\n\n}\n\ngrid_search = GridSearchCV(SVC(random_state=42, probability=True, gamma='scale'),\n                           param_grid,\n                           cv=5,\n                           scoring='roc_auc',\n                           n_jobs=-1)\n\ngrid_search.fit(X_train_valid, y_train_valid)\n\nbest_SVC = grid_search.best_estimator_\nprint(best_SVC.get_params())\n</pre> %%time  \"\"\"  - \u0411\u0443\u0434\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u0438\u0442\u044c \u0438\u0437 \u043f\u0440\u0435\u0434\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \u0447\u0442\u043e \u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u0435\u0441\u0442\u044c \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0441\u0442\u044c, \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0435 \u044f\u0434\u0440\u043e \u0438 \u043f\u043e\u043b\u0438\u043d\u043e\u043c\u0438\u0430\u043b\u044c\u043d\u043e\u0435 - probability = True : \u041d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u043d\u043e\u0435 \u0440\u0430\u0441\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435  'kernel': 'poly' \u0433\u043e\u0432\u043e\u0440\u0438\u0442 \u043d\u0430\u043c \u043e \u0442\u043e\u043c \u0447\u0442\u043e \u043f\u043e\u043b\u0438\u043d\u043e\u043c\u0438\u0430\u043b\u043d\u043e\u0435 \u044f\u0434\u043d\u043e \u0434\u0430\u0435\u0442 \u043b\u0443\u0447\u0448\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0418\u0437 \u044d\u0442\u043e\u0433\u043e \u043c\u044b \u043e\u0434\u043d\u043e\u0437\u043d\u0430\u0447\u043d\u043e \u0434\u0435\u043b\u0430\u0435\u043c \u0432\u044b\u0432\u043e\u0434 \u0447\u0442\u043e \u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u0435\u0441\u0442\u044c \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0441\u0442\u044c   \"\"\"  param_grid = {     # 'C': np.linspace(0.001, 2, 50),     # 'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],     'kernel': ['linear', 'poly']  }  grid_search = GridSearchCV(SVC(random_state=42, probability=True, gamma='scale'),                            param_grid,                            cv=5,                            scoring='roc_auc',                            n_jobs=-1)  grid_search.fit(X_train_valid, y_train_valid)  best_SVC = grid_search.best_estimator_ print(best_SVC.get_params()) <pre>{'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'poly', 'max_iter': -1, 'probability': True, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}\nCPU times: user 9.52 s, sys: 305 ms, total: 9.82 s\nWall time: 52.1 s\n</pre> In\u00a0[\u00a0]: Copied! <pre>res_SVC = print_metrics(best_SVC, X_train, y_train, X_valid, y_valid, X_test, y_test)\nres_SVC\n</pre> res_SVC = print_metrics(best_SVC, X_train, y_train, X_valid, y_valid, X_test, y_test) res_SVC Out[\u00a0]: precision recall f1 roc_auc train 0.856 0.247 0.383 0.825 valid 0.885 0.246 0.385 0.841 test 0.850 0.241 0.376 0.812 <p>\u0418\u043d\u0442\u0435\u0440\u043f\u0440\u0438\u0442\u0430\u0446\u0438\u044f \u0412\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u041f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432</p> <p>Permutation Importance</p> <ul> <li>\u0418\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0432\u0435\u0441\u0430 \u0443 \u043f\u043e\u043b\u0438\u043d\u043e\u043c\u0438\u0430\u043b\u044c\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043a\u0430\u043a \u0441 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u044b \u043d\u0435 \u0441\u043c\u043e\u0436\u0435\u043c</li> <li>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0434\u0440\u0443\u0433\u0438\u043c \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u043c permutation importance</li> <li>\u041e\u043d\u0430 \u0441\u043c\u043e\u0442\u0440\u0438\u0442 \u043d\u0430 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u0432 \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442\u0435<ul> <li>\u041f\u0440\u0438\u0441\u0442\u0430\u0432\u0438\u043c \u043f\u0440\u0438\u043d\u043d\u0430\u043a h</li> <li>PI \u043f\u0435\u0440\u0435\u043c\u0435\u0448\u0438\u0432\u0430\u0435\u0442 \u0432\u0441\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0432 \u044d\u0442\u043e\u043c \u0441\u0442\u043e\u043b\u0431\u0446\u0435 (\u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u0447\u0442\u043e \u043e\u043d \u0438\u0441\u043f\u043e\u0440\u0442\u0438\u043b \u043a\u043e\u043b\u043e\u043d\u043a\u0443)</li> <li>\u041e\u0431\u0443\u0447\u0430\u0435\u0442 \u043c\u043e\u0434\u0435\u043b\u044c \u0441 \u0443\u0447\u0435\u0442\u043e\u043c \u0438\u0441\u043f\u043e\u0440\u0447\u0435\u043d\u043d\u043e\u0439 \u0444\u0438\u0447\u0438</li> <li>\u0437\u0430\u043f\u043e\u043c\u0438\u043d\u0430\u0435\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0437\u0430\u043f\u043e\u043c\u0438\u043d\u0430\u0435\u043c \u0435\u0435</li> <li>\u0430\u043d\u043e\u043b\u043e\u0433\u0438\u0447\u043d\u043e \u0434\u0435\u043b\u0430\u0435\u043c \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a</li> <li>\u0447\u0435\u043c \u0441\u0438\u043b\u044c\u043d\u0435\u0435 \u043f\u0430\u0434\u0430\u0435\u0442 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0440\u0438 \u043f\u0435\u0440\u0435\u043c\u0435\u0448\u0435\u043d\u043d\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0442\u0435\u043c \u0431\u043e\u043b\u044c\u0448\u0435 \u0432\u043a\u043b\u0430\u0434 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u0432 \u043c\u043e\u0434\u0435\u043b\u0438</li> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>from sklearn.inspection import permutation_importance\n\nperm_importance = permutation_importance(best_SVC, X_test, y_test)\n\nfeatures = np.array(X_test.columns)\n\nsorted_idx = perm_importance.importances_mean.argsort()\nplt.barh(features[sorted_idx], perm_importance.importances_mean[sorted_idx])\nplt.xlabel(\"Permutation Importance\");\n</pre> from sklearn.inspection import permutation_importance  perm_importance = permutation_importance(best_SVC, X_test, y_test)  features = np.array(X_test.columns)  sorted_idx = perm_importance.importances_mean.argsort() plt.barh(features[sorted_idx], perm_importance.importances_mean[sorted_idx]) plt.xlabel(\"Permutation Importance\"); In\u00a0[\u00a0]: Copied! <pre>%%time\n\nparam_grid = {\n    'criterion': ['gini', 'entropy', 'log_loss'],\n    # 'splitter': ['best', 'random'],\n    'max_depth': range(1, 11),\n    # 'min_samples_split': range(2, 10),\n    'min_samples_leaf': range(2, 10)\n}\n\ngrid_search = GridSearchCV(DecisionTreeClassifier(random_state=42),\n                           param_grid,\n                           cv=5,\n                           scoring='roc_auc',\n                           n_jobs=-1)\n\ngrid_search.fit(X_train_valid, y_train_valid)\n\nbest_tree = grid_search.best_estimator_\nprint(best_tree.get_params())\n</pre> %%time  param_grid = {     'criterion': ['gini', 'entropy', 'log_loss'],     # 'splitter': ['best', 'random'],     'max_depth': range(1, 11),     # 'min_samples_split': range(2, 10),     'min_samples_leaf': range(2, 10) }  grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42),                            param_grid,                            cv=5,                            scoring='roc_auc',                            n_jobs=-1)  grid_search.fit(X_train_valid, y_train_valid)  best_tree = grid_search.best_estimator_ print(best_tree.get_params()) <pre>{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 6, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 9, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 42, 'splitter': 'best'}\nCPU times: user 1.17 s, sys: 127 ms, total: 1.29 s\nWall time: 31.1 s\n</pre> In\u00a0[\u00a0]: Copied! <pre>res_tree = print_metrics(best_tree, X_train, y_train, X_valid, y_valid, X_test, y_test)\nres_tree\n</pre> res_tree = print_metrics(best_tree, X_train, y_train, X_valid, y_valid, X_test, y_test) res_tree Out[\u00a0]: precision recall f1 roc_auc train 0.767 0.485 0.594 0.868 valid 0.813 0.459 0.587 0.869 test 0.744 0.440 0.553 0.841 In\u00a0[\u00a0]: Copied! <pre>def plot_importance(model, X):\n\n  # \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n  feature_importances = model.feature_importances_\n\n  # \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 DataFrame \u0434\u043b\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n  importance_df = pd.DataFrame({\n      'Feature': X.columns,\n      'Importance': feature_importances\n  })\n\n  # \u0421\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0430 DataFrame \u043f\u043e \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438\n  importance_df = importance_df.sort_values(by='Importance', ascending=False)\n\n  # \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n  plt.figure(figsize=(10, 6))\n  sns.barplot(x='Importance', y='Feature', data=importance_df)\n  plt.title('Feature Importances in Decision Tree')\n  plt.show()\n</pre> def plot_importance(model, X):    # \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432   feature_importances = model.feature_importances_    # \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 DataFrame \u0434\u043b\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432   importance_df = pd.DataFrame({       'Feature': X.columns,       'Importance': feature_importances   })    # \u0421\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0430 DataFrame \u043f\u043e \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438   importance_df = importance_df.sort_values(by='Importance', ascending=False)    # \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432   plt.figure(figsize=(10, 6))   sns.barplot(x='Importance', y='Feature', data=importance_df)   plt.title('Feature Importances in Decision Tree')   plt.show() In\u00a0[\u00a0]: Copied! <pre>plot_importance(best_tree, X_train_valid)\n</pre> plot_importance(best_tree, X_train_valid) In\u00a0[\u00a0]: Copied! <pre>!pip install optuna -qqq\n</pre> !pip install optuna -qqq In\u00a0[\u00a0]: Copied! <pre>import optuna\n\n# \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u0435\u043c\ndef objective(trial):\n\n    # \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u043b\u0435\u0441\u0430\n    param = {\n        'criterion': trial.suggest_categorical(\"criterion\", ['gini', 'entropy', 'log_loss']),\n        'n_estimators': trial.suggest_int(\"n_estimators\", 10, 100),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10), # \u0441\u043e\u043a\u0440\u0430\u0442\u0438\u0442\u044c \u043a\u043e\u043b-\u0432\u043e \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432\n        'min_samples_split': trial.suggest_int(\"min_samples_split\", 2, 10),\n        'min_samples_leaf': trial.suggest_int(\"min_samples_leaf\", 2, 10),\n\n    }\n\n    model = RandomForestClassifier(**param, random_state=random_state)\n    model.fit(X_train, y_train)\n\n    preds = model.predict_proba(X_valid)[:,1]\n    auc = roc_auc_score(y_valid, preds)\n\n    return auc\n</pre> import optuna  # \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u0435\u043c def objective(trial):      # \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u043b\u0435\u0441\u0430     param = {         'criterion': trial.suggest_categorical(\"criterion\", ['gini', 'entropy', 'log_loss']),         'n_estimators': trial.suggest_int(\"n_estimators\", 10, 100),         \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10), # \u0441\u043e\u043a\u0440\u0430\u0442\u0438\u0442\u044c \u043a\u043e\u043b-\u0432\u043e \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432         'min_samples_split': trial.suggest_int(\"min_samples_split\", 2, 10),         'min_samples_leaf': trial.suggest_int(\"min_samples_leaf\", 2, 10),      }      model = RandomForestClassifier(**param, random_state=random_state)     model.fit(X_train, y_train)      preds = model.predict_proba(X_valid)[:,1]     auc = roc_auc_score(y_valid, preds)      return auc In\u00a0[\u00a0]: Copied! <pre># \u043c\u0430\u043a\u0441\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u0435\u043c roc-auc\nstudy = optuna.create_study(direction=\"maximize\", \n                            study_name='RandomForestClassifier')\nstudy.optimize(objective, n_trials=10) # \u043f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0442\u044c\n\nprint(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"Value: {}\".format(trial.value))\n\nprint(\"Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n</pre> # \u043c\u0430\u043a\u0441\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u0435\u043c roc-auc study = optuna.create_study(direction=\"maximize\",                              study_name='RandomForestClassifier') study.optimize(objective, n_trials=10) # \u043f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0442\u044c  print(\"Number of finished trials: {}\".format(len(study.trials)))  print(\"Best trial:\") trial = study.best_trial  print(\"Value: {}\".format(trial.value))  print(\"Params: \") for key, value in trial.params.items():     print(\"    {}: {}\".format(key, value)) <pre>[I 2024-08-14 03:46:14,901] A new study created in memory with name: RandomForestClassifier\n[I 2024-08-14 03:46:15,008] Trial 0 finished with value: 0.8334212486754861 and parameters: {'criterion': 'log_loss', 'n_estimators': 15, 'max_depth': 5, 'min_samples_split': 8, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.8334212486754861.\n[I 2024-08-14 03:46:15,213] Trial 1 finished with value: 0.8172085799204443 and parameters: {'criterion': 'gini', 'n_estimators': 38, 'max_depth': 2, 'min_samples_split': 8, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.8334212486754861.\n[I 2024-08-14 03:46:15,408] Trial 2 finished with value: 0.8193139210088363 and parameters: {'criterion': 'log_loss', 'n_estimators': 17, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.8334212486754861.\n[I 2024-08-14 03:46:16,070] Trial 3 finished with value: 0.8264767078326399 and parameters: {'criterion': 'log_loss', 'n_estimators': 61, 'max_depth': 2, 'min_samples_split': 3, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.8334212486754861.\n[I 2024-08-14 03:46:16,818] Trial 4 finished with value: 0.8437150555794624 and parameters: {'criterion': 'log_loss', 'n_estimators': 53, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 7}. Best is trial 4 with value: 0.8437150555794624.\n[I 2024-08-14 03:46:18,630] Trial 5 finished with value: 0.8498282566079176 and parameters: {'criterion': 'gini', 'n_estimators': 68, 'max_depth': 9, 'min_samples_split': 6, 'min_samples_leaf': 9}. Best is trial 5 with value: 0.8498282566079176.\n[I 2024-08-14 03:46:18,932] Trial 6 finished with value: 0.8170481729803764 and parameters: {'criterion': 'log_loss', 'n_estimators': 13, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 7}. Best is trial 5 with value: 0.8498282566079176.\n[I 2024-08-14 03:46:19,959] Trial 7 finished with value: 0.8418210197871214 and parameters: {'criterion': 'log_loss', 'n_estimators': 65, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 5}. Best is trial 5 with value: 0.8498282566079176.\n[I 2024-08-14 03:46:20,197] Trial 8 finished with value: 0.8422798761781811 and parameters: {'criterion': 'log_loss', 'n_estimators': 15, 'max_depth': 6, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 5 with value: 0.8498282566079176.\n[I 2024-08-14 03:46:21,125] Trial 9 finished with value: 0.8426600714736308 and parameters: {'criterion': 'log_loss', 'n_estimators': 96, 'max_depth': 4, 'min_samples_split': 5, 'min_samples_leaf': 10}. Best is trial 5 with value: 0.8498282566079176.\n</pre> <pre>Number of finished trials: 10\nBest trial:\nValue: 0.8498282566079176\nParams: \n    criterion: gini\n    n_estimators: 68\n    max_depth: 9\n    min_samples_split: 6\n    min_samples_leaf: 9\n</pre> In\u00a0[\u00a0]: Copied! <pre># \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0441 \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u043c\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 \nbest_forest = RandomForestClassifier(**study.best_params, random_state=random_state)\nbest_forest.fit(X_train_valid, y_train_valid)\n\nres_forest = print_metrics(best_forest, X_train, y_train, X_valid, y_valid, X_test, y_test)\nres_forest\n</pre> # \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0441 \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u043c\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438  best_forest = RandomForestClassifier(**study.best_params, random_state=random_state) best_forest.fit(X_train_valid, y_train_valid)  res_forest = print_metrics(best_forest, X_train, y_train, X_valid, y_valid, X_test, y_test) res_forest Out[\u00a0]: precision recall f1 roc_auc train 0.863 0.463 0.603 0.914 valid 0.894 0.477 0.622 0.908 test 0.795 0.411 0.542 0.857 <p>\u0418\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u044f \u0412\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u041f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432</p> In\u00a0[\u00a0]: Copied! <pre>plot_importance(best_forest, X_train_valid)\n</pre> plot_importance(best_forest, X_train_valid) <p>\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438</p> <p>\u041d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0437\u0430\u0434\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c</p> In\u00a0[\u00a0]: Copied! <pre>from catboost import CatBoostClassifier\n\ndef objective(trial):\n    param = {\n        \"learning_rate\": trial.suggest_float('learning_rate', 0.01, 0.9),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n        \"l2_leaf_reg\":trial.suggest_float('l2_leaf_reg', 0.01, 2),\n        \"subsample\": trial.suggest_float('subsample', 0.01, 1),\n        \"random_strength\": trial.suggest_float('random_strength', 1, 200),\n        \"min_data_in_leaf\":trial.suggest_float('min_data_in_leaf', 1, 500)\n    }\n\n    cat = CatBoostClassifier(\n        logging_level=\"Silent\",\n        eval_metric=\"AUC\",\n        grow_policy=\"Lossguide\",\n        random_seed=42,\n        **param)\n    cat.fit(X_train, y_train,\n            eval_set=(X_valid, y_valid),\n            verbose=False,\n            early_stopping_rounds=10\n           )\n\n    preds = cat.predict_proba(X_valid)[:,1]\n    auc = roc_auc_score(y_valid, preds)\n\n    return auc\n</pre> from catboost import CatBoostClassifier  def objective(trial):     param = {         \"learning_rate\": trial.suggest_float('learning_rate', 0.01, 0.9),         \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),         \"l2_leaf_reg\":trial.suggest_float('l2_leaf_reg', 0.01, 2),         \"subsample\": trial.suggest_float('subsample', 0.01, 1),         \"random_strength\": trial.suggest_float('random_strength', 1, 200),         \"min_data_in_leaf\":trial.suggest_float('min_data_in_leaf', 1, 500)     }      cat = CatBoostClassifier(         logging_level=\"Silent\",         eval_metric=\"AUC\",         grow_policy=\"Lossguide\",         random_seed=42,         **param)     cat.fit(X_train, y_train,             eval_set=(X_valid, y_valid),             verbose=False,             early_stopping_rounds=10            )      preds = cat.predict_proba(X_valid)[:,1]     auc = roc_auc_score(y_valid, preds)      return auc <p>\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043e\u0431\u044a\u0435\u043a\u0442 \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u044f \u0438 \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044e</p> In\u00a0[\u00a0]: Copied! <pre>study = optuna.create_study(direction=\"maximize\", study_name='CatBoostClassifier')\nstudy.optimize(objective, n_trials=100) # \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0439\u0442\u0435 \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0442\u044c\n\nprint(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"Value: {}\".format(trial.value))\n\nprint(\"Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n</pre> study = optuna.create_study(direction=\"maximize\", study_name='CatBoostClassifier') study.optimize(objective, n_trials=100) # \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0439\u0442\u0435 \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0442\u044c  print(\"Number of finished trials: {}\".format(len(study.trials)))  print(\"Best trial:\") trial = study.best_trial  print(\"Value: {}\".format(trial.value))  print(\"Params: \") for key, value in trial.params.items():     print(\"    {}: {}\".format(key, value)) <pre>[I 2024-08-14 03:46:44,732] A new study created in memory with name: CatBoostClassifier\n[I 2024-08-14 03:46:44,948] Trial 0 finished with value: 0.5 and parameters: {'learning_rate': 0.2165534023121395, 'max_depth': 2, 'l2_leaf_reg': 1.5477322039068058, 'subsample': 0.029345419918240072, 'random_strength': 6.5809964961892495, 'min_data_in_leaf': 477.52416517082804}. Best is trial 0 with value: 0.5.\n[I 2024-08-14 03:46:45,339] Trial 1 finished with value: 0.8372625321777863 and parameters: {'learning_rate': 0.789728112113311, 'max_depth': 4, 'l2_leaf_reg': 0.6004269201561995, 'subsample': 0.55359191980997, 'random_strength': 46.57861106355321, 'min_data_in_leaf': 425.7303912933811}. Best is trial 1 with value: 0.8372625321777863.\n[I 2024-08-14 03:46:45,992] Trial 2 finished with value: 0.8485835604479672 and parameters: {'learning_rate': 0.4634724182719374, 'max_depth': 6, 'l2_leaf_reg': 0.575436612277608, 'subsample': 0.42186545862300934, 'random_strength': 123.97404853733563, 'min_data_in_leaf': 98.4208597017056}. Best is trial 2 with value: 0.8485835604479672.\n[I 2024-08-14 03:46:48,201] Trial 3 finished with value: 0.8515032752320888 and parameters: {'learning_rate': 0.09543005782339105, 'max_depth': 9, 'l2_leaf_reg': 1.2167151901623765, 'subsample': 0.06245537706270228, 'random_strength': 61.228218388468974, 'min_data_in_leaf': 38.39023875452006}. Best is trial 3 with value: 0.8515032752320888.\n[I 2024-08-14 03:46:48,708] Trial 4 finished with value: 0.8406711796542305 and parameters: {'learning_rate': 0.7331013466248556, 'max_depth': 6, 'l2_leaf_reg': 0.012765794151882687, 'subsample': 0.14335624784813372, 'random_strength': 73.50485090264708, 'min_data_in_leaf': 380.00905654802966}. Best is trial 3 with value: 0.8515032752320888.\n[I 2024-08-14 03:46:50,233] Trial 5 finished with value: 0.8435615893243013 and parameters: {'learning_rate': 0.23370405202769556, 'max_depth': 9, 'l2_leaf_reg': 0.16497446248632272, 'subsample': 0.8752850397650734, 'random_strength': 111.64581392121151, 'min_data_in_leaf': 388.6064442254268}. Best is trial 3 with value: 0.8515032752320888.\n[I 2024-08-14 03:46:51,271] Trial 6 finished with value: 0.8226369667047633 and parameters: {'learning_rate': 0.04122993099237723, 'max_depth': 2, 'l2_leaf_reg': 0.20284874078763926, 'subsample': 0.6367717313128627, 'random_strength': 161.4305343264782, 'min_data_in_leaf': 18.271195536437006}. Best is trial 3 with value: 0.8515032752320888.\n[I 2024-08-14 03:46:51,778] Trial 7 finished with value: 0.8371075235482015 and parameters: {'learning_rate': 0.8582564268296371, 'max_depth': 7, 'l2_leaf_reg': 0.6159108475992469, 'subsample': 0.6430359063610354, 'random_strength': 194.77206434441388, 'min_data_in_leaf': 395.81326570058985}. Best is trial 3 with value: 0.8515032752320888.\n[I 2024-08-14 03:46:52,106] Trial 8 finished with value: 0.8514030208945464 and parameters: {'learning_rate': 0.6220500822136842, 'max_depth': 5, 'l2_leaf_reg': 0.6614202111677746, 'subsample': 0.3763622594650606, 'random_strength': 1.5372606636099502, 'min_data_in_leaf': 210.81684346685665}. Best is trial 3 with value: 0.8515032752320888.\n[I 2024-08-14 03:46:53,023] Trial 9 finished with value: 0.8379396345498039 and parameters: {'learning_rate': 0.7670675853970256, 'max_depth': 8, 'l2_leaf_reg': 1.8403233110251145, 'subsample': 0.33221049473597575, 'random_strength': 76.59908186125371, 'min_data_in_leaf': 220.27667189607422}. Best is trial 3 with value: 0.8515032752320888.\n[I 2024-08-14 03:46:53,795] Trial 10 finished with value: 0.8352975471619539 and parameters: {'learning_rate': 0.03939269976022153, 'max_depth': 10, 'l2_leaf_reg': 1.2239191289705484, 'subsample': 0.1971957977790358, 'random_strength': 45.97984119612045, 'min_data_in_leaf': 117.68547624545266}. Best is trial 3 with value: 0.8515032752320888.\n[I 2024-08-14 03:46:54,182] Trial 11 finished with value: 0.8534204466407855 and parameters: {'learning_rate': 0.5292947899032613, 'max_depth': 4, 'l2_leaf_reg': 1.0562715822887339, 'subsample': 0.29238467023057874, 'random_strength': 1.0080155926863474, 'min_data_in_leaf': 217.05912068009718}. Best is trial 11 with value: 0.8534204466407855.\n[I 2024-08-14 03:46:54,925] Trial 12 finished with value: 0.854156159240905 and parameters: {'learning_rate': 0.46667965752333035, 'max_depth': 4, 'l2_leaf_reg': 1.1034539045748595, 'subsample': 0.2177545543283828, 'random_strength': 33.69191424519235, 'min_data_in_leaf': 287.6354438748095}. Best is trial 12 with value: 0.854156159240905.\n[I 2024-08-14 03:46:55,261] Trial 13 finished with value: 0.8481239328696957 and parameters: {'learning_rate': 0.47388030842524104, 'max_depth': 4, 'l2_leaf_reg': 0.9841376098092054, 'subsample': 0.23796767505443062, 'random_strength': 30.559857725987595, 'min_data_in_leaf': 304.8772947881302}. Best is trial 12 with value: 0.854156159240905.\n[I 2024-08-14 03:46:55,774] Trial 14 finished with value: 0.8512302749590884 and parameters: {'learning_rate': 0.5677435867325714, 'max_depth': 4, 'l2_leaf_reg': 0.9858935858858828, 'subsample': 0.2880787724849706, 'random_strength': 19.641914653188316, 'min_data_in_leaf': 296.50997464128113}. Best is trial 12 with value: 0.854156159240905.\n[I 2024-08-14 03:46:56,213] Trial 15 finished with value: 0.8494503748741036 and parameters: {'learning_rate': 0.37166572152717614, 'max_depth': 3, 'l2_leaf_reg': 1.4583249930757494, 'subsample': 0.9955315242420071, 'random_strength': 88.02174705220291, 'min_data_in_leaf': 179.3738772355934}. Best is trial 12 with value: 0.854156159240905.\n[I 2024-08-14 03:46:57,055] Trial 16 finished with value: 0.8559885000562966 and parameters: {'learning_rate': 0.3210310795870268, 'max_depth': 5, 'l2_leaf_reg': 1.9094658324753917, 'subsample': 0.4804851220395184, 'random_strength': 33.49522734292147, 'min_data_in_leaf': 287.9109110478402}. Best is trial 16 with value: 0.8559885000562966.\n[I 2024-08-14 03:46:57,763] Trial 17 finished with value: 0.847795407117441 and parameters: {'learning_rate': 0.35378097903239786, 'max_depth': 5, 'l2_leaf_reg': 1.9401989755988598, 'subsample': 0.48416569735771703, 'random_strength': 40.07318375520298, 'min_data_in_leaf': 303.24367032588754}. Best is trial 16 with value: 0.8559885000562966.\n[I 2024-08-14 03:46:58,649] Trial 18 finished with value: 0.8543381594229051 and parameters: {'learning_rate': 0.3270310046948177, 'max_depth': 5, 'l2_leaf_reg': 1.6932697259348661, 'subsample': 0.79447929672279, 'random_strength': 136.12105973617452, 'min_data_in_leaf': 329.93271902824506}. Best is trial 16 with value: 0.8559885000562966.\n[I 2024-08-14 03:46:59,576] Trial 19 finished with value: 0.8500765788901381 and parameters: {'learning_rate': 0.2708829352692093, 'max_depth': 7, 'l2_leaf_reg': 1.7038356958216352, 'subsample': 0.7632729569486497, 'random_strength': 138.4597489310068, 'min_data_in_leaf': 343.8985733074137}. Best is trial 16 with value: 0.8559885000562966.\n[I 2024-08-14 03:47:00,116] Trial 20 finished with value: 0.8531150565048871 and parameters: {'learning_rate': 0.3626267613161781, 'max_depth': 5, 'l2_leaf_reg': 1.9795846165177058, 'subsample': 0.7827032985899299, 'random_strength': 142.73839861496384, 'min_data_in_leaf': 493.0493076589869}. Best is trial 16 with value: 0.8559885000562966.\n[I 2024-08-14 03:47:00,940] Trial 21 finished with value: 0.8563910597808904 and parameters: {'learning_rate': 0.16961344589594285, 'max_depth': 3, 'l2_leaf_reg': 1.649834982836516, 'subsample': 0.5749517198100278, 'random_strength': 99.89102990362149, 'min_data_in_leaf': 271.7012421453372}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:02,167] Trial 22 finished with value: 0.8344091394938851 and parameters: {'learning_rate': 0.14552950442407062, 'max_depth': 3, 'l2_leaf_reg': 1.6649232359966202, 'subsample': 0.5985514802507333, 'random_strength': 169.99871764933434, 'min_data_in_leaf': 261.3080549135861}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:04,114] Trial 23 finished with value: 0.8545186172304817 and parameters: {'learning_rate': 0.15793176071004453, 'max_depth': 3, 'l2_leaf_reg': 1.3526112935873842, 'subsample': 0.7175761381145475, 'random_strength': 113.99285178004443, 'min_data_in_leaf': 352.6566495237683}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:06,560] Trial 24 finished with value: 0.8553638384146858 and parameters: {'learning_rate': 0.1556921373541092, 'max_depth': 3, 'l2_leaf_reg': 1.4115008286364423, 'subsample': 0.480391200689659, 'random_strength': 100.39425793454086, 'min_data_in_leaf': 350.01711545220127}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:08,806] Trial 25 finished with value: 0.8480560683950514 and parameters: {'learning_rate': 0.16409141193428203, 'max_depth': 2, 'l2_leaf_reg': 1.496137374339839, 'subsample': 0.46620575811523923, 'random_strength': 91.45570082665543, 'min_data_in_leaf': 163.57983997102036}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:10,275] Trial 26 finished with value: 0.8544893121164308 and parameters: {'learning_rate': 0.2837819547192468, 'max_depth': 3, 'l2_leaf_reg': 1.8020953679362315, 'subsample': 0.5189916467893968, 'random_strength': 101.08298523619007, 'min_data_in_leaf': 253.22233056776207}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:10,680] Trial 27 finished with value: 0.8211246685822957 and parameters: {'learning_rate': 0.1044556356752651, 'max_depth': 2, 'l2_leaf_reg': 1.3459091803909486, 'subsample': 0.431619565523395, 'random_strength': 59.294142417791434, 'min_data_in_leaf': 449.8387981624195}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:10,968] Trial 28 finished with value: 0.8035146085993543 and parameters: {'learning_rate': 0.21613547632724206, 'max_depth': 3, 'l2_leaf_reg': 1.6092822334057308, 'subsample': 0.5676045993284018, 'random_strength': 72.80957980792742, 'min_data_in_leaf': 263.0893593280764}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:11,494] Trial 29 finished with value: 0.8547792785080921 and parameters: {'learning_rate': 0.2086592133839792, 'max_depth': 2, 'l2_leaf_reg': 0.819697698152623, 'subsample': 0.6834571224386156, 'random_strength': 19.984681853813314, 'min_data_in_leaf': 453.6832846244588}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:12,284] Trial 30 finished with value: 0.8367967351018197 and parameters: {'learning_rate': 0.3944960136703665, 'max_depth': 7, 'l2_leaf_reg': 1.8676872612497761, 'subsample': 0.38308326997649067, 'random_strength': 154.4491135123389, 'min_data_in_leaf': 332.6615450443436}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:12,895] Trial 31 finished with value: 0.8546836512938208 and parameters: {'learning_rate': 0.21596851649554094, 'max_depth': 2, 'l2_leaf_reg': 0.8664350791512772, 'subsample': 0.6869620770295259, 'random_strength': 24.469562538665006, 'min_data_in_leaf': 444.9760164258388}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:13,411] Trial 32 finished with value: 0.8548571684164904 and parameters: {'learning_rate': 0.28016951231760356, 'max_depth': 2, 'l2_leaf_reg': 0.806914526399829, 'subsample': 0.5400321582582661, 'random_strength': 16.595337724789662, 'min_data_in_leaf': 425.5676845159874}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:14,332] Trial 33 finished with value: 0.8465121515968972 and parameters: {'learning_rate': 0.2943832841669326, 'max_depth': 3, 'l2_leaf_reg': 1.4002865433211302, 'subsample': 0.5244965679173971, 'random_strength': 55.18009770759243, 'min_data_in_leaf': 411.0478307080825}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:15,095] Trial 34 finished with value: 0.8532785481938023 and parameters: {'learning_rate': 0.41204275970364945, 'max_depth': 4, 'l2_leaf_reg': 1.5580897723724074, 'subsample': 0.588510970323773, 'random_strength': 119.79281757796326, 'min_data_in_leaf': 388.1140383915803}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:15,717] Trial 35 finished with value: 0.8286869303818456 and parameters: {'learning_rate': 0.08975166153572325, 'max_depth': 2, 'l2_leaf_reg': 1.2419086491253597, 'subsample': 0.44346103753859356, 'random_strength': 12.29947978876556, 'min_data_in_leaf': 364.58824440904306}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:16,582] Trial 36 finished with value: 0.852669310296429 and parameters: {'learning_rate': 0.250033922012678, 'max_depth': 6, 'l2_leaf_reg': 0.42864672108139, 'subsample': 0.5268262007033628, 'random_strength': 95.42973443090546, 'min_data_in_leaf': 429.3365607018824}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:17,024] Trial 37 finished with value: 0.8127665415801009 and parameters: {'learning_rate': 0.1691762432733271, 'max_depth': 3, 'l2_leaf_reg': 1.7829743976227164, 'subsample': 0.3765836616592204, 'random_strength': 105.88486237007761, 'min_data_in_leaf': 475.9380950106968}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:17,801] Trial 38 finished with value: 0.852903751208836 and parameters: {'learning_rate': 0.3144852057758997, 'max_depth': 5, 'l2_leaf_reg': 0.7468205513730214, 'subsample': 0.6288168983269199, 'random_strength': 81.03884687055434, 'min_data_in_leaf': 278.02630322886773}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:18,104] Trial 39 finished with value: 0.7429039208700227 and parameters: {'learning_rate': 0.0916001703115347, 'max_depth': 2, 'l2_leaf_reg': 0.3209407231862438, 'subsample': 0.48947279248425846, 'random_strength': 63.425336876240344, 'min_data_in_leaf': 312.50209443952326}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:18,299] Trial 40 finished with value: 0.5 and parameters: {'learning_rate': 0.6819438096729318, 'max_depth': 6, 'l2_leaf_reg': 1.1696390615306496, 'subsample': 0.02690876704491979, 'random_strength': 186.05663001448391, 'min_data_in_leaf': 237.24363431981527}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:18,628] Trial 41 finished with value: 0.8275224376919292 and parameters: {'learning_rate': 0.20539423781999722, 'max_depth': 2, 'l2_leaf_reg': 0.7875365906927447, 'subsample': 0.7197475219510165, 'random_strength': 14.4847764719263, 'min_data_in_leaf': 469.9772033903035}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:19,489] Trial 42 finished with value: 0.8467774399977791 and parameters: {'learning_rate': 0.11258151909406194, 'max_depth': 2, 'l2_leaf_reg': 0.8721010222525916, 'subsample': 0.6594836798807467, 'random_strength': 48.89095313702967, 'min_data_in_leaf': 402.73822377455843}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:20,194] Trial 43 finished with value: 0.8518981230845636 and parameters: {'learning_rate': 0.19221299265782907, 'max_depth': 3, 'l2_leaf_reg': 0.4867586865668939, 'subsample': 0.5571688603077394, 'random_strength': 31.93422189548409, 'min_data_in_leaf': 363.31928598127024}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:22,996] Trial 44 finished with value: 0.8487424250136115 and parameters: {'learning_rate': 0.039144856136337974, 'max_depth': 4, 'l2_leaf_reg': 0.9009428480096298, 'subsample': 0.8375833862722262, 'random_strength': 6.814579833825066, 'min_data_in_leaf': 432.94909658576285}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:23,939] Trial 45 finished with value: 0.8486205774341369 and parameters: {'learning_rate': 0.2510132838894006, 'max_depth': 2, 'l2_leaf_reg': 0.6907636418579387, 'subsample': 0.6280995182686069, 'random_strength': 24.31951969212401, 'min_data_in_leaf': 374.4772844339267}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:25,057] Trial 46 finished with value: 0.8486591367947299 and parameters: {'learning_rate': 0.4177823887952824, 'max_depth': 3, 'l2_leaf_reg': 0.5837206207668852, 'subsample': 0.10938321642670351, 'random_strength': 41.931702968776605, 'min_data_in_leaf': 178.84495282770186}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:25,696] Trial 47 finished with value: 0.823857756061146 and parameters: {'learning_rate': 0.019782576463977575, 'max_depth': 4, 'l2_leaf_reg': 1.9024552651741735, 'subsample': 0.40297843167975866, 'random_strength': 66.82247683667727, 'min_data_in_leaf': 316.8543796002272}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:26,796] Trial 48 finished with value: 0.8527418018943443 and parameters: {'learning_rate': 0.12284365746995335, 'max_depth': 8, 'l2_leaf_reg': 1.5884344189532063, 'subsample': 0.3276266788784489, 'random_strength': 51.29989880997462, 'min_data_in_leaf': 459.0054658582284}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:27,241] Trial 49 finished with value: 0.8540805828941422 and parameters: {'learning_rate': 0.3299775712117529, 'max_depth': 3, 'l2_leaf_reg': 1.0878543895513753, 'subsample': 0.6942586059012404, 'random_strength': 11.713670742817778, 'min_data_in_leaf': 495.2165248684829}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:27,758] Trial 50 finished with value: 0.8472201014573897 and parameters: {'learning_rate': 0.25324978223894085, 'max_depth': 4, 'l2_leaf_reg': 1.7363552106408617, 'subsample': 0.5877810779665728, 'random_strength': 37.972565443197155, 'min_data_in_leaf': 412.6914975963677}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:28,249] Trial 51 finished with value: 0.8474005592649662 and parameters: {'learning_rate': 0.19612243767630727, 'max_depth': 2, 'l2_leaf_reg': 0.8917975193826132, 'subsample': 0.6848753748976462, 'random_strength': 24.324814512268816, 'min_data_in_leaf': 436.96437349015264}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:28,631] Trial 52 finished with value: 0.8247153162407398 and parameters: {'learning_rate': 0.06857413116836153, 'max_depth': 2, 'l2_leaf_reg': 0.8093899327079714, 'subsample': 0.8856644877809534, 'random_strength': 22.15656724840708, 'min_data_in_leaf': 454.16888064861433}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:28,958] Trial 53 finished with value: 0.8552512450817537 and parameters: {'learning_rate': 0.2277359380751674, 'max_depth': 2, 'l2_leaf_reg': 1.0044730574652196, 'subsample': 0.5426082432903984, 'random_strength': 2.092346231014936, 'min_data_in_leaf': 283.86020680504697}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:29,458] Trial 54 finished with value: 0.8545217019793291 and parameters: {'learning_rate': 0.3128193984950219, 'max_depth': 3, 'l2_leaf_reg': 1.008665858224686, 'subsample': 0.47370734259106645, 'random_strength': 6.662500258650947, 'min_data_in_leaf': 283.74741280019464}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:29,754] Trial 55 finished with value: 0.8421464607905287 and parameters: {'learning_rate': 0.8773525297785807, 'max_depth': 2, 'l2_leaf_reg': 1.149059206077042, 'subsample': 0.5453409140242778, 'random_strength': 128.55684672230558, 'min_data_in_leaf': 233.31803747702202}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:30,145] Trial 56 finished with value: 0.8550931517033213 and parameters: {'learning_rate': 0.13656066626789937, 'max_depth': 2, 'l2_leaf_reg': 1.2919909470689186, 'subsample': 0.43933745123796614, 'random_strength': 1.7054069646826093, 'min_data_in_leaf': 62.17289685684315}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:30,493] Trial 57 finished with value: 0.8527880731270562 and parameters: {'learning_rate': 0.13041630646518754, 'max_depth': 10, 'l2_leaf_reg': 1.274378826982479, 'subsample': 0.34334490211186264, 'random_strength': 4.2565201685199305, 'min_data_in_leaf': 71.55126556167352}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:30,682] Trial 58 finished with value: 0.8514184446387836 and parameters: {'learning_rate': 0.16594689210860503, 'max_depth': 3, 'l2_leaf_reg': 1.4888320186002333, 'subsample': 0.4376508602553443, 'random_strength': 1.3885552941461277, 'min_data_in_leaf': 152.70173531249966}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:31,007] Trial 59 finished with value: 0.8540620744010574 and parameters: {'learning_rate': 0.2750010284884532, 'max_depth': 5, 'l2_leaf_reg': 1.2746287691142977, 'subsample': 0.5071138630239854, 'random_strength': 16.021246309606525, 'min_data_in_leaf': 8.92644330176222}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:31,275] Trial 60 finished with value: 0.8236294846464337 and parameters: {'learning_rate': 0.06580145985259861, 'max_depth': 4, 'l2_leaf_reg': 1.4207580932038155, 'subsample': 0.2559732452953547, 'random_strength': 34.10123336167225, 'min_data_in_leaf': 35.04352061528167}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:31,513] Trial 61 finished with value: 0.8484570857452214 and parameters: {'learning_rate': 0.23125001722207209, 'max_depth': 2, 'l2_leaf_reg': 0.951125702424557, 'subsample': 0.45236883411477885, 'random_strength': 16.37629361091796, 'min_data_in_leaf': 276.32032283099494}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:31,848] Trial 62 finished with value: 0.8491604084824422 and parameters: {'learning_rate': 0.18521171522349617, 'max_depth': 2, 'l2_leaf_reg': 1.0233833614681076, 'subsample': 0.6028311345426446, 'random_strength': 27.84478190364073, 'min_data_in_leaf': 212.63167127467798}. Best is trial 21 with value: 0.8563910597808904.\n[I 2024-08-14 03:47:32,139] Trial 63 finished with value: 0.8579966715559936 and parameters: {'learning_rate': 0.2275828875803787, 'max_depth': 2, 'l2_leaf_reg': 1.649969257251929, 'subsample': 0.41432801712540723, 'random_strength': 9.871440937029153, 'min_data_in_leaf': 339.4206240957627}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:32,353] Trial 64 finished with value: 0.8410583156345868 and parameters: {'learning_rate': 0.15443576863058872, 'max_depth': 3, 'l2_leaf_reg': 1.6356148561301187, 'subsample': 0.40829684886366496, 'random_strength': 8.920803659606767, 'min_data_in_leaf': 329.25270044657947}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:32,559] Trial 65 finished with value: 0.8475794746981188 and parameters: {'learning_rate': 0.52262191640972, 'max_depth': 2, 'l2_leaf_reg': 1.7570515967211897, 'subsample': 0.35209745591260677, 'random_strength': 79.31033248169595, 'min_data_in_leaf': 298.8152094762548}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:32,817] Trial 66 finished with value: 0.8545409816596259 and parameters: {'learning_rate': 0.3397594287720047, 'max_depth': 3, 'l2_leaf_reg': 1.51439591232287, 'subsample': 0.48690121670146136, 'random_strength': 113.64174712891179, 'min_data_in_leaf': 344.9705847573752}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:33,180] Trial 67 finished with value: 0.8527741917572427 and parameters: {'learning_rate': 0.2872741864314234, 'max_depth': 6, 'l2_leaf_reg': 1.9758241623536021, 'subsample': 0.30706088302917706, 'random_strength': 86.69768770901611, 'min_data_in_leaf': 195.60397604666105}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:33,542] Trial 68 finished with value: 0.8388542625830762 and parameters: {'learning_rate': 0.37734226459157394, 'max_depth': 7, 'l2_leaf_reg': 1.84386701582687, 'subsample': 0.5416787382022203, 'random_strength': 102.93491264871817, 'min_data_in_leaf': 100.73573695821503}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:33,798] Trial 69 finished with value: 0.8491079677520357 and parameters: {'learning_rate': 0.2396158685166121, 'max_depth': 3, 'l2_leaf_reg': 1.3497075412335753, 'subsample': 0.41533858440585675, 'random_strength': 9.011802604306009, 'min_data_in_leaf': 244.2984153117823}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:34,249] Trial 70 finished with value: 0.8551000923882279 and parameters: {'learning_rate': 0.1405771071007593, 'max_depth': 2, 'l2_leaf_reg': 1.6670425816135785, 'subsample': 0.5628243022632802, 'random_strength': 43.63893059165331, 'min_data_in_leaf': 265.0039335083252}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:34,463] Trial 71 finished with value: 0.8369964725896929 and parameters: {'learning_rate': 0.1346261793235442, 'max_depth': 2, 'l2_leaf_reg': 1.69505852020515, 'subsample': 0.5662727341567123, 'random_strength': 18.17730751314747, 'min_data_in_leaf': 261.9534789983721}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:34,721] Trial 72 finished with value: 0.8321156287257981 and parameters: {'learning_rate': 0.07724194746378436, 'max_depth': 2, 'l2_leaf_reg': 1.6529774456888673, 'subsample': 0.5130735404504633, 'random_strength': 43.04440541458727, 'min_data_in_leaf': 271.8748406602916}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:34,936] Trial 73 finished with value: 0.8534474381932009 and parameters: {'learning_rate': 0.2193541378472138, 'max_depth': 2, 'l2_leaf_reg': 1.55127358733355, 'subsample': 0.45844426802461946, 'random_strength': 1.2277571480166767, 'min_data_in_leaf': 323.54337938841485}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:35,129] Trial 74 finished with value: 0.8245140363784432 and parameters: {'learning_rate': 0.17096273713989574, 'max_depth': 3, 'l2_leaf_reg': 1.4299055079718532, 'subsample': 0.6103423491223143, 'random_strength': 35.52778769358011, 'min_data_in_leaf': 309.4874282846581}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:35,471] Trial 75 finished with value: 0.849687900535358 and parameters: {'learning_rate': 0.1424757474479632, 'max_depth': 2, 'l2_leaf_reg': 1.9040100631862928, 'subsample': 0.49178296476749883, 'random_strength': 29.861178645449915, 'min_data_in_leaf': 288.4169959906686}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:35,653] Trial 76 finished with value: 0.8166471556302065 and parameters: {'learning_rate': 0.10900450958876559, 'max_depth': 3, 'l2_leaf_reg': 1.1584526059635818, 'subsample': 0.5436945071994185, 'random_strength': 97.11608635264365, 'min_data_in_leaf': 252.08827881313073}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:36,111] Trial 77 finished with value: 0.8529130054553784 and parameters: {'learning_rate': 0.26344618455418334, 'max_depth': 2, 'l2_leaf_reg': 1.8036914133857063, 'subsample': 0.5759920935180337, 'random_strength': 149.97345964123704, 'min_data_in_leaf': 230.53437749650683}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:36,464] Trial 78 finished with value: 0.8554301605149063 and parameters: {'learning_rate': 0.30386516642561456, 'max_depth': 2, 'l2_leaf_reg': 1.5941173743706698, 'subsample': 0.388487385094279, 'random_strength': 12.04964504414562, 'min_data_in_leaf': 340.15331434948547}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:36,925] Trial 79 finished with value: 0.8556607454912538 and parameters: {'learning_rate': 0.3043185361985488, 'max_depth': 3, 'l2_leaf_reg': 1.6062543081685263, 'subsample': 0.38163010145401816, 'random_strength': 10.801512721309756, 'min_data_in_leaf': 336.9094072537885}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:37,511] Trial 80 finished with value: 0.8507459693900372 and parameters: {'learning_rate': 0.2967690808832415, 'max_depth': 4, 'l2_leaf_reg': 1.5769102540118054, 'subsample': 0.3683396074299298, 'random_strength': 119.50189785234694, 'min_data_in_leaf': 352.7530072728084}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:37,788] Trial 81 finished with value: 0.8495382902162564 and parameters: {'learning_rate': 0.3519982264605916, 'max_depth': 3, 'l2_leaf_reg': 1.491074642500498, 'subsample': 0.39339814284208235, 'random_strength': 9.046452450187513, 'min_data_in_leaf': 338.6181638900527}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:38,153] Trial 82 finished with value: 0.8492305865187221 and parameters: {'learning_rate': 0.442607098792248, 'max_depth': 2, 'l2_leaf_reg': 1.7262120187355963, 'subsample': 0.27234819741871114, 'random_strength': 12.955991756657548, 'min_data_in_leaf': 381.46339177847165}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:38,943] Trial 83 finished with value: 0.8509557323116645 and parameters: {'learning_rate': 0.18048848769077014, 'max_depth': 9, 'l2_leaf_reg': 1.6096921762481307, 'subsample': 0.31232826961375537, 'random_strength': 27.6479977169267, 'min_data_in_leaf': 362.9959768769905}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:39,536] Trial 84 finished with value: 0.8530302259115817 and parameters: {'learning_rate': 0.22708933829872321, 'max_depth': 3, 'l2_leaf_reg': 1.6632316572246608, 'subsample': 0.4206405376875545, 'random_strength': 20.498316514413297, 'min_data_in_leaf': 297.40518856402844}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:40,270] Trial 85 finished with value: 0.844975946670862 and parameters: {'learning_rate': 0.3134824967461136, 'max_depth': 8, 'l2_leaf_reg': 1.458604426862953, 'subsample': 0.46636325980991755, 'random_strength': 172.65958600142233, 'min_data_in_leaf': 308.2224094733233}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:40,626] Trial 86 finished with value: 0.853733548648803 and parameters: {'learning_rate': 0.20516007929387697, 'max_depth': 4, 'l2_leaf_reg': 1.8081881256255257, 'subsample': 0.3669610320292627, 'random_strength': 71.7353570023819, 'min_data_in_leaf': 349.71334841544535}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:40,844] Trial 87 finished with value: 0.847503898351356 and parameters: {'learning_rate': 0.3850725922826087, 'max_depth': 2, 'l2_leaf_reg': 1.3738184718048245, 'subsample': 0.4394316413220218, 'random_strength': 13.073971750314525, 'min_data_in_leaf': 319.76257608276416}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:41,130] Trial 88 finished with value: 0.8509834950512916 and parameters: {'learning_rate': 0.2486495279955683, 'max_depth': 3, 'l2_leaf_reg': 1.3060721511513962, 'subsample': 0.5036611916707044, 'random_strength': 6.582515968741947, 'min_data_in_leaf': 288.24916069068973}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:41,464] Trial 89 finished with value: 0.8411153834882648 and parameters: {'learning_rate': 0.05111799564446984, 'max_depth': 2, 'l2_leaf_reg': 1.537356157859457, 'subsample': 0.3880066956376303, 'random_strength': 4.295324207554229, 'min_data_in_leaf': 266.43818936484587}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:41,615] Trial 90 finished with value: 0.809689504604759 and parameters: {'learning_rate': 0.14849530714748765, 'max_depth': 2, 'l2_leaf_reg': 1.604654407357084, 'subsample': 0.6488514855233791, 'random_strength': 109.07773151115863, 'min_data_in_leaf': 325.72222318586404}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:41,889] Trial 91 finished with value: 0.8535808535808537 and parameters: {'learning_rate': 0.2768348084629538, 'max_depth': 2, 'l2_leaf_reg': 1.7020206569000091, 'subsample': 0.5270113119832779, 'random_strength': 19.279543505451198, 'min_data_in_leaf': 390.8245718531585}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:42,108] Trial 92 finished with value: 0.8445248021519208 and parameters: {'learning_rate': 0.2980320322130359, 'max_depth': 2, 'l2_leaf_reg': 0.9413144021105265, 'subsample': 0.18491093410744158, 'random_strength': 23.462444073545914, 'min_data_in_leaf': 371.68996169817217}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:42,275] Trial 93 finished with value: 0.8438677506474116 and parameters: {'learning_rate': 0.8018049022359041, 'max_depth': 3, 'l2_leaf_reg': 0.7207307279562393, 'subsample': 0.4817377653790264, 'random_strength': 54.11215711265974, 'min_data_in_leaf': 247.83798275985313}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:42,629] Trial 94 finished with value: 0.8526831916662426 and parameters: {'learning_rate': 0.18976503763926902, 'max_depth': 5, 'l2_leaf_reg': 1.0778005788536056, 'subsample': 0.42051153997605584, 'random_strength': 15.934790667198342, 'min_data_in_leaf': 418.39615211072214}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:42,894] Trial 95 finished with value: 0.8465136939713211 and parameters: {'learning_rate': 0.3330384372789757, 'max_depth': 2, 'l2_leaf_reg': 1.9197824427277657, 'subsample': 0.5366007428124753, 'random_strength': 44.67016583545405, 'min_data_in_leaf': 200.40708572900584}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:43,205] Trial 96 finished with value: 0.8505886471988167 and parameters: {'learning_rate': 0.26241859653774113, 'max_depth': 3, 'l2_leaf_reg': 0.6304145123151321, 'subsample': 0.4542962823423417, 'random_strength': 37.17578672829132, 'min_data_in_leaf': 399.3636999400437}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:43,443] Trial 97 finished with value: 0.8340196899518935 and parameters: {'learning_rate': 0.09242477857682523, 'max_depth': 2, 'l2_leaf_reg': 0.0645769669423184, 'subsample': 0.5820960690872515, 'random_strength': 11.65162235424116, 'min_data_in_leaf': 358.5938499475734}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:43,772] Trial 98 finished with value: 0.8560116356726526 and parameters: {'learning_rate': 0.2238295228087367, 'max_depth': 2, 'l2_leaf_reg': 1.4538002550468356, 'subsample': 0.6177874774515071, 'random_strength': 26.070466696158935, 'min_data_in_leaf': 336.943504306358}. Best is trial 63 with value: 0.8579966715559936.\n[I 2024-08-14 03:47:44,246] Trial 99 finished with value: 0.8533078533078534 and parameters: {'learning_rate': 0.12342253771037937, 'max_depth': 3, 'l2_leaf_reg': 1.446914784821623, 'subsample': 0.6193773627486576, 'random_strength': 32.4812721291412, 'min_data_in_leaf': 336.8229861132391}. Best is trial 63 with value: 0.8579966715559936.\n</pre> <pre>Number of finished trials: 100\nBest trial:\nValue: 0.8579966715559936\nParams: \n    learning_rate: 0.2275828875803787\n    max_depth: 2\n    l2_leaf_reg: 1.649969257251929\n    subsample: 0.41432801712540723\n    random_strength: 9.871440937029153\n    min_data_in_leaf: 339.4206240957627\n</pre> In\u00a0[\u00a0]: Copied! <pre>best_cat = CatBoostClassifier(**study.best_params, random_state=random_state)\nbest_cat.fit(X_train, y_train,\n            eval_set=(X_valid, y_valid),\n            verbose=False,\n             early_stopping_rounds=10\n           )\n\nres_cat = print_metrics(best_cat, X_train, y_train, X_valid, y_valid, X_test, y_test)\nres_cat\n</pre> best_cat = CatBoostClassifier(**study.best_params, random_state=random_state) best_cat.fit(X_train, y_train,             eval_set=(X_valid, y_valid),             verbose=False,              early_stopping_rounds=10            )  res_cat = print_metrics(best_cat, X_train, y_train, X_valid, y_valid, X_test, y_test) res_cat Out[\u00a0]: precision recall f1 roc_auc train 0.786 0.468 0.587 0.877 valid 0.825 0.464 0.594 0.855 test 0.754 0.442 0.557 0.856 <p>\u0418\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u044f \u0412\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u041f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432</p> <p>\u0414\u043b\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430 \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>SHAP</code> \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0438 \u0434\u043b\u044f \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u0438 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432</p> <ul> <li>\u0427\u0435\u043c \u0432\u044b\u0448\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a, \u0442\u0435\u043c \u043e\u043d \u0432\u0430\u0436\u043d\u0435\u0435</li> <li>\u0427\u0435\u043c \u043a\u0440\u0430\u0441\u043d\u0435\u0435 \u0442\u043e\u0447\u043a\u0430, \u0442\u0435\u043c \u0432\u044b\u0448\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430</li> </ul> <p>SHAP value</p> <ul> <li>\u0417\u043d\u0430\u0447\u0435\u043d\u0438\u044f SHAP \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u0441\u043e\u0431\u043e\u0439 \u0432\u043a\u043b\u0430\u0434 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u0432 \u043f\u0440\u043e\u0433\u043d\u043e\u0437 \u043c\u043e\u0434\u0435\u043b\u0438.</li> <li>\u041f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f SHAP \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0442 \u043d\u0430 \u0442\u043e, \u0447\u0442\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0432\u0430\u0435\u0442 \u043f\u0440\u043e\u0433\u043d\u043e\u0437 (\u0432 \u0441\u0442\u043e\u0440\u043e\u043d\u0443 \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430; \u043e\u0442\u0442\u043e\u043a \u043a\u043b\u0438\u0435\u043d\u0442\u0430)</li> </ul> <p>\u0420\u0430\u0441\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0422\u043e\u0447\u0435\u043a</p> <ul> <li>\u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0442\u043e\u0447\u0435\u043a \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u0438\u0437\u043c\u0435\u043d\u0447\u0438\u0432\u043e\u0441\u0442\u044c \u0432\u043b\u0438\u044f\u043d\u0438\u044f \u044d\u0442\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u043d\u0430 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u044b \u0434\u043b\u044f \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440\u043e\u0432.</li> <li>\u0411\u043e\u043b\u0435\u0435 \u0448\u0438\u0440\u043e\u043a\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u043d\u0430 \u0442\u043e, \u0447\u0442\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a \u0438\u043c\u0435\u0435\u0442 \u0440\u0430\u0437\u043d\u043e\u0435 \u0432\u043b\u0438\u044f\u043d\u0438\u0435 \u0432 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 \u0434\u0440\u0443\u0433\u0438\u0445 \u0444\u0430\u043a\u0442\u043e\u0440\u043e\u0432.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import shap\nexplainer = shap.TreeExplainer(best_cat)\nshap_values = explainer(X_train_valid)\n</pre> import shap explainer = shap.TreeExplainer(best_cat) shap_values = explainer(X_train_valid) In\u00a0[\u00a0]: Copied! <pre>shap.plots.beeswarm(shap_values)\n</pre> shap.plots.beeswarm(shap_values)"},{"location":"portfolio/course_recsys/Look-a-like_Segmentation.html#look-a-like","title":"\u0417\u0430\u0434\u0430\u0447\u0430 Look-a-Like\u00b6","text":""},{"location":"portfolio/course_recsys/Look-a-like_Segmentation.html#1","title":"1 | \u0412\u0432\u043e\u0434\u043d\u0430\u044f\u00b6","text":""},{"location":"portfolio/course_recsys/Look-a-like_Segmentation.html#look-a-like","title":"\u0417\u0430\u0434\u0430\u0447\u0430 look-a-like\u00b6","text":"<p>\u0426\u0435\u043b\u044c \u0437\u0430\u0434\u0430\u0447\u0438 look-a-like \u0437\u0430\u043a\u043b\u044e\u0447\u0430\u0435\u0442\u0441\u044f \u0432 \u0442\u043e\u043c \u0447\u0442\u043e \u0432\u044b \u0445\u043e\u0442\u0438\u043c \u043d\u0430\u0439\u0442\u0438 \u043f\u043e\u0445\u043e\u0436\u0438\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439.</p> <ul> <li>\u042d\u0442\u043e \u043d\u0443\u0436\u043d\u043e \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u043c\u043e\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u043f\u0440\u0435\u0434\u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0442\u044c \u043a\u0430\u043a\u0438\u0435 \u0442\u043e \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0435 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u044d\u0442\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438</li> </ul>"},{"location":"portfolio/course_recsys/Look-a-like_Segmentation.html","title":"\u0417\u0430\u0434\u0430\u0447\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043e\u0442\u0442\u043e\u043a\u0430\u00b6","text":"<p>\u0411\u0443\u0434\u0435\u043c \u0441\u0442\u0440\u043e\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0433\u0443\u0442 \u0443\u0439\u0442\u0438</p> <ul> <li>\u0417\u0430\u0434\u0430\u0447\u0443 look-a-like \u0431\u0443\u0434\u0435\u043c \u0440\u0435\u0448\u0430\u0442\u044c \u043d\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u043f\u043e\u0438\u0441\u043a\u0430 \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432, <code>\u0441\u043a\u043b\u043e\u043d\u043d\u044b\u0445 \u043a \u043e\u0442\u0442\u043e\u043a\u0443</code> \u0438\u0437 \u0431\u0430\u043d\u043a\u0430.</li> <li>\u0414\u0430\u0442\u0430\u0441\u0435\u0442 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0440\u0435\u0442\u0440\u043e-\u0434\u0430\u043d\u043d\u044b\u0435 \u043e \u043a\u043b\u0438\u0435\u043d\u0442\u0430\u0445, \u043e\u0442\u0442\u0435\u043a\u0448\u0438\u0445 \u0438\u0437 \u0431\u0430\u043d\u043a\u0430 - \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u0441\u0435\u0433\u043c\u0435\u043d\u0442. \u0410\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u043e - \u0438\u043c\u0435\u044e\u0442\u0441\u044f \u0434\u0430\u043d\u043d\u044b\u0435 \u043e \u0442\u0435\u0445, \u043a\u0442\u043e \u043d\u0435 \u043e\u0442\u0442\u0435\u043a.</li> <li>\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0434\u043b\u044f \u043b\u044e\u0431\u043e\u0433\u043e \u0434\u0440\u0443\u0433\u043e\u0433\u043e \u043a\u043b\u0438\u0435\u043d\u0442\u0430 \u0438\u0437 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c (\u0441\u043a\u043b\u043e\u043d\u043d\u043e\u0441\u0442\u044c \u043a \u043e\u0442\u0442\u043e\u043a\u0443).</li> </ul> <p>\u0417\u0430\u0434\u0430\u0447\u0430: \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0441 \u043f\u0440\u0435\u0434\u0435\u043b\u044c\u043d\u043e \u0431\u043e\u043b\u044c\u0448\u0438\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\u043c ROC-AUC</p>"},{"location":"portfolio/course_recsys/Look-a-like_Segmentation.html#2","title":"2 | \u0427\u0442\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":""},{"location":"portfolio/course_recsys/Look-a-like_Segmentation.html","title":"\u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\u00b6","text":"<ul> <li><code>RowNumber</code> \u2014 \u0438\u043d\u0434\u0435\u043a\u0441 \u0441\u0442\u0440\u043e\u043a\u0438 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445</li> <li><code>CustomerId</code> \u2014 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0439 \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u043a\u043b\u0438\u0435\u043d\u0442\u0430</li> <li><code>Surname</code> \u2014 \u0444\u0430\u043c\u0438\u043b\u0438\u044f</li> <li><code>CreditScore</code> \u2014 \u043a\u0440\u0435\u0434\u0438\u0442\u043d\u044b\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433</li> <li><code>Geography</code> \u2014 \u0441\u0442\u0440\u0430\u043d\u0430 \u043f\u0440\u043e\u0436\u0438\u0432\u0430\u043d\u0438\u044f</li> <li><code>Gender</code> \u2014 \u043f\u043e\u043b</li> <li><code>Age</code> \u2014 \u0432\u043e\u0437\u0440\u0430\u0441\u0442</li> <li><code>Tenure</code> \u2014 \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043b\u0435\u0442 \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u043c \u0431\u0430\u043d\u043a\u0430</li> <li><code>Balance</code> \u2014 \u0431\u0430\u043b\u0430\u043d\u0441 \u043d\u0430 \u0441\u0447\u0451\u0442\u0435</li> <li><code>NumOfProducts</code> \u2014 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432 \u0431\u0430\u043d\u043a\u0430, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044b\u0445 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u043c</li> <li><code>HasCrCard</code> \u2014 \u043d\u0430\u043b\u0438\u0447\u0438\u0435 \u043a\u0440\u0435\u0434\u0438\u0442\u043d\u043e\u0439 \u043a\u0430\u0440\u0442\u044b</li> <li><code>IsActiveMember</code> \u2014 \u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c \u043a\u043b\u0438\u0435\u043d\u0442\u0430</li> <li><code>EstimatedSalary</code> \u2014 \u043f\u0440\u0435\u0434\u043f\u043e\u043b\u0430\u0433\u0430\u0435\u043c\u0430\u044f \u0437\u0430\u0440\u043f\u043b\u0430\u0442\u0430</li> </ul>"},{"location":"portfolio/course_recsys/Look-a-like_Segmentation.html","title":"\u0426\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u00b6","text":"<ul> <li><code>Exited</code> \u2014 \u0444\u0430\u043a\u0442 \u0443\u0445\u043e\u0434\u0430 \u043a\u043b\u0438\u0435\u043d\u0442\u0430 (1 - \u041e\u0442\u0442\u043e\u043a)</li> </ul>"},{"location":"portfolio/course_recsys/Look-a-like_Segmentation.html#3","title":"3 | \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0414\u0430\u043d\u043d\u044b\u0445\u00b6","text":""},{"location":"portfolio/course_recsys/Look-a-like_Segmentation.html#4","title":"4 | \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430\u00b6","text":""},{"location":"portfolio/course_recsys/Look-a-like_Segmentation.html","title":"\u041a\u043e\u0434\u0438\u0440\u043e\u0432\u043a\u0430 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\u00b6","text":"<ul> <li>\u0412 \u0434\u0430\u043d\u043d\u044b\u0445 \u0435\u0441\u0442\u044c \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0438 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438.</li> <li>\u041a\u043e \u0432\u0441\u0435\u043c\u0443 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0443 \u043f\u0440\u0438\u043c\u0435\u043d\u0438\u043c One-Hot Endoding \u0438 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439</li> </ul> <p>\u0414\u0430\u043b\u0435\u0435 \u0431\u0443\u0434\u0435\u043c \u043e\u0431\u0443\u0447\u0430\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438:</p> <ul> <li>\u041b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e</li> <li>SVM</li> <li>\u0420\u0435\u0448\u0430\u044e\u0449\u0435\u0435 \u0434\u0435\u0440\u0435\u0432\u043e</li> <li>\u0421\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441</li> <li>\u0411\u0443\u0441\u0442\u0438\u043d\u0433</li> </ul>"},{"location":"portfolio/course_recsys/Look-a-like_Segmentation.html#5-traintest","title":"5 | Train/Test \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438\u00b6","text":"<p>\u0414\u0435\u043b\u0438\u043c \u043d\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0438:</p>"},{"location":"portfolio/course_recsys/Look-a-like_Segmentation.html#6","title":"6 | \u041c\u0430\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\u00b6","text":"<ul> <li>\u0421\u0440\u0435\u0434\u0438 \u043c\u043e\u0434\u0435\u043b\u0435\u0439, \u0432\u044b\u0431\u0440\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u044f, \u0435\u0441\u0442\u044c \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0435; \u041a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0430 \u0434\u0430\u043d\u043d\u044b\u0445. \u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u044b.</li> <li>\u0415\u0441\u043b\u0438 \u043c\u0430\u0441\u0448\u0442\u0430\u0431 \u043e\u0434\u043d\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u0441\u0438\u043b\u044c\u043d\u043e \u043f\u0440\u0435\u0432\u043e\u0441\u0445\u043e\u0434\u0438\u0442 \u043c\u0430\u0441\u0448\u0442\u0430\u0431 \u0434\u0440\u0443\u0433\u0438\u0445, \u0442\u043e \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0436\u0435\u0442 \u0440\u0435\u0437\u043a\u043e \u0443\u043f\u0430\u0441\u0442\u044c.</li> <li>\u0414\u043b\u044f \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u0430\u0446\u0438\u044e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432: \u0432\u043e\u0437\u044c\u043c\u0435\u043c \u043d\u0430\u0431\u043e\u0440 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u043d\u0430 \u0432\u0441\u0435\u0445 \u043e\u0431\u044a\u0435\u043a\u0442\u0430\u0445, \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u043c \u0438\u0445 \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0438 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0435 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0435.</li> <li>\u041f\u043e\u0441\u043b\u0435 \u044d\u0442\u043e\u0433\u043e \u0438\u0437 \u0432\u0441\u0435\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u0432\u044b\u0447\u0442\u0435\u043c \u0441\u0440\u0435\u0434\u043d\u0435\u0435, \u0438 \u0437\u0430\u0442\u0435\u043c \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u0443\u044e \u0440\u0430\u0437\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u0434\u0435\u043b\u0438\u043c \u043d\u0430 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0435 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0435. \u0421\u0434\u0435\u043b\u0430\u0435\u0442 \u044d\u0442\u043e StandardScaler()...</li> </ul>"},{"location":"portfolio/course_recsys/Look-a-like_Segmentation.html#7","title":"7 | \u041c\u0430\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/course_recsys/Look-a-like_Segmentation.html","title":"\u041b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0420\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438\u00b6","text":"<p>\u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u0413\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432</p> <p>\u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u0443\u044e \u043f\u0435\u0440\u0435\u0431\u043e\u0440\u043a\u0443 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0441 GridSearchCV</p>"},{"location":"portfolio/course_recsys/Look-a-like_Segmentation.html","title":"\u041c\u0435\u0442\u043e\u0434 \u041e\u043f\u043e\u0440\u043d\u044b\u0445 \u0412\u0435\u043a\u0442\u043e\u0440\u043e\u0432\u00b6","text":""},{"location":"portfolio/course_recsys/Look-a-like_Segmentation.html","title":"\u0420\u0435\u0448\u0430\u044e\u0449\u0435\u0435 \u0414\u0435\u0440\u0435\u0432\u043e\u00b6","text":"<p>\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b</p> <ul> <li><code>criterion</code> : \u041a\u0440\u0438\u0442\u0435\u0440\u0438\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 \u0434\u0432\u0430 \u043a\u043b\u0430\u0441\u0441\u0430 , \u043e\u0442\u0432\u0435\u0447\u0430\u0435\u0442 \u0437\u0430 \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f</li> <li>max_depth : \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0433\u043b\u0443\u0431\u0438\u043d\u0430 \u0434\u0435\u0440\u0435\u0432\u0430</li> <li>min_samples_leaf : \u041c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u044f\u0434\u043e\u0432 \u0432 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\u0445</li> </ul>"},{"location":"portfolio/course_recsys/Look-a-like_Segmentation.html","title":"\u0421\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u041b\u0435\u0441\u00b6","text":"<p>\u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u0413\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432</p> <ul> <li>\u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440 \u0434\u043b\u044f \u043f\u043e\u0434\u0431\u043e\u0440\u0430 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432</li> <li>\u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u0431\u0430\u0439\u0435\u0432\u0441\u043a\u0443\u044e \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044e</li> </ul>"},{"location":"portfolio/course_recsys/Look-a-like_Segmentation.html","title":"\u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0411\u0443\u0441\u0442\u0438\u043d\u0433\u00b6","text":"<p>\u0412\u044b\u0431\u043e\u0440 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432</p> <ul> <li><p>learning_Rate : \u0421\u043a\u043e\u0440\u043e\u0441\u0442\u044c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u044e\u0449\u0430\u044f, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u0438\u043b\u044c\u043d\u043e \u043e\u0431\u043d\u043e\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0432\u0435\u0441\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043a\u0430\u0436\u0434\u043e\u043c \u0448\u0430\u0433\u0435</p> </li> <li><p>max_depth : \u041c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0433\u043b\u0443\u0431\u0438\u043d\u0430 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 \u0440\u0435\u0448\u0435\u043d\u0438\u0439</p> </li> <li><p>l2_leaf_reg : \u0420\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044f L2 \u0434\u043b\u044f \u043b\u0438\u0441\u0442\u044c\u0435\u0432 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432</p> </li> <li><p>subsample : \u0414\u043e\u043b\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0430\u044f \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0434\u0435\u0440\u0435\u0432\u0430</p> </li> <li><p>random_strength</p> <ul> <li>\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440, \u043e\u0442\u0432\u0435\u0447\u0430\u044e\u0449\u0438\u0439 \u0437\u0430 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0441\u0442\u044c \u0432 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0435 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432</li> <li>\u041e\u043d \u0432\u043b\u0438\u044f\u0435\u0442 \u043d\u0430 \u0442\u043e, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u0438\u043b\u044c\u043d\u043e \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0435 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0432\u043b\u0438\u044f\u044e\u0442 \u043d\u0430 \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u0443\u0437\u043b\u0435 \u0434\u0435\u0440\u0435\u0432\u0430</li> </ul> </li> <li><p>min_data_in_leaf : \u041c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0434\u0430\u043d\u043d\u044b\u0445, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0445 \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u043b\u0438\u0441\u0442\u0430 \u0432 \u0434\u0435\u0440\u0435\u0432\u0435.</p> </li> </ul>"},{"location":"portfolio/course_recsys/NBA_Modeling.html","title":"NBA Modeling","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport random\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder\nfrom sklearn.utils import shuffle\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# \u0423\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u043d\u0430\u0441\u0442\u0440\u043e\u0435\u043a \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0432\u0441\u0435\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u0438 \u0441\u0442\u0440\u043e\u043a \u043f\u0440\u0438 \u043f\u0435\u0447\u0430\u0442\u0438\npd.set_option('display.max_columns', None)\n# pd.set_option('display.max_rows', None)\n\n# \u0437\u0430\u0440\u0430\u043d\u0435\u0435 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043c \u0432 \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u0443 random_state\nrandom_state = 47\n\nsns.set(style=\"whitegrid\")\n</pre> import pandas as pd import numpy as np import seaborn as sns import random from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV  from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score from sklearn.preprocessing import StandardScaler, OrdinalEncoder from sklearn.utils import shuffle  import warnings warnings.filterwarnings(\"ignore\")  # \u0423\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u043d\u0430\u0441\u0442\u0440\u043e\u0435\u043a \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0432\u0441\u0435\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u0438 \u0441\u0442\u0440\u043e\u043a \u043f\u0440\u0438 \u043f\u0435\u0447\u0430\u0442\u0438 pd.set_option('display.max_columns', None) # pd.set_option('display.max_rows', None)  # \u0437\u0430\u0440\u0430\u043d\u0435\u0435 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043c \u0432 \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u0443 random_state random_state = 47  sns.set(style=\"whitegrid\") <ul> <li>\u041d\u0430 \u043a\u0430\u0436\u0434\u044b\u0439 \u0442\u043e\u0432\u0430\u0439 \u043e\u0434\u043d\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0430, \u0437\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u0438 \u0438\u0441\u0441\u043b\u0435\u0434\u0443\u0435\u043c \u043d\u0430\u0448\u0438 \u0444\u0438\u0447\u0438. \u0412 \u043a\u0430\u0436\u0434\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0443 \u043d\u0430\u0441 \u043f\u043e 10000 \u0437\u0430\u043f\u0438\u0441\u0435\u0439 (\u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432)</li> <li>\u041c\u043e\u0436\u043d\u043e \u0432\u044b\u0434\u0435\u043b\u0438\u0442\u044c \u043a\u043e\u043b\u043e\u043d\u043ay Purchase Flag, \u044d\u0442\u043e \u0443 \u043d\u0430\u0441 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439 \u0441 \u043a\u043b\u0438\u0435\u0442\u043d\u043e\u043c, \u043b\u0438\u0431\u043e \u043a\u043b\u0438\u0435\u043d\u0442 \u043a\u0443\u043f\u0438\u043b \u0442\u043e\u0432\u0430\u0440 (1) \u043b\u0438\u0431\u043e \u043d\u0435 \u043a\u0443\u043f\u0438\u043b (0)</li> <li>\u0412\u0441\u0435 \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0435 \u0444\u0438\u0447\u0438 \u043a\u0430\u043a \u0442\u043e \u0445\u0430\u0440\u0430\u043a\u0442\u0438\u0440\u0435\u0437\u0443\u044e\u0442 \u044d\u0442\u043e\u0433\u043e \u043a\u043b\u0438\u0435\u043d\u0442\u0430</li> </ul> In\u00a0[\u00a0]: Copied! <pre>df_Laptop = pd.read_csv('synthetic_laptop_data.csv')\ndf_Charger = pd.read_csv('synthetic_charger_data.csv')\ndf_Phone = pd.read_csv('synthetic_mobile_phone_data.csv')\n\ndf = pd.concat([df_Laptop, df_Charger, df_Phone])\nprint(df.shape)\ndf.head()\n</pre> df_Laptop = pd.read_csv('synthetic_laptop_data.csv') df_Charger = pd.read_csv('synthetic_charger_data.csv') df_Phone = pd.read_csv('synthetic_mobile_phone_data.csv')  df = pd.concat([df_Laptop, df_Charger, df_Phone]) print(df.shape) df.head() <pre>(30000, 18)\n</pre> Out[\u00a0]: Age Gender Geography PreviousPurchases Salary NumChildren Product EducationLevel MaritalStatus EmploymentStatus HousingStatus CreditScore InternetUsage NumberOfCars HealthStatus ShoppingFrequency MembershipDuration PurchaseFlag 0 56 Male North America 8386.985501 42217.503431 4 Laptop Bachelor Divorced Employed Rent 754 1.545836 0 Fair 6 9 1 1 69 Female South America 7391.924025 44369.165492 3 Laptop Master Single Employed Own 745 8.005712 0 Good 1 3 1 2 46 Female South America 7951.832329 85406.301432 3 Laptop High School Widowed Unemployed Own 552 8.154606 1 Fair 5 9 0 3 32 Female Africa 2284.841225 116197.240945 4 Laptop PhD Widowed Retired Living with Parents 458 1.938812 1 Fair 17 2 1 4 60 Male Asia 6520.780649 55461.014410 0 Laptop High School Married Employed Living with Parents 434 7.656888 0 Fair 10 15 0 <p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u0430 PurchaseFlag, \u0438 \u0443\u0431\u0435\u0434\u0438\u043b\u0438\u0441\u044c \u0447\u0442\u043e \u043a\u043b\u0430\u0441\u0441\u044b \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u044b</p> In\u00a0[\u00a0]: Copied! <pre>df.groupby(['Product', 'PurchaseFlag']).agg({'Gender': 'count'})\n</pre> df.groupby(['Product', 'PurchaseFlag']).agg({'Gender': 'count'}) Out[\u00a0]: Gender Product PurchaseFlag Charger 0 6541 1 3459 Laptop 0 4594 1 5406 Mobile Phone 0 5049 1 4951 <p>\u041c\u043e\u0436\u043d\u043e \u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c \u0440\u0430\u0434\u0438 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u0430 \u043f\u0440\u043e\u043f\u043e\u0440\u0446\u0438\u044e \u0443\u0441\u043f\u0435\u0448\u043d\u043e\u0439 \u043a \u043d\u0435\u0443\u0441\u043f\u0435\u0448\u043d\u044b\u0445 \u043f\u0440\u043e\u0434\u0430\u0436 \u044d\u0442\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0438</p> In\u00a0[\u00a0]: Copied! <pre>df.groupby(['Product']).agg({'PurchaseFlag': 'mean'})\n</pre> df.groupby(['Product']).agg({'PurchaseFlag': 'mean'}) Out[\u00a0]: PurchaseFlag Product Charger 0.3459 Laptop 0.5406 Mobile Phone 0.4951 <p>\u041d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443 \u0434\u0430\u043d\u043d\u044b\u0445</p> <ul> <li>\u041e\u0431\u0443\u0447\u0430\u0442\u044c\u0441\u044f \u0431\u0443\u0434\u0435\u043c \u043d\u0430 \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u0435. CatBoost \u0433\u0438\u0431\u043e\u043a \u043a \u0442\u0438\u043f\u0430\u043c \u0434\u0430\u043d\u043d\u044b\u0445.</li> <li>\u042d\u0442\u043e \u0437\u043d\u0430\u0447\u0438\u0442, \u0447\u0442\u043e \u043c\u043e\u0436\u043d\u043e \u043d\u0435 \u0437\u0430\u043d\u0438\u043c\u0430\u0442\u044c\u0441\u044f \u043a\u043e\u0434\u0438\u0440\u0440\u0432\u0430\u043d\u0438\u0435\u043c \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445, \u0430 \u043f\u0440\u043e\u0441\u0442\u043e \u043f\u0440\u0438\u0441\u0432\u043e\u0438\u0442\u044c \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0439 \u0442\u0438\u043f \u0434\u0430\u043d\u043d\u044b\u0445.</li> <li>\u041a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0444\u0438\u0447\u0438 \u043d\u0430\u0434\u043e \u0431\u0443\u0434\u0435\u0442 \u0443\u043f\u043e\u043c\u0435\u043d\u0443\u0442\u044c \u043f\u0440\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438 \u043c\u043e\u0434\u0435\u043b\u0438</li> </ul> In\u00a0[\u00a0]: Copied! <pre># \u041a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c object \u0432 category\nobject_cols = list(df.drop(['Product'], axis=1).select_dtypes('object').columns)\nprint(f'we have {len(object_cols)} object_cols')\n\ndf[object_cols] = df[object_cols].astype('category')\n</pre> # \u041a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c object \u0432 category object_cols = list(df.drop(['Product'], axis=1).select_dtypes('object').columns) print(f'we have {len(object_cols)} object_cols')  df[object_cols] = df[object_cols].astype('category') <pre>we have 7 object_cols\n</pre> In\u00a0[\u00a0]: Copied! <pre>df = df.rename(columns={'PurchaseFlag': 'target'})\n</pre> df = df.rename(columns={'PurchaseFlag': 'target'}) In\u00a0[\u00a0]: Copied! <pre>df_Charger = df[df['Product'] == 'Charger']\ndf_Laptop = df[df['Product'] == 'Laptop']\ndf_Phone = df[df['Product'] == 'Mobile Phone']\n\nprint(f\"\u0420\u0430\u0437\u043c\u0435\u0440\u044b \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u043e\u0432:\\n\"\n      f\"Charger: {df_Charger.shape[0]} \u0437\u0430\u043f\u0438\u0441\u0435\u0439 \u0438 {df_Charger.shape[1]} \u0444\u0438\u0447\u0435\u0439\\n\"\n      f\"Laptop: {df_Laptop.shape[0]} \u0437\u0430\u043f\u0438\u0441\u0435\u0439 \u0438 {df_Laptop.shape[1]} \u0444\u0438\u0447\u0435\u0439\\n\"\n      f\"Mobile Phone: {df_Phone.shape[0]} \u0437\u0430\u043f\u0438\u0441\u0435\u0439 \u0438 {df_Phone.shape[1]} \u0444\u0438\u0447\u0435\u0439\\n\")\n\nprint(f\"\u0421\u0440\u0435\u0434\u043d\u044f\u044f \u0434\u043e\u043b\u044f \u043f\u043e\u043a\u0443\u043f\u043e\u043a:\\n\"\n      f\"Charger: {df_Charger.target.mean():.2f}\\n\"\n      f\"Laptop: {df_Laptop.target.mean():.2f}\\n\"\n      f\"Mobile Phone: {df_Phone.target.mean():.2f}\\n\")\n</pre> df_Charger = df[df['Product'] == 'Charger'] df_Laptop = df[df['Product'] == 'Laptop'] df_Phone = df[df['Product'] == 'Mobile Phone']  print(f\"\u0420\u0430\u0437\u043c\u0435\u0440\u044b \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u043e\u0432:\\n\"       f\"Charger: {df_Charger.shape[0]} \u0437\u0430\u043f\u0438\u0441\u0435\u0439 \u0438 {df_Charger.shape[1]} \u0444\u0438\u0447\u0435\u0439\\n\"       f\"Laptop: {df_Laptop.shape[0]} \u0437\u0430\u043f\u0438\u0441\u0435\u0439 \u0438 {df_Laptop.shape[1]} \u0444\u0438\u0447\u0435\u0439\\n\"       f\"Mobile Phone: {df_Phone.shape[0]} \u0437\u0430\u043f\u0438\u0441\u0435\u0439 \u0438 {df_Phone.shape[1]} \u0444\u0438\u0447\u0435\u0439\\n\")  print(f\"\u0421\u0440\u0435\u0434\u043d\u044f\u044f \u0434\u043e\u043b\u044f \u043f\u043e\u043a\u0443\u043f\u043e\u043a:\\n\"       f\"Charger: {df_Charger.target.mean():.2f}\\n\"       f\"Laptop: {df_Laptop.target.mean():.2f}\\n\"       f\"Mobile Phone: {df_Phone.target.mean():.2f}\\n\") <pre>\u0420\u0430\u0437\u043c\u0435\u0440\u044b \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u043e\u0432:\nCharger: 10000 \u0437\u0430\u043f\u0438\u0441\u0435\u0439 \u0438 18 \u0444\u0438\u0447\u0435\u0439\nLaptop: 10000 \u0437\u0430\u043f\u0438\u0441\u0435\u0439 \u0438 18 \u0444\u0438\u0447\u0435\u0439\nMobile Phone: 10000 \u0437\u0430\u043f\u0438\u0441\u0435\u0439 \u0438 18 \u0444\u0438\u0447\u0435\u0439\n\n\u0421\u0440\u0435\u0434\u043d\u044f\u044f \u0434\u043e\u043b\u044f \u043f\u043e\u043a\u0443\u043f\u043e\u043a:\nCharger: 0.35\nLaptop: 0.54\nMobile Phone: 0.50\n\n</pre> In\u00a0[\u00a0]: Copied! <pre># \u0443\u0431\u0438\u0440\u0430\u0435\u0442 \u0442\u043e \u0447\u0442\u043e \u043c\u044b \u0440\u0435\u043a\u043b\u0430\u043c\u0438\u0440\u043e\u0432\u0430\u043b\u0438 \u0430 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u0441\u0442\u0430\u043d\u0435\u0442 \u043a\u0443\u043f\u0438\u043b/\u043d\u0435 \u043a\u0443\u043f\u0438\u043b\n# \u043e\u043d\u0438 \u0432\u0441\u0435 \u0431\u0443\u0434\u0443\u0442 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b \u0434\u043b\u044f \u043a\u0430\u0436\u043d\u043e\u0433\u043e subset\n\nfeatures_Charger = df_Charger.drop(['Product', 'target'], axis=1)\ntarget_Charger = df_Charger['target']\n\nfeatures_Laptop = df_Laptop.drop(['Product', 'target'], axis=1)\ntarget_Laptop = df_Laptop['target']\n\nfeatures_Phone = df_Phone.drop(['Product', 'target'], axis=1)\ntarget_Phone = df_Phone['target']\n</pre> # \u0443\u0431\u0438\u0440\u0430\u0435\u0442 \u0442\u043e \u0447\u0442\u043e \u043c\u044b \u0440\u0435\u043a\u043b\u0430\u043c\u0438\u0440\u043e\u0432\u0430\u043b\u0438 \u0430 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u0441\u0442\u0430\u043d\u0435\u0442 \u043a\u0443\u043f\u0438\u043b/\u043d\u0435 \u043a\u0443\u043f\u0438\u043b # \u043e\u043d\u0438 \u0432\u0441\u0435 \u0431\u0443\u0434\u0443\u0442 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b \u0434\u043b\u044f \u043a\u0430\u0436\u043d\u043e\u0433\u043e subset  features_Charger = df_Charger.drop(['Product', 'target'], axis=1) target_Charger = df_Charger['target']  features_Laptop = df_Laptop.drop(['Product', 'target'], axis=1) target_Laptop = df_Laptop['target']  features_Phone = df_Phone.drop(['Product', 'target'], axis=1) target_Phone = df_Phone['target'] <p>\u0412\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f <code>make_samples</code> \u0440\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u0442 \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 3 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438 (train/valid/test)</p> In\u00a0[\u00a0]: Copied! <pre>def make_samples(features, target):\n  # \u043e\u0442\u0434\u0435\u043b\u044f\u0435\u043c 20% - \u043f\u044f\u0442\u0443\u044e \u0447\u0430\u0441\u0442\u044c \u0432\u0441\u0435\u0433\u043e - \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443\n  X_train_valid, X_test, y_train_valid, y_test = train_test_split(features, target,\n                                                                  test_size=0.2,\n                                                                  random_state=random_state)\n  \n  # \u043e\u0442\u0434\u0435\u043b\u044f\u0435\u043c 25% - \u0447\u0435\u0442\u0432\u0435\u0440\u0442\u0443\u044e \u0447\u0430\u0441\u0442\u044c \u0442\u0440\u0435\u0439\u043d+\u0432\u0430\u043b\u0438\u0434 - \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u0443\u044e\u0449\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443\n  X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid,\n                                                        test_size=0.25,\n                                                        random_state=random_state)\n\n  s1 = y_train.size\n  s2 = y_valid.size\n  s3 = y_test.size\n  \n  print('\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 train:valid:test \u0432 \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0438 '\n        + str(round(s1/s3)) + ':' + str(round(s2/s3)) + ':' + str(round(s3/s3)))\n  print('target rate \u043d\u0430 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f\u0445:', round(y_train.mean(), 4), round(y_valid.mean(), 4), round(y_test.mean(), 4))\n  return X_train, X_valid, X_test, y_train, y_valid, y_test\n</pre> def make_samples(features, target):   # \u043e\u0442\u0434\u0435\u043b\u044f\u0435\u043c 20% - \u043f\u044f\u0442\u0443\u044e \u0447\u0430\u0441\u0442\u044c \u0432\u0441\u0435\u0433\u043e - \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443   X_train_valid, X_test, y_train_valid, y_test = train_test_split(features, target,                                                                   test_size=0.2,                                                                   random_state=random_state)      # \u043e\u0442\u0434\u0435\u043b\u044f\u0435\u043c 25% - \u0447\u0435\u0442\u0432\u0435\u0440\u0442\u0443\u044e \u0447\u0430\u0441\u0442\u044c \u0442\u0440\u0435\u0439\u043d+\u0432\u0430\u043b\u0438\u0434 - \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u0443\u044e\u0449\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443   X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid,                                                         test_size=0.25,                                                         random_state=random_state)    s1 = y_train.size   s2 = y_valid.size   s3 = y_test.size      print('\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 train:valid:test \u0432 \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0438 '         + str(round(s1/s3)) + ':' + str(round(s2/s3)) + ':' + str(round(s3/s3)))   print('target rate \u043d\u0430 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f\u0445:', round(y_train.mean(), 4), round(y_valid.mean(), 4), round(y_test.mean(), 4))   return X_train, X_valid, X_test, y_train, y_valid, y_test In\u00a0[\u00a0]: Copied! <pre>X_train_Charger, X_valid_Charger, X_test_Charger, y_train_Charger, y_valid_Charger, y_test_Charger = make_samples(features_Charger, target_Charger)\nX_train_Laptop, X_valid_Laptop, X_test_Laptop, y_train_Laptop, y_valid_Laptop, y_test_Laptop = make_samples(features_Laptop, target_Laptop)\nX_train_Phone, X_valid_Phone, X_test_Phone, y_train_Phone, y_valid_Phone, y_test_Phone = make_samples(features_Phone, target_Phone)\n</pre> X_train_Charger, X_valid_Charger, X_test_Charger, y_train_Charger, y_valid_Charger, y_test_Charger = make_samples(features_Charger, target_Charger) X_train_Laptop, X_valid_Laptop, X_test_Laptop, y_train_Laptop, y_valid_Laptop, y_test_Laptop = make_samples(features_Laptop, target_Laptop) X_train_Phone, X_valid_Phone, X_test_Phone, y_train_Phone, y_valid_Phone, y_test_Phone = make_samples(features_Phone, target_Phone) <pre>\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 train:valid:test \u0432 \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0438 3:1:1\ntarget rate \u043d\u0430 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f\u0445: 0.3532 0.3325 0.3375\n\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 train:valid:test \u0432 \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0438 3:1:1\ntarget rate \u043d\u0430 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f\u0445: 0.5388 0.549 0.5375\n\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 train:valid:test \u0432 \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0438 3:1:1\ntarget rate \u043d\u0430 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f\u0445: 0.4938 0.506 0.488\n</pre> In\u00a0[\u00a0]: Copied! <pre>X_train_Charger.head(2)\n</pre> X_train_Charger.head(2) Out[\u00a0]: Age Gender Geography PreviousPurchases Salary NumChildren EducationLevel MaritalStatus EmploymentStatus HousingStatus CreditScore InternetUsage NumberOfCars HealthStatus ShoppingFrequency MembershipDuration 9731 23 Female South America 6042.440703 67691.979045 2 Master Single Student Living with Parents 655 4.588958 0 Poor 2 11 6920 64 Male North America 2497.156731 35648.888199 1 High School Widowed Student Rent 379 6.842913 0 Poor 11 1 In\u00a0[\u00a0]: Copied! <pre># \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438\ndef calc_metrics(model, X_train, y_train, X_valid, y_valid, X_test, y_test):\n    # \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\n    y_train_pred = model.predict(X_train)\n    y_train_proba = model.predict_proba(X_train)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_train)\n\n    # \u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f\n    y_valid_pred = model.predict(X_valid)\n    y_valid_proba = model.predict_proba(X_valid)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_valid)\n\n    # \u0422\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\n    y_test_pred = model.predict(X_test)\n    y_test_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test)\n\n    train_metrics = {\n        'precision': precision_score(y_train, y_train_pred),\n        'recall': recall_score(y_train, y_train_pred),\n        'f1': f1_score(y_train, y_train_pred),\n        'roc_auc': roc_auc_score(y_train, y_train_proba)\n    }\n\n    valid_metrics = {\n        'precision': precision_score(y_valid, y_valid_pred),\n        'recall': recall_score(y_valid, y_valid_pred),\n        'f1': f1_score(y_valid, y_valid_pred),\n        'roc_auc': roc_auc_score(y_valid, y_valid_proba)\n    }\n\n    test_metrics = {\n        'precision': precision_score(y_test, y_test_pred),\n        'recall': recall_score(y_test, y_test_pred),\n        'f1': f1_score(y_test, y_test_pred),\n        'roc_auc': roc_auc_score(y_test, y_test_proba)\n    }\n\n    return train_metrics, valid_metrics, test_metrics\n\ndef print_metrics(model, X_train, y_train, X_valid, y_valid, X_test, y_test):\n    res = calc_metrics(model, X_train, y_train, X_valid, y_valid, X_test, y_test)\n    metrics = pd.DataFrame(res, index=['train', 'valid', 'test'])\n    return metrics\n</pre> # \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 def calc_metrics(model, X_train, y_train, X_valid, y_valid, X_test, y_test):     # \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435     y_train_pred = model.predict(X_train)     y_train_proba = model.predict_proba(X_train)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_train)      # \u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f     y_valid_pred = model.predict(X_valid)     y_valid_proba = model.predict_proba(X_valid)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_valid)      # \u0422\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435     y_test_pred = model.predict(X_test)     y_test_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test)      train_metrics = {         'precision': precision_score(y_train, y_train_pred),         'recall': recall_score(y_train, y_train_pred),         'f1': f1_score(y_train, y_train_pred),         'roc_auc': roc_auc_score(y_train, y_train_proba)     }      valid_metrics = {         'precision': precision_score(y_valid, y_valid_pred),         'recall': recall_score(y_valid, y_valid_pred),         'f1': f1_score(y_valid, y_valid_pred),         'roc_auc': roc_auc_score(y_valid, y_valid_proba)     }      test_metrics = {         'precision': precision_score(y_test, y_test_pred),         'recall': recall_score(y_test, y_test_pred),         'f1': f1_score(y_test, y_test_pred),         'roc_auc': roc_auc_score(y_test, y_test_proba)     }      return train_metrics, valid_metrics, test_metrics  def print_metrics(model, X_train, y_train, X_valid, y_valid, X_test, y_test):     res = calc_metrics(model, X_train, y_train, X_valid, y_valid, X_test, y_test)     metrics = pd.DataFrame(res, index=['train', 'valid', 'test'])     return metrics In\u00a0[\u00a0]: Copied! <pre>!pip install catboost optuna -qqq\n</pre> !pip install catboost optuna -qqq <pre>   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 98.2/98.2 MB 6.8 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 380.1/380.1 kB 17.7 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 233.0/233.0 kB 10.1 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.6/78.6 kB 4.6 MB/s eta 0:00:00\n</pre> In\u00a0[\u00a0]: Copied! <pre>from catboost import CatBoostClassifier\nimport optuna\n</pre> from catboost import CatBoostClassifier import optuna In\u00a0[\u00a0]: Copied! <pre>def objective(trial, X_train, y_train, X_valid, y_valid):\n\n    # \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u043f\u0435\u0440\u0435\u0431\u043e\u0440\u0430 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \n    param = {\n        \"learning_rate\": trial.suggest_float('learning_rate', 0.01, 0.9),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 7),\n        \"l2_leaf_reg\":trial.suggest_float('l2_leaf_reg', 0.01, 2),\n        \"subsample\": trial.suggest_float('subsample', 0.01, 1),\n        \"random_strength\": trial.suggest_float('random_strength', 1, 200),\n        \"min_data_in_leaf\":trial.suggest_float('min_data_in_leaf', 1, 500)\n    }\n\n    # \u043c\u043e\u0434\u0435\u043b\u044c\n    cat = CatBoostClassifier(\n        logging_level=\"Silent\",\n        eval_metric=\"AUC\",\n        grow_policy=\"Lossguide\",\n        random_seed=42,\n        **param)\n\n    # \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \n    cat.fit(X_train, y_train,\n            cat_features=object_cols, # \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438\n            eval_set=(X_valid, y_valid),\n            verbose=False,\n            early_stopping_rounds=10\n           )\n\n    # \u0412\u044b\u0432\u043e\u0434\u0438\u043c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u043d\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435\n    preds = cat.predict_proba(X_valid)[:,1]\n    auc = roc_auc_score(y_valid, preds)\n\n    return auc\n</pre> def objective(trial, X_train, y_train, X_valid, y_valid):      # \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u043f\u0435\u0440\u0435\u0431\u043e\u0440\u0430 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432      param = {         \"learning_rate\": trial.suggest_float('learning_rate', 0.01, 0.9),         \"max_depth\": trial.suggest_int(\"max_depth\", 2, 7),         \"l2_leaf_reg\":trial.suggest_float('l2_leaf_reg', 0.01, 2),         \"subsample\": trial.suggest_float('subsample', 0.01, 1),         \"random_strength\": trial.suggest_float('random_strength', 1, 200),         \"min_data_in_leaf\":trial.suggest_float('min_data_in_leaf', 1, 500)     }      # \u043c\u043e\u0434\u0435\u043b\u044c     cat = CatBoostClassifier(         logging_level=\"Silent\",         eval_metric=\"AUC\",         grow_policy=\"Lossguide\",         random_seed=42,         **param)      # \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435      cat.fit(X_train, y_train,             cat_features=object_cols, # \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438             eval_set=(X_valid, y_valid),             verbose=False,             early_stopping_rounds=10            )      # \u0412\u044b\u0432\u043e\u0434\u0438\u043c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u043d\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435     preds = cat.predict_proba(X_valid)[:,1]     auc = roc_auc_score(y_valid, preds)      return auc In\u00a0[\u00a0]: Copied! <pre>optuna.logging.set_verbosity(optuna.logging.WARNING)\n\ndef train_model(X_train, y_train, X_valid, y_valid, X_test, y_test):\n\n  # \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u0446\u0438\u043a\u043b\n  study = optuna.create_study(direction=\"maximize\", \n                              study_name='CatBoostClassifier')\n  \n  # \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0435 \u0446\u0438\u043a\u043b\u044b\n  study.optimize(lambda trial: objective(trial, X_train, y_train, X_valid, y_valid), n_trials=100)\n\n  '''\n  \n  \u043d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0441 \u0441\u0430\u043c\u044b\u043c \u0432\u044b\u0441\u043e\u043a\u0438\u043c ROC-AUC\n  \n  '''\n\n  # \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \n  best_cat = CatBoostClassifier(**study.best_params, random_state=random_state)\n\n  # \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u0441 \u044d\u0442\u0438\u043c\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438\n  best_cat.fit(X_train, y_train, cat_features=object_cols,\n              eval_set=(X_valid, y_valid),\n              verbose=False,\n              early_stopping_rounds=10\n            )\n\n  # \u0412\u044b\u0432\u043e\u0434\u0438\u043c \u043c\u0435\u0442\u0440\u0438\u043a\u0438\n  res_cat = print_metrics(best_cat, X_train, y_train, X_valid, y_valid, X_test, y_test)\n  return res_cat, best_cat\n</pre> optuna.logging.set_verbosity(optuna.logging.WARNING)  def train_model(X_train, y_train, X_valid, y_valid, X_test, y_test):    # \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u0446\u0438\u043a\u043b   study = optuna.create_study(direction=\"maximize\",                                study_name='CatBoostClassifier')      # \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0435 \u0446\u0438\u043a\u043b\u044b   study.optimize(lambda trial: objective(trial, X_train, y_train, X_valid, y_valid), n_trials=100)    '''      \u043d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0441 \u0441\u0430\u043c\u044b\u043c \u0432\u044b\u0441\u043e\u043a\u0438\u043c ROC-AUC      '''    # \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b    best_cat = CatBoostClassifier(**study.best_params, random_state=random_state)    # \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u0441 \u044d\u0442\u0438\u043c\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438   best_cat.fit(X_train, y_train, cat_features=object_cols,               eval_set=(X_valid, y_valid),               verbose=False,               early_stopping_rounds=10             )    # \u0412\u044b\u0432\u043e\u0434\u0438\u043c \u043c\u0435\u0442\u0440\u0438\u043a\u0438   res_cat = print_metrics(best_cat, X_train, y_train, X_valid, y_valid, X_test, y_test)   return res_cat, best_cat In\u00a0[\u00a0]: Copied! <pre># \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u0438 (\u0432\u0441\u0435 \u0442\u0440\u0438 \u043c\u043e\u0434\u0435\u043b\u0438)\nres_Laptop, model_Laptop = train_model(X_train_Laptop, y_train_Laptop, X_valid_Laptop, y_valid_Laptop, X_test_Laptop, y_test_Laptop)\nres_Charger, model_Charger = train_model(X_train_Charger, y_train_Charger, X_valid_Charger, y_valid_Charger, X_test_Charger, y_test_Charger)\nres_Phone, model_Phone = train_model(X_train_Phone, y_train_Phone, X_valid_Phone, y_valid_Phone, X_test_Phone, y_test_Phone)\n</pre> # \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u0438 (\u0432\u0441\u0435 \u0442\u0440\u0438 \u043c\u043e\u0434\u0435\u043b\u0438) res_Laptop, model_Laptop = train_model(X_train_Laptop, y_train_Laptop, X_valid_Laptop, y_valid_Laptop, X_test_Laptop, y_test_Laptop) res_Charger, model_Charger = train_model(X_train_Charger, y_train_Charger, X_valid_Charger, y_valid_Charger, X_test_Charger, y_test_Charger) res_Phone, model_Phone = train_model(X_train_Phone, y_train_Phone, X_valid_Phone, y_valid_Phone, X_test_Phone, y_test_Phone) In\u00a0[\u00a0]: Copied! <pre>res_Laptop\n</pre> res_Laptop Out[\u00a0]: precision recall f1 roc_auc train 0.631678 0.727807 0.676344 0.672488 valid 0.655340 0.737705 0.694087 0.686782 test 0.609277 0.720930 0.660418 0.635050 In\u00a0[\u00a0]: Copied! <pre>res_Phone\n</pre> res_Phone Out[\u00a0]: precision recall f1 roc_auc train 0.545998 0.400607 0.462137 0.548935 valid 0.538163 0.411067 0.466106 0.541741 test 0.508941 0.379098 0.434527 0.519428 In\u00a0[\u00a0]: Copied! <pre>res_Charger\n</pre> res_Charger Out[\u00a0]: precision recall f1 roc_auc train 0.639498 0.288815 0.397919 0.706019 valid 0.553977 0.293233 0.383481 0.698588 test 0.559659 0.291852 0.383642 0.667592 In\u00a0[\u00a0]: Copied! <pre># \u043a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0431\u0443\u0434\u0435\u043c \u043d\u0430 \u0438\u0437\u043e\u0442\u043e\u043d\u0438\u043a\u0435\nfrom sklearn.isotonic import IsotonicRegression\n\n# \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u043a\u0438\ndef calibrate(model, X_valid, y_valid):\n  y_pred = model.predict_proba(X_valid)[:, 1]\n  # y_pred2 = model.predict_proba(X_valid)[:,0]\n  iso_reg = IsotonicRegression(out_of_bounds='clip')\n  iso_reg.fit(y_pred, y_valid)\n  y_pred_iso = iso_reg.transform(y_pred)\n  return iso_reg\n</pre> # \u043a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0431\u0443\u0434\u0435\u043c \u043d\u0430 \u0438\u0437\u043e\u0442\u043e\u043d\u0438\u043a\u0435 from sklearn.isotonic import IsotonicRegression  # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u043a\u0438 def calibrate(model, X_valid, y_valid):   y_pred = model.predict_proba(X_valid)[:, 1]   # y_pred2 = model.predict_proba(X_valid)[:,0]   iso_reg = IsotonicRegression(out_of_bounds='clip')   iso_reg.fit(y_pred, y_valid)   y_pred_iso = iso_reg.transform(y_pred)   return iso_reg <p>\u0412\u0441\u0435, \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438 \u043d\u0430\u0448\u0438 \u043e\u0442\u043a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0442\u043e\u0432\u0430\u0440\u0430</p> In\u00a0[\u00a0]: Copied! <pre># \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u043a\u0443 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 (\u043c\u043e\u0436\u043d\u043e \u0438 \u043d\u0430 \u0442\u0440\u0435\u0439\u043d\u0435)\niso_reg_Laptop = calibrate(model_Laptop, X_valid_Laptop, y_valid_Laptop)\niso_reg_Charger = calibrate(model_Charger, X_valid_Charger, y_valid_Charger)\niso_reg_Phone = calibrate(model_Phone, X_valid_Phone, y_valid_Phone)\n</pre> # \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u043a\u0443 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 (\u043c\u043e\u0436\u043d\u043e \u0438 \u043d\u0430 \u0442\u0440\u0435\u0439\u043d\u0435) iso_reg_Laptop = calibrate(model_Laptop, X_valid_Laptop, y_valid_Laptop) iso_reg_Charger = calibrate(model_Charger, X_valid_Charger, y_valid_Charger) iso_reg_Phone = calibrate(model_Phone, X_valid_Phone, y_valid_Phone) In\u00a0[\u00a0]: Copied! <pre># net present value\n# \u0446\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0442\u043e\u0432\u0430\u0440\u0430\n# \u0443 \u043a\u0430\u0436\u043d\u043e\u0433\u043e \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430 \u0441\u0432\u043e\u0438, \u0444\u0438\u043d\u0430\u043d\u0441\u0438\u0441\u0442\u044b \u044d\u0442\u0438\u043c \u0437\u0430\u043d\u0438\u043c\u0430\u044e\u0442\u0441\u044f\n\nnpv_values = {\n    'Laptop': 100.0,\n    'Phone': 300.0,\n    'Charger': 200.0\n}\n</pre> # net present value # \u0446\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0442\u043e\u0432\u0430\u0440\u0430 # \u0443 \u043a\u0430\u0436\u043d\u043e\u0433\u043e \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430 \u0441\u0432\u043e\u0438, \u0444\u0438\u043d\u0430\u043d\u0441\u0438\u0441\u0442\u044b \u044d\u0442\u0438\u043c \u0437\u0430\u043d\u0438\u043c\u0430\u044e\u0442\u0441\u044f  npv_values = {     'Laptop': 100.0,     'Phone': 300.0,     'Charger': 200.0 } In\u00a0[\u00a0]: Copied! <pre># \u0441\u0431\u043e\u0440\u043a\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u0434\u043b\u044f \u0443\u0434\u043e\u0431\u043d\u043e\u0433\u043e \u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\ncols = X_test_Charger.columns.tolist()\n\n# \u0434\u0430\u0435\u043c \u0442\u043e\u0442 \u0436\u0435 \u0442\u044d\u0433 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430\nX_test_Charger['product'] = 'Charger'\nX_test_Laptop['product'] = 'Laptop'\nX_test_Phone['product'] = 'Phone'\n\n# \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0435 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\nX_test = pd.concat([X_test_Charger, X_test_Laptop, X_test_Phone])\ny_test = pd.concat([y_test_Charger, y_test_Laptop, y_test_Phone])\n\ntest_df = X_test[['product']]\ntest_df['NPV'] = test_df['product'].map(npv_values)\ntest_df['target'] = y_test\ntest_df['predict_Charger'] = model_Charger.predict_proba(X_test[cols])[:, 1]\ntest_df['predict_Laptop'] = model_Laptop.predict_proba(X_test[cols])[:, 1]\ntest_df['predict_Phone'] = model_Phone.predict_proba(X_test[cols])[:, 1]\n</pre> # \u0441\u0431\u043e\u0440\u043a\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u0434\u043b\u044f \u0443\u0434\u043e\u0431\u043d\u043e\u0433\u043e \u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 cols = X_test_Charger.columns.tolist()  # \u0434\u0430\u0435\u043c \u0442\u043e\u0442 \u0436\u0435 \u0442\u044d\u0433 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430 X_test_Charger['product'] = 'Charger' X_test_Laptop['product'] = 'Laptop' X_test_Phone['product'] = 'Phone'  # \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0435 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 X_test = pd.concat([X_test_Charger, X_test_Laptop, X_test_Phone]) y_test = pd.concat([y_test_Charger, y_test_Laptop, y_test_Phone])  test_df = X_test[['product']] test_df['NPV'] = test_df['product'].map(npv_values) test_df['target'] = y_test test_df['predict_Charger'] = model_Charger.predict_proba(X_test[cols])[:, 1] test_df['predict_Laptop'] = model_Laptop.predict_proba(X_test[cols])[:, 1] test_df['predict_Phone'] = model_Phone.predict_proba(X_test[cols])[:, 1] <p>\u0412 \u0434\u0430\u043d\u043d\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435, \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435</p> In\u00a0[\u00a0]: Copied! <pre># \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c, \u0447\u0442\u043e \u0441\u0431\u043e\u0440\u043a\u0430 \u043f\u0440\u043e\u0448\u043b\u0430 \u0443\u0434\u0430\u0447\u043d\u043e, \u043d\u0438\u0433\u0434\u0435 \u043d\u0435 \u043e\u0448\u0438\u0431\u043b\u0438\u0441\u044c, \u0438 ROC_AUC \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u043e\u0436\u0438\u0434\u0430\u0435\u043c\u044b\u0439\nfor product in ['Charger', 'Laptop', 'Phone']:\n  tmp = test_df[test_df['product'] == product]\n  print(f'ROC_AUC for {product}:', roc_auc_score(tmp['target'], tmp[f'predict_{product}']))\n</pre> # \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c, \u0447\u0442\u043e \u0441\u0431\u043e\u0440\u043a\u0430 \u043f\u0440\u043e\u0448\u043b\u0430 \u0443\u0434\u0430\u0447\u043d\u043e, \u043d\u0438\u0433\u0434\u0435 \u043d\u0435 \u043e\u0448\u0438\u0431\u043b\u0438\u0441\u044c, \u0438 ROC_AUC \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u043e\u0436\u0438\u0434\u0430\u0435\u043c\u044b\u0439 for product in ['Charger', 'Laptop', 'Phone']:   tmp = test_df[test_df['product'] == product]   print(f'ROC_AUC for {product}:', roc_auc_score(tmp['target'], tmp[f'predict_{product}'])) <pre>ROC_AUC for Charger: 0.6675924528301888\nROC_AUC for Laptop: 0.635050157133878\nROC_AUC for Phone: 0.5194281906378073\n</pre> <p>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u043e\u0442\u043a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u043c\u043e\u0434\u0435\u043b\u044f\u043c\u0438 \u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0436\u0435\u043c \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043a\u043b\u0438\u0435\u043d\u0442\u0430 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u043a\u0443\u043f\u043a\u0438 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0442\u043e\u0432\u0430\u0440\u0430</p> In\u00a0[\u00a0]: Copied! <pre># \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u043a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u043a\u0443 \u043a \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u043c \u0441\u0435\u043c\u043f\u043b\u0430\u043c\ntest_df['predict_Charger_calibrated'] = iso_reg_Charger.transform(test_df['predict_Charger'])\ntest_df['predict_Laptop_calibrated'] = iso_reg_Laptop.transform(test_df['predict_Laptop'])\n9test_df['predict_Phone_calibrated'] = iso_reg_Phone.transform(test_df['predict_Phone'])\n</pre> # \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u043a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u043a\u0443 \u043a \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u043c \u0441\u0435\u043c\u043f\u043b\u0430\u043c test_df['predict_Charger_calibrated'] = iso_reg_Charger.transform(test_df['predict_Charger']) test_df['predict_Laptop_calibrated'] = iso_reg_Laptop.transform(test_df['predict_Laptop']) 9test_df['predict_Phone_calibrated'] = iso_reg_Phone.transform(test_df['predict_Phone']) <p>\u041a\u0430\u0436\u0434\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0441 \u0443\u0447\u0435\u0442\u043e\u043c NPV (Max_Score_NPV_Product) \u0438 \u0435\u0441\u0442\u044c \u0442\u043e \u0447\u0442\u043e \u043d\u0430\u043c \u0441\u043b\u0435\u0434\u0443\u0435\u0442 \u0440\u0435\u043a\u043e\u043c\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u0442\u044c \u043d\u0430\u0448\u0435\u043c\u0443 \u043a\u043b\u0438\u0435\u0442\u043d\u0443</p> In\u00a0[\u00a0]: Copied! <pre># \u041d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432 \u0438 \u0438\u0445 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0435 \u0441\u043a\u043e\u0440\u0430\u043c\nproduct_scores = {\n    'predict_Charger_calibrated': 'Charger',\n    'predict_Laptop_calibrated': 'Laptop',\n    'predict_Phone_calibrated': 'Phone'\n}\n\n# \u0412\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0441\u043a\u043e\u0440\u0430 \u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0433\u043e \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430\ntest_df['Max_Score_Product'] = test_df[\n    ['predict_Charger_calibrated', 'predict_Laptop_calibrated', 'predict_Phone_calibrated']\n].idxmax(axis=1).map(product_scores)\n\n# \u0412\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0441\u043a\u043e\u0440 * NPV \u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0433\u043e \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430\ndef calculate_max_score_npv_product(row):\n    scores_with_npv = {\n        'Charger': row['predict_Charger_calibrated'] * npv_values['Charger'],\n        'Laptop': row['predict_Laptop_calibrated'] * npv_values['Laptop'],\n        'Phone': row['predict_Phone_calibrated'] * npv_values['Phone']\n    }\n    max_product = max(scores_with_npv, key=scores_with_npv.get)\n    return max_product\n\ntest_df['Max_Score_NPV_Product'] = test_df.apply(calculate_max_score_npv_product, axis=1)\n</pre> # \u041d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432 \u0438 \u0438\u0445 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0435 \u0441\u043a\u043e\u0440\u0430\u043c product_scores = {     'predict_Charger_calibrated': 'Charger',     'predict_Laptop_calibrated': 'Laptop',     'predict_Phone_calibrated': 'Phone' }  # \u0412\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0441\u043a\u043e\u0440\u0430 \u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0433\u043e \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430 test_df['Max_Score_Product'] = test_df[     ['predict_Charger_calibrated', 'predict_Laptop_calibrated', 'predict_Phone_calibrated'] ].idxmax(axis=1).map(product_scores)  # \u0412\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0441\u043a\u043e\u0440 * NPV \u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0433\u043e \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430 def calculate_max_score_npv_product(row):     scores_with_npv = {         'Charger': row['predict_Charger_calibrated'] * npv_values['Charger'],         'Laptop': row['predict_Laptop_calibrated'] * npv_values['Laptop'],         'Phone': row['predict_Phone_calibrated'] * npv_values['Phone']     }     max_product = max(scores_with_npv, key=scores_with_npv.get)     return max_product  test_df['Max_Score_NPV_Product'] = test_df.apply(calculate_max_score_npv_product, axis=1) In\u00a0[\u00a0]: Copied! <pre>test_df[['product', 'NPV', 'target', 'predict_Charger_calibrated','predict_Laptop_calibrated',\n         'predict_Phone_calibrated',\t'Max_Score_Product',\t'Max_Score_NPV_Product']].sample(6)\n</pre> test_df[['product', 'NPV', 'target', 'predict_Charger_calibrated','predict_Laptop_calibrated',          'predict_Phone_calibrated',\t'Max_Score_Product',\t'Max_Score_NPV_Product']].sample(6) Out[\u00a0]: product NPV target predict_Charger_calibrated predict_Laptop_calibrated predict_Phone_calibrated Max_Score_Product Max_Score_NPV_Product 528 Phone 300.0 0 0.383764 0.446309 0.530378 Phone Phone 8662 Phone 300.0 1 0.264151 0.565217 0.489831 Laptop Phone 2287 Phone 300.0 0 0.525836 0.700935 0.530378 Laptop Phone 252 Laptop 100.0 1 0.216418 0.404255 0.530378 Phone Phone 7716 Charger 200.0 0 0.216418 0.646552 0.489831 Laptop Phone 5078 Phone 300.0 1 0.388889 0.700935 0.535545 Laptop Phone In\u00a0[\u00a0]: Copied! <pre>test_df['Max_Score_Product'].value_counts()\n</pre> test_df['Max_Score_Product'].value_counts() Out[\u00a0]: count Max_Score_Product Laptop 3606 Phone 1966 Charger 428 dtype: int64 In\u00a0[\u00a0]: Copied! <pre>test_df['Max_Score_NPV_Product'].value_counts()\n</pre> test_df['Max_Score_NPV_Product'].value_counts() Out[\u00a0]: count Max_Score_NPV_Product Phone 5944 Charger 56 dtype: int64"},{"location":"portfolio/course_recsys/NBA_Modeling.html#nba","title":"\u0417\u0430\u0434\u0430\u0447\u0430 NBA\u00b6","text":""},{"location":"portfolio/course_recsys/NBA_Modeling.html","title":"\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0433\u043e \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f\u00b6","text":"<ul> <li><p>\u0417\u0430\u0434\u0430\u0447\u0443 NBA \u0431\u0443\u0434\u0435\u043c \u0440\u0435\u0448\u0430\u0442\u044c \u043d\u0430 \u0441\u0438\u043d\u0442\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435.</p> </li> <li><p>\u0412 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442\u0441\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u043e \u043a\u043b\u0438\u0435\u043d\u0442\u0430\u0445 \u0432\u044b\u043c\u044b\u0448\u043b\u0435\u043d\u043d\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d\u0430</p> </li> <li><p>\u041a\u043b\u0438\u0435\u043d\u0442\u0430\u043c \u0440\u0435\u043a\u043b\u0430\u043c\u0438\u0440\u043e\u0432\u0430\u043b\u0438</p> <ul> <li>\u041d\u043e\u0443\u0442\u0431\u0443\u043a (\u0431/\u0443, \u043e\u0447\u0435\u043d\u044c \u0441\u0442\u0430\u0440\u044b\u0439)</li> <li>\u0422\u0435\u043b\u0435\u0444\u043e\u043d</li> <li>\u0417\u0430\u0440\u044f\u0434\u043d\u043e\u0435 \u0443\u0441\u0442\u0440\u043e\u0439\u0441\u0442\u0432\u043e</li> </ul> </li> </ul>"},{"location":"portfolio/course_recsys/NBA_Modeling.html","title":"\u0427\u0442\u043e \u043d\u0430\u043c \u043f\u0440\u0435\u0434\u0441\u0442\u043e\u0438\u0442 \u0441\u0434\u0435\u043b\u0430\u0442\u044c\u00b6","text":"<ul> <li>\u041f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c NBA \u043a\u0430\u043a \u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u044e \u0431\u0438\u043d\u0430\u0440\u043d\u044b\u0439 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043e\u0442\u043a\u043b\u0438\u043a\u0430 \u0438 \u0441\u0440\u0430\u0432\u0438\u0442\u044c \u043b\u0443\u0447\u0448\u0435\u0435 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439 \u0441 \u043b\u0443\u0447\u0448\u0438\u043c \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435\u043c \u0441 \u0443\u0447\u0435\u0442\u043e\u043c NPV \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430.</li> <li>\u0422\u0435. \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0441\u0442\u0440\u043e\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u0443\u0434\u0443\u0442 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0433\u043e \u0442\u043e\u0432\u0430\u0440\u0430, \u043a\u0443\u043f\u0438\u043b \u0438\u043b\u0438 \u043d\u0435 \u043a\u0443\u043f\u0438\u043b. \u042d\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e\u0442\u043e\u043c \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043d\u044f\u0442\u044c \u0447\u0442\u043e \u043b\u0443\u0447\u0448\u0435 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0438\u0442\u044c \u043a\u043b\u0438\u0435\u043d\u0442\u0430\u043c (\u043a\u0430\u043a \u0432\u0430\u0440\u0438\u0430\u043d\u0442 \u043d\u043e\u0432\u044b\u043c)</li> </ul>"},{"location":"portfolio/course_recsys/NBA_Modeling.html","title":"\u041a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u043a\u0430 \u041c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<p>\u0427\u0442\u043e \u0442\u0430\u043a\u043e\u0435 \u043a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0435\u0439:</p> <ul> <li>\u0412 \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u043f\u0441\u0435\u0432\u0434\u043e \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435</li> <li>\u041a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u043a\u0430 \u044d\u0442\u043e \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u0438\u0440\u043e\u0432\u043a\u0438 (\u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f) \u043e\u0442 \u043f\u0441\u0435\u0432\u0434\u043e\u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0438 \u0442\u0430\u043a \u0447\u0442\u043e\u0431\u044b \u043e\u043d\u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0432\u043e\u0432\u0430\u043b\u0438 \u0438\u0441\u0442\u0438\u043d\u044b\u043c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044f\u043c \u0441\u043e\u0431\u044b\u0442\u0438\u0439</li> <li>\u0414\u0435\u043b\u0430\u0435\u0442\u0441\u044f \u044d\u0442\u043e \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0442\u044c \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0445 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0432\u0442\u0435\u0439</li> <li>\u043d\u0430\u043f\u0440. \u0435\u0441\u043b\u0438 \u043c\u043e\u0434\u0435\u043b\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 0.8 \u0434\u043b\u044f \u0441\u043e\u0431\u044b\u0442\u0438\u044f \u0442\u043e \u044d\u0442\u043e \u0441\u043e\u0431\u044b\u0442\u0438\u0435 \u0434\u043e\u043b\u0436\u043d\u043e \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442\u044c \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e 80% \u0441\u043b\u0443\u0447\u0430\u0435\u0432</li> <li>\u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u0430\u0442\u044c \u0432\u044b\u0445\u043e\u0434 \u043a\u0430\u0436\u0434\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438</li> </ul> <p>\u0422\u0438\u043f \u041a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u043e\u043a:</p> <p>\u0412 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0435 scikit-learn \u0435\u0441\u0442\u044c \u0434\u0432\u0430 \u043f\u043e\u0434\u0445\u043e\u0434\u0430:</p> <ul> <li><p>\u0418\u0437\u043e\u0442\u0430\u043d\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0439 (\u043f\u0440\u0435\u0434\u043f\u043e\u0447\u0442\u0438\u0442\u0435\u043b\u044c\u043d\u0435\u0435)</p> <ul> <li>\u0423\u043f\u043e\u0440\u044f\u0434\u043e\u0447\u0438\u0432\u0430\u0435\u0442 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u0438\u0440\u0443\u0435\u0442 \u0438\u0445 \u0442\u0430\u043a \u0447\u0442\u043e\u0431\u044b \u043e\u043d\u0438 \u043b\u0443\u0447\u0448\u0435 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0432\u043e\u0432\u0430\u043b\u0438 \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u043c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044f\u043c</li> </ul> </li> <li><p>\u041f\u043b\u0430\u0442\u0442 \u041a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u043a\u0430</p> <ul> <li>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e \u0434\u043b\u044f \u043a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u043a\u0438  \u0432\u044b\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0438</li> <li>\u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u0442 \u043e\u0446\u0435\u043d\u043a\u0438 \u0432 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0441\u0438\u0433\u043c\u043e\u0438\u0434\u0430\u043b\u044c\u043d\u0443\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u044e</li> </ul> </li> </ul>"},{"location":"portfolio/course_recsys/NBA_Modeling.html","title":"\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<p>\u0417\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u0438 \u0438\u0441\u0441\u043b\u0435\u0434\u0443\u0435\u043c \u043d\u0430\u0448\u0438 \u0434\u0430\u043d\u043d\u044b\u0435</p>"},{"location":"portfolio/course_recsys/NBA_Modeling.html","title":"\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0424\u0438\u0447\u00b6","text":"<p>\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0444\u0438\u0447\u0435\u0439:</p> <ul> <li>Age: \u0412\u043e\u0437\u0440\u0430\u0441\u0442 \u043a\u043b\u0438\u0435\u043d\u0442\u0430</li> <li>Gender: \u041f\u043e\u043b \u043a\u043b\u0438\u0435\u043d\u0442\u0430</li> <li>Geography: \u0420\u0435\u0433\u0438\u043e\u043d \u043f\u0440\u043e\u0436\u0438\u0432\u0430\u043d\u0438\u044f \u043a\u043b\u0438\u0435\u043d\u0442\u0430</li> <li>PreviousPurchases: \u0421\u0443\u043c\u043c\u0430 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0445 \u043f\u043e\u043a\u0443\u043f\u043e\u043a \u043a\u043b\u0438\u0435\u043d\u0442\u0430 \u0432 \u0432\u0430\u043b\u044e\u0442\u0435</li> <li>Salary: \u0413\u043e\u0434\u043e\u0432\u0430\u044f \u0437\u0430\u0440\u043f\u043b\u0430\u0442\u0430 \u043a\u043b\u0438\u0435\u043d\u0442\u0430 \u0432 \u0432\u0430\u043b\u044e\u0442\u0435</li> <li>NumChildren: \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0434\u0435\u0442\u0435\u0439 \u0443 \u043a\u043b\u0438\u0435\u043d\u0442\u0430</li> <li>EducationLevel: \u0423\u0440\u043e\u0432\u0435\u043d\u044c \u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043a\u043b\u0438\u0435\u043d\u0442\u0430</li> <li>MaritalStatus: \u0421\u0435\u043c\u0435\u0439\u043d\u043e\u0435 \u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u043a\u043b\u0438\u0435\u043d\u0442\u0430</li> <li>EmploymentStatus: \u0421\u0442\u0430\u0442\u0443\u0441 \u0437\u0430\u043d\u044f\u0442\u043e\u0441\u0442\u0438 \u043a\u043b\u0438\u0435\u043d\u0442\u0430</li> <li>HousingStatus: \u0416\u0438\u043b\u0438\u0449\u043d\u044b\u0439 \u0441\u0442\u0430\u0442\u0443\u0441 \u043a\u043b\u0438\u0435\u043d\u0442\u0430</li> <li>CreditScore: \u041a\u0440\u0435\u0434\u0438\u0442\u043d\u044b\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u043a\u043b\u0438\u0435\u043d\u0442\u0430</li> <li>InternetUsage: \u0412\u0440\u0435\u043c\u044f, \u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u043d\u043e\u0435 \u0432 \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442\u0435, \u0432 \u0447\u0430\u0441\u0430\u0445 \u0432 \u0434\u0435\u043d\u044c</li> <li>NumberOfCars: \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0430\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u0435\u0439 \u0443 \u043a\u043b\u0438\u0435\u043d\u0442\u0430</li> <li>HealthStatus: \u0421\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435 \u0437\u0434\u043e\u0440\u043e\u0432\u044c\u044f \u043a\u043b\u0438\u0435\u043d\u0442\u0430</li> <li>ShoppingFrequency: \u0427\u0430\u0441\u0442\u043e\u0442\u0430 \u043f\u043e\u043a\u0443\u043f\u043e\u043a \u043a\u043b\u0438\u0435\u043d\u0442\u0430, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u043a\u0443\u043f\u043e\u043a \u0432 \u043c\u0435\u0441\u044f\u0446</li> <li>MembershipDuration: \u0414\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0447\u043b\u0435\u043d\u0441\u0442\u0432\u0430 \u043a\u043b\u0438\u0435\u043d\u0442\u0430 \u0432 \u0433\u043e\u0434\u0430\u0445</li> <li>Product : \u041f\u0440\u043e\u0434\u0443\u043a\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043f\u043e\u043a\u0443\u043f\u043a\u0438</li> <li>PurchaseFlag : \u0424\u043b\u0430\u0433 \u043f\u043e\u043a\u0443\u043f\u043a\u0438, \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u043d\u0430 \u0442\u043e, \u0441\u043e\u0432\u0435\u0440\u0448\u0438\u043b \u043b\u0438 \u043a\u043b\u0438\u0435\u043d\u0442 \u043f\u043e\u043a\u0443\u043f\u043a\u0443</li> </ul> <p>\u042d\u0442\u0438 \u0444\u0438\u0447\u0438 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 \u0434\u0435\u043c\u043e\u0433\u0440\u0430\u0444\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432, \u0430 \u0442\u0430\u043a\u0436\u0435 \u0438\u0445 \u043f\u043e\u043a\u0443\u043f\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0435 \u0438 \u0444\u0438\u043d\u0430\u043d\u0441\u043e\u0432\u044b\u0435 \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043f\u043e\u043a\u0443\u043f\u043a\u0438 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0430</p>"},{"location":"portfolio/course_recsys/NBA_Modeling.html","title":"\u041a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/course_recsys/NBA_Modeling.html","title":"\u0421\u0435\u043c\u043f\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/course_recsys/NBA_Modeling.html","title":"\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<ul> <li><p>\u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u043e\u0432\u0443\u044e \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044e \u0438 optuna, \u0442\u0430\u043a \u043a\u0430\u043a \u043e\u043d\u0430 \u043d\u0430 \u043c\u043d\u043e\u0433\u043e \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u0435\u0435 \u0434\u043b\u044f \u043f\u0435\u0440\u0435\u0431\u043e\u0440\u0430 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432</p> </li> <li><p>\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u0443\u0434\u0435\u043c \u043f\u0435\u0440\u0435\u0431\u0438\u0440\u0430\u0442\u044c <code>param</code></p> <ul> <li>learning_rate : \u0448\u0430\u0433 \u0432 \u0433\u0440\u0430\u0434\u0438\u0435\u0442\u043d\u043e\u043c \u0441\u043f\u0443\u0441\u043a\u0435</li> <li>max_depth : \u0433\u043b\u0443\u0431\u0438\u043d\u0430 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432</li> <li>l2_leaf_reg : \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438</li> <li>subsample : \u0432\u0430\u0436\u0435\u043d \u0435\u0441\u043b\u0438 \u043c\u0430\u043b\u043e \u0434\u0430\u043d\u043d\u044b\u0445, \u0438\u0441\u043a\u0443\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0432\u0430\u0435\u0442 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0434\u0430\u043d\u043d\u044b\u0445</li> <li>random_strength : \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438</li> <li>min_data_in_leaf : \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0434\u0430\u043d\u043d\u044b\u0445 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0432 \u043b\u0438\u0441\u0442\u0438\u043a\u0435</li> </ul> </li> </ul> <p>\u041c\u0435\u0442\u0440\u0438\u043a\u0430 \u043e\u0446\u0435\u043d\u043a\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c ROC-AUC (\u043f\u043b\u043e\u0449\u0430\u0434\u044c \u043f\u043e\u0434 TPR/FPR \u043a\u0440\u0438\u0432\u043e\u0439)</p> <p>\u0414\u043b\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u0435\u0440\u0435\u0431\u043e\u0440\u0430 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0441 Optuna \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0443\u043a\u0430\u0437\u0430\u0442\u044c \"object function\", \u0432\u044b\u0445\u043e\u0434 \u0438\u0437 \u044d\u0442\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0438 \u0431\u0443\u0434\u0435\u0442 \u043d\u0430\u0448\u0430 \u043c\u0435\u0442\u0440\u0438\u043a\u0430</p>"},{"location":"portfolio/course_recsys/NBA_Modeling.html","title":"\u041a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u043a\u0430 \u041c\u043e\u0434\u0435\u043b\u0435\u0439\u00b6","text":"<p>\u041e\u0431\u0443\u0447\u0438\u0432 \u043c\u043e\u0434\u0435\u043b\u0438, \u043d\u0430\u043c \u0432\u0441\u0435\u0433\u0434\u0430 \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u043a\u0430\u043b\u0438\u0431\u0440\u043e\u0432\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438, \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438, \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e \u0435\u0441\u043b\u0438 \u043d\u0430 \u043d\u0430\u0434\u043e \u0441\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0442\u044c \u0440\u0430\u0437\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u044b.</p>"},{"location":"portfolio/course_recsys/NBA_Modeling.html#nba","title":"\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c NBA \u043d\u0430 \u043d\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\u00b6","text":"<ul> <li>\u0423 \u043d\u0430\u0441 \u043e\u0441\u0442\u0430\u043b\u0430\u0441\u044c \u0442\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043c\u044b \u043d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u0438 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043d\u0430\u0448\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439. \u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u044d\u0442\u0443 \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u0434\u043b\u044f \u043f\u0440\u0435\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0448\u0435\u0433\u043e \u0432\u0435\u0439\u0441\u0442\u0432\u0438\u044f.</li> <li>\u041e\u0442 \u0431\u0438\u0437\u043d\u0435\u0441\u0441\u0430 \u043c\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u043e \u0446\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0442\u043e\u0432\u0430\u0440\u0430 (NPV), \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u044d\u0442\u0438 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0434\u043b\u044f \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u043d\u043e \u0441 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044f\u043c\u0438</li> </ul>"},{"location":"portfolio/course_recsys/Ranker_CatBoost.html","title":"Ranker CatBoost","text":"In\u00a0[25]: Copied! <pre>!pip install catboost category_encoders sentence_transformers -qqq\n</pre> !pip install catboost category_encoders sentence_transformers -qqq <pre>/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n</pre> In\u00a0[26]: Copied! <pre>%matplotlib inline\nimport pandas as pd\nfrom catboost import CatBoostRanker, Pool\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nfrom sklearn.metrics import ndcg_score\nfrom category_encoders.cat_boost import CatBoostEncoder\nfrom sklearn.decomposition import PCA\nfrom sentence_transformers import SentenceTransformer\n</pre> %matplotlib inline import pandas as pd from catboost import CatBoostRanker, Pool import seaborn as sns import matplotlib.pyplot as plt import numpy as np import random from sklearn.metrics import ndcg_score from category_encoders.cat_boost import CatBoostEncoder from sklearn.decomposition import PCA from sentence_transformers import SentenceTransformer In\u00a0[27]: Copied! <pre>ratings_df = pd.read_csv('/kaggle/input/book-recommendation-dataset/Ratings.csv')\nratings_df.head()\n</pre> ratings_df = pd.read_csv('/kaggle/input/book-recommendation-dataset/Ratings.csv') ratings_df.head() Out[27]: User-ID ISBN Book-Rating 0 276725 034545104X 0 1 276726 0155061224 5 2 276727 0446520802 0 3 276729 052165615X 3 4 276729 0521795028 6 <p>\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439 \u043e\u043a\u043e\u043b\u043e \u043c\u0438\u043b\u043b\u0438\u043e\u043d \u0441\u0442\u0440\u043e\u043a</p> In\u00a0[28]: Copied! <pre>ratings_df.shape\n</pre> ratings_df.shape Out[28]: <pre>(1149780, 3)</pre> In\u00a0[29]: Copied! <pre>users_df = pd.read_csv('/kaggle/input/book-recommendation-dataset/Users.csv')\nusers_df.head()\n</pre> users_df = pd.read_csv('/kaggle/input/book-recommendation-dataset/Users.csv') users_df.head() Out[29]: User-ID Location Age 0 1 nyc, new york, usa NaN 1 2 stockton, california, usa 18.0 2 3 moscow, yukon territory, russia NaN 3 4 porto, v.n.gaia, portugal 17.0 4 5 farnborough, hants, united kingdom NaN In\u00a0[30]: Copied! <pre>books_df = pd.read_csv('/kaggle/input/book-recommendation-dataset/Books.csv')\nbooks_df.head()\n</pre> books_df = pd.read_csv('/kaggle/input/book-recommendation-dataset/Books.csv') books_df.head() <pre>/tmp/ipykernel_30/3442253096.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  books_df = pd.read_csv('/kaggle/input/book-recommendation-dataset/Books.csv')\n</pre> Out[30]: ISBN Book-Title Book-Author Year-Of-Publication Publisher Image-URL-S Image-URL-M Image-URL-L 0 0195153448 Classical Mythology Mark P. O. Morford 2002 Oxford University Press http://images.amazon.com/images/P/0195153448.0... http://images.amazon.com/images/P/0195153448.0... http://images.amazon.com/images/P/0195153448.0... 1 0002005018 Clara Callan Richard Bruce Wright 2001 HarperFlamingo Canada http://images.amazon.com/images/P/0002005018.0... http://images.amazon.com/images/P/0002005018.0... http://images.amazon.com/images/P/0002005018.0... 2 0060973129 Decision in Normandy Carlo D'Este 1991 HarperPerennial http://images.amazon.com/images/P/0060973129.0... http://images.amazon.com/images/P/0060973129.0... http://images.amazon.com/images/P/0060973129.0... 3 0374157065 Flu: The Story of the Great Influenza Pandemic... Gina Bari Kolata 1999 Farrar Straus Giroux http://images.amazon.com/images/P/0374157065.0... http://images.amazon.com/images/P/0374157065.0... http://images.amazon.com/images/P/0374157065.0... 4 0393045218 The Mummies of Urumchi E. J. W. Barber 1999 W. W. Norton &amp;amp; Company http://images.amazon.com/images/P/0393045218.0... http://images.amazon.com/images/P/0393045218.0... http://images.amazon.com/images/P/0393045218.0... In\u00a0[31]: Copied! <pre>df = pd.merge(ratings_df, users_df, on='User-ID', how='left')\ndf = pd.merge(books_df, df, on='ISBN', how='left')\ndf.head()\n</pre> df = pd.merge(ratings_df, users_df, on='User-ID', how='left') df = pd.merge(books_df, df, on='ISBN', how='left') df.head() Out[31]: ISBN Book-Title Book-Author Year-Of-Publication Publisher Image-URL-S Image-URL-M Image-URL-L User-ID Book-Rating Location Age 0 0195153448 Classical Mythology Mark P. O. Morford 2002 Oxford University Press http://images.amazon.com/images/P/0195153448.0... http://images.amazon.com/images/P/0195153448.0... http://images.amazon.com/images/P/0195153448.0... 2.0 0.0 stockton, california, usa 18.0 1 0002005018 Clara Callan Richard Bruce Wright 2001 HarperFlamingo Canada http://images.amazon.com/images/P/0002005018.0... http://images.amazon.com/images/P/0002005018.0... http://images.amazon.com/images/P/0002005018.0... 8.0 5.0 timmins, ontario, canada NaN 2 0002005018 Clara Callan Richard Bruce Wright 2001 HarperFlamingo Canada http://images.amazon.com/images/P/0002005018.0... http://images.amazon.com/images/P/0002005018.0... http://images.amazon.com/images/P/0002005018.0... 11400.0 0.0 ottawa, ontario, canada 49.0 3 0002005018 Clara Callan Richard Bruce Wright 2001 HarperFlamingo Canada http://images.amazon.com/images/P/0002005018.0... http://images.amazon.com/images/P/0002005018.0... http://images.amazon.com/images/P/0002005018.0... 11676.0 8.0 n/a, n/a, n/a NaN 4 0002005018 Clara Callan Richard Bruce Wright 2001 HarperFlamingo Canada http://images.amazon.com/images/P/0002005018.0... http://images.amazon.com/images/P/0002005018.0... http://images.amazon.com/images/P/0002005018.0... 41385.0 0.0 sudbury, ontario, canada NaN In\u00a0[32]: Copied! <pre>df['Book-Author'] = df['Book-Author'].fillna('unknown')\ndf['Publisher'] = df['Publisher'].fillna('unknown')\n</pre> df['Book-Author'] = df['Book-Author'].fillna('unknown') df['Publisher'] = df['Publisher'].fillna('unknown') In\u00a0[33]: Copied! <pre># \u0412\u0441\u0435 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u0433\u043e\u0434\u044b \u0432\u044b\u043f\u0443\u0441\u043a\u0430 \u043a\u043d\u0438\u0433\u0438 \ndf['Year-Of-Publication'].unique()\n</pre> # \u0412\u0441\u0435 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u0433\u043e\u0434\u044b \u0432\u044b\u043f\u0443\u0441\u043a\u0430 \u043a\u043d\u0438\u0433\u0438  df['Year-Of-Publication'].unique() Out[33]: <pre>array([2002, 2001, 1991, 1999, 2000, 1993, 1996, 1988, 2004, 1998, 1994,\n       2003, 1997, 1983, 1979, 1995, 1982, 1985, 1992, 1986, 1978, 1980,\n       1952, 1987, 1990, 1981, 1989, 1984, 0, 1968, 1961, 1958, 1974,\n       1976, 1971, 1977, 1975, 1965, 1941, 1970, 1962, 1973, 1972, 1960,\n       1966, 1920, 1956, 1959, 1953, 1951, 1942, 1963, 1964, 1969, 1954,\n       1950, 1967, 2005, 1957, 1940, 1937, 1955, 1946, 1936, 1930, 2011,\n       1925, 1948, 1943, 1947, 1945, 1923, 2020, 1939, 1926, 1938, 2030,\n       1911, 1904, 1949, 1932, 1928, 1929, 1927, 1931, 1914, 2050, 1934,\n       1910, 1933, 1902, 1924, 1921, 1900, 2038, 2026, 1944, 1917, 1901,\n       2010, 1908, 1906, 1935, 1806, 2021, '2000', '1995', '1999', '2004',\n       '2003', '1990', '1994', '1986', '1989', '2002', '1981', '1993',\n       '1983', '1982', '1976', '1991', '1977', '1998', '1992', '1996',\n       '0', '1997', '2001', '1974', '1968', '1987', '1984', '1988',\n       '1963', '1956', '1970', '1985', '1978', '1973', '1980', '1979',\n       '1975', '1969', '1961', '1965', '1939', '1958', '1950', '1953',\n       '1966', '1971', '1959', '1972', '1955', '1957', '1945', '1960',\n       '1967', '1932', '1924', '1964', '2012', '1911', '1927', '1948',\n       '1962', '2006', '1952', '1940', '1951', '1931', '1954', '2005',\n       '1930', '1941', '1944', 'DK Publishing Inc', '1943', '1938',\n       '1900', '1942', '1923', '1920', '1933', 'Gallimard', '1909',\n       '1946', '2008', '1378', '2030', '1936', '1947', '2011', '2020',\n       '1919', '1949', '1922', '1897', '2024', '1376', '1926', '2037'],\n      dtype=object)</pre> In\u00a0[34]: Copied! <pre>df['Year-Of-Publication'].value_counts().iloc[:10]\n</pre> df['Year-Of-Publication'].value_counts().iloc[:10] Out[34]: <pre>Year-Of-Publication\n2002    87297\n2001    75328\n1999    70228\n2003    69235\n2000    67595\n1998    59655\n1997    55196\n1996    54716\n1995    49964\n1994    42765\nName: count, dtype: int64</pre> In\u00a0[35]: Copied! <pre>print(df.shape)\ndf['Year-Of-Publication'] = pd.to_numeric(df['Year-Of-Publication'], errors='coerce')\ndf['Year-Of-Publication'].unique()\n</pre> print(df.shape) df['Year-Of-Publication'] = pd.to_numeric(df['Year-Of-Publication'], errors='coerce') df['Year-Of-Publication'].unique() <pre>(1032345, 12)\n</pre> Out[35]: <pre>array([2002., 2001., 1991., 1999., 2000., 1993., 1996., 1988., 2004.,\n       1998., 1994., 2003., 1997., 1983., 1979., 1995., 1982., 1985.,\n       1992., 1986., 1978., 1980., 1952., 1987., 1990., 1981., 1989.,\n       1984.,    0., 1968., 1961., 1958., 1974., 1976., 1971., 1977.,\n       1975., 1965., 1941., 1970., 1962., 1973., 1972., 1960., 1966.,\n       1920., 1956., 1959., 1953., 1951., 1942., 1963., 1964., 1969.,\n       1954., 1950., 1967., 2005., 1957., 1940., 1937., 1955., 1946.,\n       1936., 1930., 2011., 1925., 1948., 1943., 1947., 1945., 1923.,\n       2020., 1939., 1926., 1938., 2030., 1911., 1904., 1949., 1932.,\n       1928., 1929., 1927., 1931., 1914., 2050., 1934., 1910., 1933.,\n       1902., 1924., 1921., 1900., 2038., 2026., 1944., 1917., 1901.,\n       2010., 1908., 1906., 1935., 1806., 2021., 2012., 2006.,   nan,\n       1909., 2008., 1378., 1919., 1922., 1897., 2024., 1376., 2037.])</pre> In\u00a0[36]: Copied! <pre>df['Age'] = np.where(df['Age'] &gt; 100, None, df['Age'])\n\ndf['Year-Of-Publication'] = np.where(\n    df['Year-Of-Publication'] &lt;= 0,\n    np.nanmedian(df['Year-Of-Publication']),\n    df['Year-Of-Publication']\n    ).clip(0, 2021).astype(str)\n\ndf = df[df['Book-Rating'] &gt; 0]\n\ndf['city'] = df['Location'].apply(lambda x: x.split(',')[0].strip())\ndf['state'] = df['Location'].apply(lambda x: x.split(',')[1].strip())\ndf['country'] = df['Location'].apply(lambda x: x.split(',')[2].strip())\n</pre> df['Age'] = np.where(df['Age'] &gt; 100, None, df['Age'])  df['Year-Of-Publication'] = np.where(     df['Year-Of-Publication'] &lt;= 0,     np.nanmedian(df['Year-Of-Publication']),     df['Year-Of-Publication']     ).clip(0, 2021).astype(str)  df = df[df['Book-Rating'] &gt; 0]  df['city'] = df['Location'].apply(lambda x: x.split(',')[0].strip()) df['state'] = df['Location'].apply(lambda x: x.split(',')[1].strip()) df['country'] = df['Location'].apply(lambda x: x.split(',')[2].strip()) In\u00a0[37]: Copied! <pre># \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439\nusers = df['User-ID'].unique()\nrandom.shuffle(users)\n\n# \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043d\u0430 train, validation \u0438 test \u0432 \u043f\u0440\u043e\u043f\u043e\u0440\u0446\u0438\u0438 0.7 : 0.1 : 0.2\ntrain_users = users[:int(0.7*len(users))]\nval_users = users[int(0.7*len(users)):int(0.8*len(users))]\ntest_users = users[int(0.8*len(users)):]\n\n# train, val \u0438 test df\ntrain_df = df[df['User-ID'].isin(train_users)]\nval_df = df[df['User-ID'].isin(val_users)]\ntest_df = df[df['User-ID'].isin(test_users)]\n</pre> # \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 users = df['User-ID'].unique() random.shuffle(users)  # \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043d\u0430 train, validation \u0438 test \u0432 \u043f\u0440\u043e\u043f\u043e\u0440\u0446\u0438\u0438 0.7 : 0.1 : 0.2 train_users = users[:int(0.7*len(users))] val_users = users[int(0.7*len(users)):int(0.8*len(users))] test_users = users[int(0.8*len(users)):]  # train, val \u0438 test df train_df = df[df['User-ID'].isin(train_users)] val_df = df[df['User-ID'].isin(val_users)] test_df = df[df['User-ID'].isin(test_users)] In\u00a0[38]: Copied! <pre>train_df['Book-Title']\n</pre> train_df['Book-Title'] Out[38]: <pre>1                                               Clara Callan\n3                                               Clara Callan\n5                                               Clara Callan\n8                                               Clara Callan\n11                                              Clara Callan\n                                 ...                        \n1032308                                     You Got an Ology\n1032310                    Illustrated Encyclopedia of Cacti\n1032314    Lewis Carroll: A Traves Del Espejo Y Lo Que Al...\n1032337                                    Cocktail Classics\n1032339        Flashpoints: Promise and Peril in a New World\nName: Book-Title, Length: 271753, dtype: object</pre> In\u00a0[39]: Copied! <pre># \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0442\u0435\u043a\u0441\u0442\u043e\u043c\nmodel = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n</pre> # \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0442\u0435\u043a\u0441\u0442\u043e\u043c model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\") <pre>/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n</pre> In\u00a0[40]: Copied! <pre># \u0441\u043e\u0437\u0434\u0430\u0435\u043c train, val \u0438 test \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438\ntrain_books = train_df.loc[:, [\"ISBN\", \"Book-Title\"]].drop_duplicates()\nval_books = val_df.loc[:, [\"ISBN\", \"Book-Title\"]].drop_duplicates()\ntest_books = test_df.loc[:, [\"ISBN\", \"Book-Title\"]].drop_duplicates()\n\ntrain_embeddings = model.encode(train_books[\"Book-Title\"].tolist(), normalize_embeddings=True)\nval_embeddings = model.encode(val_books[\"Book-Title\"].tolist(), normalize_embeddings=True)\ntest_embeddings = model.encode(test_books[\"Book-Title\"].tolist(), normalize_embeddings=True)\n</pre> # \u0441\u043e\u0437\u0434\u0430\u0435\u043c train, val \u0438 test \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 train_books = train_df.loc[:, [\"ISBN\", \"Book-Title\"]].drop_duplicates() val_books = val_df.loc[:, [\"ISBN\", \"Book-Title\"]].drop_duplicates() test_books = test_df.loc[:, [\"ISBN\", \"Book-Title\"]].drop_duplicates()  train_embeddings = model.encode(train_books[\"Book-Title\"].tolist(), normalize_embeddings=True) val_embeddings = model.encode(val_books[\"Book-Title\"].tolist(), normalize_embeddings=True) test_embeddings = model.encode(test_books[\"Book-Title\"].tolist(), normalize_embeddings=True) <pre>Batches:   0%|          | 0/3707 [00:00&lt;?, ?it/s]</pre> <pre>Batches:   0%|          | 0/860 [00:00&lt;?, ?it/s]</pre> <pre>Batches:   0%|          | 0/1398 [00:00&lt;?, ?it/s]</pre> In\u00a0[41]: Copied! <pre>train_embeddings\n</pre> train_embeddings Out[41]: <pre>array([[-0.03592037, -0.03050607, -0.00902679, ..., -0.02955211,\n        -0.038664  ,  0.0858156 ],\n       [-0.02361225,  0.08952442, -0.01865784, ..., -0.01826494,\n        -0.01039198,  0.06050469],\n       [-0.03252224,  0.00791598, -0.05199765, ..., -0.01043319,\n        -0.05725905,  0.02445595],\n       ...,\n       [ 0.00055834, -0.00024567,  0.00981628, ..., -0.01239159,\n        -0.01077607, -0.01399653],\n       [-0.00267682, -0.02064378,  0.00749625, ...,  0.03351401,\n         0.02761846,  0.02537339],\n       [-0.03020654, -0.05159961,  0.04820484, ...,  0.01754397,\n        -0.06948989,  0.02908983]], dtype=float32)</pre> In\u00a0[42]: Copied! <pre>train_embeddings.shape\n</pre> train_embeddings.shape Out[42]: <pre>(118618, 384)</pre> <p>\u041c\u043e\u0434\u0435\u043b\u044c \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0431\u043e\u043b\u044c\u0448\u0443\u044e \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432 \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u043c\u0435\u0442\u043e\u0434\u043e\u043c \u043f\u043e\u043d\u0438\u0436\u0435\u043d\u0438\u044f \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438</p> In\u00a0[43]: Copied! <pre># \u0441\u043e\u043a\u0440\u0430\u0442\u0438\u043c \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0441 PCA\npca = PCA(n_components=0.8, random_state=42)\ntrain_embeddings = pca.fit_transform(train_embeddings)\nval_embeddings = pca.transform(val_embeddings)\ntest_embeddings = pca.transform(test_embeddings)\n</pre> # \u0441\u043e\u043a\u0440\u0430\u0442\u0438\u043c \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0441 PCA pca = PCA(n_components=0.8, random_state=42) train_embeddings = pca.fit_transform(train_embeddings) val_embeddings = pca.transform(val_embeddings) test_embeddings = pca.transform(test_embeddings) In\u00a0[44]: Copied! <pre>train_embeddings.shape\n</pre> train_embeddings.shape Out[44]: <pre>(118618, 87)</pre> In\u00a0[45]: Copied! <pre>train_books = train_df.loc[:, [\"ISBN\", \"Book-Title\"]].drop_duplicates()\n</pre> train_books = train_df.loc[:, [\"ISBN\", \"Book-Title\"]].drop_duplicates() In\u00a0[46]: Copied! <pre># \u0434\u043e\u0431\u0430\u0432\u0438\u043c \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 \u0432 \u0432\u0438\u0434\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\ndef add_embeddings(df, embeddings, books):\n    embeddings_df = pd.DataFrame(embeddings)\n    embeddings_df.columns = [f\"Book-Title_{i}\" for i in embeddings_df.columns]\n    books = pd.merge(books.reset_index(drop=True), embeddings_df, left_index=True, right_index=True)\n    return pd.merge(df, books, on=[\"ISBN\", \"Book-Title\"])\n\ntrain_df = add_embeddings(train_df, train_embeddings, train_books)\nval_df = add_embeddings(val_df, val_embeddings, val_books)\ntest_df = add_embeddings(test_df, test_embeddings, test_books)\n</pre> # \u0434\u043e\u0431\u0430\u0432\u0438\u043c \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 \u0432 \u0432\u0438\u0434\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 def add_embeddings(df, embeddings, books):     embeddings_df = pd.DataFrame(embeddings)     embeddings_df.columns = [f\"Book-Title_{i}\" for i in embeddings_df.columns]     books = pd.merge(books.reset_index(drop=True), embeddings_df, left_index=True, right_index=True)     return pd.merge(df, books, on=[\"ISBN\", \"Book-Title\"])  train_df = add_embeddings(train_df, train_embeddings, train_books) val_df = add_embeddings(val_df, val_embeddings, val_books) test_df = add_embeddings(test_df, test_embeddings, test_books) In\u00a0[47]: Copied! <pre>train_df.shape\n</pre> train_df.shape Out[47]: <pre>(271753, 102)</pre> In\u00a0[49]: Copied! <pre># \u0424\u0438\u0447\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \nEXCLUDE_FEATURES = ['city', 'Image-URL-S', 'Image-URL-M', 'Image-URL-L', 'User-ID', 'ISBN', 'Location','Book-Title','Book-Rating']\n\n# \u041a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0444\u0438\u0447\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u0443\u0434\u0435\u043c \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u0442\u044c \nCATEGORICAL_FEATURES = ['Book-Author', 'Year-Of-Publication', 'Publisher', 'state', 'country']\n\n# \u0426\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f\nTARGET = ['Book-Rating']\n</pre> # \u0424\u0438\u0447\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c  EXCLUDE_FEATURES = ['city', 'Image-URL-S', 'Image-URL-M', 'Image-URL-L', 'User-ID', 'ISBN', 'Location','Book-Title','Book-Rating']  # \u041a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0444\u0438\u0447\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u0443\u0434\u0435\u043c \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u0442\u044c  CATEGORICAL_FEATURES = ['Book-Author', 'Year-Of-Publication', 'Publisher', 'state', 'country']  # \u0426\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f TARGET = ['Book-Rating'] In\u00a0[56]: Copied! <pre>encoder = CatBoostEncoder()\n\ntrain_df[CATEGORICAL_FEATURES] = encoder.fit_transform(train_df[CATEGORICAL_FEATURES],\n                                                       train_df[TARGET])\nval_df[CATEGORICAL_FEATURES] = encoder.transform(val_df[CATEGORICAL_FEATURES])\ntest_df[CATEGORICAL_FEATURES] = encoder.transform(test_df[CATEGORICAL_FEATURES])\n</pre> encoder = CatBoostEncoder()  train_df[CATEGORICAL_FEATURES] = encoder.fit_transform(train_df[CATEGORICAL_FEATURES],                                                        train_df[TARGET]) val_df[CATEGORICAL_FEATURES] = encoder.transform(val_df[CATEGORICAL_FEATURES]) test_df[CATEGORICAL_FEATURES] = encoder.transform(test_df[CATEGORICAL_FEATURES]) <pre>Warning: No categorical columns found. Calling 'transform' will only return input data.\n</pre> <p>\u0412\u0441\u0435 \u0444\u0438\u0447\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u043f\u043e\u0434\u0430\u0432\u0430\u0442\u044c \u0432 \u043c\u043e\u0434\u0435\u043b\u044c \u0440\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f</p> In\u00a0[57]: Copied! <pre>FEATURES = [feat for feat in train_df.columns if feat not in EXCLUDE_FEATURES]\nprint('\\n',', '.join(map(repr, FEATURES)))\n</pre> FEATURES = [feat for feat in train_df.columns if feat not in EXCLUDE_FEATURES] print('\\n',', '.join(map(repr, FEATURES))) <pre>\n 'Book-Author', 'Year-Of-Publication', 'Publisher', 'Age', 'state', 'country', 'Book-Title_0', 'Book-Title_1', 'Book-Title_2', 'Book-Title_3', 'Book-Title_4', 'Book-Title_5', 'Book-Title_6', 'Book-Title_7', 'Book-Title_8', 'Book-Title_9', 'Book-Title_10', 'Book-Title_11', 'Book-Title_12', 'Book-Title_13', 'Book-Title_14', 'Book-Title_15', 'Book-Title_16', 'Book-Title_17', 'Book-Title_18', 'Book-Title_19', 'Book-Title_20', 'Book-Title_21', 'Book-Title_22', 'Book-Title_23', 'Book-Title_24', 'Book-Title_25', 'Book-Title_26', 'Book-Title_27', 'Book-Title_28', 'Book-Title_29', 'Book-Title_30', 'Book-Title_31', 'Book-Title_32', 'Book-Title_33', 'Book-Title_34', 'Book-Title_35', 'Book-Title_36', 'Book-Title_37', 'Book-Title_38', 'Book-Title_39', 'Book-Title_40', 'Book-Title_41', 'Book-Title_42', 'Book-Title_43', 'Book-Title_44', 'Book-Title_45', 'Book-Title_46', 'Book-Title_47', 'Book-Title_48', 'Book-Title_49', 'Book-Title_50', 'Book-Title_51', 'Book-Title_52', 'Book-Title_53', 'Book-Title_54', 'Book-Title_55', 'Book-Title_56', 'Book-Title_57', 'Book-Title_58', 'Book-Title_59', 'Book-Title_60', 'Book-Title_61', 'Book-Title_62', 'Book-Title_63', 'Book-Title_64', 'Book-Title_65', 'Book-Title_66', 'Book-Title_67', 'Book-Title_68', 'Book-Title_69', 'Book-Title_70', 'Book-Title_71', 'Book-Title_72', 'Book-Title_73', 'Book-Title_74', 'Book-Title_75', 'Book-Title_76', 'Book-Title_77', 'Book-Title_78', 'Book-Title_79', 'Book-Title_80', 'Book-Title_81', 'Book-Title_82', 'Book-Title_83', 'Book-Title_84', 'Book-Title_85', 'Book-Title_86'\n</pre> In\u00a0[60]: Copied! <pre># Catboost Ranker \u0442\u0440\u0435\u0431\u0443\u0435\u0442 \u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0438 \u043f\u043e \u0433\u0440\u0443\u043f\u043f\u0430\u043c (\u0432 \u043d\u0430\u0448\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u043f\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c)\ntrain_df = train_df.sort_values(by='User-ID')\nval_df = val_df.sort_values(by='User-ID')\ntest_df = test_df.sort_values(by='User-ID')\n\ntrain_df['User-ID'] = train_df['User-ID'].astype(str)\nval_df['User-ID'] = val_df['User-ID'].astype(str)\ntest_df['User-ID'] = test_df['User-ID'].astype(str)\n</pre> # Catboost Ranker \u0442\u0440\u0435\u0431\u0443\u0435\u0442 \u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0438 \u043f\u043e \u0433\u0440\u0443\u043f\u043f\u0430\u043c (\u0432 \u043d\u0430\u0448\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u043f\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c) train_df = train_df.sort_values(by='User-ID') val_df = val_df.sort_values(by='User-ID') test_df = test_df.sort_values(by='User-ID')  train_df['User-ID'] = train_df['User-ID'].astype(str) val_df['User-ID'] = val_df['User-ID'].astype(str) test_df['User-ID'] = test_df['User-ID'].astype(str) In\u00a0[61]: Copied! <pre>train_pool = Pool(\n    data=train_df[FEATURES],\n    label=train_df[TARGET],\n    group_id=train_df['User-ID'].tolist(),\n)\nval_pool = Pool(\n    data=val_df[FEATURES],\n    label=val_df[TARGET],\n    group_id=val_df[\"User-ID\"].tolist(),\n)\ntest_pool = Pool(\n    data=test_df[FEATURES],\n    group_id=test_df[\"User-ID\"].tolist(),\n)\n</pre> train_pool = Pool(     data=train_df[FEATURES],     label=train_df[TARGET],     group_id=train_df['User-ID'].tolist(), ) val_pool = Pool(     data=val_df[FEATURES],     label=val_df[TARGET],     group_id=val_df[\"User-ID\"].tolist(), ) test_pool = Pool(     data=test_df[FEATURES],     group_id=test_df[\"User-ID\"].tolist(), ) In\u00a0[70]: Copied! <pre>model = CatBoostRanker(loss_function=\"YetiRank\", \n                       verbose=100)\nmodel.fit(train_pool, \n          eval_set=val_pool, \n          early_stopping_rounds=100)\n</pre> model = CatBoostRanker(loss_function=\"YetiRank\",                         verbose=100) model.fit(train_pool,            eval_set=val_pool,            early_stopping_rounds=100) <pre>0:\ttest: 0.9793909\tbest: 0.9793909 (0)\ttotal: 787ms\tremaining: 13m 6s\n100:\ttest: 0.9847290\tbest: 0.9847870 (88)\ttotal: 1m 19s\tremaining: 11m 51s\n200:\ttest: 0.9848414\tbest: 0.9849120 (194)\ttotal: 2m 37s\tremaining: 10m 26s\n300:\ttest: 0.9850033\tbest: 0.9850304 (237)\ttotal: 3m 55s\tremaining: 9m 6s\n400:\ttest: 0.9852593\tbest: 0.9852885 (396)\ttotal: 5m 13s\tremaining: 7m 48s\n500:\ttest: 0.9852816\tbest: 0.9853377 (410)\ttotal: 6m 30s\tremaining: 6m 29s\nStopped by overfitting detector  (100 iterations wait)\n\nbestTest = 0.9853376892\nbestIteration = 410\n\nShrink model to first 411 iterations.\n</pre> Out[70]: <pre>&lt;catboost.core.CatBoostRanker at 0x7b1d1172b340&gt;</pre> In\u00a0[81]: Copied! <pre>test_df[\"score\"] = model.predict(test_pool)\ntest_df[['ISBN','Book-Title','Book-Rating','score']].head(10)\n</pre> test_df[\"score\"] = model.predict(test_pool) test_df[['ISBN','Book-Title','Book-Rating','score']].head(10) Out[81]: ISBN Book-Title Book-Rating score 194 0345402871 Airframe 9.0 -0.190028 34957 0891075275 Piercing the Darkness 6.0 0.156112 30245 0891076182 Prophet 3.0 -0.064853 45547 0553264990 Bant/Spec.Last of the Breed 5.0 0.038086 32009 0425099148 Death in the Clouds 7.0 0.082281 246 0375759778 Prague : A Novel 7.0 -0.347915 333 0553582747 From the Corner of His Eye 7.0 0.064243 517 0375410538 Anil's Ghost 5.0 -0.156534 520 0966986105 Prescription for Terror 10.0 -0.253021 525 0553062042 Daybreakers Louis Lamour Collection 7.0 -0.227025 In\u00a0[74]: Copied! <pre>feature_importance = model.get_feature_importance(data=train_pool, verbose=0)\nfeature_importance_df = (\n        pd.DataFrame(\n            feature_importance,\n            index=FEATURES,\n            columns=[\"Importance\"],\n        )\n        .sort_values(by=\"Importance\", ascending=False)\n        .reset_index()\n    )\n\nfeature_importance_df\n</pre> feature_importance = model.get_feature_importance(data=train_pool, verbose=0) feature_importance_df = (         pd.DataFrame(             feature_importance,             index=FEATURES,             columns=[\"Importance\"],         )         .sort_values(by=\"Importance\", ascending=False)         .reset_index()     )  feature_importance_df Out[74]: index Importance 0 state 9.617470e-05 1 Book-Title_3 9.159373e-05 2 Book-Title_56 5.298438e-05 3 country 5.202056e-05 4 Book-Title_24 5.140638e-05 ... ... ... 88 Book-Title_52 1.224288e-07 89 Age -1.621962e-06 90 Book-Title_25 -3.205877e-06 91 Book-Title_10 -7.567529e-06 92 Book-Author -4.965173e-04 <p>93 rows \u00d7 2 columns</p> In\u00a0[75]: Copied! <pre>users_count = test_df.groupby('User-ID')['ISBN'].count()\nusers = users_count[users_count.values &gt; 1].index\n</pre> users_count = test_df.groupby('User-ID')['ISBN'].count() users = users_count[users_count.values &gt; 1].index In\u00a0[78]: Copied! <pre>def get_user_ndcg(user_id):\n    true_relevance = np.asarray([test_df[test_df['User-ID'] == user_id][TARGET[0]].tolist()])\n    y_relevance = np.asarray([test_df[test_df['User-ID'] == user_id]['score'].tolist()])\n    return ndcg_score(true_relevance, y_relevance)\n\nndcg_scores = []\nfor user_id in users:\n    ndcg_scores.append(get_user_ndcg(user_id))\n    \nnp.mean(ndcg_scores)\n</pre> def get_user_ndcg(user_id):     true_relevance = np.asarray([test_df[test_df['User-ID'] == user_id][TARGET[0]].tolist()])     y_relevance = np.asarray([test_df[test_df['User-ID'] == user_id]['score'].tolist()])     return ndcg_score(true_relevance, y_relevance)  ndcg_scores = [] for user_id in users:     ndcg_scores.append(get_user_ndcg(user_id))      np.mean(ndcg_scores) Out[78]: <pre>0.9651886257470081</pre>"},{"location":"portfolio/course_recsys/Ranker_CatBoost.html#1","title":"1 | \u0417\u0430\u0434\u0430\u0447\u0430 \u0440\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f\u00b6","text":""},{"location":"portfolio/course_recsys/Ranker_CatBoost.html","title":"\u258e\u0420\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u041a\u043d\u0438\u0433\u00b6","text":"<ul> <li><code>\u0417\u0430\u0434\u0430\u0447\u0430 \u0440\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f</code> \u0432 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0441\u0438\u0441\u0442\u0435\u043c\u0430\u0445 \u0437\u0430\u043a\u043b\u044e\u0447\u0430\u0435\u0442\u0441\u044f \u0432 \u0442\u043e\u043c, \u0447\u0442\u043e\u0431\u044b \u0443\u043f\u043e\u0440\u044f\u0434\u043e\u0447\u0438\u0442\u044c \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u044b (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0442\u043e\u0432\u0430\u0440\u044b, \u0444\u0438\u043b\u044c\u043c\u044b, \u0441\u0442\u0430\u0442\u044c\u0438 \u0438 \u0442. \u0434.) \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0438\u0445 \u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f.</li> <li>\u041e\u0441\u043d\u043e\u0432\u043d\u0430\u044f \u0446\u0435\u043b\u044c \u2014 \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044e \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0449\u0438\u0435 \u0438 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u044b\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438, \u0447\u0442\u043e\u0431\u044b \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u0435\u0433\u043e \u043e\u043f\u044b\u0442 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u0441 \u0441\u0438\u0441\u0442\u0435\u043c\u043e\u0439.</li> <li>\u0412 \u044d\u0442\u043e\u043c \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0440\u0435\u0438\u0442\u0438\u043d\u0433\u043e\u0432 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u043a\u043d\u0438\u0433. \u041f\u043e \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0439 \u0441 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u043c \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435, \u0432 \u044d\u0442\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u043e\u043a\u043e\u043b\u043e \u043c\u0438\u043b\u043b\u0438\u043e\u043d\u0430 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432 \u043a\u043d\u0438\u0433 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439.</li> </ul>"},{"location":"portfolio/course_recsys/Ranker_CatBoost.html#2","title":"2 | \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":""},{"location":"portfolio/course_recsys/Ranker_CatBoost.html","title":"\u258e\u041f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0435 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439\u00b6","text":"<ul> <li><code>User-ID</code> : \u041f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c</li> <li><code>ISBN</code> : \u041a\u043d\u0438\u0433\u0430</li> <li><code>Book-Rating</code> : \u041f\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433</li> </ul> <p>\u0412 \u044d\u0442\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u0443 \u043d\u0430\u0441 \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0434\u043d\u0430 \u0444\u0438\u0447\u0430 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0441 \u043a\u043d\u0438\u0433\u0430\u043c\u0438, \u044d\u0442\u043e \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0438 \u043a\u043d\u0438\u0433 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u0441\u0442\u0430\u0432\u0438\u043b\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438 <code>Book-Rating</code>, \u044d\u0442\u043e \u0438 \u0431\u0443\u0434\u0435\u0442 \u043d\u0430\u0448\u0430 \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f</p>"},{"location":"portfolio/course_recsys/Ranker_CatBoost.html","title":"\u258e\u041f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438\u00b6","text":"<p>\u0422\u0430\u043a \u0436\u0435 \u0443 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0444\u0438\u0447 \u043e \u0441\u0430\u043c\u0438\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u0442\u0430\u0432\u0438\u043b\u0438 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0438</p>"},{"location":"portfolio/course_recsys/Ranker_CatBoost.html","title":"\u258e\u041f\u0440\u0435\u0434\u043c\u0435\u0442\u044b\u00b6","text":"<p>\u041a\u0430\u043a \u0438 \u043f\u0440\u043e\u0448\u043b\u044b\u0439 \u0440\u0430\u0437 \u043d\u0430\u0448\u0438 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u044b \u044d\u0442\u043e \u043a\u043d\u0438\u0433\u0438,</p>"},{"location":"portfolio/course_recsys/Ranker_CatBoost.html","title":"\u258e\u041e\u0431\u044a\u0435\u0434\u0435\u043d\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435\u00b6","text":"<p>\u0414\u043e\u0431\u0430\u0432\u0438\u043c \u0444\u0438\u0447\u0438 \u043f\u043e\u043b\u044c\u0437\u0449\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0438 \u043a\u043d\u0438\u0433 \u043a \u0434\u0430\u043d\u043d\u044b\u043c \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432</p>"},{"location":"portfolio/course_recsys/Ranker_CatBoost.html#3-preprocessing","title":"3 | Preprocessing\u00b6","text":""},{"location":"portfolio/course_recsys/Ranker_CatBoost.html","title":"\u258e\u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432 \u0438 \u043d\u0435\u0441\u0442\u0430\u0442\u044b\u043a\u043e\u0432\u043e\u043a\u00b6","text":"<p>\u041e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0435\u043c \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438 \u0434\u043b\u044f \u043a\u043e\u043b\u043e\u043d\u043e\u043a <code>Book-Author</code> \u0438 <code>Publisher</code></p>"},{"location":"portfolio/course_recsys/Ranker_CatBoost.html","title":"\u258e\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432\u00b6","text":"<p>\u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432\u044b\u0435 \u0444\u0438\u0447\u0438 \u0438\u0437 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u043a\u043d\u0438\u0433\u0438 <code>Book-Title</code></p>"},{"location":"portfolio/course_recsys/Ranker_CatBoost.html","title":"\u258e\u0422\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\u00b6","text":"<p>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u043c\u0435\u0442\u043e\u0434\u043e\u043c \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 Catboost <code>CatBoostEncoder</code></p>"},{"location":"portfolio/course_recsys/Ranker_CatBoost.html#catboost-pool","title":"\u258e\u0421\u043e\u0437\u0434\u0430\u0435\u043c Catboost Pool\u00b6","text":"<p>\u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u043e\u0440\u043c\u0430\u0442 <code>Pool()</code> \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 Catboost</p>"},{"location":"portfolio/course_recsys/Ranker_CatBoost.html#4","title":"4 | \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438 \u0420\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/course_recsys/Ranker_CatBoost.html","title":"\u258e\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\u00b6","text":"<p>\u041d\u0430 \u0432\u0445\u043e\u0434 \u043f\u0435\u0440\u0435\u0434\u0430\u0435\u043c \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u044e \u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439 Pool</p>"},{"location":"portfolio/course_recsys/Ranker_CatBoost.html","title":"\u258e\u0420\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\u00b6","text":""},{"location":"portfolio/course_recsys/Ranker_CatBoost.html","title":"\u258e\u041e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u043c \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\u00b6","text":""},{"location":"portfolio/course_recsys/Ranker_CatBoost.html","title":"\u258e\u041c\u0435\u0442\u0440\u0438\u043a\u0430 \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u0440\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f\u00b6","text":"<p>NDCG (Normalized Discounted Cumulative Gain) \u2014 \u044d\u0442\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0430, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0430\u044f \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u0440\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0432 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0441\u0438\u0441\u0442\u0435\u043c\u0430\u0445 \u0438 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u043c \u043f\u043e\u0438\u0441\u043a\u0435. \u041e\u043d\u0430 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0438\u0437\u043c\u0435\u0440\u0438\u0442\u044c, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0445\u043e\u0440\u043e\u0448\u043e \u0441\u0438\u0441\u0442\u0435\u043c\u0430 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u044b\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0432 \u0437\u0430\u0434\u0430\u043d\u043d\u043e\u043c \u043f\u043e\u0440\u044f\u0434\u043a\u0435.</p>"},{"location":"portfolio/course_recsys/Ranker_Regressor.html","title":"Ranker Regressor","text":"In\u00a0[1]: Copied! <pre>pip install catboost -qqq\n</pre> pip install catboost -qqq <pre>Note: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[2]: Copied! <pre># import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import mean_absolute_percentage_error as mape\nfrom catboost import CatBoostRegressor\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# set the color palette\nsns.set_palette(sns.color_palette('deep'))\nsns.set(rc = {'figure.figsize': (10, 5)})\nsns.set_style('whitegrid')\n</pre> # import necessary libraries import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt  from sklearn.metrics import mean_absolute_percentage_error as mape from catboost import CatBoostRegressor import warnings warnings.filterwarnings('ignore')  # set the color palette sns.set_palette(sns.color_palette('deep')) sns.set(rc = {'figure.figsize': (10, 5)}) sns.set_style('whitegrid') In\u00a0[3]: Copied! <pre>path = '/kaggle/input/bestseller/bestsellers with categories.csv'\ndf = pd.read_csv(path)\ndf.head()\n</pre> path = '/kaggle/input/bestseller/bestsellers with categories.csv' df = pd.read_csv(path) df.head() Out[3]: Name Author User Rating Reviews Price Year Genre 0 10-Day Green Smoothie Cleanse JJ Smith 4.7 17350 8 2016 Non Fiction 1 11/22/63: A Novel Stephen King 4.6 2052 22 2011 Fiction 2 12 Rules for Life: An Antidote to Chaos Jordan B. Peterson 4.7 18979 15 2018 Non Fiction 3 1984 (Signet Classics) George Orwell 4.7 21424 6 2017 Fiction 4 5,000 Awesome Facts (About Everything!) (Natio... National Geographic Kids 4.8 7665 12 2019 Non Fiction In\u00a0[4]: Copied! <pre>df.shape\n</pre> df.shape Out[4]: <pre>(550, 7)</pre> In\u00a0[5]: Copied! <pre>df['Name'].value_counts()\ndf[df['Name'] == \"Oh, the Places You'll Go!\"]\n</pre> df['Name'].value_counts() df[df['Name'] == \"Oh, the Places You'll Go!\"] Out[5]: Name Author User Rating Reviews Price Year Genre 245 Oh, the Places You'll Go! Dr. Seuss 4.9 21834 8 2012 Fiction 246 Oh, the Places You'll Go! Dr. Seuss 4.9 21834 8 2013 Fiction 247 Oh, the Places You'll Go! Dr. Seuss 4.9 21834 8 2014 Fiction 248 Oh, the Places You'll Go! Dr. Seuss 4.9 21834 8 2015 Fiction 249 Oh, the Places You'll Go! Dr. Seuss 4.9 21834 8 2016 Fiction 250 Oh, the Places You'll Go! Dr. Seuss 4.9 21834 8 2017 Fiction 251 Oh, the Places You'll Go! Dr. Seuss 4.9 21834 8 2018 Fiction 252 Oh, the Places You'll Go! Dr. Seuss 4.9 21834 8 2019 Fiction In\u00a0[6]: Copied! <pre>df.drop_duplicates(subset=['Name', 'Author'], inplace=True)\ndf.shape\n</pre> df.drop_duplicates(subset=['Name', 'Author'], inplace=True) df.shape Out[6]: <pre>(351, 7)</pre> In\u00a0[7]: Copied! <pre># ratings distribution\nplt.title('Rating Distribution')\nsns.histplot(x = 'User Rating', data = df);\n</pre> # ratings distribution plt.title('Rating Distribution') sns.histplot(x = 'User Rating', data = df); In\u00a0[8]: Copied! <pre># ratings distribution\nplt.title('Number of Reviews')\nsns.histplot(x = 'Reviews', data = df);\n</pre> # ratings distribution plt.title('Number of Reviews') sns.histplot(x = 'Reviews', data = df); In\u00a0[9]: Copied! <pre>df['Name Length'] = df['Name'].apply(lambda name: len(name))\n</pre> df['Name Length'] = df['Name'].apply(lambda name: len(name)) In\u00a0[10]: Copied! <pre>train = df[df['Year'] &lt; 2019]\ntest = df[df['Year'] == 2019]\n</pre> train = df[df['Year'] &lt; 2019] test = df[df['Year'] == 2019] In\u00a0[11]: Copied! <pre>train.head()\n</pre> train.head() Out[11]: Name Author User Rating Reviews Price Year Genre Name Length 0 10-Day Green Smoothie Cleanse JJ Smith 4.7 17350 8 2016 Non Fiction 29 1 11/22/63: A Novel Stephen King 4.6 2052 22 2011 Fiction 17 2 12 Rules for Life: An Antidote to Chaos Jordan B. Peterson 4.7 18979 15 2018 Non Fiction 39 3 1984 (Signet Classics) George Orwell 4.7 21424 6 2017 Fiction 22 5 A Dance with Dragons (A Song of Ice and Fire) George R. R. Martin 4.4 12643 11 2011 Fiction 45 In\u00a0[12]: Copied! <pre># \u0424\u0438\u0447\u0438 \u0438 \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \nfeatures = ['Name','Author','Price','Genre','Name Length']\ntarget = 'User Rating'\n\n# \u0414\u043b\u044f catboost \u043c\u043e\u0436\u043d\u043e \u0443\u043a\u0430\u0437\u0430\u0442\u044c \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0444\u0438\u0447\u0438 \u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0435 \ncategorical_features = ['Author', 'Genre'] \ntext_features = ['Name']\n</pre> # \u0424\u0438\u0447\u0438 \u0438 \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f  features = ['Name','Author','Price','Genre','Name Length'] target = 'User Rating'  # \u0414\u043b\u044f catboost \u043c\u043e\u0436\u043d\u043e \u0443\u043a\u0430\u0437\u0430\u0442\u044c \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0444\u0438\u0447\u0438 \u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0435  categorical_features = ['Author', 'Genre']  text_features = ['Name'] In\u00a0[13]: Copied! <pre>X_train = train[features]\ny_train = train[target]\nX_test = test[features]\ny_test = test[target]\n</pre> X_train = train[features] y_train = train[target] X_test = test[features] y_test = test[target] In\u00a0[14]: Copied! <pre>model = CatBoostRegressor(loss_function='RMSE', \n                          verbose=False)\nmodel.fit(X_train, \n          y_train,\n          categorical_features,\n          text_features)\n\npreds = model.predict(X_test)\nprint(f'Mean Absolute Percentage Error (MAPE): {round(mape(y_test, preds) * 100, 2)}')\n</pre> model = CatBoostRegressor(loss_function='RMSE',                            verbose=False) model.fit(X_train,            y_train,           categorical_features,           text_features)  preds = model.predict(X_test) print(f'Mean Absolute Percentage Error (MAPE): {round(mape(y_test, preds) * 100, 2)}') <pre>Mean Absolute Percentage Error (MAPE): 3.21\n</pre> In\u00a0[15]: Copied! <pre>rating_pred = test.copy()\nrating_pred['Pred Rating'] = preds\nrating_pred.sort_values(by='Pred Rating', ascending=False, inplace=True)\nrating_pred.head()\n</pre> rating_pred = test.copy() rating_pred['Pred Rating'] = preds rating_pred.sort_values(by='Pred Rating', ascending=False, inplace=True) rating_pred.head() Out[15]: Name Author User Rating Reviews Price Year Genre Name Length Pred Rating 86 Dog Man: For Whom the Ball Rolls: From the Cre... Dav Pilkey 4.9 9089 8 2019 Fiction 85 4.872677 85 Dog Man: Fetch-22: From the Creator of Captain... Dav Pilkey 4.9 12619 8 2019 Fiction 70 4.865559 545 Wrecking Ball (Diary of a Wimpy Kid Book 14) Jeff Kinney 4.9 9413 8 2019 Fiction 44 4.785464 155 Harry Potter and the Goblet of Fire: The Illus... J. K. Rowling 4.9 7758 18 2019 Fiction 87 4.771625 150 Guts Raina Telgemeier 4.8 5476 7 2019 Non Fiction 4 4.733733 In\u00a0[16]: Copied! <pre>model.feature_importances_\n</pre> model.feature_importances_ Out[16]: <pre>array([44.15442025, 17.23367217, 18.09923482,  2.28755613, 18.22511664])</pre> In\u00a0[17]: Copied! <pre>imp_catboost = pd.DataFrame(X_train.columns, \n                            columns = ['feature'])\nimp_catboost['importance'] = model.feature_importances_\n\nsns.set_theme(rc={'figure.figsize':(5,3)},style='whitegrid')\nax = sns.barplot(data=imp_catboost.sort_values(by='importance', ascending=False), \n            x='importance', y='feature', palette=\"BuGn_r\")\nplt.title('Top feature importances');\n</pre> imp_catboost = pd.DataFrame(X_train.columns,                              columns = ['feature']) imp_catboost['importance'] = model.feature_importances_  sns.set_theme(rc={'figure.figsize':(5,3)},style='whitegrid') ax = sns.barplot(data=imp_catboost.sort_values(by='importance', ascending=False),              x='importance', y='feature', palette=\"BuGn_r\") plt.title('Top feature importances'); In\u00a0[18]: Copied! <pre>target = 'Reviews'\n</pre> target = 'Reviews' In\u00a0[19]: Copied! <pre>X_train = train[features]\ny_train = train[target]\nX_test = test[features]\ny_test = test[target]\n</pre> X_train = train[features] y_train = train[target] X_test = test[features] y_test = test[target] In\u00a0[20]: Copied! <pre>model = CatBoostRegressor(loss_function='RMSE', \n                          verbose=False)\nmodel.fit(X_train, \n          y_train,\n          categorical_features,\n          text_features)\n\npreds = model.predict(X_test)\nprint(f'Mean Absolute Percentage Error (MAPE): {round(mape(y_test, preds) * 100, 2)}')\n</pre> model = CatBoostRegressor(loss_function='RMSE',                            verbose=False) model.fit(X_train,            y_train,           categorical_features,           text_features)  preds = model.predict(X_test) print(f'Mean Absolute Percentage Error (MAPE): {round(mape(y_test, preds) * 100, 2)}') <pre>Mean Absolute Percentage Error (MAPE): 58.13\n</pre> In\u00a0[21]: Copied! <pre>reviews_pred = test.copy()\nreviews_pred['Pred Reviews'] = preds\nreviews_pred.sort_values(by='Pred Reviews', ascending=False, inplace=True)\nreviews_pred.head()\n</pre> reviews_pred = test.copy() reviews_pred['Pred Reviews'] = preds reviews_pred.sort_values(by='Pred Reviews', ascending=False, inplace=True) reviews_pred.head() Out[21]: Name Author User Rating Reviews Price Year Genre Name Length Pred Reviews 150 Guts Raina Telgemeier 4.8 5476 7 2019 Non Fiction 4 36632.891881 462 The Silent Patient Alex Michaelides 4.5 27536 14 2019 Fiction 18 15312.348405 534 Where the Crawdads Sing Delia Owens 4.8 87841 15 2019 Fiction 23 14982.666031 176 I Am Confident, Brave &amp; Beautiful: A Coloring ... Hopscotch Girls 4.8 9737 7 2019 Non Fiction 60 14977.741160 345 The Body Keeps the Score: Brain, Mind, and Bod... Bessel van der Kolk M.D. 4.8 12361 12 2019 Non Fiction 72 11709.129867 In\u00a0[22]: Copied! <pre>pd.DataFrame({'rating': rating_pred['Name'].tolist(),\n              'review': reviews_pred['Name'].tolist(),\n})\n</pre> pd.DataFrame({'rating': rating_pred['Name'].tolist(),               'review': reviews_pred['Name'].tolist(), }) Out[22]: rating review 0 Dog Man: For Whom the Ball Rolls: From the Cre... Guts 1 Dog Man: Fetch-22: From the Creator of Captain... The Silent Patient 2 Wrecking Ball (Diary of a Wimpy Kid Book 14) Where the Crawdads Sing 3 Harry Potter and the Goblet of Fire: The Illus... I Am Confident, Brave &amp; Beautiful: A Coloring ... 4 Guts The Body Keeps the Score: Brain, Mind, and Bod... 5 It's Not Supposed to Be This Way: Finding Unex... Strange Planet (Strange Planet Series) 6 The Mueller Report What Should Danny Do? (The Power to Choose Ser... 7 Difficult Riddles For Smart Kids: 300 Difficul... The Guardians: A Novel 8 Where the Crawdads Sing Harry Potter and the Goblet of Fire: The Illus... 9 National Geographic Kids Why?: Over 1,111 Answ... Unfreedom of the Press 10 The Body Keeps the Score: Brain, Mind, and Bod... Girl, Stop Apologizing: A Shame-Free Plan for ... 11 Can't Hurt Me: Master Your Mind and Defy the Odds Good Days Start With Gratitude: A 52 Week Guid... 12 Howard Stern Comes Again Instant Pot Pressure Cooker Cookbook: 500 Ever... 13 I Am Confident, Brave &amp; Beautiful: A Coloring ... The Mueller Report 14 Good Days Start With Gratitude: A 52 Week Guid... The Total Money Makeover: Classic Edition: A P... 15 Strange Planet (Strange Planet Series) Howard Stern Comes Again 16 The Silent Patient National Geographic Little Kids First Big Book... 17 5,000 Awesome Facts (About Everything!) (Natio... 5,000 Awesome Facts (About Everything!) (Natio... 18 Instant Pot Pressure Cooker Cookbook: 500 Ever... The Unofficial Harry Potter Cookbook: From Cau... 19 Unicorn Coloring Book: For Kids Ages 4-8 (US E... Difficult Riddles For Smart Kids: 300 Difficul... 20 The Total Money Makeover: Classic Edition: A P... It's Not Supposed to Be This Way: Finding Unex... 21 The Unofficial Harry Potter Cookbook: From Cau... National Geographic Kids Why?: Over 1,111 Answ... 22 National Geographic Little Kids First Big Book... Dog Man: Fetch-22: From the Creator of Captain... 23 What Should Danny Do? (The Power to Choose Ser... Dog Man: For Whom the Ball Rolls: From the Cre... 24 Girl, Stop Apologizing: A Shame-Free Plan for ... Can't Hurt Me: Master Your Mind and Defy the Odds 25 Unfreedom of the Press Unicorn Coloring Book: For Kids Ages 4-8 (US E... 26 The Guardians: A Novel Wrecking Ball (Diary of a Wimpy Kid Book 14)"},{"location":"portfolio/course_recsys/Ranker_Regressor.html#1","title":"1 | \u0412\u0432\u043e\u0434\u043d\u0430\u044f \u0427\u0430\u0441\u0442\u044c\u00b6","text":""},{"location":"portfolio/course_recsys/Ranker_Regressor.html","title":"\u258e\u0420\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u041a\u043d\u0438\u0433\u00b6","text":"<p>\u0421\u0435\u0433\u043e\u0434\u043d\u044f \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0437\u0430\u0434\u0430\u0447\u0443 <code>\u0440\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0442\u043e\u0432\u0430\u0440\u043e\u0432</code>. \u041f\u043e \u0441\u0443\u0442\u0438 \u044d\u0442\u043e \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u0443\u043f\u043e\u0440\u044f\u0434\u043e\u0447\u0438\u0432\u0430\u043d\u0438\u044f \u0442\u043e\u0432\u0430\u0440\u043e\u0432 \u043f\u043e \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u044b\u043c \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u044f\u043c.</p> <p>\u0417\u0430\u0447\u0435\u043c \u043d\u0430\u043c \u044d\u0442\u043e \u043d\u0443\u0436\u043d\u043e; \u044d\u0442\u043e \u0432\u043b\u0438\u044f\u043d\u0438\u0435 \u043d\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0439 \u043e\u043f\u044b\u0442:</p> <ul> <li>\u0423\u0434\u043e\u0431\u0441\u0442\u0432\u043e \u043f\u043e\u0438\u0441\u043a\u0430 \u0438 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438 \u0442\u043e\u0432\u0430\u0440\u043e\u0432</li> <li>\u0418\u043d\u0434\u0438\u0432\u0438\u0434\u0443\u0430\u043b\u044c\u043d\u044b\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</li> <li>\u041f\u0435\u0440\u0441\u043e\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0438\u0441\u0442\u043e\u0440\u0438\u0438 \u043f\u043e\u043a\u0443\u043f\u043e\u043a \u0438 \u043f\u0440\u0435\u0434\u043f\u043e\u0447\u0442\u0435\u043d\u0438\u0439 (\u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0432 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u043c \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435)</li> </ul> <p>\u041f\u043e\u0434\u0445\u043e\u0434 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043c\u044b \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c; \u042d\u0442\u0443 \u0437\u0430\u0434\u0430\u0447\u0443 \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0440\u0435\u0448\u0438\u0442\u044c \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f. \u0418\u043c\u0435\u044f \u0438\u0441\u0442\u043e\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043e \u043f\u0440\u0438\u043e\u0440\u0438\u0442\u0435\u0442\u0430\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c <code>implicit</code>/<code>explicit</code> \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043e\u0446\u0435\u043d\u043a\u0438 \u0442\u043e\u0432\u0430\u0440\u0430.</p>"},{"location":"portfolio/course_recsys/Ranker_Regressor.html#2-amazon-best-seller-dataset","title":"2 | Amazon Best Seller Dataset\u00b6","text":""},{"location":"portfolio/course_recsys/Ranker_Regressor.html","title":"\u258e\u0427\u0442\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<p>\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0440\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0442\u043e\u0432\u0430\u0440\u043e\u0432 \u043d\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 Amazon Top 50 Bestselling Books 2009 \u2013 2019, \u0414\u0430\u0442\u0430\u0441\u0435\u0442 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 550 \u043a\u043d\u0438\u0433</p> <p><code>\u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438:</code> </p> <ol> <li>Name: \u041d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u043a\u043d\u0438\u0433\u0438 </li> <li>Author: \u0410\u0432\u0442\u043e\u0440 \u043a\u043d\u0438\u0433\u0438 </li> <li>User Rating: \u0420\u0435\u0439\u0442\u0438\u043d\u0433 </li> <li>Reviews: \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0435\u0432\u044c\u044e </li> <li>Price: \u0421\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u044c \u043a\u043d\u0438\u0433\u0438 </li> <li>Year: \u0413\u043e\u0434 \u043a\u043e\u0433\u0434\u0430 \u043a\u043d\u0438\u0433\u0430 \u043f\u043e\u043f\u0430\u043b\u0430 \u0432 \u0431\u0435\u0441\u0442\u0441\u0435\u043b\u043b\u0435\u0440\u044b </li> <li>Genre: \u0416\u0430\u043d\u0440 Fiction \u0438\u043b\u0438 Non-fiction </li> </ol>"},{"location":"portfolio/course_recsys/Ranker_Regressor.html#3","title":"3 | \u0420\u0430\u0437\u0432\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0410\u043d\u0430\u043b\u0438\u0437\u00b6","text":""},{"location":"portfolio/course_recsys/Ranker_Regressor.html","title":"\u258e\u041a\u043e\u043b\u043e\u043d\u043a\u0438 \u0434\u043b\u044f \u0440\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f\u00b6","text":"<p>\u0423 \u043d\u0430\u0441 \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u0434\u0432\u0435 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043b\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u0440\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0442\u043e\u0432\u0430\u0440\u043e\u0432; \u044d\u0442\u043e explicit \u0438 implicit feedback; \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0435\u0432\u044c\u044e\u0435\u0440\u043e\u0432 <code>Reviews</code> \u0438 \u0438\u0445 \u0441\u043e\u0432\u043e\u043a\u0443\u043f\u043d\u0430\u044f \u043e\u0446\u0435\u043d\u043a\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 <code>User Rating</code>; \u044d\u0442\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0431\u0443\u0434\u0443\u0442 \u043d\u0430\u0448\u0438\u043c\u0438 <code>\u0442\u0430\u0440\u0433\u0435\u0442\u0430\u043c\u0438</code> \u043f\u0440\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438 \u043c\u043e\u0434\u0435\u043b\u0435\u0439</p>"},{"location":"portfolio/course_recsys/Ranker_Regressor.html#4","title":"4 | \u0420\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0422\u043e\u0432\u0430\u0440\u043e\u0432\u00b6","text":""},{"location":"portfolio/course_recsys/Ranker_Regressor.html","title":"\u258e\u041c\u043e\u0434\u0435\u043b\u044c \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0421\u043e\u0432\u043e\u043a\u0443\u043f\u043d\u043e\u0433\u043e \u0420\u0435\u0439\u0442\u0438\u043d\u0433\u0430 \u0424\u0438\u043b\u044c\u043c\u0430\u00b6","text":""},{"location":"portfolio/course_recsys/Ranker_Regressor.html","title":"\u258e\u041c\u043e\u0434\u0435\u043b\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432\u00b6","text":"<p>\u0414\u043b\u044f \u0432\u0442\u043e\u0440\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c <code>Reviews</code></p>"},{"location":"portfolio/course_recsys/Uplift_Modeling.html","title":"Uplift Modeling","text":"In\u00a0[2]: Copied! <pre># import sys\n# !{sys.executable} -m pip install scikit-uplift==0.5.1 catboost pandas==1.3.5\n</pre> # import sys # !{sys.executable} -m pip install scikit-uplift==0.5.1 catboost pandas==1.3.5 In\u00a0[1]: Copied! <pre>from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklift.datasets import fetch_x5\nfrom sklift.metrics import uplift_at_k\nimport pandas as pd\nimport numpy as np\n\nrandom_state = 47\nk = 0.1\n</pre> from sklearn.model_selection import train_test_split from sklearn.model_selection import StratifiedKFold from sklift.datasets import fetch_x5 from sklift.metrics import uplift_at_k import pandas as pd import numpy as np  random_state = 47 k = 0.1 In\u00a0[8]: Copied! <pre>df_clients = pd.read_csv('fetch_x5_clients.csv')\ndf_train = pd.read_csv('fetch_x5_train.csv')\ndf_clients = df_clients.dropna()\ndf_all = df_clients.merge(df_train,on='client_id')\ndf_clients = df_all[['client_id','first_issue_date','first_redeem_date','age','gender']]\ndf_train = df_all[['client_id','treatment_flg','target']]\n</pre> df_clients = pd.read_csv('fetch_x5_clients.csv') df_train = pd.read_csv('fetch_x5_train.csv') df_clients = df_clients.dropna() df_all = df_clients.merge(df_train,on='client_id') df_clients = df_all[['client_id','first_issue_date','first_redeem_date','age','gender']] df_train = df_all[['client_id','treatment_flg','target']] In\u00a0[3]: Copied! <pre>df_clients.head(2)\n</pre> df_clients.head(2) Out[3]: client_id first_issue_date first_redeem_date age gender 0 000012768d 2017-08-05 15:40:48 2018-01-04 19:30:07 45 U 1 000036f903 2017-04-10 13:54:23 2017-04-23 12:37:56 72 F In\u00a0[7]: Copied! <pre>df_train.head(2)\n</pre> df_train.head(2) Out[7]: client_id treatment_flg target 0 000012768d 0 1 1 000036f903 1 1 In\u00a0[9]: Copied! <pre>print(f\"Dataset features shape: {df_clients.shape}\")\nprint(f\"Dataset train shape: {df_train.shape}\")\nprint(f\"Dataset target mean: {df_train.target.mean()}\")\nprint(f\"Dataset treatment mean: {df_train.treatment_flg.mean()}\")\n</pre> print(f\"Dataset features shape: {df_clients.shape}\") print(f\"Dataset train shape: {df_train.shape}\") print(f\"Dataset target mean: {df_train.target.mean()}\") print(f\"Dataset treatment mean: {df_train.treatment_flg.mean()}\") <pre>Dataset features shape: (182493, 5)\nDataset train shape: (182493, 3)\nDataset target mean: 0.6444904736072069\nDataset treatment mean: 0.5016247198522683\n</pre> In\u00a0[10]: Copied! <pre>df_clients = df_clients.set_index('client_id')\ndf_train = df_train.set_index('client_id')\n</pre> df_clients = df_clients.set_index('client_id') df_train = df_train.set_index('client_id') In\u00a0[11]: Copied! <pre>from sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\ndf_clients['gender'] = encoder.fit_transform(df_clients['gender'])\n</pre> from sklearn.preprocessing import LabelEncoder  encoder = LabelEncoder() df_clients['gender'] = encoder.fit_transform(df_clients['gender']) <p>\u041f\u0440\u0435\u0432\u0440\u0430\u0449\u0430\u0435\u043c <code>datetime</code> \u0432 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435 \u0432\u0440\u0435\u043c\u044f (\u0432 \u0441\u0435\u043a\u0443\u043d\u0434\u0430\u0445) \u043e\u0442 1970 \u0433\u043e\u0434\u0430</p> In\u00a0[12]: Copied! <pre># \u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\ndf_features = df_clients.copy()\ndf_features['first_issue_time'] = (pd.to_datetime(df_features['first_issue_date'])- pd.Timestamp('1970-01-01')) // pd.Timedelta('1s')\ndf_features['first_redeem_time'] = (pd.to_datetime(df_features['first_redeem_date']) - pd.Timestamp('1970-01-01')) // pd.Timedelta('1s')\ndf_features['issue_redeem_delay'] = df_features['first_redeem_time'] - df_features['first_issue_time']\ndf_features = df_features.drop(['first_issue_date', 'first_redeem_date'], axis=1)\n\ndf_features.head(2)\n</pre> # \u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 df_features = df_clients.copy() df_features['first_issue_time'] = (pd.to_datetime(df_features['first_issue_date'])- pd.Timestamp('1970-01-01')) // pd.Timedelta('1s') df_features['first_redeem_time'] = (pd.to_datetime(df_features['first_redeem_date']) - pd.Timestamp('1970-01-01')) // pd.Timedelta('1s') df_features['issue_redeem_delay'] = df_features['first_redeem_time'] - df_features['first_issue_time'] df_features = df_features.drop(['first_issue_date', 'first_redeem_date'], axis=1)  df_features.head(2) Out[12]: age gender first_issue_time first_redeem_time issue_redeem_delay client_id 000012768d 45 2 1501947648 1515094207 13146559 000036f903 72 0 1491832463 1492951076 1118613 <p>\u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043d\u0430 \u0434\u0432\u0435 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438</p> In\u00a0[13]: Copied! <pre># \u043e\u0442\u0434\u0435\u043b\u044f\u0435\u043c 30% - \u043f\u044f\u0442\u0443\u044e \u0447\u0430\u0441\u0442\u044c \u0432\u0441\u0435\u0433\u043e - \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443\ntrain, test = train_test_split(df_features, test_size=0.3, random_state=random_state)\n\nindices_train = train.index.tolist()\nindices_test = test.index.tolist()\n\nX_train = df_features.loc[indices_train, :]\ny_train = df_train.loc[indices_train, 'target']\ntreat_train = df_train.loc[indices_train, 'treatment_flg']\n\nX_test = df_features.loc[indices_test, :]\ny_test = df_train.loc[indices_test, 'target']\ntreat_test =  df_train.loc[indices_test, 'treatment_flg']\n\ntargets = [y_train, y_test]\ntreatmnets = [treat_train, treat_test]\nnames = ['train:',  'test:']\n</pre> # \u043e\u0442\u0434\u0435\u043b\u044f\u0435\u043c 30% - \u043f\u044f\u0442\u0443\u044e \u0447\u0430\u0441\u0442\u044c \u0432\u0441\u0435\u0433\u043e - \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443 train, test = train_test_split(df_features, test_size=0.3, random_state=random_state)  indices_train = train.index.tolist() indices_test = test.index.tolist()  X_train = df_features.loc[indices_train, :] y_train = df_train.loc[indices_train, 'target'] treat_train = df_train.loc[indices_train, 'treatment_flg']  X_test = df_features.loc[indices_test, :] y_test = df_train.loc[indices_test, 'target'] treat_test =  df_train.loc[indices_test, 'treatment_flg']  targets = [y_train, y_test] treatmnets = [treat_train, treat_test] names = ['train:',  'test:'] In\u00a0[14]: Copied! <pre>print('\u0411\u0430\u043b\u0430\u043d\u0441 \u0442\u0430\u0440\u0433\u0435\u0442\u0430 \u043d\u0430 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f\u0445:\\n')\ni = 0\nfor target in targets:\n    pc = target.mean()\n    print(names[i], pc)\n    i += 1\n\nprint('\\n\u0411\u0430\u043b\u0430\u043d\u0441 \u0432\u043e\u0437\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043d\u0430 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f\u0445:\\n')\ni = 0\nfor treatmnet in treatmnets:\n    pc = treatmnet.mean()\n    print(names[i], pc)\n    i += 1\n</pre> print('\u0411\u0430\u043b\u0430\u043d\u0441 \u0442\u0430\u0440\u0433\u0435\u0442\u0430 \u043d\u0430 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f\u0445:\\n') i = 0 for target in targets:     pc = target.mean()     print(names[i], pc)     i += 1  print('\\n\u0411\u0430\u043b\u0430\u043d\u0441 \u0432\u043e\u0437\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043d\u0430 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f\u0445:\\n') i = 0 for treatmnet in treatmnets:     pc = treatmnet.mean()     print(names[i], pc)     i += 1 <pre>\u0411\u0430\u043b\u0430\u043d\u0441 \u0442\u0430\u0440\u0433\u0435\u0442\u0430 \u043d\u0430 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f\u0445:\n\ntrain: 0.644831500254413\ntest: 0.6436947468400672\n\n\u0411\u0430\u043b\u0430\u043d\u0441 \u0432\u043e\u0437\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043d\u0430 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f\u0445:\n\ntrain: 0.5018983130455204\ntest: 0.500986337400453\n</pre> <p>\u0421\u043e\u0437\u0434\u0430\u043b\u0438\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u043a\u0443\u0434\u0430 \u0431\u0443\u0434\u0435\u043c \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043c\u0435\u0442\u0440\u0438\u043a\u0438 <code>uplift</code></p> In\u00a0[15]: Copied! <pre>models_results = {\n    'approach': [],\n    f'train_uplift@{k*100}%': [],\n    f'test_uplift@{k*100}%': []\n}\n</pre> models_results = {     'approach': [],     f'train_uplift@{k*100}%': [],     f'test_uplift@{k*100}%': [] } In\u00a0[17]: Copied! <pre>from sklift.models import ClassTransformation\nfrom sklearn.ensemble import GradientBoostingClassifier\n</pre> from sklift.models import ClassTransformation from sklearn.ensemble import GradientBoostingClassifier In\u00a0[18]: Copied! <pre>ct = ClassTransformation(\n    GradientBoostingClassifier()\n    )\n\nct = ct.fit(X_train, y_train, treat_train)\n\nuplift_ct_train = ct.predict(X_train)\nuplift_ct = ct.predict(X_test)\nprint(uplift_ct[:5])\n</pre> ct = ClassTransformation(     GradientBoostingClassifier()     )  ct = ct.fit(X_train, y_train, treat_train)  uplift_ct_train = ct.predict(X_train) uplift_ct = ct.predict(X_test) print(uplift_ct[:5]) <pre>[ 0.03508148  0.03092437  0.04037575 -0.05105889  0.01682562]\n</pre> <p>2 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0438 (<code>strategy</code>) \u0432 <code>uplift_at_k</code></p> <ul> <li>by_group (\u0434\u0435\u043b\u0430\u0435\u043c \u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0443 \u043f\u043e uplift \u0432\u043d\u0443\u0442\u0440\u0438 \u043a\u0430\u0436\u0434\u043e\u0439 \u043f\u0435\u0440\u0441\u0435\u043d\u0442\u0438\u043b\u044c\u043a\u0438)</li> <li>overall (\u0434\u0435\u043b\u0430\u0435\u043c \u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0443 \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u043e)</li> </ul> In\u00a0[19]: Copied! <pre>ct_score_train = uplift_at_k(y_true=y_train, \n                             uplift=uplift_ct_train, \n                             treatment=treat_train, \n                             strategy='by_group', k=k)\n\nct_score = uplift_at_k(y_true=y_test, \n                       uplift=uplift_ct, \n                       treatment=treat_test, \n                       strategy='by_group', k=k)\n\nmodels_results['approach'].append('ClassTransformation')\nmodels_results[f'train_uplift@{k*100}%'].append(ct_score_train)\nmodels_results[f'test_uplift@{k*100}%'].append(ct_score)\n\n# \u044f\u0432\u043d\u043e\u0435 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\npd.DataFrame(models_results) \n</pre> ct_score_train = uplift_at_k(y_true=y_train,                               uplift=uplift_ct_train,                               treatment=treat_train,                               strategy='by_group', k=k)  ct_score = uplift_at_k(y_true=y_test,                         uplift=uplift_ct,                         treatment=treat_test,                         strategy='by_group', k=k)  models_results['approach'].append('ClassTransformation') models_results[f'train_uplift@{k*100}%'].append(ct_score_train) models_results[f'test_uplift@{k*100}%'].append(ct_score)  # \u044f\u0432\u043d\u043e\u0435 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 pd.DataFrame(models_results)  Out[19]: approach train_uplift@10.0% test_uplift@10.0% 0 ClassTransformation 0.201149 0.118589 <p>\u041a\u0430\u043a \u0438 \u0434\u0440\u0443\u0433\u0438\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438, \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0441\u0440\u0430\u0432\u043d\u0438\u0442\u044c \u043c\u0435\u0442\u0440\u0438\u043a\u0443 \u043d\u0430 train \u0438 test \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0430\u0445, \u0441\u0434\u0435\u0441\u044c \u0443 \u043d\u0430\u0441 \u044f\u0432\u043d\u043e\u0435 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435</p> In\u00a0[21]: Copied! <pre>from sklift.metrics import uplift_at_k\nfrom sklift.viz import plot_uplift_preds\nfrom sklift.models import SoloModel\n</pre> from sklift.metrics import uplift_at_k from sklift.viz import plot_uplift_preds from sklift.models import SoloModel In\u00a0[23]: Copied! <pre>sm = SoloModel(\n    GradientBoostingClassifier()\n    )\n\n# \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\nsm = sm.fit(X_train, y_train, treat_train)\n\nuplift_sm_train = sm.predict(X_train)\nuplift_sm = sm.predict(X_test)\nprint(uplift_sm[:5])\n</pre> sm = SoloModel(     GradientBoostingClassifier()     )  # \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c sm = sm.fit(X_train, y_train, treat_train)  uplift_sm_train = sm.predict(X_train) uplift_sm = sm.predict(X_test) print(uplift_sm[:5]) <pre>[ 0.01861624  0.03623787  0.02475317 -0.01291694  0.02082576]\n</pre> In\u00a0[24]: Copied! <pre>sm_score_train = uplift_at_k(y_true=y_train, uplift=uplift_sm_train, treatment=treat_train, strategy='by_group', k=k)\nsm_score = uplift_at_k(y_true=y_test, uplift=uplift_sm, treatment=treat_test, strategy='by_group', k=k)\n\nmodels_results['approach'].append('SoloModel')\nmodels_results[f'train_uplift@{k*100}%'].append(sm_score_train)\nmodels_results[f'test_uplift@{k*100}%'].append(sm_score)\n\npd.DataFrame(models_results)\n</pre> sm_score_train = uplift_at_k(y_true=y_train, uplift=uplift_sm_train, treatment=treat_train, strategy='by_group', k=k) sm_score = uplift_at_k(y_true=y_test, uplift=uplift_sm, treatment=treat_test, strategy='by_group', k=k)  models_results['approach'].append('SoloModel') models_results[f'train_uplift@{k*100}%'].append(sm_score_train) models_results[f'test_uplift@{k*100}%'].append(sm_score)  pd.DataFrame(models_results) Out[24]: approach train_uplift@10.0% test_uplift@10.0% 0 ClassTransformation 0.201149 0.118589 1 SoloModel 0.088193 0.083730 In\u00a0[25]: Copied! <pre># \u041f\u043e\u043b\u0443\u0447\u0438\u043c \u0443\u0441\u043b\u043e\u0432\u043d\u044b\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0446\u0435\u043b\u0435\u0432\u043e\u0433\u043e \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043f\u0440\u0438 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430\nsm_trmnt_preds = sm.trmnt_preds_\n# \u0418 \u0443\u0441\u043b\u043e\u0432\u043d\u044b\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0446\u0435\u043b\u0435\u0432\u043e\u0433\u043e \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u0431\u0435\u0437 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430\nsm_ctrl_preds = sm.ctrl_preds_\n\n# \u041e\u0442\u0440\u0438\u0441\u0443\u0435\u043c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439 \u0438 \u0438\u0445 \u0440\u0430\u0437\u043d\u043e\u0441\u0442\u044c (uplift)\nplot_uplift_preds(trmnt_preds=sm_trmnt_preds, ctrl_preds=sm_ctrl_preds);\n</pre> # \u041f\u043e\u043b\u0443\u0447\u0438\u043c \u0443\u0441\u043b\u043e\u0432\u043d\u044b\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0446\u0435\u043b\u0435\u0432\u043e\u0433\u043e \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043f\u0440\u0438 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 sm_trmnt_preds = sm.trmnt_preds_ # \u0418 \u0443\u0441\u043b\u043e\u0432\u043d\u044b\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0446\u0435\u043b\u0435\u0432\u043e\u0433\u043e \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u0431\u0435\u0437 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 sm_ctrl_preds = sm.ctrl_preds_  # \u041e\u0442\u0440\u0438\u0441\u0443\u0435\u043c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439 \u0438 \u0438\u0445 \u0440\u0430\u0437\u043d\u043e\u0441\u0442\u044c (uplift) plot_uplift_preds(trmnt_preds=sm_trmnt_preds, ctrl_preds=sm_ctrl_preds); In\u00a0[27]: Copied! <pre>from sklift.models import TwoModels\n\ntm = TwoModels(\n    estimator_trmnt = GradientBoostingClassifier(),\n    estimator_ctrl = GradientBoostingClassifier(),\n    method='vanilla'\n)\n\ntm = tm.fit(X_train, y_train, treat_train)\n\nuplift_tm_train = tm.predict(X_train)\nuplift_tm = tm.predict(X_test)\nprint(uplift_tm[:5])\n</pre> from sklift.models import TwoModels  tm = TwoModels(     estimator_trmnt = GradientBoostingClassifier(),     estimator_ctrl = GradientBoostingClassifier(),     method='vanilla' )  tm = tm.fit(X_train, y_train, treat_train)  uplift_tm_train = tm.predict(X_train) uplift_tm = tm.predict(X_test) print(uplift_tm[:5]) <pre>[ 0.03633365  0.03156543  0.03352265 -0.05856611  0.02573542]\n</pre> In\u00a0[28]: Copied! <pre>tm_score_train = uplift_at_k(y_true=y_train, uplift=uplift_tm_train, treatment=treat_train, strategy='by_group', k=k)\ntm_score = uplift_at_k(y_true=y_test, uplift=uplift_tm, treatment=treat_test, strategy='by_group', k=k)\n\nmodels_results['approach'].append('TwoModels')\nmodels_results[f'train_uplift@{k*100}%'].append(tm_score_train)\nmodels_results[f'test_uplift@{k*100}%'].append(tm_score)\n\npd.DataFrame(models_results)\n</pre> tm_score_train = uplift_at_k(y_true=y_train, uplift=uplift_tm_train, treatment=treat_train, strategy='by_group', k=k) tm_score = uplift_at_k(y_true=y_test, uplift=uplift_tm, treatment=treat_test, strategy='by_group', k=k)  models_results['approach'].append('TwoModels') models_results[f'train_uplift@{k*100}%'].append(tm_score_train) models_results[f'test_uplift@{k*100}%'].append(tm_score)  pd.DataFrame(models_results) Out[28]: approach train_uplift@10.0% test_uplift@10.0% 0 ClassTransformation 0.201149 0.118589 1 SoloModel 0.088193 0.083730 2 TwoModels 0.159321 0.070591 In\u00a0[29]: Copied! <pre># \u041f\u043e\u043b\u0443\u0447\u0438\u043c \u0443\u0441\u043b\u043e\u0432\u043d\u044b\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0446\u0435\u043b\u0435\u0432\u043e\u0433\u043e \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043f\u0440\u0438 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430\ntm_trmnt_preds = tm.trmnt_preds_\n# \u0418 \u0443\u0441\u043b\u043e\u0432\u043d\u044b\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0446\u0435\u043b\u0435\u0432\u043e\u0433\u043e \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u0431\u0435\u0437 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430\ntm_ctrl_preds = tm.ctrl_preds_\n\n# \u041e\u0442\u0440\u0438\u0441\u0443\u0435\u043c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439 \u0438 \u0438\u0445 \u0440\u0430\u0437\u043d\u043e\u0441\u0442\u044c (uplift)\nplot_uplift_preds(trmnt_preds=tm_trmnt_preds, ctrl_preds=tm_ctrl_preds);\n</pre> # \u041f\u043e\u043b\u0443\u0447\u0438\u043c \u0443\u0441\u043b\u043e\u0432\u043d\u044b\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0446\u0435\u043b\u0435\u0432\u043e\u0433\u043e \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043f\u0440\u0438 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 tm_trmnt_preds = tm.trmnt_preds_ # \u0418 \u0443\u0441\u043b\u043e\u0432\u043d\u044b\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0446\u0435\u043b\u0435\u0432\u043e\u0433\u043e \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u0431\u0435\u0437 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 tm_ctrl_preds = tm.ctrl_preds_  # \u041e\u0442\u0440\u0438\u0441\u0443\u0435\u043c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439 \u0438 \u0438\u0445 \u0440\u0430\u0437\u043d\u043e\u0441\u0442\u044c (uplift) plot_uplift_preds(trmnt_preds=tm_trmnt_preds, ctrl_preds=tm_ctrl_preds); In\u00a0[35]: Copied! <pre>tm_ctrl = TwoModels(\n    estimator_trmnt=GradientBoostingClassifier(),\n    estimator_ctrl=GradientBoostingClassifier(),\n    method='ddr_control'\n)\n\ntm_ctrl = tm_ctrl.fit(X_train, y_train, treat_train)\n\nuplift_tm_ctrl_train = tm_ctrl.predict(X_train)\nuplift_tm_ctrl = tm_ctrl.predict(X_test)\nprint(uplift_tm_ctrl[:5])\n</pre> tm_ctrl = TwoModels(     estimator_trmnt=GradientBoostingClassifier(),     estimator_ctrl=GradientBoostingClassifier(),     method='ddr_control' )  tm_ctrl = tm_ctrl.fit(X_train, y_train, treat_train)  uplift_tm_ctrl_train = tm_ctrl.predict(X_train) uplift_tm_ctrl = tm_ctrl.predict(X_test) print(uplift_tm_ctrl[:5]) <pre>[ 0.03621028  0.03075091  0.03341193 -0.04307654  0.0292164 ]\n</pre> In\u00a0[36]: Copied! <pre>tm_ctrl_score_train = uplift_at_k(y_true=y_train, \n                                  uplift=uplift_tm_ctrl_train, \n                                  treatment=treat_train, \n                                  strategy='by_group', k=k)\n\ntm_ctrl_score = uplift_at_k(y_true=y_test, \n                            uplift=uplift_tm_ctrl, \n                            treatment=treat_test, \n                            strategy='by_group', k=k)\n\nmodels_results['approach'].append('TwoModels_ddr_control')\nmodels_results[f'train_uplift@{k*100}%'].append(tm_ctrl_score_train)\nmodels_results[f'test_uplift@{k*100}%'].append(tm_ctrl_score)\n\n\npd.DataFrame(models_results)\n</pre> tm_ctrl_score_train = uplift_at_k(y_true=y_train,                                    uplift=uplift_tm_ctrl_train,                                    treatment=treat_train,                                    strategy='by_group', k=k)  tm_ctrl_score = uplift_at_k(y_true=y_test,                              uplift=uplift_tm_ctrl,                              treatment=treat_test,                              strategy='by_group', k=k)  models_results['approach'].append('TwoModels_ddr_control') models_results[f'train_uplift@{k*100}%'].append(tm_ctrl_score_train) models_results[f'test_uplift@{k*100}%'].append(tm_ctrl_score)   pd.DataFrame(models_results) Out[36]: approach train_uplift@10.0% test_uplift@10.0% 0 ClassTransformation 0.201149 0.118589 1 SoloModel 0.088193 0.083730 2 TwoModels 0.159321 0.070591 3 TwoModels_ddr_control 0.172037 0.123702 In\u00a0[37]: Copied! <pre>plot_uplift_preds(trmnt_preds=tm_ctrl.trmnt_preds_, ctrl_preds=tm_ctrl.ctrl_preds_);\n</pre> plot_uplift_preds(trmnt_preds=tm_ctrl.trmnt_preds_, ctrl_preds=tm_ctrl.ctrl_preds_); <p>*\u0410\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c \u043c\u043e\u0436\u043d\u043e \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 $P^T$, \u0430 \u0437\u0430\u0442\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0435\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430 $P^C$.</p> In\u00a0[39]: Copied! <pre>tm_trmnt = TwoModels(\n    estimator_trmnt=GradientBoostingClassifier(),\n    estimator_ctrl=GradientBoostingClassifier(),\n    method='ddr_treatment'\n)\n\ntm_trmnt = tm_trmnt.fit(\n    X_train, y_train, treat_train\n)\n\nuplift_tm_trmnt_train = tm_trmnt.predict(X_train)\nuplift_tm_trmnt = tm_trmnt.predict(X_test)\nprint(uplift_tm_trmnt[:5])\n</pre> tm_trmnt = TwoModels(     estimator_trmnt=GradientBoostingClassifier(),     estimator_ctrl=GradientBoostingClassifier(),     method='ddr_treatment' )  tm_trmnt = tm_trmnt.fit(     X_train, y_train, treat_train )  uplift_tm_trmnt_train = tm_trmnt.predict(X_train) uplift_tm_trmnt = tm_trmnt.predict(X_test) print(uplift_tm_trmnt[:5]) <pre>[ 0.03124574  0.01680991  0.03615345 -0.09655285  0.02634465]\n</pre> In\u00a0[40]: Copied! <pre>tm_trmnt_score_train = uplift_at_k(y_true=y_train, uplift=uplift_tm_trmnt_train, treatment=treat_train, strategy='by_group', k=k)\ntm_trmnt_score = uplift_at_k(y_true=y_test, uplift=uplift_tm_trmnt, treatment=treat_test, strategy='by_group', k=k)\n\nmodels_results['approach'].append('TwoModels_ddr_treatment')\nmodels_results[f'train_uplift@{k*100}%'].append(tm_trmnt_score_train)\nmodels_results[f'test_uplift@{k*100}%'].append(tm_trmnt_score)\n\npd.DataFrame(models_results)\n</pre> tm_trmnt_score_train = uplift_at_k(y_true=y_train, uplift=uplift_tm_trmnt_train, treatment=treat_train, strategy='by_group', k=k) tm_trmnt_score = uplift_at_k(y_true=y_test, uplift=uplift_tm_trmnt, treatment=treat_test, strategy='by_group', k=k)  models_results['approach'].append('TwoModels_ddr_treatment') models_results[f'train_uplift@{k*100}%'].append(tm_trmnt_score_train) models_results[f'test_uplift@{k*100}%'].append(tm_trmnt_score)  pd.DataFrame(models_results) Out[40]: approach train_uplift@10.0% test_uplift@10.0% 0 ClassTransformation 0.201149 0.118589 1 SoloModel 0.088193 0.083730 2 TwoModels 0.159321 0.070591 3 TwoModels_ddr_control 0.172037 0.123702 4 TwoModels_ddr_treatment 0.141788 0.051622 In\u00a0[41]: Copied! <pre>plot_uplift_preds(trmnt_preds=tm_trmnt.trmnt_preds_, ctrl_preds=tm_trmnt.ctrl_preds_);\n</pre> plot_uplift_preds(trmnt_preds=tm_trmnt.trmnt_preds_, ctrl_preds=tm_trmnt.ctrl_preds_); In\u00a0[42]: Copied! <pre>pd.DataFrame(data=models_results).sort_values(f'test_uplift@{k*100}%', ascending=False)\n</pre> pd.DataFrame(data=models_results).sort_values(f'test_uplift@{k*100}%', ascending=False) Out[42]: approach train_uplift@10.0% test_uplift@10.0% 3 TwoModels_ddr_control 0.172037 0.123702 0 ClassTransformation 0.201149 0.118589 1 SoloModel 0.088193 0.083730 2 TwoModels 0.159321 0.070591 4 TwoModels_ddr_treatment 0.141788 0.051622 In\u00a0[53]: Copied! <pre># \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b \u0434\u0430\u043d\u043d\u044b\u0445. \u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e, \u0447\u0442\u043e\u0431\u044b \u0437\u0430\u043f\u0443\u0441\u0442\u0438\u043b\u0441\u044f \u0440\u0430\u0441\u0447\u0435\u0442 \u043c\u0435\u0442\u0440\u0438\u043a \u043d\u0430 \u043e\u0441\u043e\u0431\u044b\u0445 \u0432\u0435\u0440\u0441\u0438\u044f\u0445 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\nprint(f'y_test \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430: {y_test.shape}, \u0442\u0438\u043f: {y_test.dtype}')\nprint(f'uplift_ct \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430: {uplift_ct.shape}, \u0442\u0438\u043f: {uplift_ct.dtype}')\nprint(f'treat_test \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430: {treat_test.shape}, \u0442\u0438\u043f: {treat_test.dtype}')\n\n# \u0415\u0441\u043b\u0438 \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u044b \u0432\u043b\u043e\u0436\u0435\u043d\u043d\u044b\u0435 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b, \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u0442\u044c \u0438\u0445 \u0432 \u043e\u0434\u043d\u043e\u043c\u0435\u0440\u043d\u044b\u0435 \u043c\u0430\u0441\u0441\u0438\u0432\u044b\nif y_test.ndim &gt; 1:\n    y_test = y_test.flatten()\n\nif uplift_ct.ndim &gt; 1:\n    uplift_ct = uplift_ct.flatten()\n\nif treat_test.ndim &gt; 1:\n    treat_test = treat_test.flatten()\n</pre> # \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b \u0434\u0430\u043d\u043d\u044b\u0445. \u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e, \u0447\u0442\u043e\u0431\u044b \u0437\u0430\u043f\u0443\u0441\u0442\u0438\u043b\u0441\u044f \u0440\u0430\u0441\u0447\u0435\u0442 \u043c\u0435\u0442\u0440\u0438\u043a \u043d\u0430 \u043e\u0441\u043e\u0431\u044b\u0445 \u0432\u0435\u0440\u0441\u0438\u044f\u0445 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a print(f'y_test \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430: {y_test.shape}, \u0442\u0438\u043f: {y_test.dtype}') print(f'uplift_ct \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430: {uplift_ct.shape}, \u0442\u0438\u043f: {uplift_ct.dtype}') print(f'treat_test \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430: {treat_test.shape}, \u0442\u0438\u043f: {treat_test.dtype}')  # \u0415\u0441\u043b\u0438 \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u044b \u0432\u043b\u043e\u0436\u0435\u043d\u043d\u044b\u0435 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b, \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u0442\u044c \u0438\u0445 \u0432 \u043e\u0434\u043d\u043e\u043c\u0435\u0440\u043d\u044b\u0435 \u043c\u0430\u0441\u0441\u0438\u0432\u044b if y_test.ndim &gt; 1:     y_test = y_test.flatten()  if uplift_ct.ndim &gt; 1:     uplift_ct = uplift_ct.flatten()  if treat_test.ndim &gt; 1:     treat_test = treat_test.flatten() <pre>y_test \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430: (54748,), \u0442\u0438\u043f: int64\nuplift_ct \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430: (54748,), \u0442\u0438\u043f: float64\ntreat_test \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430: (54748,), \u0442\u0438\u043f: int64\n</pre> In\u00a0[55]: Copied! <pre># from sklift.metrics import uplift_by_percentile\n# import warnings; warnings.filterwarnings('ignore')\n\n# # \u043f\u043e \u0431\u0430\u043a\u0435\u0442\u0430\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043e response_rate_treatment / response_rate_control\n# uplift_by_percentile(y_test, uplift_ct, treat_test,\n#                      strategy='overall',\n#                      total=True, std=True, bins=10)\n</pre> # from sklift.metrics import uplift_by_percentile # import warnings; warnings.filterwarnings('ignore')  # # \u043f\u043e \u0431\u0430\u043a\u0435\u0442\u0430\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043e response_rate_treatment / response_rate_control # uplift_by_percentile(y_test, uplift_ct, treat_test, #                      strategy='overall', #                      total=True, std=True, bins=10) In\u00a0[56]: Copied! <pre>from sklift.metrics import weighted_average_uplift\n\nuplift_full_data_ct = weighted_average_uplift(y_test, uplift_ct, treat_test, bins=10)\nuplift_full_data_sm = weighted_average_uplift(y_test, uplift_sm, treat_test, bins=10)\nprint(f\"average uplift on test data CT: {uplift_full_data_ct:.4f}\")\nprint(f\"average uplift on test data SM: {uplift_full_data_sm:.4f}\")\n</pre> from sklift.metrics import weighted_average_uplift  uplift_full_data_ct = weighted_average_uplift(y_test, uplift_ct, treat_test, bins=10) uplift_full_data_sm = weighted_average_uplift(y_test, uplift_sm, treat_test, bins=10) print(f\"average uplift on test data CT: {uplift_full_data_ct:.4f}\") print(f\"average uplift on test data SM: {uplift_full_data_sm:.4f}\") <pre>average uplift on test data CT: 0.0346\naverage uplift on test data SM: 0.0356\n</pre> In\u00a0[62]: Copied! <pre># from sklift.viz import plot_uplift_by_percentile\n\n# # line plot\n# plot_uplift_by_percentile(y_test, uplift_ct, treat_test,\n#                           strategy='overall', kind='line');\n</pre> # from sklift.viz import plot_uplift_by_percentile  # # line plot # plot_uplift_by_percentile(y_test, uplift_ct, treat_test, #                           strategy='overall', kind='line'); In\u00a0[64]: Copied! <pre># # bar plot\n# plot_uplift_by_percentile(y_test, uplift_ct, treat_test, strategy='overall', kind='bar');\n</pre> # # bar plot # plot_uplift_by_percentile(y_test, uplift_ct, treat_test, strategy='overall', kind='bar'); In\u00a0[65]: Copied! <pre>from sklift.viz import plot_uplift_curve\n\n# with ideal curve\n# perfect=True\nplot_uplift_curve(y_test, uplift_ct, treat_test, perfect=False);\n</pre> from sklift.viz import plot_uplift_curve  # with ideal curve # perfect=True plot_uplift_curve(y_test, uplift_ct, treat_test, perfect=False); In\u00a0[66]: Copied! <pre>from sklift.viz import plot_qini_curve\n\n# with ideal Qini curve (red line)\n# perfect=True\nplot_qini_curve(y_test, uplift_ct, treat_test, perfect=False);\n</pre> from sklift.viz import plot_qini_curve  # with ideal Qini curve (red line) # perfect=True plot_qini_curve(y_test, uplift_ct, treat_test, perfect=False);"},{"location":"portfolio/course_recsys/Uplift_Modeling.html#uplift","title":"Uplift \u041c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f\u00b6","text":""},{"location":"portfolio/course_recsys/Uplift_Modeling.html","title":"\u041e\u0431\u0437\u043e\u0440 \u0431\u0430\u0437\u043e\u0432\u044b\u0445 \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u0432\u00b6","text":"<ul> <li>\u0412 \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0447\u0430\u0441\u0442\u0438 \u0432\u0435\u0431\u0438\u043d\u0430\u0440\u0430 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0431\u0430\u0437\u043e\u0432\u044b\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044b <code>Uplift</code> \u041c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f</li> </ul> <ul> <li>\u0417\u0430\u0434\u0430\u0447\u0430 <code>Uplift</code> \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u043e\u043c\u043e\u0433\u0430\u0435\u0442 \u043d\u0430\u043c \u0432\u044b\u044f\u0432\u0438\u0442\u044c \u0447\u0438\u0441\u0442\u044b\u0439 \u044d\u0444\u0444\u0435\u043a\u0442 \u043e\u0442 \u043a\u043e\u043c\u043c\u0443\u043d\u0438\u043a\u0430\u0446\u0438\u0438, \u0438\u043b\u0438 \u0432\u044b\u044f\u0432\u0438\u0442\u044c \u0443\u0431\u0435\u0436\u0434\u0430\u0435\u043c\u044b\u0445 \u043a\u043b\u0438\u0435\u0442\u043d\u043e\u0432</li> <li><code>Uplift</code> \u042d\u0442\u043e \u0440\u0430\u0437\u043d\u0438\u0446\u0430 \u043c\u0435\u0436\u0434\u0443 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c\u0438 \u0433\u0440\u0443\u043f\u043f\u044b, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u043e\u0434\u0432\u0435\u0440\u0433\u043b\u0430\u0441\u044c \u0432\u043e\u0437\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044e (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0443\u0432\u0438\u0434\u0435\u043b\u0430 \u0440\u0435\u043a\u043b\u0430\u043c\u0443), \u0438 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u043e\u0439, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043d\u0435 \u043f\u043e\u0434\u0432\u0435\u0440\u0433\u0430\u043b\u0430\u0441\u044c \u0432\u043e\u0437\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044e.</li> </ul> <ul> <li>\u0414\u043b\u044f \u043f\u0440\u043e\u0441\u0442\u043e\u0442\u044b, \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443 <code>sklift</code></li> </ul> <p>\u0412\u0430\u0436\u043d\u043e \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c</p> <ul> <li><code>\u041a\u0413</code> (\u041a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u0430\u044f \u0433\u0440\u0443\u043f\u043f\u0430) \u043a\u043e\u043c\u043c\u0443\u043d\u0438\u043a\u0430\u0446\u0438\u0438 \u043d\u0435\u0442 \u0438 <code>\u0426\u0413</code> (\u0426\u0435\u043b\u0435\u0432\u0430\u044f \u0433\u0440\u0443\u043f\u043f\u0430) \u043a\u043e\u043c\u043c\u0443\u043d\u0438\u043a\u0430\u0446\u0438\u044f \u0435\u0441\u0442\u044c</li> </ul>"},{"location":"portfolio/course_recsys/Uplift_Modeling.html","title":"\u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438\u00b6","text":""},{"location":"portfolio/course_recsys/Uplift_Modeling.html","title":"\u0427\u0438\u0442\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\u00b6","text":"<p>\u0414\u0430\u043d\u043d\u044b\u0435 \u0443 \u043d\u0430\u0441 \u0431\u0443\u0434\u0443\u0442 \u043e\u0442 X5, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0447\u0430\u0441\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0445, \u043e\u043f\u0438\u0440\u0430\u044f\u0441\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u043d\u0430 5 \u0444\u0438\u0447\u0435\u0439 (\u0434\u043b\u044f \u043f\u0440\u043e\u0441\u0442\u043e\u0442\u044b)</p>"},{"location":"portfolio/course_recsys/Uplift_Modeling.html","title":"\u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430\u00b6","text":"<p>\u0423\u0434\u043e\u0441\u0442\u043e\u0432\u0435\u0440\u0438\u043c\u0441\u044f \u0447\u0442\u043e \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0444\u0438\u0447\u0438 \u0438\u043c\u0435\u044e\u0442 \u0444\u043e\u0440\u043c\u0430\u0442 <code>category</code></p>"},{"location":"portfolio/course_recsys/Uplift_Modeling.html","title":"\u041f\u043e\u0434\u0445\u043e\u0434\u044b \u0441 \u041e\u0434\u043d\u043e\u0439 \u041c\u043e\u0434\u0435\u043b\u0438\u00b6","text":""},{"location":"portfolio/course_recsys/Uplift_Modeling.html#1","title":"(1) \u0422\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u041a\u043b\u0430\u0441\u0441\u0430\u00b6","text":""},{"location":"portfolio/course_recsys/Uplift_Modeling.html#2","title":"(2) \u041e\u0434\u043d\u0430 \u043c\u043e\u0434\u0435\u043b\u044c \u0441 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u043c \u043a\u043e\u043c\u043c\u0443\u043d\u0438\u043a\u0430\u0446\u0438\u0438\u00b6","text":"<p>\u0421\u0430\u043c\u043e\u0435 \u043f\u0440\u043e\u0441\u0442\u043e\u0435 \u0438 \u0438\u043d\u0442\u0443\u0438\u0442\u0438\u0432\u043d\u043e\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u0435:</p> <ul> <li>\u043c\u043e\u0434\u0435\u043b\u044c \u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u043e\u0434\u043d\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e \u043d\u0430 \u0434\u0432\u0443\u0445 \u0433\u0440\u0443\u043f\u043f\u0430\u0445, \u043f\u0440\u0438 \u044d\u0442\u043e\u043c \u0431\u0438\u043d\u0430\u0440\u043d\u044b\u0439 \u0444\u043b\u0430\u0433 \u043a\u043e\u043c\u043c\u0443\u043d\u0438\u043a\u0430\u0446\u0438\u0438 \u0432\u044b\u0441\u0442\u0443\u043f\u0430\u0435\u0442 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 (k+1) \u0444\u0438\u0447\u0443</li> <li>\u041a\u0430\u0436\u0434\u044b\u0439 \u043e\u0431\u044a\u0435\u043a\u0442 \u0438\u0437 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0441\u043a\u043e\u0440\u0438\u043c \u0434\u0432\u0430\u0436\u0434\u044b: \u0441 \u0444\u043b\u0430\u0433\u043e\u043c \u043a\u043e\u043c\u043c\u0443\u043d\u0438\u043a\u0430\u0446\u0438\u0438 \u0440\u0430\u0432\u043d\u044b\u043c 1 \u0438 \u0440\u0430\u0432\u043d\u044b\u043c 0.</li> <li>\u0412\u044b\u0447\u0438\u0442\u0430\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043f\u043e \u043a\u0430\u0436\u0434\u043e\u043c\u0443 \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044e, \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u0438\u0441\u043a\u043e\u043c\u044b uplift.</li> <li>\u0412 <code>sklift</code> \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432 \u043c\u0435\u0442\u043e\u0434\u0435 <code>SoloModel</code></li> </ul>"},{"location":"portfolio/course_recsys/Uplift_Modeling.html","title":"\u041f\u043e\u0434\u0445\u043e\u0434\u044b \u0441 \u0434\u0432\u0443\u043c\u044f \u043c\u043e\u0434\u0435\u043b\u044f\u043c\u0438\u00b6","text":""},{"location":"portfolio/course_recsys/Uplift_Modeling.html#1-t-learner","title":"(1) \u0414\u0432\u0435 \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 (t-Learner)\u00b6","text":"<ul> <li><p>\u0420\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f <code>sklift</code> \u0432 <code>TwoModels</code> + vanilla</p> </li> <li><p>\u041e\u0431\u0443\u0447\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b (t) (modelt)</p> </li> <li><p>\u041e\u0431\u0443\u0447\u0438\u0442\u044c \u0412\u0442\u043e\u0440\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b (c) (modelc)</p> </li> <li><p>\u0427\u0442\u043e\u0431\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c uplift \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432:</p> <ul> <li>(\u0430) \u043f\u0440\u043e\u0441\u043a\u043e\u0440\u0438\u0442\u044c \u0438\u0445 \u043f\u0435\u0440\u0432\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u044c\u044e</li> <li>(\u0431) \u043f\u0440\u043e\u0441\u043a\u043e\u0440\u0438\u0442\u044c \u0438\u0445 \u0432\u0442\u043e\u0440\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u044c\u044e</li> <li>\u041d\u0430\u0439\u0442\u0438 \u0440\u0430\u0437\u043d\u0438\u0446\u0443 \u0434\u0432\u0443\u0445 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432</li> </ul> </li> </ul>"},{"location":"portfolio/course_recsys/Uplift_Modeling.html#2","title":"(2) \u0414\u0432\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<ul> <li><p>\u0420\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f <code>sklift</code> \u0432 <code>TwoModels</code> + ddr_control</p> </li> <li><p>\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b (c) (modelc)</p> </li> <li><p>\u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u044d\u0442\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043f\u0440\u0435\u0434\u0438\u043a\u0442 \u043d\u0430 \u043a\u043b\u0438\u0435\u0442\u043d\u0430\u0445 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b (modelc).pred_proba(t)</p> </li> <li><p>\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u0432\u0442\u043e\u0440\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 (\u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b (t) + \u043f\u0440\u0435\u0434\u0438\u043a\u0442 \u0441 \u043f\u0440\u043e\u0448\u043b\u043e\u0433\u043e \u0448\u0430\u0433\u0430 \u043a\u0430\u043a \u0434\u043e\u043f. \u0444\u0438\u0447\u0430) (modelt)*</p> </li> <li><p>\u0427\u0442\u043e\u0431\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c uplift \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u043a\u043b\u0438\u0435\u043d\u0442\u0430\u0445</p> <ul> <li>(\u0430) \u041f\u0440\u043e\u0441\u043a\u043e\u0440\u044c\u0442\u0435 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432 \u043f\u0435\u0440\u0432\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u044c\u044e</li> <li>(\u0431) \u0414\u043e\u0431\u0430\u0432\u044c\u0442\u0435 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u0438\u043a\u0442\u044b \u043a\u0430\u043a \u0444\u0438\u0447\u0443 \u0438 \u043f\u0440\u043e\u0441\u043a\u043e\u0440\u044c\u0442\u0435 \u0432\u0442\u043e\u0440\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u044c\u044e</li> <li>\u041d\u0430\u0439\u0434\u0438\u0442\u0435 \u0440\u0430\u0437\u043d\u043e\u0441\u0442\u044c \u043f\u0443\u043d\u043a\u0442\u043e\u0432 (\u0430) \u0438 (\u0431)</li> </ul> </li> </ul> <p>*\u041c\u043e\u0436\u043d\u043e \u0438 \u043e\u0431\u0440\u0430\u0442\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c</p>"},{"location":"portfolio/course_recsys/Uplift_Modeling.html","title":"\u0421\u0440\u0430\u0432\u043d\u0438\u043c \u041c\u0435\u0442\u0440\u0438\u043a\u0438\u00b6","text":""},{"location":"portfolio/course_recsys/Uplift_Modeling.html#1-uplift_at_k","title":"(1) uplift_at_k\u00b6","text":"<ul> <li>\u043c\u0435\u0442\u0440\u0438\u043a\u0430 <code>uplift_at_k</code> \u043f\u043e\u043b\u0435\u0437\u043d\u0430 \u0434\u043b\u044f \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u0440\u0435\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0432\u043e\u0437\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043d\u0430 \u0446\u0435\u043b\u0435\u0432\u0443\u044e \u0430\u0443\u0434\u0438\u0442\u043e\u0440\u0438\u044e \u0438 \u0434\u043b\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0439 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u0441 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c\u0438.</li> </ul>"},{"location":"portfolio/course_recsys/Uplift_Modeling.html#2-uplift_by_percentile","title":"(2) uplift_by_percentile\u00b6","text":""},{"location":"portfolio/course_recsys/Uplift_Modeling.html#3-weighted_average_uplift","title":"(3) weighted_average_uplift\u00b6","text":"<p>\u041f\u043e \u0432\u0441\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435, \u0443\u0441\u0440\u0435\u0434\u043d\u0435\u043d\u043d\u0430\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0430 <code>uplift</code></p>"},{"location":"portfolio/course_recsys/Uplift_Modeling.html#5-plot_uplift_by_percentile","title":"(5) plot_uplift_by_percentile\u00b6","text":"<ul> <li>\u0413\u0440\u0430\u0444\u0438\u043a \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u043a\u0430\u043a\u043e\u0439 <code>uplift</code> \u043d\u0430 \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u043c percentile</li> <li>\u0414\u0430\u0435\u0442 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u043d\u044f\u0442\u044c \u043a \u043a\u0435\u043c \u043a\u043e\u043c\u043c\u0443\u043d\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u0442\u044c<ul> <li>\u0411\u0435\u0440\u0435\u043c \u0442\u043e\u043f 30% \u0438 \u043d\u0430 \u043d\u0438\u0445 \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044e</li> </ul> </li> <li>\u0413\u0440\u0430\u0444\u0438\u043a \u0434\u043e\u043b\u0436\u0435\u043d \u0431\u044b\u0442\u044c \u043c\u043e\u043d\u043e\u0442\u043e\u043d\u043d\u044b\u0439</li> </ul>"},{"location":"portfolio/course_recsys/Uplift_Modeling.html#6-plot_uplift_curve-plot_qini_curve","title":"(6) plot_uplift_curve, plot_qini_curve\u00b6","text":"<ul> <li><code>Uplift Curve</code> \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442, \u043a\u0430\u043a \u0438\u0437\u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f \u043f\u0440\u0438\u0440\u043e\u0441\u0442 (uplift) \u043f\u043e \u043c\u0435\u0440\u0435 \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u044f \u0434\u043e\u043b\u0438 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u0430\u0443\u0434\u0438\u0442\u043e\u0440\u0438\u0438, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u0432\u043e\u0437\u0434\u0435\u0439\u0441\u0442\u0432\u0443\u0435\u0442 \u043a\u0430\u043c\u043f\u0430\u043d\u0438\u044f. \u041e\u043d\u0430 \u043f\u043e\u043c\u043e\u0433\u0430\u0435\u0442 \u043e\u0446\u0435\u043d\u0438\u0442\u044c, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e \u0432\u043e\u0437\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0435 \u043d\u0430 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0451\u043d\u043d\u0443\u044e \u0433\u0440\u0443\u043f\u043f\u0443 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439.</li> <li><code>Qini Curve</code> \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0431\u043e\u043b\u0435\u0435 \u043f\u0440\u043e\u0434\u0432\u0438\u043d\u0443\u0442\u043e\u0439 \u0432\u0435\u0440\u0441\u0438\u0435\u0439 <code>Uplift Curve</code> \u0438 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043e\u0446\u0435\u043d\u0438\u0442\u044c \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438 \u0441 \u0442\u043e\u0447\u043a\u0438 \u0437\u0440\u0435\u043d\u0438\u044f \u0435\u0451 \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u0438 \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0432\u0430\u0442\u044c \u0434\u043e\u0445\u043e\u0434 \u0438\u043b\u0438 \u0434\u0440\u0443\u0433\u0438\u0435 \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0438.</li> </ul>"},{"location":"portfolio/course_recsys/User_Segmentation.html","title":"User Segmentation","text":"In\u00a0[7]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set(style=\"whitegrid\")\n</pre> import pandas as pd import numpy as np import datetime as dt import seaborn as sns from matplotlib import pyplot as plt  import warnings warnings.filterwarnings('ignore')  sns.set(style=\"whitegrid\") In\u00a0[5]: Copied! <pre>def plot_distrib(df):\n  # \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0438\n  fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n  # \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u043b\u044f Recency\n  sns.histplot(df['Recency'], kde=True, color='blue', ax=axes[0])\n  axes[0].set_title('Distribution of Recency')\n  axes[0].set_xlabel('Recency')\n  axes[0].set_ylabel('Frequency')\n\n  # \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u043b\u044f Frequency\n  sns.histplot(df['Frequency'], kde=True, color='green', ax=axes[1])\n  axes[1].set_title('Distribution of Frequency')\n  axes[1].set_xlabel('Frequency')\n  axes[1].set_ylabel('Frequency')\n\n  # \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u043b\u044f Monetary\n  sns.histplot(df['Monetary'], kde=True, color='red', ax=axes[2])\n  axes[2].set_title('Distribution of Monetary')\n  axes[2].set_xlabel('Monetary')\n  axes[2].set_ylabel('Frequency')\n\n  plt.tight_layout()\n  plt.show()\n</pre> def plot_distrib(df):   # \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0438   fig, axes = plt.subplots(1, 3, figsize=(15, 5))    # \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u043b\u044f Recency   sns.histplot(df['Recency'], kde=True, color='blue', ax=axes[0])   axes[0].set_title('Distribution of Recency')   axes[0].set_xlabel('Recency')   axes[0].set_ylabel('Frequency')    # \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u043b\u044f Frequency   sns.histplot(df['Frequency'], kde=True, color='green', ax=axes[1])   axes[1].set_title('Distribution of Frequency')   axes[1].set_xlabel('Frequency')   axes[1].set_ylabel('Frequency')    # \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u043b\u044f Monetary   sns.histplot(df['Monetary'], kde=True, color='red', ax=axes[2])   axes[2].set_title('Distribution of Monetary')   axes[2].set_xlabel('Monetary')   axes[2].set_ylabel('Frequency')    plt.tight_layout()   plt.show() <ul> <li>\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435, \u0441\u0440\u0430\u0437\u0443 \u0441\u0447\u0438\u0442\u044b\u0432\u0430\u044f \u0434\u0430\u0442\u044b \u0432 \u043d\u0443\u0436\u043d\u043e\u043c \u0444\u043e\u0440\u043c\u0430\u0442\u0435</li> <li>\u0412 \u0434\u0430\u043d\u043d\u044b\u0445 \u0443 \u043d\u0430\u0441 \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0430\u044e\u0442\u0441\u044f \u0442\u0440\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0438:<ul> <li>customer_id : \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0441\u043e\u0432\u0435\u0440\u0448\u0438\u043b \u043f\u043e\u043a\u0443\u043f\u043a\u0443</li> <li>trans_data : \u0434\u0430\u0442\u0430 \u043f\u043e\u043a\u0443\u043f\u043a\u0438 \u0442\u043e\u0432\u0430\u0440\u0430 (\u0447\u0442\u043e \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e \u043d\u0435 \u0443\u0442\u043e\u0447\u043d\u044f\u0435\u0442\u0441\u044f)</li> <li>tran_amount : \u0441\u0443\u043c\u043c\u0430 \u043f\u043e\u043a\u0443\u043f\u043a\u0438</li> </ul> </li> </ul> In\u00a0[6]: Copied! <pre>df = pd.read_csv('Retail_Data_Transactions.csv', parse_dates=['trans_date'])\nprint(df.shape)\ndf.head()\n</pre> df = pd.read_csv('Retail_Data_Transactions.csv', parse_dates=['trans_date']) print(df.shape) df.head() <pre>(125000, 3)\n</pre> Out[6]: customer_id trans_date tran_amount 0 CS5295 2013-02-11 35 1 CS4768 2015-03-15 39 2 CS2122 2013-02-26 52 3 CS1217 2011-11-16 99 4 CS1850 2013-11-20 78 In\u00a0[1]: Copied! <pre>print(\"\u041d\u0430\u043c \u0434\u0430\u043d\u044b \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0438 \u0437\u0430 \u0438\u043d\u0442\u0435\u0440\u0432\u0430\u043b:\")\nprint(df['trans_date'].min(), df['trans_date'].max())\n</pre> print(\"\u041d\u0430\u043c \u0434\u0430\u043d\u044b \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0438 \u0437\u0430 \u0438\u043d\u0442\u0435\u0440\u0432\u0430\u043b:\") print(df['trans_date'].min(), df['trans_date'].max()) <pre>\u041d\u0430\u043c \u0434\u0430\u043d\u044b \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0438 \u0437\u0430 \u0438\u043d\u0442\u0435\u0440\u0432\u0430\u043b:\n2011-05-16 00:00:00 2015-03-16 00:00:00\n</pre> In\u00a0[8]: Copied! <pre>sd = dt.datetime(2015,4,1)\ndf['days_from_last_transaction'] = (sd - df['trans_date']).dt.days\n# \u0432\u043e\u0437\u044c\u043c\u0435\u043c \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0438 \u0437\u0430 \u0433\u043e\u0434\ndf = df[df['days_from_last_transaction'] &lt;= 365]\nprint(df.shape)\ndf.head()\n</pre> sd = dt.datetime(2015,4,1) df['days_from_last_transaction'] = (sd - df['trans_date']).dt.days # \u0432\u043e\u0437\u044c\u043c\u0435\u043c \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0438 \u0437\u0430 \u0433\u043e\u0434 df = df[df['days_from_last_transaction'] &lt;= 365] print(df.shape) df.head() <pre>(30928, 4)\n</pre> Out[8]: customer_id trans_date tran_amount days_from_last_transaction 1 CS4768 2015-03-15 39 17 7 CS5902 2015-01-30 89 61 14 CS5552 2014-12-29 78 93 18 CS3904 2014-07-20 103 255 30 CS2859 2015-02-27 77 33 In\u00a0[6]: Copied! <pre># df.groupby('customer_id').agg(days_from_last_transaction=('tran_amount','min'))\n</pre> # df.groupby('customer_id').agg(days_from_last_transaction=('tran_amount','min')) <p>\u0413\u0440\u0443\u043f\u043f\u0438\u0440\u0443\u0435\u043c \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043a\u043b\u0438\u0435\u043d\u0442\u0430, \u043d\u0430\u0445\u043e\u0434\u0438\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0434\u043d\u0435\u0439 \u043e\u0442 \u0435\u0433\u043e \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0439 \u043f\u043e\u043a\u0443\u043f\u043a\u0438, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u043a\u0443\u043f\u043e\u043a \u0438 \u0441\u0443\u043c\u043c\u0430 \u0432\u0441\u0435\u0445 \u043f\u043e\u043a\u0443\u043f\u043e\u043a \u043a\u043b\u0438\u0435\u043d\u0442\u0430</p> In\u00a0[9]: Copied! <pre># \u0433\u0440\u0443\u043f\u043f\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0439 \u0444\u043e\u0440\u043c\u0430\u0442 \u0430\u0433\u0440\u0435\u0433\u0430\u0446\u0438\u0438\nrfmTable = df.groupby('customer_id').agg({\n    'days_from_last_transaction': lambda x: x.min(), # Recency\n    'customer_id': lambda x: len(x), # Frequency\n    'tran_amount': lambda x: x.sum() # Monetary Value\n    })\nprint(rfmTable.shape)\nrfmTable.head()\n</pre> # \u0433\u0440\u0443\u043f\u043f\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0439 \u0444\u043e\u0440\u043c\u0430\u0442 \u0430\u0433\u0440\u0435\u0433\u0430\u0446\u0438\u0438 rfmTable = df.groupby('customer_id').agg({     'days_from_last_transaction': lambda x: x.min(), # Recency     'customer_id': lambda x: len(x), # Frequency     'tran_amount': lambda x: x.sum() # Monetary Value     }) print(rfmTable.shape) rfmTable.head() <pre>(6763, 3)\n</pre> Out[9]: days_from_last_transaction customer_id tran_amount customer_id CS1112 77 4 251 CS1113 51 6 393 CS1114 48 5 315 CS1115 27 3 248 CS1116 219 3 190 In\u00a0[1]: Copied! <pre># \u0434\u043b\u044f \u0443\u0434\u043e\u0431\u0441\u0442\u0432\u0430 \u043f\u0435\u0440\u0435\u0438\u043c\u0435\u043d\u0443\u0435\u043c \u043a\u043e\u043b\u043e\u043d\u043a\u0438\nrfmTable.rename(columns={'days_from_last_transaction': 'Recency',\n                         'customer_id': 'Frequency',\n                         'tran_amount': 'Monetary'}, inplace=True)\nprint(rfmTable.shape)\nrfmTable.head()\n</pre> # \u0434\u043b\u044f \u0443\u0434\u043e\u0431\u0441\u0442\u0432\u0430 \u043f\u0435\u0440\u0435\u0438\u043c\u0435\u043d\u0443\u0435\u043c \u043a\u043e\u043b\u043e\u043d\u043a\u0438 rfmTable.rename(columns={'days_from_last_transaction': 'Recency',                          'customer_id': 'Frequency',                          'tran_amount': 'Monetary'}, inplace=True) print(rfmTable.shape) rfmTable.head() <pre>(6763, 3)\n</pre> Out[1]: Recency Frequency Monetary customer_id CS1112 77 4 251 CS1113 51 6 393 CS1114 48 5 315 CS1115 27 3 248 CS1116 219 3 190 <p>\u0421\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445</p> In\u00a0[4]: Copied! <pre># \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0444\u0438\u0447\u0435\u0439\nplot_distrib(rfmTable)\n</pre> # \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0444\u0438\u0447\u0435\u0439 plot_distrib(rfmTable) <p>\u0420\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u043c \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432 \u043d\u0430 \u0442\u0440\u0438 \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0442\u0440\u0435\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a</p> In\u00a0[\u00a0]: Copied! <pre># \u041f\u0440\u0438\u0441\u0432\u043e\u0435\u043d\u0438\u0435 \u0431\u0430\u043b\u043b\u043e\u0432 \u043f\u043e \u043a\u0432\u0430\u043d\u0442\u0438\u043b\u044f\u043c\nrfmTable['R_Score'] = pd.qcut(rfmTable['Recency'], 3, labels=range(3, 0, -1))\nrfmTable['F_Score'] = pd.qcut(rfmTable['Frequency'], 3, labels=range(3, 0, -1))\nrfmTable['M_Score'] = pd.qcut(rfmTable['Monetary'], 3, labels=range(3, 0, -1))\n</pre> # \u041f\u0440\u0438\u0441\u0432\u043e\u0435\u043d\u0438\u0435 \u0431\u0430\u043b\u043b\u043e\u0432 \u043f\u043e \u043a\u0432\u0430\u043d\u0442\u0438\u043b\u044f\u043c rfmTable['R_Score'] = pd.qcut(rfmTable['Recency'], 3, labels=range(3, 0, -1)) rfmTable['F_Score'] = pd.qcut(rfmTable['Frequency'], 3, labels=range(3, 0, -1)) rfmTable['M_Score'] = pd.qcut(rfmTable['Monetary'], 3, labels=range(3, 0, -1)) <p>\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043e\u0434\u043d\u0443 \u043c\u0435\u0442\u043a\u0443 \u0438\u0437 \u0432\u0441\u0435\u0445 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043e\u043a \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u0438\u0445 \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c</p> In\u00a0[6]: Copied! <pre># \u041e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u0435 \u0431\u0430\u043b\u043b\u043e\u0432 \u0432 \u043e\u0434\u0438\u043d RFM-\u0431\u0430\u043b\u043b\nrfmTable['RFM_Score'] = rfmTable['R_Score'].astype(str) + rfmTable['F_Score'].astype(str) + rfmTable['M_Score'].astype(str)\nprint(rfmTable.shape)\nrfmTable.head()\n</pre> # \u041e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u0435 \u0431\u0430\u043b\u043b\u043e\u0432 \u0432 \u043e\u0434\u0438\u043d RFM-\u0431\u0430\u043b\u043b rfmTable['RFM_Score'] = rfmTable['R_Score'].astype(str) + rfmTable['F_Score'].astype(str) + rfmTable['M_Score'].astype(str) print(rfmTable.shape) rfmTable.head() <pre>(6763, 7)\n</pre> Out[6]: Recency Frequency Monetary R_Score F_Score M_Score RFM_Score customer_id CS1112 77 4 251 2 2 2 222 CS1113 51 6 393 2 1 1 211 CS1114 48 5 315 2 2 2 222 CS1115 27 3 248 1 3 2 132 CS1116 219 3 190 3 3 3 333 <p>\u0421\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u043f\u0441\u0441\u043f\u0435\u0440\u0435\u0434\u0435\u043b\u0438\u0435 RFM-\u0441\u0435\u0433\u043c\u0435\u043d\u0442\u043e\u0432 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u0432</p> In\u00a0[7]: Copied! <pre># \u041a\u0440\u0443\u0433\u043e\u0432\u0430\u044f \u0434\u0438\u0430\u0433\u0440\u0430\u043c\u043c\u0430 \u0434\u043b\u044f RFM-\u0441\u0435\u0433\u043c\u0435\u043d\u0442\u043e\u0432\nplt.figure(figsize=(6, 6))\nrfm_counts = rfmTable['RFM_Score'].value_counts()\nplt.pie(rfm_counts, labels=rfm_counts.index, autopct='%1.1f%%', startangle=140)\nplt.title('Pie Chart of RFM Segments')\nplt.show()\n</pre> # \u041a\u0440\u0443\u0433\u043e\u0432\u0430\u044f \u0434\u0438\u0430\u0433\u0440\u0430\u043c\u043c\u0430 \u0434\u043b\u044f RFM-\u0441\u0435\u0433\u043c\u0435\u043d\u0442\u043e\u0432 plt.figure(figsize=(6, 6)) rfm_counts = rfmTable['RFM_Score'].value_counts() plt.pie(rfm_counts, labels=rfm_counts.index, autopct='%1.1f%%', startangle=140) plt.title('Pie Chart of RFM Segments') plt.show() <p>\u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0443 \u0434\u043b\u044f \u0441\u0430\u043c\u044b\u0445 \u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u043a\u043b\u0438\u0435\u0442\u043e\u0432</p> In\u00a0[8]: Copied! <pre># \u041f\u043e\u0441\u0442\u043e\u044f\u043d\u043d\u044b\u0435 \u043a\u043b\u0438\u0435\u043d\u0442\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0433\u043e\u0442\u043e\u0432\u044b \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u043e \u043f\u043e\u043a\u0443\u043f\u0430\u0442\u044c \u043d\u0430 \u0431\u043e\u043b\u044c\u0448\u0438\u0435 \u0441\u0443\u043c\u043c\u044b?\nrfmTable[rfmTable['RFM_Score'] == '111'][['Recency',\t'Frequency',\t'Monetary']].agg(['count', 'mean', 'median', 'min', 'max'])\n</pre> # \u041f\u043e\u0441\u0442\u043e\u044f\u043d\u043d\u044b\u0435 \u043a\u043b\u0438\u0435\u043d\u0442\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0433\u043e\u0442\u043e\u0432\u044b \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u043e \u043f\u043e\u043a\u0443\u043f\u0430\u0442\u044c \u043d\u0430 \u0431\u043e\u043b\u044c\u0448\u0438\u0435 \u0441\u0443\u043c\u043c\u044b? rfmTable[rfmTable['RFM_Score'] == '111'][['Recency',\t'Frequency',\t'Monetary']].agg(['count', 'mean', 'median', 'min', 'max']) Out[8]: Recency Frequency Monetary count 916.000000 916.000000 916.000000 mean 29.145197 7.424672 523.518559 median 28.000000 7.000000 504.000000 min 16.000000 6.000000 360.000000 max 46.000000 14.000000 951.000000 <p>\u041a\u043b\u0438\u0435\u043d\u0442\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043d\u0435\u0434\u0430\u0432\u043d\u043e \u043f\u043e\u043a\u0443\u043f\u0430\u043b\u0438 \u0438 \u0431\u044b\u043b\u0438 \u0430\u043a\u0442\u0438\u0432\u043d\u044b\u043c\u0438</p> In\u00a0[9]: Copied! <pre># \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043d\u0435\u0434\u0430\u0432\u043d\u0438\u0435 \u043a\u043b\u0438\u0435\u043d\u0442\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u043a\u0443\u043f\u0430\u044e\u0442 \u0441\u043e \u0441\u0440\u0435\u0434\u043d\u0435\u0439 \u0447\u0430\u0441\u0442\u043e\u0442\u043e\u0439 \u043d\u0430 \u0441\u0440\u0435\u0434\u043d\u044e\u044e \u0441\u0443\u043c\u043c\u0443?\nrfmTable[rfmTable['RFM_Score'] == '322'][['Recency',\t'Frequency',\t'Monetary']].agg(['count', 'mean', 'median', 'min', 'max'])\n</pre> # \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043d\u0435\u0434\u0430\u0432\u043d\u0438\u0435 \u043a\u043b\u0438\u0435\u043d\u0442\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u043a\u0443\u043f\u0430\u044e\u0442 \u0441\u043e \u0441\u0440\u0435\u0434\u043d\u0435\u0439 \u0447\u0430\u0441\u0442\u043e\u0442\u043e\u0439 \u043d\u0430 \u0441\u0440\u0435\u0434\u043d\u044e\u044e \u0441\u0443\u043c\u043c\u0443? rfmTable[rfmTable['RFM_Score'] == '322'][['Recency',\t'Frequency',\t'Monetary']].agg(['count', 'mean', 'median', 'min', 'max']) Out[9]: Recency Frequency Monetary count 505.000000 505.000000 505.000000 mean 152.233663 4.368317 282.485149 median 143.000000 4.000000 283.000000 min 101.000000 4.000000 202.000000 max 295.000000 5.000000 359.000000 <p>\u041d\u0435 \u043e\u0447\u0435\u043d\u044c \u0430\u043a\u0442\u0438\u0432\u043d\u044b\u0435 \u043a\u043b\u0438\u0435\u0442\u043d\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0440\u0435\u0434\u043a\u043e \u0445\u043e\u0434\u0438\u043b\u0438 \u0438 \u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u043c\u043d\u043e\u0433\u043e \u043f\u043e\u043a\u0443\u043f\u0430\u043b\u0438</p> In\u00a0[14]: Copied! <pre># \u0441\u0430\u043c\u044b\u0435 \u043d\u0435\u0430\u043a\u0442\u0438\u0432\u043d\u044b\u0435 \u0438 \u0440\u0430\u0437\u043e\u0432\u044b\u0435 \u043a\u043b\u0438\u0435\u043d\u0442\u044b?\nrfmTable[rfmTable['RFM_Score'] == '333'][['Recency',\t'Frequency',\t'Monetary']].agg(['count', 'mean', 'median', 'min', 'max'])\n</pre> # \u0441\u0430\u043c\u044b\u0435 \u043d\u0435\u0430\u043a\u0442\u0438\u0432\u043d\u044b\u0435 \u0438 \u0440\u0430\u0437\u043e\u0432\u044b\u0435 \u043a\u043b\u0438\u0435\u043d\u0442\u044b? rfmTable[rfmTable['RFM_Score'] == '333'][['Recency',\t'Frequency',\t'Monetary']].agg(['count', 'mean', 'median', 'min', 'max']) Out[14]: Recency Frequency Monetary count 1044.000000 1044.000000 1044.000000 mean 193.834291 2.056513 109.831418 median 180.500000 2.000000 108.000000 min 101.000000 1.000000 10.000000 max 365.000000 3.000000 201.000000 <p>General</p> <ul> <li>\u044d\u0442\u043e \u043c\u0435\u0442\u043e\u0434 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438\u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0434\u0435\u043b\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u041a \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0441\u0445\u043e\u0436\u043e\u0441\u0442\u0438, \u043c\u0438\u043d\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u044f \u0441\u0440\u0435\u0434\u043d\u0435\u043a\u0432\u0430\u0440\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u043e\u0448\u0438\u0431\u043a\u0443 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f</li> <li>\u0423 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0446\u0435\u043d\u0442\u0440\u0430 \u0435\u0441\u0442\u044c \u0441\u0432\u043e\u0439 \u0446\u0435\u043d\u0442\u0440 \u043c\u0430\u0441\u0441\u044b</li> <li>\u043d\u0443\u0436\u043d\u043e \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0440\u0430\u0437\u043d\u0438\u0446\u0443 \u043c\u0435\u0436\u0434\u0443 \u0432\u0441\u0435\u043c\u0438 \u043d\u0430\u0448\u0438\u043c\u0438 \u0442\u043e\u0447\u043a\u0430\u043c\u0438 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0446\u0435\u043d\u0442\u0440\u0430 \u043d\u0430\u0448\u0435\u0433\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0430</li> <li>\u044d\u0442\u0443 \u0441\u0443\u043c\u043c\u0443 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043c\u0438\u043d\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c</li> </ul> <p>Algorithm</p> <ul> <li>\u0412\u044b\u0431\u043e\u0440 \u041a \u043d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0445 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0445\u0446\u0435\u043d\u0442\u0440\u043e\u0432</li> <li>\u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0446\u0435\u043d\u0442\u0440\u0430 \u0440\u0430\u0441\u0441\u0447\u0438\u0442\u0438\u0432\u0430\u0435\u043c \u0440\u0430\u0441\u0442\u043e\u044f\u043d\u0438\u0435 \u0434\u043e \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0437 \u041a \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u0438 \u043e\u0442\u043d\u043e\u0441\u0438\u043c \u044d\u0433\u043e \u043a \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0435\u043c\u0443</li> <li>\u041f\u0435\u0440\u0435\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u0446\u0435\u043d\u0442\u0440\u044b \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u043a\u0430\u043a \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0442\u043e\u0447\u0435\u043a, \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u0430\u0449\u0438\u0445 \u043a\u0430\u0436\u0434\u043e\u043c\u0443 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0443</li> <li>\u043f\u043e\u0432\u0442\u043e\u0440\u044f\u0435\u043c \u0440\u0430\u0441\u0441\u0447\u0435\u0442 \u0440\u0430\u0441\u0442\u043e\u044f\u043d\u0438\u0439 \u0438 \u043f\u0435\u043f\u0435\u0441\u0447\u0435\u0442 \u0446\u0435\u043d\u0442\u0440\u043e\u0432 \u0434\u043e \u0442\u0435\u0445 \u043f\u043e\u0440 \u043f\u043e\u043a\u0430 \u0446\u0435\u043d\u0442\u0440\u044b \u043d\u0435 \u043f\u0435\u0440\u0435\u0441\u0442\u0430\u043d\u0443\u0442 \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043c\u0435\u043d\u044f\u0442\u044c\u0441\u044f</li> </ul> <p>\u041f\u043b\u044e\u0441\u044b</p> <ul> <li>\u043f\u0440\u043e\u0441\u0442\u043e\u0442\u0430 \u0438 \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c</li> <li>\u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0438\u0440\u0443\u0435\u043c\u043e\u0441\u0442\u044c</li> </ul> <p>\u041c\u0438\u043d\u0443\u0441\u044b</p> <ul> <li>\u0412\u044b\u0431\u043e\u043a \u041a</li> <li>\u0447\u0443\u0432\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043a \u043d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u043c \u0446\u0435\u043d\u0442\u0440\u043e\u0439\u0434\u0430\u043c</li> <li>\u0444\u043e\u0440\u043c\u0430 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432</li> </ul> <p>\u041c\u0435\u0442\u043e\u0434\u044b \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f</p> <ul> <li><code>\u041c\u0435\u0442\u043e\u0434 \u043b\u043e\u043a\u0442\u044f</code> (\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0433\u0440\u0430\u0444\u0438\u043a\u0430 \u0441\u0443\u043c\u043c\u044b \u043a\u0432\u0430\u0434\u0440\u0430\u0442\u043e\u0432 \u043e\u0448\u0438\u0431\u043a\u0438)</li> <li>K-means++ : \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u0435 \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0446\u0435\u043d\u0442\u0440\u043e\u0439\u0434\u043b\u0432 \u0434\u043b\u044f \u0434\u043e\u0441\u0442\u0438\u0436\u0435\u043d\u0438\u044f \u0431\u043e\u043b\u0435\u0435 \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u044b\u0445 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432</li> <li><code>\u0421\u0438\u043b\u0443\u044d\u0442\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437</code>: \u043e\u0446\u0435\u043d\u043a\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043a\u043b\u0432\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0432\u043d\u0443\u0442\u0440\u0438\u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043d\u043e\u0433\u043e \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u044f \u0438 \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u044f \u0434\u043e \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0435\u0433\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0430</li> </ul> In\u00a0[8]: Copied! <pre>from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(style=\"whitegrid\")\n</pre> from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, normalize from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import pandas as pd import seaborn as sns import matplotlib.pyplot as plt  sns.set(style=\"whitegrid\") <p>\u0414\u043b\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e 3 \u0444\u0438\u0447\u0430\u043c\u0438</p> In\u00a0[5]: Copied! <pre># \u0437\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435\n# \u0432 \u043f\u0440\u043e\u0441\u0442\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u0432\u043e\u0437\u044c\u043c\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0440\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0438\n# \u043c\u043e\u0436\u0435\u0442\u0435 \u043f\u0440\u043e\u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441 \u0431\u041e\u043b\u044c\u0448\u0438\u043c \u043d\u0430\u0431\u043e\u0440\u043e\u043c \u0434\u0430\u043d\u043d\u044b\u0445\ndf = pd.read_csv('housing.csv',\n                 usecols = ['longitude', 'latitude', 'median_house_value'])\nprint(df.shape)\ndf.head()\n</pre> # \u0437\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 # \u0432 \u043f\u0440\u043e\u0441\u0442\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u0432\u043e\u0437\u044c\u043c\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0440\u0438 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 # \u043c\u043e\u0436\u0435\u0442\u0435 \u043f\u0440\u043e\u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441 \u0431\u041e\u043b\u044c\u0448\u0438\u043c \u043d\u0430\u0431\u043e\u0440\u043e\u043c \u0434\u0430\u043d\u043d\u044b\u0445 df = pd.read_csv('housing.csv',                  usecols = ['longitude', 'latitude', 'median_house_value']) print(df.shape) df.head() <pre>(20640, 3)\n</pre> Out[5]: longitude latitude median_house_value 0 -122.23 37.88 452600.0 1 -122.22 37.86 358500.0 2 -122.24 37.85 352100.0 3 -122.25 37.85 341300.0 4 -122.25 37.85 342200.0 <p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0438\u0441\u0442\u0438\u043d\u043d\u043e\u0435 \u0440\u0430\u0441\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0438 \u0434\u043e\u043c\u043e\u0432 (\u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043c\u0435\u0442\u043a\u0430)</p> In\u00a0[6]: Copied! <pre>sns.scatterplot(data = df, \n                x = 'longitude', \n                y = 'latitude',\n                hue = 'median_house_value',s=8,\n                linewidth=0.6,\n               edgecolor='k');\n</pre> sns.scatterplot(data = df,                  x = 'longitude',                  y = 'latitude',                 hue = 'median_house_value',s=8,                 linewidth=0.6,                edgecolor='k'); <p>\u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438</p> In\u00a0[7]: Copied! <pre># \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0443\u044e \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443\nX_train, X_test, y_train, y_test = train_test_split(df[['latitude', 'longitude']],\n                                                    df[['median_house_value']],\n                                                    test_size=0.33,\n                                                    random_state=42)\nprint(X_train.shape, X_test.shape)\n</pre> # \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0443\u044e \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443 X_train, X_test, y_train, y_test = train_test_split(df[['latitude', 'longitude']],                                                     df[['median_house_value']],                                                     test_size=0.33,                                                     random_state=42) print(X_train.shape, X_test.shape) <pre>(13828, 2) (6812, 2)\n</pre> <p>\u0421\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435</p> In\u00a0[8]: Copied! <pre># \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435\nX_train_norm = normalize(X_train)\nX_test_norm = normalize(X_test)\n</pre> # \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 X_train_norm = normalize(X_train) X_test_norm = normalize(X_test) <p>\u041d\u0430 \u0433\u043b\u0430\u0437 \u043f\u043e\u0441\u0442\u0430\u0440\u0430\u0435\u043c\u0441\u044f \u0438\u043d\u0442\u0443\u0438\u0442\u0438\u0432\u043d\u043e \u0432\u044b\u0431\u0440\u0430\u0442\u044c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445</p> In\u00a0[9]: Copied! <pre># \u0437\u0430\u043f\u0443\u0441\u0442\u0438\u043c \u043a\u043b\u0430\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044e \u043d\u0430 \u0444\u0438\u043a\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u043f\u0430\u0440\u043c\u0435\u0442\u0440\u0430\u0445\nkmeans = KMeans(n_clusters = 3,\n                random_state = 42,\n                max_iter=300,\n                n_init='auto')\nkmeans.fit(X_train_norm)\n</pre> # \u0437\u0430\u043f\u0443\u0441\u0442\u0438\u043c \u043a\u043b\u0430\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044e \u043d\u0430 \u0444\u0438\u043a\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u043f\u0430\u0440\u043c\u0435\u0442\u0440\u0430\u0445 kmeans = KMeans(n_clusters = 3,                 random_state = 42,                 max_iter=300,                 n_init='auto') kmeans.fit(X_train_norm) Out[9]: <pre>KMeans(n_clusters=3, n_init='auto', random_state=42)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeans<pre>KMeans(n_clusters=3, n_init='auto', random_state=42)</pre> <p>\u0412\u044b\u0432\u0435\u0434\u0435\u043c \u0434\u0432\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043e\u0446\u0435\u043d\u043a\u0438 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0430</p> In\u00a0[10]: Copied! <pre># \u0441\u0440\u0435\u0434\u043d\u0435\u0435\u043a\u0432\u0430\u0434\u0440\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043e\u0448\u0438\u0431\u043a\u0430 \u0432\u043d\u0443\u0442\u0440\u0438 \u043e\u0431\u043b\u0430\u043a\u043e\nprint('SSE: ', kmeans.inertia_) \n\n# \u043e\u0446\u0435\u043d\u0438\u043a\u0430\u0435\u0442 \u043d\u0430 \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0431\u043b\u0438\u0437\u043a\u043e \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u043a \u043d\u0443\u0436\u043d\u0435\u043c\u0443 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0443 \u0438 \u043a\u0430\u043a \u0434\u0430\u043b\u0435\u043a\u043e \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u043e\u0442 \u0434\u0440\u0443\u0433\u0438\u0445 \u0446\u0435\u043d\u0442\u0440\u043e\u0432\nprint('silhouette', silhouette_score(X_train_norm, kmeans.labels_, metric='euclidean'))\n</pre> # \u0441\u0440\u0435\u0434\u043d\u0435\u0435\u043a\u0432\u0430\u0434\u0440\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043e\u0448\u0438\u0431\u043a\u0430 \u0432\u043d\u0443\u0442\u0440\u0438 \u043e\u0431\u043b\u0430\u043a\u043e print('SSE: ', kmeans.inertia_)   # \u043e\u0446\u0435\u043d\u0438\u043a\u0430\u0435\u0442 \u043d\u0430 \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0431\u043b\u0438\u0437\u043a\u043e \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u043a \u043d\u0443\u0436\u043d\u0435\u043c\u0443 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0443 \u0438 \u043a\u0430\u043a \u0434\u0430\u043b\u0435\u043a\u043e \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u043e\u0442 \u0434\u0440\u0443\u0433\u0438\u0445 \u0446\u0435\u043d\u0442\u0440\u043e\u0432 print('silhouette', silhouette_score(X_train_norm, kmeans.labels_, metric='euclidean')) <pre>SSE:  0.16279422179147127\nsilhouette 0.7498110566149692\n</pre> In\u00a0[11]: Copied! <pre># \u043d\u0430 \u043a\u0430\u043a\u043e\u0439 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438 \u0441\u043e\u0448\u0435\u043b\u0441\u044f \u0430\u043b\u043e\u0433\u0440\u0438\u0442\u043c?\nkmeans.n_iter_\n</pre> # \u043d\u0430 \u043a\u0430\u043a\u043e\u0439 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438 \u0441\u043e\u0448\u0435\u043b\u0441\u044f \u0430\u043b\u043e\u0433\u0440\u0438\u0442\u043c? kmeans.n_iter_ Out[11]: <pre>5</pre> In\u00a0[9]: Copied! <pre>kmeans.labels_[:10]\n</pre> kmeans.labels_[:10] Out[9]: <pre>array([1, 0, 1, 0, 0, 0, 0, 1, 2, 1], dtype=int32)</pre> <p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438</p> In\u00a0[12]: Copied! <pre>sns.scatterplot(data = X_train, \n                x = 'longitude', \n                y = 'latitude',\n                hue = kmeans.labels_,\n                s=8,linewidth=0.6,\n                edgecolor='k');\n</pre> sns.scatterplot(data = X_train,                  x = 'longitude',                  y = 'latitude',                 hue = kmeans.labels_,                 s=8,linewidth=0.6,                 edgecolor='k'); In\u00a0[13]: Copied! <pre># \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c SSE \u043d\u0430 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430\u0445 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432\nkmeans_kwargs = {\n     \"init\": \"random\",\n     \"n_init\": 10,\n     \"max_iter\": 300,\n     \"random_state\": 42,\n }\n\n# A list holds the SSE values for each k\nsse = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n    kmeans.fit(X_train_norm)\n    sse.append(kmeans.inertia_)\n\n# \u043c\u0435\u0442\u043e\u0434 \u043b\u043e\u043a\u0442\u044f\nplt.bar(range(1, 11), sse)\nplt.xticks(range(1, 11))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"SSE\")\nplt.show()\n</pre> # \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c SSE \u043d\u0430 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430\u0445 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 kmeans_kwargs = {      \"init\": \"random\",      \"n_init\": 10,      \"max_iter\": 300,      \"random_state\": 42,  }  # A list holds the SSE values for each k sse = [] for k in range(1, 11):     kmeans = KMeans(n_clusters=k, **kmeans_kwargs)     kmeans.fit(X_train_norm)     sse.append(kmeans.inertia_)  # \u043c\u0435\u0442\u043e\u0434 \u043b\u043e\u043a\u0442\u044f plt.bar(range(1, 11), sse) plt.xticks(range(1, 11)) plt.xlabel(\"Number of Clusters\") plt.ylabel(\"SSE\") plt.show() In\u00a0[14]: Copied! <pre>kmeans = KMeans(n_clusters=2, **kmeans_kwargs)\nkmeans.fit(X_train_norm)\n\nsns.scatterplot(data = X_train, \n                x = 'longitude', \n                y = 'latitude',\n                hue = kmeans.labels_,\n                s=8,linewidth=0.6,\n                edgecolor='k');\n</pre> kmeans = KMeans(n_clusters=2, **kmeans_kwargs) kmeans.fit(X_train_norm)  sns.scatterplot(data = X_train,                  x = 'longitude',                  y = 'latitude',                 hue = kmeans.labels_,                 s=8,linewidth=0.6,                 edgecolor='k'); In\u00a0[16]: Copied! <pre>!pip install kneed\n</pre> !pip install kneed <pre> System commands are not supported in Juno (yet)\n</pre> <p>\u0410 \u0435\u0441\u043b\u0438 \u043d\u0430 \u0433\u043b\u0430\u0437 \u043d\u0435 \u043d\u0440\u0430\u0432\u0438\u0442\u0441\u044f?</p> In\u00a0[1]: Copied! <pre># \u0435\u0441\u043b\u0438 \u043d\u0435 \u043d\u0430 \u0433\u043b\u0430\u0437\n\nfrom kneed import KneeLocator\n\nkl = KneeLocator(\n     range(1, 11), \n    sse, \n    curve=\"convex\", \n    direction=\"decreasing\"\n)\n\nkl.elbow # \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u043d\u044b\u0439 \n</pre> # \u0435\u0441\u043b\u0438 \u043d\u0435 \u043d\u0430 \u0433\u043b\u0430\u0437  from kneed import KneeLocator  kl = KneeLocator(      range(1, 11),      sse,      curve=\"convex\",      direction=\"decreasing\" )  kl.elbow # \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u043d\u044b\u0439  Out[1]: <pre>2</pre> In\u00a0[16]: Copied! <pre># A list holds the silhouette coefficients for each k\nsilhouette_coefficients = []\n\n# Notice you start at 2 clusters for silhouette coefficient\nfor k in range(2, 11):\n    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n    kmeans.fit(X_train_norm)\n    score = silhouette_score(X_train_norm, kmeans.labels_)\n    silhouette_coefficients.append(score)\n</pre> # A list holds the silhouette coefficients for each k silhouette_coefficients = []  # Notice you start at 2 clusters for silhouette coefficient for k in range(2, 11):     kmeans = KMeans(n_clusters=k, **kmeans_kwargs)     kmeans.fit(X_train_norm)     score = silhouette_score(X_train_norm, kmeans.labels_)     silhouette_coefficients.append(score) In\u00a0[17]: Copied! <pre>plt.bar(range(2, 11), silhouette_coefficients)\nplt.xticks(range(2, 11))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Silhouette Coefficient\")\nplt.show()\n</pre> plt.bar(range(2, 11), silhouette_coefficients) plt.xticks(range(2, 11)) plt.xlabel(\"Number of Clusters\") plt.ylabel(\"Silhouette Coefficient\") plt.show() <ul> <li>\u0420\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442\u0441\u044f \u0431\u0440\u0430\u0442\u044c \u0442\u043e \u0447\u0438\u0441\u043b\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432, \u043f\u043e\u0441\u043b\u0435 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u0443\u0435\u0442 \u043d\u0435\u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u043e\u0441\u0442\u044c.</li> <li>\u041e\u0434\u043d\u0430\u043a\u043e, \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u044d\u043a\u0441\u043f\u0435\u0440\u0442\u043d\u044b\u0435 \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0442 \u043e\u0442 \u044d\u0442\u043e\u0433\u043e \u043f\u0440\u0430\u0432\u0438\u043b\u0430 \u043e\u0442\u043e\u0439\u0442\u0438</li> </ul> In\u00a0[18]: Copied! <pre>kmeans = KMeans(n_clusters=5, **kmeans_kwargs)\nkmeans.fit(X_train_norm)\n\nsns.scatterplot(data = X_train, \n                x = 'longitude', \n                y = 'latitude',\n                hue = kmeans.labels_,\n                s=8,linewidth=0.6,\n                edgecolor='k');\n</pre> kmeans = KMeans(n_clusters=5, **kmeans_kwargs) kmeans.fit(X_train_norm)  sns.scatterplot(data = X_train,                  x = 'longitude',                  y = 'latitude',                 hue = kmeans.labels_,                 s=8,linewidth=0.6,                 edgecolor='k'); <p>\u041a\u0440\u0430\u0441\u0438\u0432\u043e. \u041d\u043e \u0441\u043c\u044b\u0441\u043b \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f \u043d\u0435 \u043e\u0441\u043e\u0431\u043e \u044f\u0441\u0435\u043d.</p> <ul> <li><p><code>eps</code> \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0435 \u043c\u0435\u0436\u0434\u0443 \u0434\u0432\u0443\u043c\u044f \u0442\u043e\u0447\u043a\u0430\u043c\u0438, \u0447\u0442\u043e\u0431\u044b \u043e\u043d\u0438 \u0441\u0447\u0438\u0442\u0430\u043b\u0438\u0441\u044c \u0441\u043e\u0441\u0435\u0434\u044f\u043c\u0438. \u0415\u0441\u043b\u0438 <code>eps</code> \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u043c\u0430\u043b, \u043c\u043d\u043e\u0433\u0438\u0435 \u0442\u043e\u0447\u043a\u0438 \u043e\u0441\u0442\u0430\u043d\u0443\u0442\u0441\u044f \u043d\u0435\u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u043c\u0438; \u0435\u0441\u043b\u0438 \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0432\u0435\u043b\u0438\u043a, \u0432\u0441\u0435 \u0442\u043e\u0447\u043a\u0438 \u043e\u043a\u0430\u0436\u0443\u0442\u0441\u044f \u0432 \u043e\u0434\u043d\u043e\u043c \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0435.</p> </li> <li><p><code>min_samples</code> \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442 \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u043e\u0447\u0435\u043a, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0445 \u0434\u043b\u044f \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u043b\u043e\u0442\u043d\u043e\u0433\u043e \u0440\u0435\u0433\u0438\u043e\u043d\u0430 (\u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0430). \u0415\u0441\u043b\u0438 <code>min_samples</code> \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u043c\u0430\u043b, \u043c\u043d\u043e\u0433\u0438\u0435 \u0432\u044b\u0431\u0440\u043e\u0441\u044b \u0431\u0443\u0434\u0443\u0442 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u043a\u0430\u043a \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u044b; \u0435\u0441\u043b\u0438 \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0432\u0435\u043b\u0438\u043a, \u043c\u043d\u043e\u0433\u0438\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u043d\u0435 \u0431\u0443\u0434\u0443\u0442 \u0441\u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u044b.</p> </li> </ul> In\u00a0[19]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\n</pre> import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.cluster import DBSCAN In\u00a0[29]: Copied! <pre>db = DBSCAN(eps=20, min_samples=4)\ndb.fit(X_train)\n</pre> db = DBSCAN(eps=20, min_samples=4) db.fit(X_train) Out[29]: <pre>DBSCAN(eps=20, min_samples=4)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DBSCAN<pre>DBSCAN(eps=20, min_samples=4)</pre> In\u00a0[30]: Copied! <pre>DB_df = X_train[['latitude', 'longitude']].copy()\nDB_df['Cluster'] = db.labels_\nDB_df['Cluster'].value_counts()\n</pre> DB_df = X_train[['latitude', 'longitude']].copy() DB_df['Cluster'] = db.labels_ DB_df['Cluster'].value_counts() Out[30]: <pre>0    13828\nName: Cluster, dtype: int64</pre> In\u00a0[22]: Copied! <pre>sns.scatterplot(data = DB_df, \n                x = 'longitude', \n                y = 'latitude',\n                hue = DB_df['Cluster'],\n                s=8,linewidth=0.6,\n                edgecolor='k');\n</pre> sns.scatterplot(data = DB_df,                  x = 'longitude',                  y = 'latitude',                 hue = DB_df['Cluster'],                 s=8,linewidth=0.6,                 edgecolor='k'); <p>\u043f\u0435\u0440\u0435\u0431\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043d\u0435 \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u043d\u044b\u0445 \u043f\u043e\u0434\u0445\u043e\u0434, \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044f \u043c\u0435\u0442\u043e\u0434\u043e\u043c \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0438\u0445 \u0441\u043e\u0441\u0435\u0434\u0435\u0439</p> <ul> <li>\u043f\u0435\u0440\u0435\u0431\u0435\u0440\u0435\u043c <code>eps</code> \u0432 \u043f\u0440\u0435\u043c\u0435\u0436\u0443\u0442\u043a\u0435 [0.01, 0.1, 0.2, 0.3, 0.4]</li> <li>\u0442\u0430\u043a \u0436\u0435 \u043f\u0435\u0440\u0435\u0431\u0435\u0440\u0435\u0441\u043c <code>min_samples</code> \u0432 \u043f\u0440\u0435\u043c\u0435\u0436\u0443\u0442\u043a\u0435 [3, 5, 7, 10, 20, 30, 40, 100]</li> <li>\u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043b\u0435\u0439\u0431\u0435\u043b\u043e\u0432</li> </ul> In\u00a0[32]: Copied! <pre>from sklearn.neighbors import NearestNeighbors\n</pre> from sklearn.neighbors import NearestNeighbors In\u00a0[36]: Copied! <pre>\"\"\"\n\n\u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u044f \u0434\u043e 5-\u0433\u043e \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0435\u0433\u043e \u0441\u043e\u0441\u0435\u0434\u0430\n\n\"\"\"\n\n# \u041f\u0440\u0438\u043c\u0435\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 (\u0437\u0430\u043c\u0435\u043d\u0438\u0442\u0435 \u043d\u0430 \u0432\u0430\u0448\u0438 \u0434\u0430\u043d\u043d\u044b\u0435)\nDB_df = X_train[['latitude', 'longitude']].copy()\nX = DB_df[['latitude', 'longitude']].values\n\n# \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0433\u0440\u0430\u0444\u0438\u043a\u0430 \"K \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0439\"\nnearest_neighbors = NearestNeighbors(n_neighbors=5)\nneighbors = nearest_neighbors.fit(X)\ndistances, indices = neighbors.kneighbors(X)\ndistances = np.sort(distances[:, 4], axis=0)  \n\nplt.figure(figsize=(10, 6))\nplt.plot(distances)\nplt.xlabel('Data Points')\nplt.ylabel('5th Nearest Neighbor Distance')\nplt.title('K-distance Graph for Determining Optimal eps')\nplt.show()\n\n\"\"\"\n\n\u041c\u0435\u0442\u043e\u0434\u043e\u043c \u043f\u0435\u0440\u0435\u0431\u043e\u0440\u043e\u043c\n\n\"\"\"\n\n# \u041f\u0440\u0438\u043c\u0435\u0440 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 eps \u0438 min_samples\nfor eps in [0.01, 0.1, 0.2, 0.3, 0.4]:\n    for min_samples in [3, 5, 7, 10, 20, 30, 40, 100]:\n        db = DBSCAN(eps=eps, min_samples=min_samples)\n        db.fit(X)\n        DB_df['Cluster'] = db.labels_\n        n_clusters = len(set(db.labels_)) - (1 if -1 in db.labels_ else 0)\n        print(f'eps: {eps}, min_samples: {min_samples}, Number of clusters: {n_clusters}')\n</pre> \"\"\"  \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u044f \u0434\u043e 5-\u0433\u043e \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0435\u0433\u043e \u0441\u043e\u0441\u0435\u0434\u0430  \"\"\"  # \u041f\u0440\u0438\u043c\u0435\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 (\u0437\u0430\u043c\u0435\u043d\u0438\u0442\u0435 \u043d\u0430 \u0432\u0430\u0448\u0438 \u0434\u0430\u043d\u043d\u044b\u0435) DB_df = X_train[['latitude', 'longitude']].copy() X = DB_df[['latitude', 'longitude']].values  # \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0433\u0440\u0430\u0444\u0438\u043a\u0430 \"K \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0439\" nearest_neighbors = NearestNeighbors(n_neighbors=5) neighbors = nearest_neighbors.fit(X) distances, indices = neighbors.kneighbors(X) distances = np.sort(distances[:, 4], axis=0)    plt.figure(figsize=(10, 6)) plt.plot(distances) plt.xlabel('Data Points') plt.ylabel('5th Nearest Neighbor Distance') plt.title('K-distance Graph for Determining Optimal eps') plt.show()  \"\"\"  \u041c\u0435\u0442\u043e\u0434\u043e\u043c \u043f\u0435\u0440\u0435\u0431\u043e\u0440\u043e\u043c  \"\"\"  # \u041f\u0440\u0438\u043c\u0435\u0440 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 eps \u0438 min_samples for eps in [0.01, 0.1, 0.2, 0.3, 0.4]:     for min_samples in [3, 5, 7, 10, 20, 30, 40, 100]:         db = DBSCAN(eps=eps, min_samples=min_samples)         db.fit(X)         DB_df['Cluster'] = db.labels_         n_clusters = len(set(db.labels_)) - (1 if -1 in db.labels_ else 0)         print(f'eps: {eps}, min_samples: {min_samples}, Number of clusters: {n_clusters}') <pre>eps: 0.01, min_samples: 3, Number of clusters: 1321\neps: 0.01, min_samples: 5, Number of clusters: 719\neps: 0.01, min_samples: 7, Number of clusters: 362\neps: 0.01, min_samples: 10, Number of clusters: 135\neps: 0.01, min_samples: 20, Number of clusters: 2\neps: 0.01, min_samples: 30, Number of clusters: 0\neps: 0.01, min_samples: 40, Number of clusters: 0\neps: 0.01, min_samples: 100, Number of clusters: 0\neps: 0.1, min_samples: 3, Number of clusters: 76\neps: 0.1, min_samples: 5, Number of clusters: 59\neps: 0.1, min_samples: 7, Number of clusters: 48\neps: 0.1, min_samples: 10, Number of clusters: 43\neps: 0.1, min_samples: 20, Number of clusters: 36\neps: 0.1, min_samples: 30, Number of clusters: 31\neps: 0.1, min_samples: 40, Number of clusters: 23\neps: 0.1, min_samples: 100, Number of clusters: 10\neps: 0.2, min_samples: 3, Number of clusters: 24\neps: 0.2, min_samples: 5, Number of clusters: 20\neps: 0.2, min_samples: 7, Number of clusters: 14\neps: 0.2, min_samples: 10, Number of clusters: 16\neps: 0.2, min_samples: 20, Number of clusters: 11\neps: 0.2, min_samples: 30, Number of clusters: 12\neps: 0.2, min_samples: 40, Number of clusters: 11\neps: 0.2, min_samples: 100, Number of clusters: 9\neps: 0.3, min_samples: 3, Number of clusters: 7\neps: 0.3, min_samples: 5, Number of clusters: 7\neps: 0.3, min_samples: 7, Number of clusters: 4\neps: 0.3, min_samples: 10, Number of clusters: 8\neps: 0.3, min_samples: 20, Number of clusters: 5\neps: 0.3, min_samples: 30, Number of clusters: 5\neps: 0.3, min_samples: 40, Number of clusters: 6\neps: 0.3, min_samples: 100, Number of clusters: 5\neps: 0.4, min_samples: 3, Number of clusters: 6\neps: 0.4, min_samples: 5, Number of clusters: 5\neps: 0.4, min_samples: 7, Number of clusters: 4\neps: 0.4, min_samples: 10, Number of clusters: 2\neps: 0.4, min_samples: 20, Number of clusters: 3\neps: 0.4, min_samples: 30, Number of clusters: 4\neps: 0.4, min_samples: 40, Number of clusters: 5\neps: 0.4, min_samples: 100, Number of clusters: 3\n</pre> In\u00a0[37]: Copied! <pre>db = DBSCAN(eps=0.18, min_samples=100)\ndb.fit(X_train)\n</pre> db = DBSCAN(eps=0.18, min_samples=100) db.fit(X_train) Out[37]: <pre>DBSCAN(eps=0.18, min_samples=100)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DBSCAN<pre>DBSCAN(eps=0.18, min_samples=100)</pre> In\u00a0[38]: Copied! <pre>DB_df = X_train[['latitude', 'longitude']].copy()\nDB_df['Cluster'] = db.labels_\n\nDB_df['Cluster'].value_counts()\n</pre> DB_df = X_train[['latitude', 'longitude']].copy() DB_df['Cluster'] = db.labels_  DB_df['Cluster'].value_counts() Out[38]: <pre> 0    5819\n 1    3048\n-1    2422\n 2    1013\n 4     552\n 6     491\n 3     322\n 5     161\nName: Cluster, dtype: int64</pre> In\u00a0[40]: Copied! <pre>sns.scatterplot(data = DB_df, \n                x = 'longitude', \n                y = 'latitude',\n                hue = DB_df['Cluster'],\n                s=8,linewidth=0.6,\n                edgecolor='k');\n</pre> sns.scatterplot(data = DB_df,                  x = 'longitude',                  y = 'latitude',                 hue = DB_df['Cluster'],                 s=8,linewidth=0.6,                 edgecolor='k'); <p>\u0412 \u0442\u0430\u043a\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u0443\u0436\u0435 \u043c\u043e\u0436\u043d\u043e \u043e\u0446\u0435\u043d\u0438\u0442\u044c \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u0441 \u0442\u043e\u0447\u043a\u0438 \u0437\u0440\u0435\u043d\u0438\u044f \u043b\u043e\u0433\u0438\u043a\u0438.</p> <p>\u041b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f</p> <ul> <li>\u041c\u0435\u0442\u043e\u0434 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u043c\u044b\u0439 \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0431\u0438\u043d\u0430\u0440\u043d\u043e\u0433\u043e \u0438\u0441\u0445\u043e\u0434\u0430 (\u043a\u043b\u0430\u0441\u0441\u0430) \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043e\u0434\u043d\u043e\u0439 \u0438\u043b\u0438 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u044b\u0445 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445</li> <li>\u041b\u043e\u0433 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u043f\u043e\u043c\u043e\u0433\u0430\u0435\u0442 \u043d\u0430\u043c \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043d\u0430 \u0434\u0432\u0435 \u0433\u0440\u0443\u043f\u043f\u044b (\u0411\u0438\u043d\u0430\u0440\u043d\u044b\u0439 \u0438\u0441\u0445\u043e\u0434)</li> <li>\u041b\u043e\u0433 \u0440\u0435\u0433 \u043f\u043e\u043c\u043e\u0433\u0430\u0435\u0442 \u043d\u0430\u043c \u043f\u043e\u043d\u044f\u0442\u044c \u0432\u0435\u0440\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u0434\u0432\u0443\u043c \u043a\u043b\u0430\u0441\u0441\u043e\u043c</li> <li>\u0418\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0431\u0443\u0434\u0435\u0442 \u043f\u043e\u043c\u043e\u0433\u0430\u0442\u044c \u0431\u0438\u0437\u043d\u0435\u0441\u0443 \u043f\u043e\u043d\u044f\u0442\u044c \u0441 \u043a\u0435\u043c \u043d\u0443\u0436\u043d\u043e \u0432 \u043f\u0435\u0440\u0432\u0443\u044e \u043e\u0447\u0435\u0440\u0435\u0434\u044c \u043f\u043e\u043e\u0431\u0449\u0430\u0442\u044c\u0441\u044f</li> </ul> In\u00a0[3]: Copied! <pre>from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, roc_auc_score, roc_curve\nimport pandas as pd\n</pre> from sklearn.linear_model import LogisticRegression from sklearn.metrics import classification_report, roc_auc_score, roc_curve import pandas as pd In\u00a0[9]: Copied! <pre>df = pd.read_csv(\"diabetes.csv\")\nprint(df.shape)\ndf.head()\n</pre> df = pd.read_csv(\"diabetes.csv\") print(df.shape) df.head() <pre>(768, 9)\n</pre> Out[9]: Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome 0 6 148 72 35 0 33.6 0.627 50 1 1 1 85 66 29 0 26.6 0.351 31 0 2 8 183 64 0 0 23.3 0.672 32 1 3 1 89 66 23 94 28.1 0.167 21 0 4 0 137 40 35 168 43.1 2.288 33 1 <p>\u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u0439</p> <ul> <li>Pregnancies : Number of times pregnant</li> <li>Glucose : Plasma glucose concentration a 2 hours in an oral glucose tolerance test</li> <li>BloodPressure : Diastolic blood pressure (mm Hg)</li> <li>SkinThickness : Triceps skin fold thickness (mm)</li> <li>Insulin : 2-Hour serum insulin (mu U/ml)</li> <li>BMI : Body mass index (weight in kg/(height in m)^2)</li> <li>DiabetesPedigreeFunction : Diabetes pedigree function</li> <li>Age : Age (years)</li> <li>Outcome : Class variable (0 or 1)</li> </ul> In\u00a0[10]: Copied! <pre>X = df.drop(columns=['Outcome'])\ny = df['Outcome']\n</pre> X = df.drop(columns=['Outcome']) y = df['Outcome'] In\u00a0[11]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\nprint(X_train.shape, X_test.shape)\n</pre> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) print(X_train.shape, X_test.shape) <pre>(576, 8) (192, 8)\n</pre> In\u00a0[12]: Copied! <pre>y.mean()\n</pre> y.mean() Out[12]: <pre>0.3489583333333333</pre> In\u00a0[13]: Copied! <pre>logreg = LogisticRegression(random_state=42)\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\n</pre> logreg = LogisticRegression(random_state=42) logreg.fit(X_train, y_train) y_pred = logreg.predict(X_test) In\u00a0[14]: Copied! <pre>target_names = ['without diabetes', 'with diabetes']\nprint(classification_report(y_test, y_pred, target_names=target_names))\n</pre> target_names = ['without diabetes', 'with diabetes'] print(classification_report(y_test, y_pred, target_names=target_names)) <pre>                  precision    recall  f1-score   support\n\nwithout diabetes       0.80      0.77      0.79       123\n   with diabetes       0.62      0.65      0.63        69\n\n        accuracy                           0.73       192\n       macro avg       0.71      0.71      0.71       192\n    weighted avg       0.73      0.73      0.73       192\n\n</pre> In\u00a0[16]: Copied! <pre>y_pred_proba = logreg.predict_proba(X_test)[:, 1]\ny_pred_proba[:10]\n</pre> y_pred_proba = logreg.predict_proba(X_test)[:, 1] y_pred_proba[:10] Out[16]: <pre>array([0.27138722, 0.18792281, 0.10917411, 0.15183889, 0.47795318,\n       0.44452342, 0.0140959 , 0.62424835, 0.56286436, 0.78383318])</pre> <p>\u041e\u0446\u0435\u043d\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438</p> <p>\u041e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u043c \u0431\u0438\u043d\u0430\u0440\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u0443 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0435\u0441\u0442\u044c \u0434\u0438\u0441\u0431\u0430\u043b\u0430\u043d\u0441 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u043d\u0430 ROC-AUC \u043a\u0440\u0438\u0432\u043e\u0439</p> <ul> <li>\u0422\u043e\u043b\u044c\u043a\u043e \u0435\u0441\u043b\u0438 \u043c\u043e\u0434\u0435\u043b\u044c \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u0445\u043e\u0440\u043e\u0448\u0438\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442, \u043c\u043e\u0436\u0435\u043c \u043f\u0440\u0438\u043c\u0442\u0443\u043f\u0430\u0442\u044c \u043a \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u0438</li> </ul> In\u00a0[20]: Copied! <pre>fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\nauc = roc_auc_score(y_test, y_pred_proba)\n\n# \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0433\u0440\u0430\u0444\u0438\u043a\u0430\nplt.figure(figsize=(5, 5))\nplt.plot(fpr, tpr, color='blue', lw=2, label=f\"ROC curve (AUC = {auc:.2f})\")\nplt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=10)\nplt.ylabel('True Positive Rate', fontsize=10)\nplt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=10)\nplt.legend(loc=\"lower right\", fontsize=10)\nplt.grid(True)\nplt.show()\n</pre> fpr, tpr, _ = roc_curve(y_test, y_pred_proba) auc = roc_auc_score(y_test, y_pred_proba)  # \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0433\u0440\u0430\u0444\u0438\u043a\u0430 plt.figure(figsize=(5, 5)) plt.plot(fpr, tpr, color='blue', lw=2, label=f\"ROC curve (AUC = {auc:.2f})\") plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')  plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel('False Positive Rate', fontsize=10) plt.ylabel('True Positive Rate', fontsize=10) plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=10) plt.legend(loc=\"lower right\", fontsize=10) plt.grid(True) plt.show() In\u00a0[56]: Copied! <pre>import shap\n</pre> import shap In\u00a0[57]: Copied! <pre>explainer = shap.Explainer(\n    logreg, X_train, feature_names=X_train.columns\n)\nshap_values = explainer(X_test)\n</pre> explainer = shap.Explainer(     logreg, X_train, feature_names=X_train.columns ) shap_values = explainer(X_test) <p>\u041d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u044b\u0432\u043e\u0434\u044b:</p> <ul> <li>\u0427\u0435\u043c \u0432\u044b\u0448\u0435 BMI, \u0442\u0435\u043c \u0432\u044b\u0448\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043d\u0430\u043b\u0438\u0447\u0438\u044f \u0434\u0438\u0430\u0431\u0435\u0442\u0430</li> <li>\u0427\u0435\u043c \u0432\u044b\u0448\u0435 \u0432\u043e\u0437\u0440\u0430\u0441\u0442, \u0442\u0435\u043c \u0431\u043e\u043b\u0435\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e \u0447\u0442\u043e \u0443 \u043f\u0430\u0446\u0438\u0435\u043d\u0442\u0430 \u0434\u0438\u0430\u0431\u0435\u0442</li> </ul> In\u00a0[\u00a0]: Copied! <pre>shap.plots.beeswarm(shap_values)\n</pre> shap.plots.beeswarm(shap_values) <p>\u0411\u0438\u043d\u0430\u0440\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u0434\u0430\u0435\u0442 \u0431\u0438\u0437\u043d\u0435\u0441\u0441\u0443 \u043f\u043e\u043d\u044f\u0442\u044c \u0441 \u043a\u0435\u043c \u043d\u0443\u0436\u043d\u043e \u043e\u0431\u0449\u0430\u0442\u044c\u0441\u044f</p>"},{"location":"portfolio/course_recsys/User_Segmentation.html","title":"\u0421\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439\u00b6","text":""},{"location":"portfolio/course_recsys/User_Segmentation.html#1-background","title":"1 | Background\u00b6","text":"<p>\u0412 \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0447\u0430\u0441\u0442\u0438 \u0432\u0435\u0431\u0438\u043d\u0430\u0440\u0430 \u043d\u0430 \u043f\u0440\u043e\u0441\u0442\u044b\u0445 \u043f\u0440\u0438\u043c\u0435\u0440\u0430\u0445 \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0435\u043d\u043d\u044b\u0435 \u0442\u0435\u043e\u0440\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0430\u0441\u043f\u0435\u043a\u0442\u044b:</p> <ol> <li><code>RFM-\u0430\u043d\u0430\u043b\u0438\u0437</code> \u043d\u0430 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445</li> <li><code>\u041a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f</code> K-means</li> <li><code>\u041a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f</code> DBSCAN</li> <li><code>\u041b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f</code> \u0434\u043b\u044f \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438</li> </ol>"},{"location":"portfolio/course_recsys/User_Segmentation.html#2-rfm","title":"2 | RFM \u0410\u043d\u0430\u043b\u0438\u0437\u00b6","text":""},{"location":"portfolio/course_recsys/User_Segmentation.html#21","title":"2.1. \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<p>\u0414\u0430\u0442\u0430\u0441\u0435\u0442 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0434\u0430\u043d\u043d\u044b\u0435 \u043e \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u044f\u0445 \u043d\u0430 \u0443\u0440\u043e\u0432\u043d\u0435 \u043a\u043b\u0438\u0435\u043d\u0442\u0430 \u043f\u043e \u0434\u0430\u0442\u0435.</p>"},{"location":"portfolio/course_recsys/User_Segmentation.html","title":"\u0414\u0430\u0432\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0439 \u043f\u043e\u043a\u0443\u043f\u043a\u0438\u00b6","text":"<ul> <li>RFM-\u0430\u043d\u0430\u043b\u0438\u0437 \u0442\u0440\u0435\u0431\u0443\u0435\u0442 \u043f\u0440\u043e\u0432\u0435\u0441\u0442\u0438 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u043e\u043b \u0441\u0443\u043c\u043c\u0435, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0443 \u0438 \"\u0434\u0430\u0432\u043d\u043e\u0441\u0442\u0438\" \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0439.</li> <li>\u0421\u0443\u043c\u043c\u0430 \u0443 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043f\u043e\u0437\u0436\u0435 \u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u043e\u0439. \u041d\u0435 \u0445\u0432\u0430\u0442\u0430\u0435\u0442 \"\u0434\u0430\u0432\u043d\u043e\u0441\u0442\u0438\":</li> <li>\u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c 2015-04-01 \u043a\u0430\u043a \u0434\u0430\u0442\u0443 \u043e\u0442 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u043d\u0430\u0434\u043e \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0434\u043d\u0435\u0439 \u043e\u0442 \u043a\u0430\u0436\u0434\u043e\u0439 \u043f\u043e\u043a\u0443\u043f\u043a\u0438</li> </ul>"},{"location":"portfolio/course_recsys/User_Segmentation.html#2-k-means","title":"2 | \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f k-means\u00b6","text":"<p>\u041c\u0435\u0442\u043e\u0434\u044b \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0434\u043e\u044e\u0442 \u043d\u0430\u043c \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u0440\u0430\u0437\u043d\u044b\u0435 \u043f\u043e\u0434\u0433\u0440\u0443\u043f\u043f\u044b</p>"},{"location":"portfolio/course_recsys/User_Segmentation.html#k-means","title":"K-means\u00b6","text":""},{"location":"portfolio/course_recsys/User_Segmentation.html","title":"\u041a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u0421\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0438 \u0414\u043e\u043c\u043e\u0432\u00b6","text":"<ul> <li>\u0414\u0430\u043d\u043d\u044b\u0435 \u043e\u0442\u043d\u043e\u0441\u044f\u0442\u0441\u044f \u043a \u0434\u043e\u043c\u0430\u043c, \u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u043d\u044b\u043c \u0432 \u0448\u0442\u0430\u0442\u0435 \u041a\u0430\u043b\u0438\u0444\u043e\u0440\u043d\u0438\u044f.</li> <li>\u042d\u0442\u043e \u0441\u0432\u043e\u0434\u043d\u0430\u044f \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0430 \u043e \u043d\u0435\u0434\u0432\u0438\u0436\u0438\u043c\u043e\u0441\u0442\u0438, \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u0430\u044f \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u0435\u0440\u0435\u043f\u0438\u0441\u0438 1990 \u0433\u043e\u0434\u0430.</li> <li>\u041f\u043e\u043f\u044b\u0442\u0430\u0435\u043c\u0441\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u044d\u0442\u0438 \u0434\u0430\u043d\u043d\u044b\u0435.</li> </ul>"},{"location":"portfolio/course_recsys/User_Segmentation.html#sse","title":"\u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u043f\u043e SSE\u00b6","text":"<p>\u041f\u043e\u0441\u0442\u0430\u0440\u0430\u0435\u043c\u0441\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0433\u0438\u043f\u0435\u0440\u043f\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 inertia</p>"},{"location":"portfolio/course_recsys/User_Segmentation.html#silhouette","title":"\u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0438 Silhouette\u00b6","text":"<ul> <li>\u0422\u0435\u043f\u0435\u0440\u044c \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0441 \u0442\u043e\u0447\u043a\u0438 \u0437\u0440\u0435\u043d\u0438\u044f silhouette</li> <li>\u041f\u0435\u0440\u0435\u0431\u0435\u0440\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432, \u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c silhouette</li> </ul>"},{"location":"portfolio/course_recsys/User_Segmentation.html#3-dbscan","title":"3 | \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f DBSCAN\u00b6","text":"<p>\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435</p> <ul> <li>\u043f\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u043d\u043e\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0439 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0441 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0432\u0438\u0435\u043c \u0448\u0443\u043c\u0430</li> <li>\u0443\u043c\u0435\u0435\u0442 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0430\u043c\u0438 \u0441\u043b\u043e\u0436\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u044b</li> <li>\u0443\u043c\u0435\u0435\u0442 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u044c \u0448\u0443\u043c\u044b</li> </ul> <p>\u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c</p> <ul> <li>\u0437\u0430\u0434\u0430\u0447\u0430; \u0440\u0430\u0437\u0431\u0438\u0442\u044c \u0442\u043e\u043b\u043f\u0443 \u043d\u0430 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u044b</li> <li>\u0432\u0441\u0435 \u0443 \u043a\u043e\u0433\u043e \u0435\u0441\u0442\u044c \u0445\u043e\u0442\u044f \u0431\u044b N \u0441\u043e\u0441\u0435\u0434\u0430 \u043d\u0430 \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0438 M \u043c\u0435\u0442\u0440\u0430 \u0431\u0435\u0440\u0443\u0442 \u0432 \u043a\u0443\u043a\u0438 \u0437\u0435\u043b\u0435\u043d\u044b\u0435 \u0444\u043b\u0430\u0436\u043a\u0438</li> <li>\u0435\u0441\u043b\u0438 \u043c\u0435\u043d\u044c\u0448\u0435 N \u0441\u043e\u0441\u0435\u0434\u0430? \u0415\u0441\u043b\u0438 \u0445\u043e\u0442\u044f \u0431\u044b \u043e\u0434\u0438\u043d \u0441\u043e\u0441\u0435\u0434 \u0434\u0435\u0440\u0436\u0438\u0442 \u0437\u0435\u043b\u0435\u043d\u044b\u0439 \u0444\u043b\u0430\u0433, \u0432\u0440\u0443\u0447\u0438\u043c \u0436\u0435\u043b\u0442\u044b\u0435 \u0444\u043b\u0430\u0436\u043a\u0438 (\u043f\u043e\u0433\u0440\u0430\u043d\u0438\u0447\u043d\u044b\u0439)</li> <li>\u043a\u0440\u0430\u0441\u043d\u044b\u0435 : \u043c\u0435\u043d\u044c\u0448\u0435 \u0442\u0440\u0435\u0445 \u0441\u043e\u0441\u0435\u0434\u0435\u0439 \u0438 \u043d\u0435\u0442 \u0441\u043e\u0441\u0435\u0434\u0435\u0439 \u0441 \u0437\u0435\u043b\u0435\u043d\u044b\u043c\u0438 \u0444\u043b\u0430\u0436\u043a\u0430\u043c\u0438 (outlier)</li> </ul> <p>\u043f\u043b\u044e\u0441\u044b</p> <ul> <li>\u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u043b\u044c\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u044b</li> <li>\u0440\u0430\u0431\u043e\u0442\u0430 \u0441 \u0448\u0443\u043c\u043e\u043c \u0438\u0432\u044b\u0431\u0440\u043e\u0441\u0430\u043c\u0438</li> <li>\u043d\u0435 \u0442\u0440\u0435\u0431\u0443\u0435\u0442 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u0437\u043d\u0430\u043d\u0438\u044f \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432</li> <li>\u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u0435\u043d \u0434\u043b\u044f \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u043d\u0430\u0431\u043e\u0440\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445 (\u043f\u043e \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044e \u0441 \u043a\u043c\u0435\u0430\u043d\u0441)</li> </ul> <p>\u043c\u0438\u043d\u0443\u0441\u044b</p> <ul> <li>\u0447\u0443\u0432\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u043a \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c (2 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430)</li> <li>\u0442\u0440\u0443\u0434\u043d\u043e\u0441\u0442\u0438 \u0441 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0435\u0439 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u043f\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u044c\u044e</li> <li>\u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0430 \u0441 \u043c\u0430\u0448\u0442\u0430\u0431\u0438\u0440\u0443\u0435\u043c\u043e\u0441\u0442\u044c\u044e \u0434\u043b\u044f \u043e\u0447\u0435\u043d\u044c \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445</li> <li>\u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438 \u0432\u044b\u0431\u043e\u0440\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432</li> </ul>"},{"location":"portfolio/course_recsys/User_Segmentation.html#4","title":"4 | \u041b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0420\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f\u00b6","text":""},{"location":"portfolio/course_recsys/User_Segmentation.html#41","title":"4.1. \u041d\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u0435\u00b6","text":"<p>\u0417\u0430\u0434\u0430\u0447\u0430 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u043f\u0430\u0446\u0438\u0435\u043d\u0442\u043e\u0432 \u043d\u0430 \u043d\u0430\u043b\u0438\u0447\u0438\u044f \u0434\u0438\u0430\u0431\u0435\u0442\u0430</p> <ul> <li>\u0414\u0430\u0442\u0430\u0441\u0435\u0442 \u0432\u0437\u044f\u0442 \u0438\u0437 \u041d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0438\u043d\u0441\u0442\u0438\u0442\u0443\u0442\u0430 \u0434\u0438\u0430\u0431\u0435\u0442 \u0438 \u0437\u0430\u0431\u043e\u043b\u0435\u0432\u0430\u043d\u0438\u0439 \u043e\u0440\u0433\u0430\u043d\u043e\u0432 \u043f\u0438\u0449\u0435\u0432\u0430\u0440\u0435\u043d\u0438\u044f \u0438 \u043f\u043e\u0447\u0435\u043a</li> <li>\u0414\u0430\u043d\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u044e\u0442\u0441\u044f \u0434\u043b\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u0437\u0430\u0434\u0430\u0447\u0438 \u0434\u0438\u0430\u0433\u043d\u043e\u0441\u0442\u0438\u043a\u0438 \u043d\u0430\u043b\u0438\u0447\u0438\u044f \u0443 \u043f\u0430\u0446\u0438\u0435\u043d\u0442\u0430 \u0434\u0438\u0430\u0431\u0435\u0442\u0430 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0438\u0437\u043c\u0435\u0440\u0435\u043d\u0438\u0439 (\u0434\u0430\u043d\u043d\u044b\u0435 \u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435)</li> </ul>"},{"location":"portfolio/course_recsys/User_Segmentation.html#42","title":"4.2. \u041f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438\u00b6","text":"<p>\u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u0434\u0432\u0435 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438</p>"},{"location":"portfolio/course_recsys/User_Segmentation.html#43","title":"4.3. \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<p>\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u043e\u0434\u043d\u043e\u0439 \u0438\u0437 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043e\u043a</p>"},{"location":"portfolio/course_recsys/User_Segmentation.html#44","title":"4.4. \u041e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u043c \u043e\u0431\u043e\u0431\u0449\u0430\u044e\u0449\u0438\u044e \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c\u00b6","text":"<p>\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u043d\u0430 \u043d\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445</p> <p>\u041d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043d\u0430 \u043e\u0442\u043b\u043e\u0436\u0435\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435, \u0438 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043e\u0446\u0435\u043d\u0443 \u043e\u0431\u043e\u0431\u0449\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043d\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445</p>"},{"location":"portfolio/course_recsys/User_Segmentation.html#45","title":"4.5. \u0418\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0438\u0440\u0443\u0435\u043c \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\u00b6","text":"<p>\u0418\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0438\u0440\u0443\u0435\u043c \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432</p> <p>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u043e\u0439 SHAP \u0434\u043b\u044f \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0438 \u0438\u0445 \u0432\u043b\u0438\u044f\u043d\u0438\u0435 \u0438 \u043a\u0430\u043a \u0432\u043e\u043e\u0431\u0449\u0435 \u0438\u0445 \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0438\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c</p>"},{"location":"portfolio/course_recsys/dssm-simple.html","title":"Dssm simple","text":"In\u00a0[1]: Copied! <pre>!pip install -q replay-rec rs_datasets --quiet\n</pre> !pip install -q replay-rec rs_datasets --quiet <pre>     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.8/196.8 kB 2.2 MB/s eta 0:00:0000:010:01\n  Preparing metadata (setup.py) ... done\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 235.9/235.9 kB 4.8 MB/s eta 0:00:0000:01\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 390.6/390.6 kB 7.4 MB/s eta 0:00:00a 0:00:01\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 31.0/31.0 MB 49.5 MB/s eta 0:00:00:00:0100:01\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 290.5/290.5 kB 18.1 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 82.0/82.0 MB 19.9 MB/s eta 0:00:00:00:0100:01\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 67.9/67.9 kB 4.4 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.0/3.0 MB 80.1 MB/s eta 0:00:00:00:01\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 93.3/93.3 kB 6.1 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 49.6/49.6 kB 3.2 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 139.0/139.0 kB 8.8 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 413.7/413.7 kB 26.2 MB/s eta 0:00:00\n  Building wheel for fixed-install-nmslib (setup.py) ... done\n  Building wheel for hnswlib (pyproject.toml) ... done\n</pre> In\u00a0[2]: Copied! <pre>from datetime import datetime as dt\nimport numpy as np\nimport pandas as pd\nimport random\nimport torch\nimport torch.nn as nn\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm.auto import tqdm\n\nfrom rs_datasets import MovieLens\nfrom replay.splitters.time_splitter import TimeSplitter\nfrom replay.preprocessing.filters import MinCountFilter, LowRatingFilter\nfrom replay.metrics import NDCG, HitRate, Coverage, Experiment\nfrom dataclasses import dataclass\n</pre> from datetime import datetime as dt import numpy as np import pandas as pd import random import torch import torch.nn as nn from sklearn.preprocessing import LabelEncoder from tqdm.auto import tqdm  from rs_datasets import MovieLens from replay.splitters.time_splitter import TimeSplitter from replay.preprocessing.filters import MinCountFilter, LowRatingFilter from replay.metrics import NDCG, HitRate, Coverage, Experiment from dataclasses import dataclass In\u00a0[3]: Copied! <pre>from dataclasses import dataclass\n\n@dataclass\nclass config:\n\n    USER_COL : str = 'user_id'\n    ITEM_COL : str = 'item_id'\n    RATING_COL : str = 'rating'\n    TIMESTAMP : str = 'timestamp'\n    NUM_EPOCHS : int = 30\n\n    K = 10\n    SEED = 123\n\nconfig = config()\nrandom.seed(config.SEED)\ntorch.manual_seed(config.SEED)\nnp.random.seed(config.SEED)\n</pre> from dataclasses import dataclass  @dataclass class config:      USER_COL : str = 'user_id'     ITEM_COL : str = 'item_id'     RATING_COL : str = 'rating'     TIMESTAMP : str = 'timestamp'     NUM_EPOCHS : int = 30      K = 10     SEED = 123  config = config() random.seed(config.SEED) torch.manual_seed(config.SEED) np.random.seed(config.SEED) In\u00a0[4]: Copied! <pre>class MovieLensPrepare:\n\n    def __init__(self):\n        rs = MovieLens('1m')\n        self.data = rs.ratings\n        self.u_features = rs.users\n        self.i_features = rs.items\n\n\n    def preprocess(self):\n\n        data = self.data\n        u_features = self.u_features\n        i_features = self.i_features\n        \n        data = MinCountFilter(num_entries=20).transform(data)\n\n        # interactions and user &amp; item features must be synchronised\n        data = data[data[config.USER_COL].isin(u_features[config.USER_COL].unique())]\n        data = data[data[config.ITEM_COL].isin(i_features[config.ITEM_COL].unique())]\n\n        print(f\"Number of unique users {data['user_id'].nunique()}\")\n        print(f\"Number of unique items {data['item_id'].nunique()}\")\n\n        # interactions and user &amp; item features must be synchronised\n        data = data[data[config.USER_COL].isin(u_features[config.USER_COL].unique())]\n        data = data[data[config.ITEM_COL].isin(i_features[config.ITEM_COL].unique())]\n\n        data[config.TIMESTAMP] = pd.to_datetime(data['timestamp'],unit='s')\n\n        self.data = data\n\n    def split_data(self):\n\n        data = self.data\n        u_features = self.u_features\n        i_features = self.i_features\n\n        splitter = TimeSplitter(time_threshold=0.2,  # 20% into test subset\n                            drop_cold_users=True,\n                            drop_cold_items=True,\n                            query_column=config.USER_COL)\n        \n        train,test = splitter.split(data)\n        print('train size',train.shape[0])\n        print('test size', test.shape[0])\n\n        # user features and item features must be present in interactions dataset and only\n        u_features = u_features[u_features[config.USER_COL].isin(train[config.USER_COL].unique())]\n        i_features = i_features[i_features[config.ITEM_COL].isin(train[config.ITEM_COL].unique())]\n\n        # encoders for users\n        encoder_user = LabelEncoder()\n        encoder_user.fit(train[config.USER_COL])\n        \n        # encoders for items\n        encoder_item = LabelEncoder()\n        encoder_item.fit(train[config.ITEM_COL])\n\n        train[config.USER_COL] = encoder_user.transform(train[config.USER_COL])\n        train[config.ITEM_COL] = encoder_item.transform(train[config.ITEM_COL])\n        \n        test[config.USER_COL] = encoder_user.transform(test[config.USER_COL])\n        test[config.ITEM_COL] = encoder_item.transform(test[config.ITEM_COL])\n        \n        u_features[config.USER_COL] = encoder_user.transform(u_features[config.USER_COL])\n        i_features[config.ITEM_COL] = encoder_item.transform(i_features[config.ITEM_COL])\n\n        self.train = train \n        self.test = test\n\n        self.u_features = u_features\n        self.i_features = i_features\n        \n    def filter_test(self):\n        filter_rating = LowRatingFilter(value=4)        \n        self.test = filter_rating.transform(self.test)\n        \nstudy = MovieLensPrepare()\n</pre> class MovieLensPrepare:      def __init__(self):         rs = MovieLens('1m')         self.data = rs.ratings         self.u_features = rs.users         self.i_features = rs.items       def preprocess(self):          data = self.data         u_features = self.u_features         i_features = self.i_features                  data = MinCountFilter(num_entries=20).transform(data)          # interactions and user &amp; item features must be synchronised         data = data[data[config.USER_COL].isin(u_features[config.USER_COL].unique())]         data = data[data[config.ITEM_COL].isin(i_features[config.ITEM_COL].unique())]          print(f\"Number of unique users {data['user_id'].nunique()}\")         print(f\"Number of unique items {data['item_id'].nunique()}\")          # interactions and user &amp; item features must be synchronised         data = data[data[config.USER_COL].isin(u_features[config.USER_COL].unique())]         data = data[data[config.ITEM_COL].isin(i_features[config.ITEM_COL].unique())]          data[config.TIMESTAMP] = pd.to_datetime(data['timestamp'],unit='s')          self.data = data      def split_data(self):          data = self.data         u_features = self.u_features         i_features = self.i_features          splitter = TimeSplitter(time_threshold=0.2,  # 20% into test subset                             drop_cold_users=True,                             drop_cold_items=True,                             query_column=config.USER_COL)                  train,test = splitter.split(data)         print('train size',train.shape[0])         print('test size', test.shape[0])          # user features and item features must be present in interactions dataset and only         u_features = u_features[u_features[config.USER_COL].isin(train[config.USER_COL].unique())]         i_features = i_features[i_features[config.ITEM_COL].isin(train[config.ITEM_COL].unique())]          # encoders for users         encoder_user = LabelEncoder()         encoder_user.fit(train[config.USER_COL])                  # encoders for items         encoder_item = LabelEncoder()         encoder_item.fit(train[config.ITEM_COL])          train[config.USER_COL] = encoder_user.transform(train[config.USER_COL])         train[config.ITEM_COL] = encoder_item.transform(train[config.ITEM_COL])                  test[config.USER_COL] = encoder_user.transform(test[config.USER_COL])         test[config.ITEM_COL] = encoder_item.transform(test[config.ITEM_COL])                  u_features[config.USER_COL] = encoder_user.transform(u_features[config.USER_COL])         i_features[config.ITEM_COL] = encoder_item.transform(i_features[config.ITEM_COL])          self.train = train          self.test = test          self.u_features = u_features         self.i_features = i_features              def filter_test(self):         filter_rating = LowRatingFilter(value=4)                 self.test = filter_rating.transform(self.test)          study = MovieLensPrepare() <pre>5.93MB [00:00, 10.8MB/s]                            \n</pre> <p>Extract the relevant subsets of data; interactions, user features &amp; item features. We should also note that the <code>timestamp</code> feature is in unix seconds time, which we'll need to convert to <code>datetime</code> later</p> In\u00a0[5]: Copied! <pre>study.preprocess()\n</pre> study.preprocess() <pre>Number of unique users 6040\nNumber of unique items 3706\n</pre> <p>Lets check the <code>user_id</code> and <code>item_id</code> statistics after our filtration and make sure our interactions are synchronised with both the user and item feature subsets</p> <p>Now let's convert the time feature to datetime, so we can more easily interpret how to split the dataset in time.</p> <p>In our problem, we assume that the test interactions have not been made yet</p> In\u00a0[6]: Copied! <pre>study.split_data()\n</pre> study.split_data() <pre>train size 800164\ntest size 104452\n</pre> <pre>&lt;ipython-input-4-142900e37db3&gt;:66: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  u_features[config.USER_COL] = encoder_user.transform(u_features[config.USER_COL])\n&lt;ipython-input-4-142900e37db3&gt;:67: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  i_features[config.ITEM_COL] = encoder_item.transform(i_features[config.ITEM_COL])\n</pre> In\u00a0[7]: Copied! <pre>study.filter_test()\n</pre> study.filter_test() In\u00a0[8]: Copied! <pre>from torch.utils.data import Dataset, DataLoader\n\nclass TowerTrain(Dataset):\n    \n    def __init__(self, \n                 data, \n                 num_negatives=10, \n                 i_features=None, \n                 u_features=None):\n\n        # user, item\n        self.data = data[[config.USER_COL,config.ITEM_COL]].to_numpy()\n        self.num_negatives = num_negatives\n        self.num_items = len(np.unique(self.data[:, 1]))\n        self.i_features = i_features\n        self.u_features = u_features\n\n    def __len__(self):\n        return len(self.data)\n\n    # get item of row in data\n    def __getitem__(self, idx):\n\n        # index to -&gt; user_id, item_id\n        user_id, positive_item_id = self.data[idx, 0], self.data[idx, 1]\n\n        # create positive, negative samples\n        # torch tensor for each item_id (pos sample) create 10 neg samples\n        items = torch.tensor(np.hstack([positive_item_id,\n                                       np.random.randint(\n                                           low=0,\n                                           high=self.num_items,\n                                           size=self.num_negatives)]),\n                             dtype=torch.int32)\n\n        # set all labels to 0\n        labels = torch.zeros(self.num_negatives + 1, dtype=torch.float32)\n        labels[0] = 1. # positive label\n\n        return {'user_ids': torch.tensor([user_id], dtype=torch.int32),\n                'item_ids': items,\n                'labels': labels}\n</pre> from torch.utils.data import Dataset, DataLoader  class TowerTrain(Dataset):          def __init__(self,                   data,                   num_negatives=10,                   i_features=None,                   u_features=None):          # user, item         self.data = data[[config.USER_COL,config.ITEM_COL]].to_numpy()         self.num_negatives = num_negatives         self.num_items = len(np.unique(self.data[:, 1]))         self.i_features = i_features         self.u_features = u_features      def __len__(self):         return len(self.data)      # get item of row in data     def __getitem__(self, idx):          # index to -&gt; user_id, item_id         user_id, positive_item_id = self.data[idx, 0], self.data[idx, 1]          # create positive, negative samples         # torch tensor for each item_id (pos sample) create 10 neg samples         items = torch.tensor(np.hstack([positive_item_id,                                        np.random.randint(                                            low=0,                                            high=self.num_items,                                            size=self.num_negatives)]),                              dtype=torch.int32)          # set all labels to 0         labels = torch.zeros(self.num_negatives + 1, dtype=torch.float32)         labels[0] = 1. # positive label          return {'user_ids': torch.tensor([user_id], dtype=torch.int32),                 'item_ids': items,                 'labels': labels} <p>We create the dataset and dataloaders containing a batch size of 1024 rows, we'll define the training dataloader and show a batch sample output <code>batch</code>, which is the result of the <code>__forward__</code> method pass for the number of items in the batch</p> In\u00a0[9]: Copied! <pre># create dataset\nds_train = TowerTrain(study.train)\n\n# create data loader\ndl_train = DataLoader(ds_train,\n                          batch_size=2,\n                          shuffle=True,\n                          num_workers=0)\n\nbatch = next(iter(dl_train))\nbatch\n</pre> # create dataset ds_train = TowerTrain(study.train)  # create data loader dl_train = DataLoader(ds_train,                           batch_size=2,                           shuffle=True,                           num_workers=0)  batch = next(iter(dl_train)) batch Out[9]: <pre>{'user_ids': tensor([[ 163],\n         [3579]], dtype=torch.int32),\n 'item_ids': tensor([[1036, 3582, 3437, 3454, 1346, 1122, 1766, 3089, 2154, 1147, 1593],\n         [1806, 3286, 1761,   96, 2161, 2686,   47,   73, 1568,  942, 2272]],\n        dtype=torch.int32),\n 'labels': tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])}</pre> In\u00a0[10]: Copied! <pre># subclass contains only embedding layer but we can \n# expand on this by importing user, item features\nclass SimpleTower(nn.Module):\n    def __init__(self, num_embeddings, emb_dim):\n        super().__init__()\n        self.emb = nn.Embedding(num_embeddings, emb_dim)\n\n    def forward(self, ids, features=None):\n        return self.emb(ids)\n\n\nclass BaseTwoHead(nn.Module):\n    \n    def __init__(self, \n                 emb_dim, \n                 user_config=None,\n                 item_config=None):\n        \n        super().__init__()\n        self.emb_dim = emb_dim\n        self.user_tower = SimpleTower(emb_dim=emb_dim, **user_config) # (emb_dim,n_users)\n        self.item_tower = SimpleTower(emb_dim=emb_dim, **item_config) # (emb_dim,n_items)\n\n    # forward method defines two 'towers'\n    # and the scalar product of the two\n    # which will gives us the scores\n    def forward(self, batch):\n        item_emb = self.item_tower(batch[\"item_ids\"]) # (batch,1,16) \n        user_emb = self.user_tower(batch[\"user_ids\"]) # (batch,11,16)\n        dot_product = (user_emb * item_emb).sum(dim=-1) # (batch,11)\n        return dot_product\n\n    # methods for extracting embeddings\n    def infer_users(self, batch):\n        return self.user_tower(batch[\"user_ids\"])\n\n    def infer_items(self, batch):\n        return self.item_tower(batch[\"item_ids\"])\n</pre> # subclass contains only embedding layer but we can  # expand on this by importing user, item features class SimpleTower(nn.Module):     def __init__(self, num_embeddings, emb_dim):         super().__init__()         self.emb = nn.Embedding(num_embeddings, emb_dim)      def forward(self, ids, features=None):         return self.emb(ids)   class BaseTwoHead(nn.Module):          def __init__(self,                   emb_dim,                   user_config=None,                  item_config=None):                  super().__init__()         self.emb_dim = emb_dim         self.user_tower = SimpleTower(emb_dim=emb_dim, **user_config) # (emb_dim,n_users)         self.item_tower = SimpleTower(emb_dim=emb_dim, **item_config) # (emb_dim,n_items)      # forward method defines two 'towers'     # and the scalar product of the two     # which will gives us the scores     def forward(self, batch):         item_emb = self.item_tower(batch[\"item_ids\"]) # (batch,1,16)          user_emb = self.user_tower(batch[\"user_ids\"]) # (batch,11,16)         dot_product = (user_emb * item_emb).sum(dim=-1) # (batch,11)         return dot_product      # methods for extracting embeddings     def infer_users(self, batch):         return self.user_tower(batch[\"user_ids\"])      def infer_items(self, batch):         return self.item_tower(batch[\"item_ids\"]) <p>Let's check the contents of the model, if we utilise an embedding size of 16</p> In\u00a0[11]: Copied! <pre># model parameters\nembed_config = {'emb_dim' : 16}  # embedding dimension\nuser_config = {'num_embeddings' : study.train[config.USER_COL].max() + 1,} # number of users\nitem_config = {'num_embeddings' : study.train[config.ITEM_COL].max() + 1,} # number of items\n\n# import the embedding dimension \nmodel = BaseTwoHead(**embed_config, \n                    user_config=user_config, \n                    item_config=item_config)\nmodel\n</pre> # model parameters embed_config = {'emb_dim' : 16}  # embedding dimension user_config = {'num_embeddings' : study.train[config.USER_COL].max() + 1,} # number of users item_config = {'num_embeddings' : study.train[config.ITEM_COL].max() + 1,} # number of items  # import the embedding dimension  model = BaseTwoHead(**embed_config,                      user_config=user_config,                      item_config=item_config) model Out[11]: <pre>BaseTwoHead(\n  (user_tower): SimpleTower(\n    (emb): Embedding(5400, 16)\n  )\n  (item_tower): SimpleTower(\n    (emb): Embedding(3662, 16)\n  )\n)</pre> <p>Model <code>forward</code> pass</p> <ul> <li>The output of the model will give us the logits for each of the 11 items, for each user row</li> </ul> In\u00a0[12]: Copied! <pre># output for a single batch\noutput = model(batch)\noutput\n</pre> # output for a single batch output = model(batch) output Out[12]: <pre>tensor([[  1.6632,   5.8888,   0.0997,   7.6885,   8.2156,   4.0495,   3.0272,\n           1.9775,  -1.8750,   4.3952,   0.2714],\n        [  5.3873, -10.4797,  -4.2230,  -0.4488,   0.9215,  -5.0823,  -0.5018,\n           4.9579,   0.8251,  -6.3608,  -4.5723]], grad_fn=&lt;SumBackward1&gt;)</pre> <p>Output size</p> In\u00a0[13]: Copied! <pre>output.size()\n</pre> output.size() Out[13]: <pre>torch.Size([2, 11])</pre> In\u00a0[14]: Copied! <pre># extract embeddings from model (here items)\ni_embeddings = model.infer_items(batch)\ni_embeddings.size()\n</pre> # extract embeddings from model (here items) i_embeddings = model.infer_items(batch) i_embeddings.size() Out[14]: <pre>torch.Size([2, 11, 16])</pre> In\u00a0[15]: Copied! <pre>u_embeddings = model.infer_users(batch)\nu_embeddings.size()\n</pre> u_embeddings = model.infer_users(batch) u_embeddings.size() Out[15]: <pre>torch.Size([2, 1, 16])</pre> In\u00a0[16]: Copied! <pre>optimizer = torch.optim.Adam(model.parameters(),\n                             lr=0.001)\nloss_fn = nn.BCEWithLogitsLoss()\n\n# create train dataset\nds_train = TowerTrain(study.train)\n\n# create train data loader\ndl_train = DataLoader(ds_train,\n                      batch_size=1024,\n                      shuffle=True,\n                      num_workers=0)\n\n# create test dataset\nds_test = TowerTrain(study.test)\n\n# create test data loader\ndl_test = DataLoader(ds_test,\n                      batch_size=1024,\n                      shuffle=True,\n                      num_workers=0)\n</pre> optimizer = torch.optim.Adam(model.parameters(),                              lr=0.001) loss_fn = nn.BCEWithLogitsLoss()  # create train dataset ds_train = TowerTrain(study.train)  # create train data loader dl_train = DataLoader(ds_train,                       batch_size=1024,                       shuffle=True,                       num_workers=0)  # create test dataset ds_test = TowerTrain(study.test)  # create test data loader dl_test = DataLoader(ds_test,                       batch_size=1024,                       shuffle=True,                       num_workers=0) In\u00a0[17]: Copied! <pre>train_loss_per_epoch = []\ntest_loss_per_epoch = []\n\n# loop through all epochs\nfor epoch in tqdm(range(config.NUM_EPOCHS)):\n\n    # training loop for all batches\n    model.train()\n    train_loss = 0.0\n    for iteration, batch in enumerate(dl_train):\n        optimizer.zero_grad()\n        preds = model(batch)\n        loss = loss_fn(preds, batch['labels'])\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    train_loss /= len(dl_train)\n\n    # evaluation loop for all batches\n    model.eval()\n    test_loss = 0\n    for iteration, batch in enumerate(dl_test):\n        preds = model(batch)\n        loss = loss_fn(preds, batch['labels'])\n        test_loss += loss.item()\n\n    # evaluation of loss\n    test_loss /= len(dl_test)\n    test_loss_per_epoch.append(test_loss)\n    train_loss_per_epoch.append(train_loss)\n</pre> train_loss_per_epoch = [] test_loss_per_epoch = []  # loop through all epochs for epoch in tqdm(range(config.NUM_EPOCHS)):      # training loop for all batches     model.train()     train_loss = 0.0     for iteration, batch in enumerate(dl_train):         optimizer.zero_grad()         preds = model(batch)         loss = loss_fn(preds, batch['labels'])         loss.backward()         optimizer.step()         train_loss += loss.item()     train_loss /= len(dl_train)      # evaluation loop for all batches     model.eval()     test_loss = 0     for iteration, batch in enumerate(dl_test):         preds = model(batch)         loss = loss_fn(preds, batch['labels'])         test_loss += loss.item()      # evaluation of loss     test_loss /= len(dl_test)     test_loss_per_epoch.append(test_loss)     train_loss_per_epoch.append(train_loss) <pre>  0%|          | 0/30 [00:00&lt;?, ?it/s]</pre> <p>Save our trained model for later use!</p> In\u00a0[18]: Copied! <pre># save our model state\ntorch.save(model.state_dict(), f\"/content/model_{config.NUM_EPOCHS}\")\n</pre> # save our model state torch.save(model.state_dict(), f\"/content/model_{config.NUM_EPOCHS}\") In\u00a0[19]: Copied! <pre>import warnings; warnings.filterwarnings('ignore')\n\nmodel = BaseTwoHead(**embed_config, \n                    user_config=user_config, \n                    item_config=item_config)\nmodel.load_state_dict(torch.load(f\"/content/model_{config.NUM_EPOCHS}\"))\nmodel.eval()\n</pre> import warnings; warnings.filterwarnings('ignore')  model = BaseTwoHead(**embed_config,                      user_config=user_config,                      item_config=item_config) model.load_state_dict(torch.load(f\"/content/model_{config.NUM_EPOCHS}\")) model.eval() Out[19]: <pre>BaseTwoHead(\n  (user_tower): SimpleTower(\n    (emb): Embedding(5400, 16)\n  )\n  (item_tower): SimpleTower(\n    (emb): Embedding(3662, 16)\n  )\n)</pre> In\u00a0[20]: Copied! <pre>test_users = study.test[[config.USER_COL]].drop_duplicates().reset_index(drop=True)\ntest_users.head()\n</pre> test_users = study.test[[config.USER_COL]].drop_duplicates().reset_index(drop=True) test_users.head() Out[20]: user_id 0 1238 1 0 2 2146 3 1380 4 3180 In\u00a0[21]: Copied! <pre># extract the user / item embedding weights\nuser_embed = model.user_tower.emb.weight.detach().cpu().numpy()\nitem_embed = model.item_tower.emb.weight.detach().cpu().numpy()\nuser_embed.shape, item_embed.shape\n</pre> # extract the user / item embedding weights user_embed = model.user_tower.emb.weight.detach().cpu().numpy() item_embed = model.item_tower.emb.weight.detach().cpu().numpy() user_embed.shape, item_embed.shape Out[21]: <pre>((5400, 16), (3662, 16))</pre> In\u00a0[22]: Copied! <pre># calcualate the scores (751,1616)\nscores = user_embed[test_users[config.USER_COL].values] @ item_embed.T\nprint(scores)\n</pre> # calcualate the scores (751,1616) scores = user_embed[test_users[config.USER_COL].values] @ item_embed.T print(scores) <pre>[[-8.98283534e-03 -2.23088098e+00 -1.21248198e+00 ... -5.93518734e+00\n  -6.74669886e+00 -3.57267737e+00]\n [ 1.21680886e-01 -1.92409587e+00 -2.67904949e+00 ... -5.99316978e+00\n  -3.68449354e+00 -1.20745528e+00]\n [-3.68786573e-01 -3.50460958e+00 -3.39616084e+00 ... -4.76325846e+00\n  -2.48652792e+00 -9.43809271e-01]\n ...\n [ 5.35706937e-01 -3.02921438e+00 -2.88318610e+00 ... -3.14549780e+00\n  -3.66571522e+00 -1.52589762e+00]\n [-6.87072992e-01 -1.82784998e+00 -2.26169515e+00 ... -6.18041801e+00\n  -5.24198675e+00 -3.18532085e+00]\n [-7.03839183e-01 -1.79352736e+00 -2.59766245e+00 ... -9.07467270e+00\n  -7.98052073e+00 -4.78053665e+00]]\n</pre> In\u00a0[23]: Copied! <pre># get top 10 idx by value &amp; get its value\nids = np.argpartition(scores, -config.K)[:, -config.K:]\nscores = np.take_along_axis(scores, ids, axis=1)\nscores[:5]\n</pre> # get top 10 idx by value &amp; get its value ids = np.argpartition(scores, -config.K)[:, -config.K:] scores = np.take_along_axis(scores, ids, axis=1) scores[:5] Out[23]: <pre>array([[ 0.37186688,  0.3992171 ,  0.4394618 ,  0.4884671 ,  0.5174091 ,\n         0.87457657,  0.6083931 ,  0.58975196,  0.8642359 ,  0.78722566],\n       [ 0.4257152 ,  0.4830652 ,  0.50204533,  0.50319844,  0.6602011 ,\n         0.8257402 ,  0.8278695 ,  0.69933504,  0.88355327,  0.8429366 ],\n       [ 0.9651828 ,  0.9905893 ,  1.058987  ,  1.0769    ,  1.2267288 ,\n         1.4366896 ,  1.284672  ,  1.255043  ,  1.2931157 ,  1.2413594 ],\n       [-0.11466098, -0.10546018, -0.1026658 , -0.09919392, -0.0560105 ,\n        -0.03253222,  0.02805819, -0.07913139,  0.00199754, -0.0307106 ],\n       [ 0.5338577 ,  0.558586  ,  0.6305356 ,  0.6141302 ,  0.632113  ,\n         0.7719943 ,  0.6591141 ,  0.8380035 ,  0.65727115,  0.7047775 ]],\n      dtype=float32)</pre> In\u00a0[24]: Copied! <pre># prepare recommendations matrix\ndef prepare_recs(test_users, \n                 rec_item_ids, \n                 rec_relevances):\n    \n    predict = test_users.copy()\n    predict[config.ITEM_COL] = rec_item_ids.tolist()  # add list of indicies for each user\n    predict['rating'] = rec_relevances.tolist() # add rating list of scores for each user\n    predict = predict.explode(column=[config.ITEM_COL, 'rating']).reset_index(drop=True) # expand both lists\n    predict[config.ITEM_COL] = predict[config.ITEM_COL].astype(int)\n    predict['rating'] = predict['rating'].astype(\"double\")\n    return predict\n\n\nmodel_recommendations = prepare_recs(test_users,      # user columns \n                                     rec_item_ids=ids,  # indicies of top 10 in scores\n                                     rec_relevances=scores) # scores of top 10\nmodel_recommendations\n</pre> # prepare recommendations matrix def prepare_recs(test_users,                   rec_item_ids,                   rec_relevances):          predict = test_users.copy()     predict[config.ITEM_COL] = rec_item_ids.tolist()  # add list of indicies for each user     predict['rating'] = rec_relevances.tolist() # add rating list of scores for each user     predict = predict.explode(column=[config.ITEM_COL, 'rating']).reset_index(drop=True) # expand both lists     predict[config.ITEM_COL] = predict[config.ITEM_COL].astype(int)     predict['rating'] = predict['rating'].astype(\"double\")     return predict   model_recommendations = prepare_recs(test_users,      # user columns                                       rec_item_ids=ids,  # indicies of top 10 in scores                                      rec_relevances=scores) # scores of top 10 model_recommendations Out[24]: user_id item_id rating 0 1238 2133 0.371867 1 1238 1198 0.399217 2 1238 346 0.439462 3 1238 1156 0.488467 4 1238 2620 0.517409 ... ... ... ... 11215 5309 1115 0.197690 11216 5309 2130 0.171694 11217 5309 2480 0.256298 11218 5309 1505 0.161175 11219 5309 1795 0.279239 <p>11220 rows \u00d7 3 columns</p> <p>We'll evaluate the prediction &amp; test overlapping items using hitrate, to measure how well the model predicts at least one relevant recommendation for users. NDCG, for the evaluation of how well the model can correcly order the relevant items &amp; coverage to measure how well the model predicts a range of items from all available items</p> In\u00a0[25]: Copied! <pre>metrics = Experiment(\n    [NDCG(config.K), HitRate(config.K), Coverage(config.K)],\n    study.test,\n    study.train,\n    query_column=config.USER_COL, \n    item_column=config.ITEM_COL,\n)\n</pre> metrics = Experiment(     [NDCG(config.K), HitRate(config.K), Coverage(config.K)],     study.test,     study.train,     query_column=config.USER_COL,      item_column=config.ITEM_COL, ) In\u00a0[26]: Copied! <pre>metrics.add_result(\"dssm_model\", model_recommendations)\nmetrics.results\n</pre> metrics.add_result(\"dssm_model\", model_recommendations) metrics.results Out[26]: NDCG@10 HitRate@10 Coverage@10 dssm_model 0.058016 0.317291 0.231294"},{"location":"portfolio/course_recsys/dssm-simple.html#background","title":"Background\u00b6","text":"<p>In this notebook we will look at how to use a neural network approach to recommendations</p> <ul> <li>Implicit feedback will be used</li> <li>Scalar product of both the <code>user_id</code> and <code>item_id</code> embeddings will be our relevancy scores</li> <li>User film interactions will be <code>positive</code> feedback &amp; negative samples which will be created randomly are our <code>negative</code> samples</li> <li>The dataset is split into two, <code>train</code> will be used to train a model on historical user data, <code>test</code> will be used to provide user recommendations</li> <li>What we will be telling the model is to learn and differentiate between</li> </ul>"},{"location":"portfolio/course_recsys/dssm-simple.html#1-load-dataset","title":"1 | Load Dataset\u00b6","text":"<p>We will be using a simplified dataset <code>MovieLens</code> with 100,000 interactions</p>"},{"location":"portfolio/course_recsys/dssm-simple.html#2-preprocessing","title":"2 | Preprocessing\u00b6","text":"<ul> <li><code>Replay</code> contains a handy &amp; quick way for preprocessing interactions</li> <li><code>MinCountFilter</code> can be used for filtering our interactions that have less than num_entries</li> <li>Lets use this method for removing user interactions with less than 20 items</li> </ul>"},{"location":"portfolio/course_recsys/dssm-simple.html#3-splitting-dataset-in-time","title":"3 | Splitting Dataset in time\u00b6","text":"<ul> <li>The next step after preprocessing the dataset to our liking is to split it into subsets, so we can train the model on one subset and use another for model validation (20%)</li> <li>replay has a function named <code>TimeSplitter</code>, which we will to create our subsets</li> </ul> <p>class TimeSplitter(replay.splitters.base_splitter.Splitter) |  TimeSplitter(time_threshold: Union[datetime.datetime, str, float], query_column: str = 'query_id', drop_cold_users: bool = False, drop_cold_items: bool = False, item_column: str = 'item_id', timestamp_column: str = 'timestamp', session_id_column: Optional[str] = None, session_id_processing_strategy: str = 'test', time_column_format: str = '%Y-%m-%d %H:%M:%S')</p>"},{"location":"portfolio/course_recsys/dssm-simple.html#4-rating-filter","title":"4 | Rating Filter\u00b6","text":"<p>We want to recommend only items that have been rated highly, so for the <code>test</code> subset, we will be using <code>LowRatingFilter</code> to remove iteractions with low ratings</p>"},{"location":"portfolio/course_recsys/dssm-simple.html#5-create-torch-dataset","title":"5 | Create Torch Dataset\u00b6","text":"<p>We need to create a torch dataset from our matrix of interactions <code>data</code></p> <ul> <li>The dataset <code>TowerTrain</code> <code>get_item</code> for each index inputs the <code>user_id</code> and <code>item_id</code> (which will be our positive feedback) from the interaction dataset : (positive_item_id)</li> <li>Additionally for this user <code>user_id</code>, we generate an additional number of random <code>item_id</code> which will be the negative samples, which the user hasn't watched</li> <li>Both of these are concatenated into a single array vector (items)</li> <li>Lastly we also return the labels, corresponding to either the positive (1) or negative (0) sample id</li> </ul>"},{"location":"portfolio/course_recsys/dssm-simple.html#7-model-definition","title":"7 | Model Definition\u00b6","text":"<p>We will be creating a subclass <code>SimpleTower</code>, which only includes the embeddings of both <code>user_id</code> and <code>item_id</code></p> <ul> <li>The <code>forward</code> method, when called simply returns the user/item row of the corresponding embedding matrix</li> <li>Calculates the dot product between the <code>user_id</code> &amp; <code>item_id</code> matrices</li> </ul>"},{"location":"portfolio/course_recsys/dssm-simple.html#8-extracting-embeddings","title":"8 | Extracting Embeddings\u00b6","text":"<p>We can extract the embeddings for both the user and items using the following method and use it for <code>inference</code>. In this type of simple model, it won't be such a big problem to extract it anyway. Below is an example for a batch size of two</p>"},{"location":"portfolio/course_recsys/dssm-simple.html#9-training-the-network","title":"9 | Training the network\u00b6","text":"<p>We need to define an <code>optimiser</code>, <code>loss function</code> and the datasets in the form of a <code>data loader</code>. As we are setting up the problem as a binary classification problem, we'll be using BCEWithLogitsLoss</p>"},{"location":"portfolio/course_recsys/dssm-simple.html#8-generating-user-recommendations","title":"8 | Generating user recommendations\u00b6","text":"<p>Time has come to use our trained model!</p> <ul> <li>We will be making recommendations by using the model that we trained on the train dataset and using the test users to make predictions</li> <li>To make predictions, we will extract the embedding matrix weights for user and items, calculate the scores, get the top k results for each user based on the largest score values</li> </ul>"},{"location":"portfolio/course_recsys/dssm-simple.html#81-load-weights","title":"8.1. Load Weights\u00b6","text":"<p>First things first, we need to load the model weights</p>"},{"location":"portfolio/course_recsys/dssm-simple.html#82-get-test-users","title":"8.2. Get test users\u00b6","text":"<p>Get the user identifiers that are in the test test, the test set was saved in <code>study.test</code></p>"},{"location":"portfolio/course_recsys/dssm-simple.html#83-extract-weights","title":"8.3. Extract Weights\u00b6","text":"<p>Extract the embedding weights for all users and items which is located in the model</p>"},{"location":"portfolio/course_recsys/dssm-simple.html#84-scalar-product","title":"8.4. Scalar product\u00b6","text":"<p>Calculate the scores for each user &amp; item combination by calculating the scalar product of them</p>"},{"location":"portfolio/course_recsys/dssm-simple.html#85-get-highest-scores","title":"8.5. Get highest scores\u00b6","text":"<p>Get the highest value indicies (idx) &amp; their corresponding values (scores). The scores correspond to the index of the item in the encoder <code>encoder_item</code>, which we stored in class instance <code>study</code></p>"},{"location":"portfolio/course_recsys/dssm-simple.html#86-recommendations-matrix","title":"8.6. Recommendations Matrix\u00b6","text":"<p>Prepare the usual format, <code>user_id</code>, <code>item_id</code> and rating <code>rating</code>, which will enable us to quickly evaluate the metrics using <code>experiment</code> function from replay. We need to add both lists to each user &amp; expand them together</p>"},{"location":"portfolio/course_recsys/dssm-towers.html","title":"Dssm towers","text":"In\u00a0[70]: Copied! <pre>!pip install pytorch-lightning -qqq\n</pre> !pip install pytorch-lightning -qqq In\u00a0[71]: Copied! <pre>import json\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom collections import Counter\nfrom random import randint, random\nfrom scipy.sparse import coo_matrix, hstack\nimport torch\nfrom pathlib import Path\nfrom tqdm import tqdm\n</pre> import json import matplotlib.pyplot as plt import numpy as np import pandas as pd import os import warnings warnings.filterwarnings('ignore') from collections import Counter from random import randint, random from scipy.sparse import coo_matrix, hstack import torch from pathlib import Path from tqdm import tqdm In\u00a0[72]: Copied! <pre>users_df = pd.read_csv('/kaggle/input/kion-dataset/users.csv')  # user dataframe\nitems_df = pd.read_csv('/kaggle/input/kion-dataset/items.csv')\ninteractions_df = pd.read_csv('/kaggle/input/kion-dataset/interactions.csv') # user feature interaction dataframe\n</pre> users_df = pd.read_csv('/kaggle/input/kion-dataset/users.csv')  # user dataframe items_df = pd.read_csv('/kaggle/input/kion-dataset/items.csv') interactions_df = pd.read_csv('/kaggle/input/kion-dataset/interactions.csv') # user feature interaction dataframe In\u00a0[73]: Copied! <pre># 5476251 user/item interactions\ninteractions_df.shape\n</pre> # 5476251 user/item interactions interactions_df.shape Out[73]: <pre>(5476251, 5)</pre> In\u00a0[74]: Copied! <pre># interactions 35% + only \n# user interactions must be more than 10\n# items must have been watched more than 10 times\n\nprint(f\"N users before: {interactions_df.user_id.nunique()}\")\nprint(f\"N items before: {interactions_df.item_id.nunique()}\\n\")\n\n# (1) \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u043b \u0444\u0438\u043b\u044c\u043c \u043c\u0435\u043d\u0435\u0435 \u0447\u0435\u043c \u043d\u0430 35 \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u043e\u0432\ninteractions_df = interactions_df[interactions_df.watched_pct &gt; 35]\n\n# \u0441\u043e\u0431\u0435\u0440\u0435\u043c \u0432\u0441\u0435\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u043b\u0438\n# \u0431\u043e\u043b\u044c\u0448\u0435 10 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 (\u043c\u043e\u0436\u0435\u0442\u0435 \u0432\u044b\u0431\u0440\u0430\u0442\u044c \u0434\u0440\u0443\u0433\u043e\u0439 \u043f\u043e\u0440\u043e\u0433)\nvalid_users = []\nc = Counter(interactions_df.user_id)\nfor user_id, entries in c.most_common():\n  if entries &gt; 10:\n    valid_users.append(user_id)\n\n# \u0438 \u0441\u043e\u0431\u0435\u0440\u0435\u043c \u0432\u0441\u0435 \u0444\u0438\u043b\u044c\u043c\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u043b\u0438 \u0431\u043e\u043b\u044c\u0448\u0435 10 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439\nvalid_items = []\nc = Counter(interactions_df.item_id)\nfor item_id, entries in c.most_common():\n  if entries &gt; 10:\n    valid_items.append(item_id)\n\n# \u043e\u0442\u0431\u0440\u043e\u0441\u0438\u043c \u043d\u0435\u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u0438 \u043d\u0435\u0430\u043a\u0442\u0438\u0432\u043d\u044b\u0445 \u044e\u0437\u0435\u0440\u043e\u0432\ninteractions_df = interactions_df[interactions_df.user_id.isin(valid_users)]\ninteractions_df = interactions_df[interactions_df.item_id.isin(valid_items)]\n\nprint(f\"Number of users after filtration: {interactions_df.user_id.nunique()}\")\nprint(f\"Number of items after filtration: {interactions_df.item_id.nunique()}\")\n</pre> # interactions 35% + only  # user interactions must be more than 10 # items must have been watched more than 10 times  print(f\"N users before: {interactions_df.user_id.nunique()}\") print(f\"N items before: {interactions_df.item_id.nunique()}\\n\")  # (1) \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u043b \u0444\u0438\u043b\u044c\u043c \u043c\u0435\u043d\u0435\u0435 \u0447\u0435\u043c \u043d\u0430 35 \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u043e\u0432 interactions_df = interactions_df[interactions_df.watched_pct &gt; 35]  # \u0441\u043e\u0431\u0435\u0440\u0435\u043c \u0432\u0441\u0435\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u043b\u0438 # \u0431\u043e\u043b\u044c\u0448\u0435 10 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 (\u043c\u043e\u0436\u0435\u0442\u0435 \u0432\u044b\u0431\u0440\u0430\u0442\u044c \u0434\u0440\u0443\u0433\u043e\u0439 \u043f\u043e\u0440\u043e\u0433) valid_users = [] c = Counter(interactions_df.user_id) for user_id, entries in c.most_common():   if entries &gt; 10:     valid_users.append(user_id)  # \u0438 \u0441\u043e\u0431\u0435\u0440\u0435\u043c \u0432\u0441\u0435 \u0444\u0438\u043b\u044c\u043c\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u043b\u0438 \u0431\u043e\u043b\u044c\u0448\u0435 10 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 valid_items = [] c = Counter(interactions_df.item_id) for item_id, entries in c.most_common():   if entries &gt; 10:     valid_items.append(item_id)  # \u043e\u0442\u0431\u0440\u043e\u0441\u0438\u043c \u043d\u0435\u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u0438 \u043d\u0435\u0430\u043a\u0442\u0438\u0432\u043d\u044b\u0445 \u044e\u0437\u0435\u0440\u043e\u0432 interactions_df = interactions_df[interactions_df.user_id.isin(valid_users)] interactions_df = interactions_df[interactions_df.item_id.isin(valid_items)]  print(f\"Number of users after filtration: {interactions_df.user_id.nunique()}\") print(f\"Number of items after filtration: {interactions_df.item_id.nunique()}\") <pre>N users before: 962179\nN items before: 15706\n\nNumber of users after filtration: 54213\nNumber of items after filtration: 6140\n</pre> In\u00a0[75]: Copied! <pre># Common Users &amp; Items in interactions/User/Items after filtration\n\n# intersection between interactions &amp; users; intersection between interactions &amp; items \ncommon_users = set(interactions_df.user_id.unique()).intersection(set(users_df.user_id.unique()))\nprint(len(common_users))\n\ncommon_items = set(interactions_df.item_id.unique()).intersection(set(items_df.item_id.unique()))\nprint(len(common_items))\n\ninteractions_df = interactions_df[interactions_df.item_id.isin(common_items)]\ninteractions_df = interactions_df[interactions_df.user_id.isin(common_users)]\n\n# filtered items &amp; users (we keep only users/items in interactions)\nitems_df_filtered = items_df[items_df.item_id.isin(interactions_df['item_id'].unique())].copy()\nusers_df_filtered = users_df[users_df.user_id.isin(interactions_df['user_id'].unique())].copy()\n</pre> # Common Users &amp; Items in interactions/User/Items after filtration  # intersection between interactions &amp; users; intersection between interactions &amp; items  common_users = set(interactions_df.user_id.unique()).intersection(set(users_df.user_id.unique())) print(len(common_users))  common_items = set(interactions_df.item_id.unique()).intersection(set(items_df.item_id.unique())) print(len(common_items))  interactions_df = interactions_df[interactions_df.item_id.isin(common_items)] interactions_df = interactions_df[interactions_df.user_id.isin(common_users)]  # filtered items &amp; users (we keep only users/items in interactions) items_df_filtered = items_df[items_df.item_id.isin(interactions_df['item_id'].unique())].copy() users_df_filtered = users_df[users_df.user_id.isin(interactions_df['user_id'].unique())].copy() <pre>44959\n6140\n</pre> In\u00a0[76]: Copied! <pre>'''\n\nItem features which need to be converted\n\n'''\n\nitem_cat_feats = ['content_type', 'release_year',\n                  'for_kids', 'age_rating',\n                  'studios', 'countries', 'directors']\n\ndisplay(items_df_filtered[item_cat_feats].head())\n\nfor col in item_cat_feats:\n  items_df_filtered[col] = items_df_filtered[col].fillna('unknown')\n  items_df_filtered[f'{col}_encoded'] = items_df_filtered[col].astype('category').cat.codes\n</pre> '''  Item features which need to be converted  '''  item_cat_feats = ['content_type', 'release_year',                   'for_kids', 'age_rating',                   'studios', 'countries', 'directors']  display(items_df_filtered[item_cat_feats].head())  for col in item_cat_feats:   items_df_filtered[col] = items_df_filtered[col].fillna('unknown')   items_df_filtered[f'{col}_encoded'] = items_df_filtered[col].astype('category').cat.codes content_type release_year for_kids age_rating studios countries directors 8 film 2018.0 NaN 16.0 NaN \u0418\u0441\u043f\u0430\u043d\u0438\u044f \u0410\u0441\u0433\u0430\u0440 \u0424\u0430\u0440\u0445\u0430\u0434\u0438 10 film 2018.0 NaN 18.0 NaN \u0412\u0435\u043b\u0438\u043a\u043e\u0431\u0440\u0438\u0442\u0430\u043d\u0438\u044f \u0422\u0440\u0435\u0432\u043e\u0440 \u041d\u0430\u043d\u043d 16 film 2019.0 NaN 18.0 NaN \u0421\u0428\u0410 \u0411\u0440\u0435\u0442\u0442 \u041f\u0438\u0440\u0441, \u0414\u0440\u044e \u0422. \u041f\u0438\u0440\u0441 20 film 2019.0 NaN 6.0 NaN \u0421\u0428\u0410 NaN 38 film 2004.0 NaN 12.0 NaN \u0421\u0428\u0410 \u0414\u044d\u0432\u0438\u0434 \u041a\u0435\u043f\u043f In\u00a0[77]: Copied! <pre>user_cat_feats = [\"age\", \"income\", \"sex\", \"kids_flg\"]\n\nfor col in user_cat_feats:\n  users_df_filtered[col] = users_df_filtered[col].fillna('unknown')\n  users_df_filtered[f'{col}_encoded'] = users_df_filtered[col].astype('category').cat.codes\n\nusers_df_filtered[user_cat_feats].head()\n</pre> user_cat_feats = [\"age\", \"income\", \"sex\", \"kids_flg\"]  for col in user_cat_feats:   users_df_filtered[col] = users_df_filtered[col].fillna('unknown')   users_df_filtered[f'{col}_encoded'] = users_df_filtered[col].astype('category').cat.codes  users_df_filtered[user_cat_feats].head() Out[77]: age income sex kids_flg 24 age_35_44 income_20_40 \u0416 1 27 age_25_34 income_40_60 \u041c 1 66 age_25_34 income_20_40 \u041c 0 81 age_25_34 income_20_40 \u041c 0 136 age_65_inf income_20_40 \u041c 0 In\u00a0[78]: Copied! <pre>'''\n\nConvert item / user ids to new normalised numbering from 0...\n\n'''\n\n# converted in order from 0 for user/items\ninteractions_df[\"uid\"] = interactions_df[\"user_id\"].astype(\"category\")\ninteractions_df[\"uid\"] = interactions_df[\"uid\"].cat.codes\n\ninteractions_df[\"iid\"] = interactions_df[\"item_id\"].astype(\"category\")\ninteractions_df[\"iid\"] = interactions_df[\"iid\"].cat.codes\n\n# lets confirm they start from 0\nprint(sorted(interactions_df.iid.unique())[:5])\nprint(sorted(interactions_df.uid.unique())[:5])\n</pre> '''  Convert item / user ids to new normalised numbering from 0...  '''  # converted in order from 0 for user/items interactions_df[\"uid\"] = interactions_df[\"user_id\"].astype(\"category\") interactions_df[\"uid\"] = interactions_df[\"uid\"].cat.codes  interactions_df[\"iid\"] = interactions_df[\"item_id\"].astype(\"category\") interactions_df[\"iid\"] = interactions_df[\"iid\"].cat.codes  # lets confirm they start from 0 print(sorted(interactions_df.iid.unique())[:5]) print(sorted(interactions_df.uid.unique())[:5]) <pre>[0, 1, 2, 3, 4]\n[0, 1, 2, 3, 4]\n</pre> In\u00a0[79]: Copied! <pre>interactions_df.head()\n</pre> interactions_df.head() Out[79]: user_id item_id last_watch_dt total_dur watched_pct uid iid 0 176549 9506 2021-05-11 4250 72.0 7234 3500 1 699317 1659 2021-05-29 8317 100.0 28657 594 14 5324 8437 2021-04-18 6598 92.0 208 3084 18 927973 9617 2021-06-19 8422 100.0 37990 3536 20 896751 8081 2021-05-17 6358 100.0 36735 2953 In\u00a0[80]: Copied! <pre># extract from interaction all mappers for items\niid_to_item_id = interactions_df[[\"iid\", \"item_id\"]].drop_duplicates().set_index(\"iid\").to_dict()[\"item_id\"]\nitem_id_to_iid = interactions_df[[\"iid\", \"item_id\"]].drop_duplicates().set_index(\"item_id\").to_dict()[\"iid\"]\n\n# extract from interaction all mappers for users\nuid_to_user_id = interactions_df[[\"uid\", \"user_id\"]].drop_duplicates().set_index(\"uid\").to_dict()[\"user_id\"]\nuser_id_to_uid = interactions_df[[\"uid\", \"user_id\"]].drop_duplicates().set_index(\"user_id\").to_dict()[\"uid\"]\n\n# add iid to item dataframe\nitems_df_filtered[\"iid\"] = items_df_filtered[\"item_id\"].apply(lambda x: item_id_to_iid[x])\nitems_df_filtered = items_df_filtered.set_index(\"iid\")\n\n# add uid to user dataframe\nusers_df_filtered[\"uid\"] = users_df_filtered[\"user_id\"].apply(lambda x: user_id_to_uid[x])\nusers_df_filtered = users_df_filtered.set_index(\"uid\")\n</pre> # extract from interaction all mappers for items iid_to_item_id = interactions_df[[\"iid\", \"item_id\"]].drop_duplicates().set_index(\"iid\").to_dict()[\"item_id\"] item_id_to_iid = interactions_df[[\"iid\", \"item_id\"]].drop_duplicates().set_index(\"item_id\").to_dict()[\"iid\"]  # extract from interaction all mappers for users uid_to_user_id = interactions_df[[\"uid\", \"user_id\"]].drop_duplicates().set_index(\"uid\").to_dict()[\"user_id\"] user_id_to_uid = interactions_df[[\"uid\", \"user_id\"]].drop_duplicates().set_index(\"user_id\").to_dict()[\"uid\"]  # add iid to item dataframe items_df_filtered[\"iid\"] = items_df_filtered[\"item_id\"].apply(lambda x: item_id_to_iid[x]) items_df_filtered = items_df_filtered.set_index(\"iid\")  # add uid to user dataframe users_df_filtered[\"uid\"] = users_df_filtered[\"user_id\"].apply(lambda x: user_id_to_uid[x]) users_df_filtered = users_df_filtered.set_index(\"uid\") In\u00a0[81]: Copied! <pre>from torch.utils.data import Dataset, DataLoader\n\nSEED = 42\nclass TupleDataset(Dataset):\n\n  def __init__(self,\n               user_pos_pairs: np.ndarray, # two dimentional user/item interaction\n               user_features: pd.DataFrame, # numerical user features\n               item_features: pd.DataFrame, # item features\n               n_negatives: int = 1) -&gt; None:\n\n    self.user_pos_pairs = user_pos_pairs   # user, item pair numpy matrix\n    self.user_features = user_features  # user feature dataframe\n    self.item_features = item_features  # item feature dataframe\n    self.all_items = item_features.index.values\n    self.rng = np.random.default_rng(SEED)\n    self.n_negatives = n_negatives\n\n  def __len__(self):\n    return len(self.user_pos_pairs)\n\n  def __getitem__(self, index):\n\n    # user value &amp; user item value \n    user, pos = self.user_pos_pairs[index]\n\n    # for the user pick random item, it will be our negative\n    negative = self.rng.choice(self.all_items, size=self.n_negatives).item() \n\n    user_features = self.user_features.loc[user].to_dict()   # user features\n    pos_features = self.item_features.loc[pos].to_dict()     # positive item features\n    neg_features = self.item_features.loc[negative].to_dict() # random sample from user/item as negative sample(s)\n\n    return {\n                'user_features': user_features,\n                'pos_features': pos_features,\n                'neg_features': neg_features\n            }\n</pre> from torch.utils.data import Dataset, DataLoader  SEED = 42 class TupleDataset(Dataset):    def __init__(self,                user_pos_pairs: np.ndarray, # two dimentional user/item interaction                user_features: pd.DataFrame, # numerical user features                item_features: pd.DataFrame, # item features                n_negatives: int = 1) -&gt; None:      self.user_pos_pairs = user_pos_pairs   # user, item pair numpy matrix     self.user_features = user_features  # user feature dataframe     self.item_features = item_features  # item feature dataframe     self.all_items = item_features.index.values     self.rng = np.random.default_rng(SEED)     self.n_negatives = n_negatives    def __len__(self):     return len(self.user_pos_pairs)    def __getitem__(self, index):      # user value &amp; user item value      user, pos = self.user_pos_pairs[index]      # for the user pick random item, it will be our negative     negative = self.rng.choice(self.all_items, size=self.n_negatives).item()       user_features = self.user_features.loc[user].to_dict()   # user features     pos_features = self.item_features.loc[pos].to_dict()     # positive item features     neg_features = self.item_features.loc[negative].to_dict() # random sample from user/item as negative sample(s)      return {                 'user_features': user_features,                 'pos_features': pos_features,                 'neg_features': neg_features             } In\u00a0[82]: Copied! <pre>from pytorch_lightning import LightningDataModule\n\nclass DssmDataModule(LightningDataModule):\n\n  def __init__(self,\n               train_ds: TupleDataset,\n               train_batch_size: int = 1):\n\n    super().__init__()\n    self.train_ds = train_ds\n    self.train_batch_size = train_batch_size\n\n  def train_dataloader(self):\n    return DataLoader(self.train_ds,\n                      batch_size=self.train_batch_size)\n</pre> from pytorch_lightning import LightningDataModule  class DssmDataModule(LightningDataModule):    def __init__(self,                train_ds: TupleDataset,                train_batch_size: int = 1):      super().__init__()     self.train_ds = train_ds     self.train_batch_size = train_batch_size    def train_dataloader(self):     return DataLoader(self.train_ds,                       batch_size=self.train_batch_size) In\u00a0[83]: Copied! <pre># user / item features dataframe \nuser_feature_cols = [f'{col}_encoded' for col in user_cat_feats] # user features column names\nitem_feature_cols = [f'{col}_encoded' for col in item_cat_feats] # item feature column names\n\nuser_features = users_df_filtered[user_feature_cols]\nitem_features = items_df_filtered[item_feature_cols]\n</pre> # user / item features dataframe  user_feature_cols = [f'{col}_encoded' for col in user_cat_feats] # user features column names item_feature_cols = [f'{col}_encoded' for col in item_cat_feats] # item feature column names  user_features = users_df_filtered[user_feature_cols] item_features = items_df_filtered[item_feature_cols] In\u00a0[84]: Copied! <pre>user_features.head()\n</pre> user_features.head() Out[84]: age_encoded income_encoded sex_encoded kids_flg_encoded uid 11047 2 2 1 1 15756 1 3 2 1 8852 1 2 2 0 21134 1 2 2 0 33752 5 2 2 0 <p>User, item interaction matrix</p> In\u00a0[85]: Copied! <pre># user / item interactions\npairs = interactions_df[['uid', 'iid']].values # user uid interacted with iid\npairs\n</pre> # user / item interactions pairs = interactions_df[['uid', 'iid']].values # user uid interacted with iid pairs Out[85]: <pre>array([[ 7234,  3500],\n       [28657,   594],\n       [  208,  3084],\n       ...,\n       [17861,  4980],\n       [24986,  2581],\n       [15738,  6017]], dtype=int32)</pre> In\u00a0[86]: Copied! <pre># create a dictionary of features for \n# creates positive, negative feature samples\n\ntrain_ds = TupleDataset(user_pos_pairs=pairs,\n                        user_features=user_features,\n                        item_features=item_features)\n</pre> # create a dictionary of features for  # creates positive, negative feature samples  train_ds = TupleDataset(user_pos_pairs=pairs,                         user_features=user_features,                         item_features=item_features) In\u00a0[87]: Copied! <pre># example of data from TupleDataset\nimport pprint; pprint.pprint(train_ds[0])\n</pre> # example of data from TupleDataset import pprint; pprint.pprint(train_ds[0]) <pre>{'neg_features': {'age_rating_encoded': 2,\n                  'content_type_encoded': 1,\n                  'countries_encoded': 459,\n                  'directors_encoded': 1626,\n                  'for_kids_encoded': 2,\n                  'release_year_encoded': 87,\n                  'studios_encoded': 24},\n 'pos_features': {'age_rating_encoded': 0,\n                  'content_type_encoded': 0,\n                  'countries_encoded': 322,\n                  'directors_encoded': 1946,\n                  'for_kids_encoded': 2,\n                  'release_year_encoded': 83,\n                  'studios_encoded': 24},\n 'user_features': {'age_encoded': 2,\n                   'income_encoded': 3,\n                   'kids_flg_encoded': 0,\n                   'sex_encoded': 2}}\n</pre> <p>b) Create batched dataset</p> In\u00a0[88]: Copied! <pre>batch_size = 4\ndm = DssmDataModule(train_ds=train_ds,           # created dataset\n                    train_batch_size=batch_size) # size of group\n</pre> batch_size = 4 dm = DssmDataModule(train_ds=train_ds,           # created dataset                     train_batch_size=batch_size) # size of group <p>Sample of batch data</p> In\u00a0[105]: Copied! <pre># next batch data \n# next(iter(dm.train_dataloader())).keys()\nbatch = next(iter(dm.train_dataloader()))\npprint.pprint(batch)\n</pre> # next batch data  # next(iter(dm.train_dataloader())).keys() batch = next(iter(dm.train_dataloader())) pprint.pprint(batch) <pre>{'neg_features': {'age_rating_encoded': tensor([2, 2, 4, 0]),\n                  'content_type_encoded': tensor([1, 0, 0, 0]),\n                  'countries_encoded': tensor([294, 332, 294, 322]),\n                  'directors_encoded': tensor([ 618, 2630,  208, 3926]),\n                  'for_kids_encoded': tensor([2, 2, 2, 2]),\n                  'release_year_encoded': tensor([54, 90, 82, 73]),\n                  'studios_encoded': tensor([24, 24, 24, 24])},\n 'pos_features': {'age_rating_encoded': tensor([0, 1, 3, 2]),\n                  'content_type_encoded': tensor([0, 0, 0, 0]),\n                  'countries_encoded': tensor([322, 294, 323, 322]),\n                  'directors_encoded': tensor([1946, 1749,  371,  559]),\n                  'for_kids_encoded': tensor([2, 2, 2, 2]),\n                  'release_year_encoded': tensor([83, 84, 89, 86]),\n                  'studios_encoded': tensor([24, 24, 24, 24])},\n 'user_features': {'age_encoded': tensor([2, 2, 0, 1]),\n                   'income_encoded': tensor([3, 3, 2, 2]),\n                   'kids_flg_encoded': tensor([0, 0, 0, 0]),\n                   'sex_encoded': tensor([2, 2, 1, 2])}}\n</pre> <p>Set parameters for embedding matrices</p> In\u00a0[90]: Copied! <pre>N_FACTORS = 64  # number of factors in each embedding\nCAT_EMBEDDING_DIM = 16 \n\n# \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\u0445 \u0435\u0441\u0442\u044c \u0441\u0442\u043e\u043b\u0431\u0435\u0446 user_id/item_id, \u043f\u043e\u043c\u043d\u0438\u043c, \u0447\u0442\u043e \u043e\u043d \u043d\u0435 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0444\u0438\u0447\u0435\u0439 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f!\nITEM_MODEL_SHAPE = len(item_feature_cols) * CAT_EMBEDDING_DIM\nUSER_META_MODEL_SHAPE = len(user_feature_cols) * CAT_EMBEDDING_DIM\n\n# USER_INTERACTION_MODEL_SHAPE = (interactions_vec.shape[1], )\nprint(f\"N_FACTORS: {N_FACTORS}\")\nprint(f\"ITEM_MODEL_SHAPE: {ITEM_MODEL_SHAPE}\")\nprint(f\"USER_META_MODEL_SHAPE: {USER_META_MODEL_SHAPE}\")\n</pre> N_FACTORS = 64  # number of factors in each embedding CAT_EMBEDDING_DIM = 16   # \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\u0445 \u0435\u0441\u0442\u044c \u0441\u0442\u043e\u043b\u0431\u0435\u0446 user_id/item_id, \u043f\u043e\u043c\u043d\u0438\u043c, \u0447\u0442\u043e \u043e\u043d \u043d\u0435 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0444\u0438\u0447\u0435\u0439 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f! ITEM_MODEL_SHAPE = len(item_feature_cols) * CAT_EMBEDDING_DIM USER_META_MODEL_SHAPE = len(user_feature_cols) * CAT_EMBEDDING_DIM  # USER_INTERACTION_MODEL_SHAPE = (interactions_vec.shape[1], ) print(f\"N_FACTORS: {N_FACTORS}\") print(f\"ITEM_MODEL_SHAPE: {ITEM_MODEL_SHAPE}\") print(f\"USER_META_MODEL_SHAPE: {USER_META_MODEL_SHAPE}\") <pre>N_FACTORS: 64\nITEM_MODEL_SHAPE: 112\nUSER_META_MODEL_SHAPE: 64\n</pre> In\u00a0[91]: Copied! <pre>import torch.nn as nn\n\n# inividual tower part of model \nclass DssmTower(nn.Module):\n\n  def __init__(self,\n               feat_vocab_sizes: dict[str, int],\n               cat_feat_emb_dim: int,\n               hidden_dim: int,\n               out_dim: int):\n\n    super().__init__()\n\n    # embeddings for each unique value in user / item\n    self.embedding = nn.ModuleDict({\n        cat_feat: nn.Embedding(cat_feat_vocab_size, cat_feat_emb_dim) for cat_feat, cat_feat_vocab_size in feat_vocab_sizes.items()\n    })\n    self.features = list(feat_vocab_sizes.keys())\n    self.layer_1 = nn.Linear(cat_feat_emb_dim * len(feat_vocab_sizes), hidden_dim)\n    self.layer_2 = nn.Linear(hidden_dim, hidden_dim)\n    self.layer_3 = nn.Linear(hidden_dim, out_dim)\n\n  def forward(self, batch):\n\n    # concatenate all embeddings (they have same embedding dimenion)\n    embeddings = []\n    for feature in self.features:\n      feature_embedding = self.embedding[feature](batch[feature])\n      embeddings.append(feature_embedding)\n    embedding = torch.cat(embeddings, dim=1)\n\n    layer_1 = self.layer_1(embedding)\n    layer_2 = self.layer_2(layer_1)\n    layer_2 += layer_1\n    output = self.layer_3(layer_2)\n    return output\n</pre> import torch.nn as nn  # inividual tower part of model  class DssmTower(nn.Module):    def __init__(self,                feat_vocab_sizes: dict[str, int],                cat_feat_emb_dim: int,                hidden_dim: int,                out_dim: int):      super().__init__()      # embeddings for each unique value in user / item     self.embedding = nn.ModuleDict({         cat_feat: nn.Embedding(cat_feat_vocab_size, cat_feat_emb_dim) for cat_feat, cat_feat_vocab_size in feat_vocab_sizes.items()     })     self.features = list(feat_vocab_sizes.keys())     self.layer_1 = nn.Linear(cat_feat_emb_dim * len(feat_vocab_sizes), hidden_dim)     self.layer_2 = nn.Linear(hidden_dim, hidden_dim)     self.layer_3 = nn.Linear(hidden_dim, out_dim)    def forward(self, batch):      # concatenate all embeddings (they have same embedding dimenion)     embeddings = []     for feature in self.features:       feature_embedding = self.embedding[feature](batch[feature])       embeddings.append(feature_embedding)     embedding = torch.cat(embeddings, dim=1)      layer_1 = self.layer_1(embedding)     layer_2 = self.layer_2(layer_1)     layer_2 += layer_1     output = self.layer_3(layer_2)     return output  In\u00a0[92]: Copied! <pre># number of unique elements in column \nuser_feat_vocab_size = dict()\nfor col in user_features.columns:\n  user_feat_vocab_size[col] = user_features[col].nunique()\n\nuser_feat_vocab_size\n</pre> # number of unique elements in column  user_feat_vocab_size = dict() for col in user_features.columns:   user_feat_vocab_size[col] = user_features[col].nunique()  user_feat_vocab_size Out[92]: <pre>{'age_encoded': 7,\n 'income_encoded': 7,\n 'sex_encoded': 3,\n 'kids_flg_encoded': 2}</pre> In\u00a0[93]: Copied! <pre># number of unique elements in column \nitem_feat_vocab_size = dict()\nfor col in item_features.columns:\n  item_feat_vocab_size[col] = item_features[col].nunique()\n\nitem_feat_vocab_size\n</pre> # number of unique elements in column  item_feat_vocab_size = dict() for col in item_features.columns:   item_feat_vocab_size[col] = item_features[col].nunique()  item_feat_vocab_size Out[93]: <pre>{'content_type_encoded': 2,\n 'release_year_encoded': 92,\n 'for_kids_encoded': 3,\n 'age_rating_encoded': 6,\n 'studios_encoded': 28,\n 'countries_encoded': 535,\n 'directors_encoded': 4041}</pre> In\u00a0[94]: Copied! <pre>from pytorch_lightning import LightningModule\nfrom torch.optim import Adam\n\n# Lightning Module\nclass DssmLitModule(LightningModule):\n    \n  def __init__(self, user_tower: DssmTower, \n               item_tower: DssmTower, \n               optim_hparams: dict):\n      \n    super().__init__()\n    self.user_tower = user_tower  # neural network for users\n    self.item_tower = item_tower  # neural network for items\n    self.optim_hparams = optim_hparams\n    self.loss = torch.nn.BCEWithLogitsLoss()\n\n  def training_step(self, batch, batch_idx):\n\n    # extract feature data from dictionary (dictionary format)\n    user_feats = batch['user_features'] # feature : tensor values of batch\n    pos_feats = batch['pos_features'] # ''\n    neg_feats = batch['neg_features'] # ''\n      \n    # for each user/item input into each tower &amp; activate (forward method)\n    # for each batch item tower, one logits item for usrfeat/posfeat/negfeat x batch number\n    user_embs = self.user_tower(user_feats) # (batch_size x out_dim)\n    pos_embs = self.item_tower(pos_feats) # (batch_size x out_dim)\n    neg_embs = self.item_tower(neg_feats) # (batch_size x out_dim)\n\n    # (batch_size x 2 x out_dim) -&gt; for both positive / negative samples\n    item_embs = torch.cat((pos_embs.unsqueeze(1), \n                           neg_embs.unsqueeze(1)), dim=1) \n\n    # dot scores for both positive &amp; negative samples\n    dot_scores = (user_embs.unsqueeze(1) @ item_embs.transpose(1, 2)).squeeze()  \n\n    # everything in second dimension is negative sample (item) (label = 0)\n    labels = torch.zeros_like(dot_scores) # (zeros of shape 2 x out_dims)\n    labels[:, 0] = 1 # everything in first dimension is positive sample (item) (label = 1)\n\n    loss = self.loss(dot_scores, labels)  # loss per batch\n    self.log('train_loss',\n             loss.item(), \n             on_epoch=True, \n             on_step=True, \n             prog_bar=True)\n\n    return loss\n\n  def configure_optimizers(self):\n    return Adam(self.parameters(), **self.optim_hparams)\n</pre> from pytorch_lightning import LightningModule from torch.optim import Adam  # Lightning Module class DssmLitModule(LightningModule):        def __init__(self, user_tower: DssmTower,                 item_tower: DssmTower,                 optim_hparams: dict):            super().__init__()     self.user_tower = user_tower  # neural network for users     self.item_tower = item_tower  # neural network for items     self.optim_hparams = optim_hparams     self.loss = torch.nn.BCEWithLogitsLoss()    def training_step(self, batch, batch_idx):      # extract feature data from dictionary (dictionary format)     user_feats = batch['user_features'] # feature : tensor values of batch     pos_feats = batch['pos_features'] # ''     neg_feats = batch['neg_features'] # ''            # for each user/item input into each tower &amp; activate (forward method)     # for each batch item tower, one logits item for usrfeat/posfeat/negfeat x batch number     user_embs = self.user_tower(user_feats) # (batch_size x out_dim)     pos_embs = self.item_tower(pos_feats) # (batch_size x out_dim)     neg_embs = self.item_tower(neg_feats) # (batch_size x out_dim)      # (batch_size x 2 x out_dim) -&gt; for both positive / negative samples     item_embs = torch.cat((pos_embs.unsqueeze(1),                             neg_embs.unsqueeze(1)), dim=1)       # dot scores for both positive &amp; negative samples     dot_scores = (user_embs.unsqueeze(1) @ item_embs.transpose(1, 2)).squeeze()        # everything in second dimension is negative sample (item) (label = 0)     labels = torch.zeros_like(dot_scores) # (zeros of shape 2 x out_dims)     labels[:, 0] = 1 # everything in first dimension is positive sample (item) (label = 1)      loss = self.loss(dot_scores, labels)  # loss per batch     self.log('train_loss',              loss.item(),               on_epoch=True,               on_step=True,               prog_bar=True)      return loss    def configure_optimizers(self):     return Adam(self.parameters(), **self.optim_hparams) <p>Define the tower parts of the neural network. In our model, we will be training two towers, one for user embedding features, and another for the item embedding.</p> In\u00a0[95]: Copied! <pre># user tower of nn (input into dssm_module)\nuser_tower = DssmTower(\n                        feat_vocab_sizes=user_feat_vocab_size,\n                        cat_feat_emb_dim=CAT_EMBEDDING_DIM,\n                        hidden_dim=64,\n                        out_dim=64 # output dimension for each tower\n                        )\n\n# item tower segment of nn (input into dssm_module)\nitem_tower = DssmTower(\n                        feat_vocab_sizes=item_feat_vocab_size,\n                        cat_feat_emb_dim=CAT_EMBEDDING_DIM,\n                        hidden_dim=64,\n                        out_dim=64 # output dimension for each tower\n                        )\n\n# entire network using Lightning Module\ndssm_module = DssmLitModule(user_tower, \n                            item_tower, \n                            optim_hparams={'lr': 1e-3})\n</pre> # user tower of nn (input into dssm_module) user_tower = DssmTower(                         feat_vocab_sizes=user_feat_vocab_size,                         cat_feat_emb_dim=CAT_EMBEDDING_DIM,                         hidden_dim=64,                         out_dim=64 # output dimension for each tower                         )  # item tower segment of nn (input into dssm_module) item_tower = DssmTower(                         feat_vocab_sizes=item_feat_vocab_size,                         cat_feat_emb_dim=CAT_EMBEDDING_DIM,                         hidden_dim=64,                         out_dim=64 # output dimension for each tower                         )  # entire network using Lightning Module dssm_module = DssmLitModule(user_tower,                              item_tower,                              optim_hparams={'lr': 1e-3}) In\u00a0[97]: Copied! <pre># (user_tower) 16 x 4 -&gt; 64\n# (item_tower) 16 x 7 -&gt; 112 features\n</pre> # (user_tower) 16 x 4 -&gt; 64 # (item_tower) 16 x 7 -&gt; 112 features In\u00a0[99]: Copied! <pre># one batch prediction\nwith torch.no_grad():\n    print(dssm_module.training_step(batch, 0))\n\n# # dot scores for batch 4\n# tensor([[-0.1367, -0.7055],\n#         [ 0.8072,  1.3774],\n#         [ 0.4891,  0.5485],\n#         [ 1.1119,  1.1324]])\n\n# # labels \n# tensor([[1., 0.],\n#         [1., 0.],\n#         [1., 0.],\n#         [1., 0.]])\n\n# # loss per batch\n# tensor(0.7894)\n</pre> # one batch prediction with torch.no_grad():     print(dssm_module.training_step(batch, 0))  # # dot scores for batch 4 # tensor([[-0.1367, -0.7055], #         [ 0.8072,  1.3774], #         [ 0.4891,  0.5485], #         [ 1.1119,  1.1324]])  # # labels  # tensor([[1., 0.], #         [1., 0.], #         [1., 0.], #         [1., 0.]])  # # loss per batch # tensor(0.7894) <pre>tensor(1.1566)\n</pre> In\u00a0[100]: Copied! <pre>from pytorch_lightning import Trainer\n\n# trainer = Trainer(max_epochs=1, enable_checkpointing=False)\n# trainer.fit(dssm_module, dm)\n</pre> from pytorch_lightning import Trainer  # trainer = Trainer(max_epochs=1, enable_checkpointing=False) # trainer.fit(dssm_module, dm) In\u00a0[101]: Copied! <pre># select user\n\n# \u0431\u0435\u0440\u0435\u043c \u0440\u0430\u043d\u0434\u043e\u043c\u043d\u043e\u0433\u043e \u044e\u0437\u0435\u0440\u0430\nrand_uid = np.random.choice(list(user_features.index))\n\n# \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0444\u0438\u0447\u0438 \u044e\u0437\u0435\u0440\u0430 \u0438 \u0432\u0435\u043a\u0442\u043e\u0440 \u0435\u0433\u043e \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432 \u0430\u0439\u0442\u0435\u043c\u043e\u0432\nrand_uid_feats = user_features.loc[rand_uid].to_dict()\n\n# select item\n\n# \u0431\u0435\u0440\u0435\u043c \u0440\u0430\u043d\u0434\u043e\u043c\u043d\u044b\u0439 \u0430\u0439\u0442\u0435\u043c\nrand_iid = np.random.choice(list(item_features.index))\n# \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0444\u0438\u0447\u0438 \u0430\u0439\u0442\u0435\u043c\u0430\nrand_iid_feats = item_features.loc[rand_iid].to_dict()\n\nprint('random user',rand_uid)\nprint('random user features')\npprint.pprint(rand_uid_feats)\nprint('')\nprint('random item',rand_iid)\nprint('random item features')\npprint.pprint(rand_iid_feats)\n</pre> # select user  # \u0431\u0435\u0440\u0435\u043c \u0440\u0430\u043d\u0434\u043e\u043c\u043d\u043e\u0433\u043e \u044e\u0437\u0435\u0440\u0430 rand_uid = np.random.choice(list(user_features.index))  # \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0444\u0438\u0447\u0438 \u044e\u0437\u0435\u0440\u0430 \u0438 \u0432\u0435\u043a\u0442\u043e\u0440 \u0435\u0433\u043e \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432 \u0430\u0439\u0442\u0435\u043c\u043e\u0432 rand_uid_feats = user_features.loc[rand_uid].to_dict()  # select item  # \u0431\u0435\u0440\u0435\u043c \u0440\u0430\u043d\u0434\u043e\u043c\u043d\u044b\u0439 \u0430\u0439\u0442\u0435\u043c rand_iid = np.random.choice(list(item_features.index)) # \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0444\u0438\u0447\u0438 \u0430\u0439\u0442\u0435\u043c\u0430 rand_iid_feats = item_features.loc[rand_iid].to_dict()  print('random user',rand_uid) print('random user features') pprint.pprint(rand_uid_feats) print('') print('random item',rand_iid) print('random item features') pprint.pprint(rand_iid_feats) <pre>random user 40981\nrandom user features\n{'age_encoded': 1, 'income_encoded': 3, 'kids_flg_encoded': 0, 'sex_encoded': 1}\n\nrandom item 5654\nrandom item features\n{'age_rating_encoded': 3,\n 'content_type_encoded': 0,\n 'countries_encoded': 322,\n 'directors_encoded': 3012,\n 'for_kids_encoded': 2,\n 'release_year_encoded': 70,\n 'studios_encoded': 24}\n</pre> In\u00a0[102]: Copied! <pre># create a tensor\nfor key in rand_uid_feats:\n    rand_uid_feats[key] = torch.tensor([rand_uid_feats[key]])\nfor key in rand_iid_feats:\n    rand_iid_feats[key] = torch.tensor([rand_iid_feats[key]])\n\npprint.pprint(rand_uid_feats)\n</pre> # create a tensor for key in rand_uid_feats:     rand_uid_feats[key] = torch.tensor([rand_uid_feats[key]]) for key in rand_iid_feats:     rand_iid_feats[key] = torch.tensor([rand_iid_feats[key]])  pprint.pprint(rand_uid_feats) <pre>{'age_encoded': tensor([1]),\n 'income_encoded': tensor([3]),\n 'kids_flg_encoded': tensor([0]),\n 'sex_encoded': tensor([1])}\n</pre> <p>Using the tower classes, calculate the forward pass prediction using existing model weights</p> In\u00a0[107]: Copied! <pre>dssm_module.user_tower(rand_uid_feats).size()\n</pre> dssm_module.user_tower(rand_uid_feats).size() Out[107]: <pre>torch.Size([1, 64])</pre> In\u00a0[109]: Copied! <pre>dssm_module.item_tower(rand_iid_feats).size()\n</pre> dssm_module.item_tower(rand_iid_feats).size() Out[109]: <pre>torch.Size([1, 64])</pre> <p>Calculate the dot scores for user/item combination, this can be done a couple of ways</p> In\u00a0[110]: Copied! <pre>(dssm_module.user_tower(rand_uid_feats) * dssm_module.item_tower(rand_iid_feats)).sum()\n</pre> (dssm_module.user_tower(rand_uid_feats) * dssm_module.item_tower(rand_iid_feats)).sum() Out[110]: <pre>tensor(-2.4099, grad_fn=&lt;SumBackward0&gt;)</pre> In\u00a0[113]: Copied! <pre>dssm_module.user_tower(rand_uid_feats) @ dssm_module.item_tower(rand_iid_feats).transpose(0,1)\n</pre> dssm_module.user_tower(rand_uid_feats) @ dssm_module.item_tower(rand_iid_feats).transpose(0,1) Out[113]: <pre>tensor([[-2.4099]], grad_fn=&lt;MmBackward0&gt;)</pre>"},{"location":"portfolio/course_recsys/dssm-towers.html#deep-similarity-similarity-model","title":"Deep Similarity Similarity Model\u00b6","text":""},{"location":"portfolio/course_recsys/dssm-towers.html#1-background","title":"1 | Background\u00b6","text":"<p>Notebook contents:</p> <ul> <li><p>In this notebook, we'll implement a simple DSSM neural network model for recommendation systems.</p> </li> <li><p>We will be using embeddings for each unique user and item, group them together and pass them into linear layers, which will be our forward method for each individual 'tower'. The value in these embeddings is that they can be used in further models as features to create a ranker model.</p> </li> <li><p>Outputs of both 'towers' are multiplied together using a scalar product, to get the scores, similar to how it is done matrix factorisation approaches. These scores can be evaluated for all items and rearranged to get the top k recommendations for each user, based on the score value (section 9)</p> </li> <li><p>Training a neural network, we will be training the embedding layers in the context of a binary classification problem, in which we label the positive samples (items) as 1 and negative samples as 0. So we train a model that will be able to differentiate between positive and negative samples.</p> </li> </ul>"},{"location":"portfolio/course_recsys/dssm-towers.html#2-read-dataset","title":"2 | Read Dataset\u00b6","text":"<ul> <li>The dataset contains three data dataframes, user features, item features and the interaction details between user and item</li> <li>Items in this dataset are movies and serials</li> </ul>"},{"location":"portfolio/course_recsys/dssm-towers.html#3-filter-interactions","title":"3 | Filter Interactions\u00b6","text":"<p>We need to filter items that have low interaction counts, as they probably won't be useful</p>"},{"location":"portfolio/course_recsys/dssm-towers.html#4-categorical-encoding","title":"4 | Categorical Encoding\u00b6","text":"<p>We will use standard encoding for item &amp; user features</p> <ul> <li><code>item_cat_feats</code> : item features to be used in model</li> <li><code>user_cat_feats</code> : user features to be used in model</li> </ul>"},{"location":"portfolio/course_recsys/dssm-towers.html#5-normalise-ids","title":"5 | Normalise IDs\u00b6","text":"<p><code>user_id</code> &amp; <code>item_id</code> are just numbers, lets create a mapper for new ids that start from 0</p>"},{"location":"portfolio/course_recsys/dssm-towers.html#6-prepare-torch-dataset","title":"6 | Prepare Torch Dataset\u00b6","text":"<p>Using the interactions data, each row will have (user_id,item_id), from these ids, the user features for this user, and the film features they interacted with, as well as a random item features are returned</p>"},{"location":"portfolio/course_recsys/dssm-towers.html#7-select-features","title":"7 | Select Features\u00b6","text":"<p>Select a subset of user and item dataframes, selecting only the encoded columns</p>"},{"location":"portfolio/course_recsys/dssm-towers.html#8-group-useritem-data-into-dataset","title":"8 | Group User/Item Data into Dataset\u00b6","text":"<p>a) Dataset containing grouped positive item features, negative item festures and its user features</p>"},{"location":"portfolio/course_recsys/dssm-towers.html#9-train-model","title":"9 | Train model\u00b6","text":"<p>In this example, we will train only one epoch, just as an example</p>"},{"location":"portfolio/course_recsys/dssm-towers.html#10-inference-example","title":"10 | Inference Example\u00b6","text":"<ul> <li>Once we have a trained model, the class <code>dssm_module</code> contains the updated model weights</li> <li>We can for a particular user and film combination, evaluate the score. We could repeat this process for all the items, and find he top k films, which we can recommend the user</li> </ul>"},{"location":"portfolio/course_recsys/prob_hybrid.html","title":"Prob hybrid","text":"In\u00a0[1]: Copied! <pre>!pip install implicit --quiet\n!pip install catboost --quiet\n</pre> !pip install implicit --quiet !pip install catboost --quiet <pre>   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8.9/8.9 MB 58.5 MB/s eta 0:00:00\r\n</pre> In\u00a0[2]: Copied! <pre>import datetime\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport scipy.sparse as sparse\nfrom catboost import CatBoostClassifier\nimport implicit\nfrom implicit.bpr import BayesianPersonalizedRanking as BPR\nimport warnings; warnings.filterwarnings('ignore')\n</pre> import datetime import numpy as np import pandas as pd from tqdm.auto import tqdm import matplotlib.pyplot as plt from sklearn.utils import shuffle from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score import scipy.sparse as sparse from catboost import CatBoostClassifier import implicit from implicit.bpr import BayesianPersonalizedRanking as BPR import warnings; warnings.filterwarnings('ignore') In\u00a0[3]: Copied! <pre>def recall(df: pd.DataFrame, pred_col='preds', true_col='item_id', k=30) -&gt; float:\n    recall_values = []\n    for _, row in df.iterrows():\n      num_relevant = len(set(row[true_col]) &amp; set(row[pred_col][:k]))\n      num_true = len(row[true_col])\n      recall_values.append(num_relevant / num_true)\n    return np.mean(recall_values)\n\ndef precision(df: pd.DataFrame, pred_col='preds', true_col='item_id', k=30) -&gt; float:\n    precision_values = []\n    for _, row in df.iterrows():\n      num_relevant = len(set(row[true_col]) &amp; set(row[pred_col][:k]))\n      num_true = min(k, len(row[true_col]))\n      precision_values.append(num_relevant / num_true)\n    return np.mean(precision_values)\n\ndef mrr(df: pd.DataFrame, pred_col='preds', true_col='item_id', k=30) -&gt; float:\n    mrr_values = []\n    for _, row in df.iterrows():\n      intersection = set(row[true_col]) &amp; set(row[pred_col][:k])\n      user_mrr = 0\n      if len(intersection) &gt; 0:\n          for item in intersection:\n              user_mrr = max(user_mrr, 1 / (row[pred_col].index(item) + 1))\n      mrr_values.append(user_mrr)\n    return np.mean(mrr_values)\n</pre> def recall(df: pd.DataFrame, pred_col='preds', true_col='item_id', k=30) -&gt; float:     recall_values = []     for _, row in df.iterrows():       num_relevant = len(set(row[true_col]) &amp; set(row[pred_col][:k]))       num_true = len(row[true_col])       recall_values.append(num_relevant / num_true)     return np.mean(recall_values)  def precision(df: pd.DataFrame, pred_col='preds', true_col='item_id', k=30) -&gt; float:     precision_values = []     for _, row in df.iterrows():       num_relevant = len(set(row[true_col]) &amp; set(row[pred_col][:k]))       num_true = min(k, len(row[true_col]))       precision_values.append(num_relevant / num_true)     return np.mean(precision_values)  def mrr(df: pd.DataFrame, pred_col='preds', true_col='item_id', k=30) -&gt; float:     mrr_values = []     for _, row in df.iterrows():       intersection = set(row[true_col]) &amp; set(row[pred_col][:k])       user_mrr = 0       if len(intersection) &gt; 0:           for item in intersection:               user_mrr = max(user_mrr, 1 / (row[pred_col].index(item) + 1))       mrr_values.append(user_mrr)     return np.mean(mrr_values) In\u00a0[4]: Copied! <pre>class readData:\n\n    def __init__(self):\n        self.read_data()\n\n    def read_data(self):\n        self.interactions = pd.read_csv(\"/kaggle/input/kion-dataset/interactions.csv\")\n        self.items = pd.read_csv(\"/kaggle/input/kion-dataset/items.csv\")\n        self.users = pd.read_csv(\"/kaggle/input/kion-dataset/users.csv\")\n    \n        # convert the column [last_watch_dt] into datetime\n        self.interactions['last_watch_dt'] = pd.to_datetime(self.interactions['last_watch_dt']).map(lambda x: x.date())\n\n        print(f\"\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u044e\u0437\u0435\u0440\u043e\u0432 \u0432 interactions: {self.interactions['user_id'].nunique()}\")\n        print(f\"\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0430\u0439\u0442\u0435\u043c\u043e\u0432 \u0432 interactions: {self.interactions['item_id'].nunique()}\")\n\n    def show_interactions(self):\n        return self.interactions.head()\n\n    def show_items(self):\n        return self.items.head()\n\n    def show_user(self):\n        return self.users.head()\n</pre> class readData:      def __init__(self):         self.read_data()      def read_data(self):         self.interactions = pd.read_csv(\"/kaggle/input/kion-dataset/interactions.csv\")         self.items = pd.read_csv(\"/kaggle/input/kion-dataset/items.csv\")         self.users = pd.read_csv(\"/kaggle/input/kion-dataset/users.csv\")              # convert the column [last_watch_dt] into datetime         self.interactions['last_watch_dt'] = pd.to_datetime(self.interactions['last_watch_dt']).map(lambda x: x.date())          print(f\"\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u044e\u0437\u0435\u0440\u043e\u0432 \u0432 interactions: {self.interactions['user_id'].nunique()}\")         print(f\"\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0430\u0439\u0442\u0435\u043c\u043e\u0432 \u0432 interactions: {self.interactions['item_id'].nunique()}\")      def show_interactions(self):         return self.interactions.head()      def show_items(self):         return self.items.head()      def show_user(self):         return self.users.head() In\u00a0[5]: Copied! <pre>class dataPreprocess(readData):\n\n    def __init__(self):\n        super().__init__()\n        self.filter_data()\n        self.tts()\n\n    def filter_data(self):\n\n        interactions = self.interactions\n        interactions = interactions[interactions['total_dur'] &gt;= 300]\n        user_interactions_count = interactions.groupby('user_id')[['item_id']].count().reset_index()\n        filtered_users = user_interactions_count[user_interactions_count['item_id'] &gt;= 10][['user_id']]\n        interactions = filtered_users.merge(interactions, how='left')\n        self.interactions = interactions\n\n    def tts(self):\n\n        interactions = self.interactions\n\n        max_date = interactions['last_watch_dt'].max()\n        min_date = interactions['last_watch_dt'].min()\n\n        print(f\"min \u0434\u0430\u0442\u0430 \u0432 interactions: {min_date}\")\n        print(f\"max \u0434\u0430\u0442\u0430 \u0432 interactions: {max_date}\")\n\n        # global test dataset starting time (7 days)\n        test_threshold = max_date - pd.Timedelta(days=7)\n\n        # validation dataset starting time (2 months)\n        val_threshold = test_threshold - pd.Timedelta(days=60) \n\n        self.test = interactions[(interactions['last_watch_dt'] &gt;= test_threshold)]\n        train_val = interactions[(interactions['last_watch_dt'] &lt; test_threshold)]\n        self.val = train_val[(train_val['last_watch_dt'] &gt;= val_threshold)]\n        self.train = train_val[(train_val['last_watch_dt'] &lt; val_threshold)]\n\n        print('Data split into subsets!')\n        print(f\"train: {self.train.shape}\")\n        print(f\"val: {self.val.shape}\")\n        print(f\"test: {self.test.shape}\")\n\n    def show_subset(self,subset='train'):\n        if(subset == 'train'):\n            return self.train\n        elif(subset == 'val'):\n            return self.val\n        elif(subset == 'test'):\n            return self.test \n\n\nkion = dataPreprocess()\nkion.show_subset('train')\n</pre> class dataPreprocess(readData):      def __init__(self):         super().__init__()         self.filter_data()         self.tts()      def filter_data(self):          interactions = self.interactions         interactions = interactions[interactions['total_dur'] &gt;= 300]         user_interactions_count = interactions.groupby('user_id')[['item_id']].count().reset_index()         filtered_users = user_interactions_count[user_interactions_count['item_id'] &gt;= 10][['user_id']]         interactions = filtered_users.merge(interactions, how='left')         self.interactions = interactions      def tts(self):          interactions = self.interactions          max_date = interactions['last_watch_dt'].max()         min_date = interactions['last_watch_dt'].min()          print(f\"min \u0434\u0430\u0442\u0430 \u0432 interactions: {min_date}\")         print(f\"max \u0434\u0430\u0442\u0430 \u0432 interactions: {max_date}\")          # global test dataset starting time (7 days)         test_threshold = max_date - pd.Timedelta(days=7)          # validation dataset starting time (2 months)         val_threshold = test_threshold - pd.Timedelta(days=60)           self.test = interactions[(interactions['last_watch_dt'] &gt;= test_threshold)]         train_val = interactions[(interactions['last_watch_dt'] &lt; test_threshold)]         self.val = train_val[(train_val['last_watch_dt'] &gt;= val_threshold)]         self.train = train_val[(train_val['last_watch_dt'] &lt; val_threshold)]          print('Data split into subsets!')         print(f\"train: {self.train.shape}\")         print(f\"val: {self.val.shape}\")         print(f\"test: {self.test.shape}\")      def show_subset(self,subset='train'):         if(subset == 'train'):             return self.train         elif(subset == 'val'):             return self.val         elif(subset == 'test'):             return self.test    kion = dataPreprocess() kion.show_subset('train') <pre>\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u044e\u0437\u0435\u0440\u043e\u0432 \u0432 interactions: 962179\n\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0430\u0439\u0442\u0435\u043c\u043e\u0432 \u0432 interactions: 15706\nmin \u0434\u0430\u0442\u0430 \u0432 interactions: 2021-03-13\nmax \u0434\u0430\u0442\u0430 \u0432 interactions: 2021-08-22\nData split into subsets!\ntrain: (894491, 5)\nval: (1253718, 5)\ntest: (173795, 5)\n</pre> Out[5]: user_id item_id last_watch_dt total_dur watched_pct 0 2 7571 2021-05-20 6151 100.0 1 2 3541 2021-06-04 4320 83.0 2 2 15266 2021-06-01 5422 100.0 4 2 12841 2021-06-09 8152 100.0 6 2 4475 2021-05-30 7029 100.0 ... ... ... ... ... ... 2321985 1097516 758 2021-04-20 796 13.0 2321986 1097516 14470 2021-04-24 553 7.0 2321989 1097516 1331 2021-05-17 2563 39.0 2321990 1097516 8454 2021-04-22 6169 100.0 2321991 1097516 15421 2021-04-23 6709 100.0 <p>894491 rows \u00d7 5 columns</p> In\u00a0[6]: Copied! <pre>class genCandidate:\n\n    def __init__(self,\n                train:pd.DataFrame,\n                val:pd.DataFrame,\n                test:pd.DataFrame):\n        self.train = train\n        self.val = val\n        self.test = test\n\n    def createRatingMatrix(self):\n\n        # train model on [train]\n        users_id = list(np.sort(self.train.user_id.unique()))\n        items_train = list(self.train.item_id.unique())\n        ratings_train = list(self.train.watched_pct)\n\n        # \u041a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c ids \n        self.rows_train = self.train.user_id.astype('category').cat.codes\n        self.cols_train = self.train.item_id.astype('category').cat.codes\n\n        # create sparse rating matrix (watched percentage [watched_pct])\n        self.rating_matrix = sparse.csr_matrix((ratings_train, (self.rows_train,\n                                                                self.cols_train)), \n                                         shape=(len(users_id), len(items_train)))\n    def decompose(self):\n\n        # \u041c\u043e\u0434\u0435\u043b\u044c \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u044b\n        algo = BPR(factors=50, \n                regularization=0.01, \n                iterations=50, \n                use_gpu=False)\n        algo.fit((self.rating_matrix).astype('double'))\n\n        # user and item matrix\n        self.user_vecs = algo.user_factors\n        self.item_vecs = algo.item_factors\n\n    # BPR implicit prediction \n    def predict(self,k=10):\n    \n        \"\"\"\n    \n        Helper function for matrix factorisation prediction\n    \n        \"\"\"\n\n        user_vecs = self.user_vecs\n        item_vecs = self.item_vecs\n    \n        id2user = dict(zip(self.rows_train, self.train.user_id))\n        id2item = dict(zip(self.cols_train, self.train.item_id))\n        scores = user_vecs.dot(item_vecs.T)\n\n        ind_part = np.argpartition(scores, -k + 1)[:, -k:].copy()\n        scores_not_sorted = np.take_along_axis(scores, ind_part, axis=1)\n        ind_sorted = np.argsort(scores_not_sorted, axis=1)\n        indices = np.take_along_axis(ind_part, ind_sorted, axis=1)\n        indices = np.flip(indices, 1)\n        preds = pd.DataFrame({\n            'user_id': range(user_vecs.shape[0]),\n            'preds': indices.tolist(),\n            })\n        preds['user_id'] = preds['user_id'].map(id2user)\n        preds['preds'] = preds['preds'].map(lambda inds: [id2item[i] for i in inds])\n\n        # prediction scores for train \n        self.preds = preds\n\n    def create_candidates(self,k=10,subset='valid'):\n\n        # films watched in [val] dataset\n        if(subset == 'valid'):\n            self.user_history = self.val.groupby('user_id')[['item_id']].agg(lambda x: list(x))\n        elif(subset == 'test'):\n            self.user_history = self.test.groupby('user_id')[['item_id']].agg(lambda x: list(x))\n\n        self.predict(k=k)\n        pred_bpr = self.user_history.merge(self.preds, how='left', on='user_id')\n        pred_bpr = pred_bpr.dropna(subset=['preds'])\n\n        print('recall@k ',round(recall(pred_bpr),2))\n        print('precision@k ',round(precision(pred_bpr),2))\n        print('mrr@k ',round(mrr(pred_bpr),2))\n\n        candidates = pred_bpr[['user_id', 'preds']]\n        candidates = candidates.explode('preds').rename(columns={'preds': 'item_id'})\n        candidates['rank'] = candidates.groupby('user_id').cumcount() + 1\n        self.candidates = candidates\n        \n\ncand = genCandidate(train=kion.train,\n                    val=kion.val,\n                    test=kion.test)\ncand.createRatingMatrix()\ncand.decompose()\n</pre> class genCandidate:      def __init__(self,                 train:pd.DataFrame,                 val:pd.DataFrame,                 test:pd.DataFrame):         self.train = train         self.val = val         self.test = test      def createRatingMatrix(self):          # train model on [train]         users_id = list(np.sort(self.train.user_id.unique()))         items_train = list(self.train.item_id.unique())         ratings_train = list(self.train.watched_pct)          # \u041a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c ids          self.rows_train = self.train.user_id.astype('category').cat.codes         self.cols_train = self.train.item_id.astype('category').cat.codes          # create sparse rating matrix (watched percentage [watched_pct])         self.rating_matrix = sparse.csr_matrix((ratings_train, (self.rows_train,                                                                 self.cols_train)),                                           shape=(len(users_id), len(items_train)))     def decompose(self):          # \u041c\u043e\u0434\u0435\u043b\u044c \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u044b         algo = BPR(factors=50,                  regularization=0.01,                  iterations=50,                  use_gpu=False)         algo.fit((self.rating_matrix).astype('double'))          # user and item matrix         self.user_vecs = algo.user_factors         self.item_vecs = algo.item_factors      # BPR implicit prediction      def predict(self,k=10):              \"\"\"              Helper function for matrix factorisation prediction              \"\"\"          user_vecs = self.user_vecs         item_vecs = self.item_vecs              id2user = dict(zip(self.rows_train, self.train.user_id))         id2item = dict(zip(self.cols_train, self.train.item_id))         scores = user_vecs.dot(item_vecs.T)          ind_part = np.argpartition(scores, -k + 1)[:, -k:].copy()         scores_not_sorted = np.take_along_axis(scores, ind_part, axis=1)         ind_sorted = np.argsort(scores_not_sorted, axis=1)         indices = np.take_along_axis(ind_part, ind_sorted, axis=1)         indices = np.flip(indices, 1)         preds = pd.DataFrame({             'user_id': range(user_vecs.shape[0]),             'preds': indices.tolist(),             })         preds['user_id'] = preds['user_id'].map(id2user)         preds['preds'] = preds['preds'].map(lambda inds: [id2item[i] for i in inds])          # prediction scores for train          self.preds = preds      def create_candidates(self,k=10,subset='valid'):          # films watched in [val] dataset         if(subset == 'valid'):             self.user_history = self.val.groupby('user_id')[['item_id']].agg(lambda x: list(x))         elif(subset == 'test'):             self.user_history = self.test.groupby('user_id')[['item_id']].agg(lambda x: list(x))          self.predict(k=k)         pred_bpr = self.user_history.merge(self.preds, how='left', on='user_id')         pred_bpr = pred_bpr.dropna(subset=['preds'])          print('recall@k ',round(recall(pred_bpr),2))         print('precision@k ',round(precision(pred_bpr),2))         print('mrr@k ',round(mrr(pred_bpr),2))          candidates = pred_bpr[['user_id', 'preds']]         candidates = candidates.explode('preds').rename(columns={'preds': 'item_id'})         candidates['rank'] = candidates.groupby('user_id').cumcount() + 1         self.candidates = candidates           cand = genCandidate(train=kion.train,                     val=kion.val,                     test=kion.test) cand.createRatingMatrix() cand.decompose() In\u00a0[7]: Copied! <pre># \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442 \u043e\u0442\n# \u0434\u0435\u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u0438 train\ncand.rating_matrix\n</pre> # \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442 \u043e\u0442 # \u0434\u0435\u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u0438 train cand.rating_matrix Out[7]: <pre>&lt;72383x11373 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n\twith 894491 stored elements in Compressed Sparse Row format&gt;</pre> In\u00a0[8]: Copied! <pre># \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0434\u043b\u044f bpr (factors=50)\ncand.user_vecs\n</pre> # \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0434\u043b\u044f bpr (factors=50) cand.user_vecs Out[8]: <pre>array([[ 0.12379745, -0.15161823,  0.21322107, ...,  0.30594668,\n         0.03928256,  1.        ],\n       [-0.10700278,  0.20008633, -0.08893105, ..., -0.14906016,\n        -0.05412726,  1.        ],\n       [-0.06540288,  0.1305269 , -0.03879986, ...,  0.10091428,\n        -0.11864812,  1.        ],\n       ...,\n       [-0.05447918,  0.1625216 , -0.00793439, ...,  0.17104897,\n         0.5004299 ,  1.        ],\n       [-0.07643098, -0.09024178,  0.1201185 , ...,  0.29327735,\n        -0.375865  ,  1.        ],\n       [-0.08880109,  0.05362898, -0.01743633, ...,  0.267542  ,\n         0.14116102,  1.        ]], dtype=float32)</pre> <ul> <li>\u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c k=100 \u0434\u043b\u044f \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0431\u044b\u0431\u043e\u0440\u043a\u0438 \u0432\u0442\u043e\u0440\u043e\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f</li> <li>\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432 \u0434\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439, \u044d\u0442\u043e \u0444\u0438\u043b\u044c\u043c\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u044c \u0441\u0447\u0438\u0442\u0430\u0435\u0442 \u0447\u0442\u043e \u043e\u043d \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u043b, \u043c\u044b \u044d\u0442\u043e \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u0438 \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0430 \u043d\u0430 \u043f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u044b\u0435 \u0438 \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438</li> </ul> In\u00a0[9]: Copied! <pre># \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 2\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f (val)\ncand.create_candidates(k=100)\n</pre> # \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 2\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f (val) cand.create_candidates(k=100) <pre>recall@k  0.12\nprecision@k  0.12\nmrr@k  0.14\n</pre> In\u00a0[10]: Copied! <pre># \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c 2\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f (\u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440)\n# \u0426\u0435\u043b\u044c \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430 : \u043f\u0440\u0435\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u043c\u043e\u0442\u0440\u0435\u043b \u043a\u043b\u0438\u0435\u043d\u0442\nclass catClassifier:\n\n    def __init__(self,val,candidates,users,items):\n        self.val = val\n        self.candidates = candidates\n        self.users = users\n        self.items = items\n\n    def gen_data(self):\n        \n        # positive candidates\n        pos = self.candidates.merge(self.val,\n                               on=['user_id', 'item_id'],\n                               how='inner')\n        pos['target'] = 1\n        self.pos = pos\n        print('number of positive samples',pos.shape)\n\n        # \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u043e\u043d\u0438 \u043d\u0435 \u0441\u043c\u043e\u0442\u0440\u0435\u043b\u0438 \u0432 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\n        neg = self.candidates.set_index(['user_id', 'item_id'])\\\n                .join(self.val.set_index(['user_id', 'item_id']))\n\n        neg = neg[neg['watched_pct'].isnull()].reset_index()\n        # print(neg.shape)\n        neg = neg.sample(frac=0.07)\n        print('number of negative samples',neg.shape)\n        neg['target'] = 0\n        self.neg = neg\n\n    def create_subsets(self):\n\n        # divide the users into 3 subgroups\n        ctb_train_users, ctb_test_users = train_test_split(self.val['user_id'].unique(),\n                                                  random_state=1,\n                                                  test_size=0.2)\n\n        ctb_train_users, ctb_eval_users = train_test_split(ctb_train_users,\n                                                  random_state=1,\n                                                  test_size=0.1)\n\n        print('number of users in ctb train',ctb_train_users)\n        print('number of users in ctb eval',ctb_eval_users)\n        print('number of users in ctb test',ctb_test_users)\n\n        # \u0412\u0441\u0435 \u0431\u0430\u0437\u043e\u0432\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438\n        select_col = ['user_id', 'item_id', 'rank', 'target']\n\n        # train (basic)\n        ctb_train = shuffle(\n        pd.concat([\n            self.pos[self.pos['user_id'].isin(ctb_train_users)],\n            self.neg[self.neg['user_id'].isin(ctb_train_users)]\n        ])[select_col]\n        )\n\n        # test (basic)\n        ctb_test = shuffle(\n        pd.concat([\n            self.pos[self.pos['user_id'].isin(ctb_test_users)],\n            self.neg[self.neg['user_id'].isin(ctb_test_users)]\n        ])[select_col]\n        )\n\n        # evaluation (basic)\n        ctb_eval = shuffle(\n            pd.concat([\n                self.pos[self.pos['user_id'].isin(ctb_eval_users)],\n                self.neg[self.neg['user_id'].isin(ctb_eval_users)]\n        ])[select_col]\n        )\n\n        '''\n        \n        1. Additional Features\n        \n        '''\n        self.user_col = ['user_id', 'age', 'income', 'sex', 'kids_flg']\n        # self.item_col = ['item_id', 'content_type', 'countries', 'for_kids', 'age_rating', 'studios']\n        self.item_col = ['item_id','content_type','countries','for_kids','age_rating','studios','release_year','directors']\n\n        # train   \n        train_feat = (ctb_train\n              .merge(self.users[self.user_col], on=['user_id'], how='left')\n              .merge(self.items[self.item_col], on=['item_id'], how='left'))\n\n        # evaluation \n        eval_feat = (ctb_eval\n             .merge(self.users[self.user_col], on=['user_id'], how='left')\n             .merge(self.items[self.item_col], on=['item_id'], how='left'))\n\n\n        # drop pointless columns and separate target\n        self.drop_col = ['user_id', 'item_id']\n        self.target_col = ['target']\n\n        # we will define the categorical columns in catboost\n        self.cat_col = ['age', 'income', 'sex', 'content_type', 'countries', 'studios','directors']\n\n        self.X_train, self.y_train = train_feat.drop(self.drop_col + self.target_col, axis=1), train_feat[self.target_col]\n        self.X_val, self.y_val = eval_feat.drop(self.drop_col + self.target_col, axis=1), eval_feat[self.target_col]\n\n        # fillna for catboost with the most frequent value\n        self.X_train = self.X_train.fillna(self.X_train.mode().iloc[0])\n        self.X_val = self.X_val.fillna(self.X_train.mode().iloc[0])\n\n\n\n        '''\n        \n        2. Prepare Test Set\n        \n        '''\n        \n        test_feat = (ctb_test\n                     .merge(self.users[self.user_col], on=['user_id'], how='left')\n                     .merge(self.items[self.item_col], on=['item_id'], how='left'))\n        \n        # fillna for catboost with the most frequent value\n        test_feat = test_feat.fillna(self.X_train.mode().iloc[0])\n        self.X_test, self.y_test = test_feat.drop(self.drop_col + self.target_col, axis=1), test_feat['target']\n\n        print(f'X_train: {self.X_train.shape}')\n        print(f'X_val: {self.X_val.shape}')\n        print(f'X_test: {self.X_test.shape}')\n\n    def train(self):\n\n        # model hyperparameters\n        est_params = {\n          'subsample': 0.9,\n          'max_depth': 5,\n          'n_estimators': 5000,\n          'learning_rate': 0.01,\n          'thread_count': 20,\n          'random_state': 42,\n          'verbose': 200,\n        }\n        \n        ctb_model = CatBoostClassifier(**est_params)\n        \n        import warnings; warnings.filterwarnings('ignore')\n        ctb_model.fit(self.X_train,\n                      self.y_train,\n                      eval_set=(self.X_val, self.y_val),\n                      early_stopping_rounds=100,\n                      cat_features=self.cat_col)\n\n        self.model = ctb_model\n\n    def predict_test(self):\n        y_pred = self.model.predict_proba(self.X_test)\n        f\"ROC AUC score = {roc_auc_score(self.y_test, y_pred[:, 1]):.2f}\"\n            \n\n# cat = catClassifier(kion.val, # kion.val : \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u0430\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438\n#                     cand.candidates, # cand.candidates : \u041a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u044b \u0438\u0437 \u043c\u043e\u0434\u0435\u043b\u0438 1\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f\n#                     kion.users, # kion.users : \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \n#                     kion.items) # kion.items : \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043e\u0432 \n\n\n# cat.gen_data()  # generate positive and negative samples from candidates based on validation set\n# cat.create_subsets() # \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c X_train,X_test,X_val \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 2\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f\n\n# \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c 2\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f\n# cat.train()\n</pre> # \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c 2\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f (\u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440) # \u0426\u0435\u043b\u044c \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430 : \u043f\u0440\u0435\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u043c\u043e\u0442\u0440\u0435\u043b \u043a\u043b\u0438\u0435\u043d\u0442 class catClassifier:      def __init__(self,val,candidates,users,items):         self.val = val         self.candidates = candidates         self.users = users         self.items = items      def gen_data(self):                  # positive candidates         pos = self.candidates.merge(self.val,                                on=['user_id', 'item_id'],                                how='inner')         pos['target'] = 1         self.pos = pos         print('number of positive samples',pos.shape)          # \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u043e\u043d\u0438 \u043d\u0435 \u0441\u043c\u043e\u0442\u0440\u0435\u043b\u0438 \u0432 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435         neg = self.candidates.set_index(['user_id', 'item_id'])\\                 .join(self.val.set_index(['user_id', 'item_id']))          neg = neg[neg['watched_pct'].isnull()].reset_index()         # print(neg.shape)         neg = neg.sample(frac=0.07)         print('number of negative samples',neg.shape)         neg['target'] = 0         self.neg = neg      def create_subsets(self):          # divide the users into 3 subgroups         ctb_train_users, ctb_test_users = train_test_split(self.val['user_id'].unique(),                                                   random_state=1,                                                   test_size=0.2)          ctb_train_users, ctb_eval_users = train_test_split(ctb_train_users,                                                   random_state=1,                                                   test_size=0.1)          print('number of users in ctb train',ctb_train_users)         print('number of users in ctb eval',ctb_eval_users)         print('number of users in ctb test',ctb_test_users)          # \u0412\u0441\u0435 \u0431\u0430\u0437\u043e\u0432\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438         select_col = ['user_id', 'item_id', 'rank', 'target']          # train (basic)         ctb_train = shuffle(         pd.concat([             self.pos[self.pos['user_id'].isin(ctb_train_users)],             self.neg[self.neg['user_id'].isin(ctb_train_users)]         ])[select_col]         )          # test (basic)         ctb_test = shuffle(         pd.concat([             self.pos[self.pos['user_id'].isin(ctb_test_users)],             self.neg[self.neg['user_id'].isin(ctb_test_users)]         ])[select_col]         )          # evaluation (basic)         ctb_eval = shuffle(             pd.concat([                 self.pos[self.pos['user_id'].isin(ctb_eval_users)],                 self.neg[self.neg['user_id'].isin(ctb_eval_users)]         ])[select_col]         )          '''                  1. Additional Features                  '''         self.user_col = ['user_id', 'age', 'income', 'sex', 'kids_flg']         # self.item_col = ['item_id', 'content_type', 'countries', 'for_kids', 'age_rating', 'studios']         self.item_col = ['item_id','content_type','countries','for_kids','age_rating','studios','release_year','directors']          # train            train_feat = (ctb_train               .merge(self.users[self.user_col], on=['user_id'], how='left')               .merge(self.items[self.item_col], on=['item_id'], how='left'))          # evaluation          eval_feat = (ctb_eval              .merge(self.users[self.user_col], on=['user_id'], how='left')              .merge(self.items[self.item_col], on=['item_id'], how='left'))           # drop pointless columns and separate target         self.drop_col = ['user_id', 'item_id']         self.target_col = ['target']          # we will define the categorical columns in catboost         self.cat_col = ['age', 'income', 'sex', 'content_type', 'countries', 'studios','directors']          self.X_train, self.y_train = train_feat.drop(self.drop_col + self.target_col, axis=1), train_feat[self.target_col]         self.X_val, self.y_val = eval_feat.drop(self.drop_col + self.target_col, axis=1), eval_feat[self.target_col]          # fillna for catboost with the most frequent value         self.X_train = self.X_train.fillna(self.X_train.mode().iloc[0])         self.X_val = self.X_val.fillna(self.X_train.mode().iloc[0])            '''                  2. Prepare Test Set                  '''                  test_feat = (ctb_test                      .merge(self.users[self.user_col], on=['user_id'], how='left')                      .merge(self.items[self.item_col], on=['item_id'], how='left'))                  # fillna for catboost with the most frequent value         test_feat = test_feat.fillna(self.X_train.mode().iloc[0])         self.X_test, self.y_test = test_feat.drop(self.drop_col + self.target_col, axis=1), test_feat['target']          print(f'X_train: {self.X_train.shape}')         print(f'X_val: {self.X_val.shape}')         print(f'X_test: {self.X_test.shape}')      def train(self):          # model hyperparameters         est_params = {           'subsample': 0.9,           'max_depth': 5,           'n_estimators': 5000,           'learning_rate': 0.01,           'thread_count': 20,           'random_state': 42,           'verbose': 200,         }                  ctb_model = CatBoostClassifier(**est_params)                  import warnings; warnings.filterwarnings('ignore')         ctb_model.fit(self.X_train,                       self.y_train,                       eval_set=(self.X_val, self.y_val),                       early_stopping_rounds=100,                       cat_features=self.cat_col)          self.model = ctb_model      def predict_test(self):         y_pred = self.model.predict_proba(self.X_test)         f\"ROC AUC score = {roc_auc_score(self.y_test, y_pred[:, 1]):.2f}\"               # cat = catClassifier(kion.val, # kion.val : \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u0430\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 #                     cand.candidates, # cand.candidates : \u041a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u044b \u0438\u0437 \u043c\u043e\u0434\u0435\u043b\u0438 1\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f #                     kion.users, # kion.users : \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f  #                     kion.items) # kion.items : \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043e\u0432    # cat.gen_data()  # generate positive and negative samples from candidates based on validation set # cat.create_subsets() # \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c X_train,X_test,X_val \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 2\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f  # \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c 2\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f # cat.train() In\u00a0[13]: Copied! <pre>import optuna\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\nclass catClassifierOptuna(catClassifier):\n\n    def __init__(self,val,candidates,users,items):\n        self.val = val\n        self.candidates = candidates\n        self.users = users\n        self.items = items\n    \n    def __init__(self,val,candidates,users,items):\n        super().__init__(val,candidates,users,items)\n\n    # \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \n    def train(self):\n        \n        study = optuna.create_study(direction=\"maximize\")\n        study.optimize(self.objective, n_trials=50, timeout=600,show_progress_bar=True)\n\n        # \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u0430\u043b\u0438 \u0441\u0430\u043c\u044b\u0439 \u0432\u044b\u0441\u043e\u043a\u0438\u0439 ROC-AUC\n        self.model = CatBoostClassifier(**study.best_params,silent=True)\n        \n        import warnings; warnings.filterwarnings('ignore')\n        self.model.fit(self.X_train,\n                      self.y_train,\n                      eval_set=(self.X_val, self.y_val),\n                      early_stopping_rounds=100,\n                      cat_features=self.cat_col)\n\n        self.model = self.model\n    \n\n    # objective function ; \u0447\u0442\u043e \u043c\u044b \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u0435\u043c -&gt; ROC AUC \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0435\n    def objective(self,trial):\n    \n        param = {\n          'subsample': trial.suggest_float('subsample',0.5,0.95),\n          \"depth\": trial.suggest_int(\"depth\", 1, 15),\n          'n_estimators': trial.suggest_int('n_estimators',5000,7000),\n          'learning_rate': trial.suggest_float('learning_rate',0.001,0.1),\n          'thread_count': 20,\n          'random_state': 42\n        }\n          \n        model = CatBoostClassifier(**param,silent=True)\n    \n        model.fit(self.X_train,\n                      self.y_train,\n                      eval_set=(self.X_val, self.y_val),\n                      early_stopping_rounds=100,\n                      cat_features=self.cat_col)\n    \n        y_pred = model.predict_proba(self.X_test)\n\n        return roc_auc_score(self.y_test, y_pred[:, 1])\n\n\ncat = catClassifierOptuna(kion.val, # kion.val : \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u0430\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438\n                          cand.candidates, # cand.candidates : \u041a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u044b \u0438\u0437 \u043c\u043e\u0434\u0435\u043b\u0438 1\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f\n                          kion.users, # kion.users : \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \n                          kion.items) # kion.items : \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043e\u0432 \n        \ncat.gen_data()  # generate positive and negative samples from candidates based on validation set\ncat.create_subsets() # \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c X_train,X_test,X_val \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 2\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f\n</pre> import optuna optuna.logging.set_verbosity(optuna.logging.WARNING)  class catClassifierOptuna(catClassifier):      def __init__(self,val,candidates,users,items):         self.val = val         self.candidates = candidates         self.users = users         self.items = items          def __init__(self,val,candidates,users,items):         super().__init__(val,candidates,users,items)      # \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438      def train(self):                  study = optuna.create_study(direction=\"maximize\")         study.optimize(self.objective, n_trials=50, timeout=600,show_progress_bar=True)          # \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u0430\u043b\u0438 \u0441\u0430\u043c\u044b\u0439 \u0432\u044b\u0441\u043e\u043a\u0438\u0439 ROC-AUC         self.model = CatBoostClassifier(**study.best_params,silent=True)                  import warnings; warnings.filterwarnings('ignore')         self.model.fit(self.X_train,                       self.y_train,                       eval_set=(self.X_val, self.y_val),                       early_stopping_rounds=100,                       cat_features=self.cat_col)          self.model = self.model           # objective function ; \u0447\u0442\u043e \u043c\u044b \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u0435\u043c -&gt; ROC AUC \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0435     def objective(self,trial):              param = {           'subsample': trial.suggest_float('subsample',0.5,0.95),           \"depth\": trial.suggest_int(\"depth\", 1, 15),           'n_estimators': trial.suggest_int('n_estimators',5000,7000),           'learning_rate': trial.suggest_float('learning_rate',0.001,0.1),           'thread_count': 20,           'random_state': 42         }                    model = CatBoostClassifier(**param,silent=True)              model.fit(self.X_train,                       self.y_train,                       eval_set=(self.X_val, self.y_val),                       early_stopping_rounds=100,                       cat_features=self.cat_col)              y_pred = model.predict_proba(self.X_test)          return roc_auc_score(self.y_test, y_pred[:, 1])   cat = catClassifierOptuna(kion.val, # kion.val : \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u0430\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438                           cand.candidates, # cand.candidates : \u041a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u044b \u0438\u0437 \u043c\u043e\u0434\u0435\u043b\u0438 1\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f                           kion.users, # kion.users : \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f                            kion.items) # kion.items : \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043e\u0432           cat.gen_data()  # generate positive and negative samples from candidates based on validation set cat.create_subsets() # \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c X_train,X_test,X_val \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 2\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f <pre>number of positive samples (121011, 7)\nnumber of negative samples (430233, 6)\nnumber of users in ctb train [267415  29717 913817 ...  39932 202551 485139]\nnumber of users in ctb eval [ 751973  836142  740671 ...    1276 1012693   80378]\nnumber of users in ctb test [ 902000 1004154  188727 ...  636142   56244  933157]\nX_train: (398276, 12)\nX_val: (43384, 12)\nX_test: (109584, 12)\n</pre> In\u00a0[14]: Copied! <pre># \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f optuna \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044e \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432\ncat.train()\n</pre> # \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f optuna \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044e \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 cat.train() <p>\u0413\u043e\u0442\u043e\u0432\u0438\u043c \u0444\u0438\u0447\u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 2\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f</p> <ul> <li><code>cat.user_col</code>,<code>cat.item_col</code> : \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0442\u0435 \u0436\u0435 \u0444\u0438\u0447\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u0438 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 2\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f</li> </ul> In\u00a0[15]: Copied! <pre># \u043c\u043e\u0434\u0435\u043b\u044c 2\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f\ncat.model\n</pre> # \u043c\u043e\u0434\u0435\u043b\u044c 2\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f cat.model Out[15]: <pre>&lt;catboost.core.CatBoostClassifier at 0x7a7331938460&gt;</pre> In\u00a0[16]: Copied! <pre># \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\ncand.create_candidates(k=100,subset='test')\n</pre> # \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 cand.create_candidates(k=100,subset='test') <pre>recall@k  0.06\nprecision@k  0.06\nmrr@k  0.03\n</pre> In\u00a0[17]: Copied! <pre># \u0432\u0441\u0435 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u044b \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\npred_bpr_ctb = cand.candidates.copy()\n\n# \u0444\u0438\u0447\u0438 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u0430\nscore_feat = (pred_bpr_ctb\n              .merge(kion.users[cat.user_col], on=['user_id'], how='left')\n              .merge(kion.items[cat.item_col], on=['item_id'], how='left'))\n\n# fillna for catboost with the most frequent value\nscore_feat = score_feat.fillna(cat.X_train.mode().iloc[0])\nscore_feat.head()\n</pre> # \u0432\u0441\u0435 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u044b \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 pred_bpr_ctb = cand.candidates.copy()  # \u0444\u0438\u0447\u0438 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u0430 score_feat = (pred_bpr_ctb               .merge(kion.users[cat.user_col], on=['user_id'], how='left')               .merge(kion.items[cat.item_col], on=['item_id'], how='left'))  # fillna for catboost with the most frequent value score_feat = score_feat.fillna(cat.X_train.mode().iloc[0]) score_feat.head() Out[17]: user_id item_id rank age income sex kids_flg content_type countries for_kids age_rating studios release_year directors 0 21 849 1 age_45_54 income_20_40 \u0416 0.0 film \u0421\u0428\u0410 0.0 18.0 HBO 2018.0 \u041a\u0435\u043d \u041a\u0443\u0448\u043d\u0435\u0440 1 21 1053 2 age_45_54 income_20_40 \u0416 0.0 film \u0421\u0428\u0410 0.0 18.0 HBO 2020.0 \u041a\u043b\u0430\u0440\u043a \u0414\u044c\u044e\u043a 2 21 24 3 age_45_54 income_20_40 \u0416 0.0 series \u0413\u0435\u0440\u043c\u0430\u043d\u0438\u044f 0.0 16.0 HBO 2020.0 \u0424\u043b\u043e\u0440\u0438\u0430\u043d \u0413\u0430\u043b\u043b\u0435\u043d\u0431\u0435\u0440\u0433\u0435\u0440 3 21 826 4 age_45_54 income_20_40 \u0416 0.0 film \u0412\u0435\u043b\u0438\u043a\u043e\u0431\u0440\u0438\u0442\u0430\u043d\u0438\u044f 0.0 16.0 HBO 2020.0 \u0414\u0436\u0435\u0441\u0441\u0438 \u041a\u0438\u043d\u043e\u043d\u0435\u0441 4 21 12975 5 age_45_54 income_20_40 \u0416 0.0 film \u0421\u0428\u0410 0.0 18.0 HBO 2019.0 \u041c\u0430\u0439\u043b\u0437 \u0414\u0436\u043e\u0440\u0438\u0441-\u041f\u0435\u0439\u0440\u0430\u0444\u0438\u0442 In\u00a0[18]: Copied! <pre># \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438\ncat.model.get_params()\n</pre> # \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438 cat.model.get_params() Out[18]: <pre>{'learning_rate': 0.02442520558033121,\n 'depth': 10,\n 'silent': True,\n 'subsample': 0.7247704767962999,\n 'n_estimators': 5613}</pre> In\u00a0[19]: Copied! <pre># prediction and sort by predict proba weak values\nctb_prediction = cat.model.predict_proba(score_feat.drop(cat.drop_col, axis=1, errors='ignore'))\n\npred_bpr_ctb['ctb_pred'] = ctb_prediction[:, 1] # prob for positive class\n\npred_bpr_ctb = pred_bpr_ctb.sort_values(\n                                        by=['user_id', 'ctb_pred'], \n                                        ascending=[True, False])\npred_bpr_ctb['rank_ctb'] = pred_bpr_ctb.groupby('user_id').cumcount() + 1\npred_bpr_ctb.head()\n</pre> # prediction and sort by predict proba weak values ctb_prediction = cat.model.predict_proba(score_feat.drop(cat.drop_col, axis=1, errors='ignore'))  pred_bpr_ctb['ctb_pred'] = ctb_prediction[:, 1] # prob for positive class  pred_bpr_ctb = pred_bpr_ctb.sort_values(                                         by=['user_id', 'ctb_pred'],                                          ascending=[True, False]) pred_bpr_ctb['rank_ctb'] = pred_bpr_ctb.groupby('user_id').cumcount() + 1 pred_bpr_ctb.head() Out[19]: user_id item_id rank ctb_pred rank_ctb 1 21 14703 62 0.493133 1 1 21 8636 75 0.429543 2 1 21 1132 64 0.422380 3 1 21 11661 36 0.419328 4 1 21 12659 10 0.346157 5 In\u00a0[20]: Copied! <pre>true_items = kion.test.groupby('user_id').agg(lambda x: list(x))[['item_id']].reset_index()\npred_items = pred_bpr_ctb.groupby('user_id').agg(lambda x: list(x))[['item_id']].reset_index().rename(columns={'item_id': 'preds'})\ntrue_pred_items = true_items.merge(pred_items, how='left')\ntrue_pred_items = true_pred_items.dropna(subset=['preds'])\n</pre> true_items = kion.test.groupby('user_id').agg(lambda x: list(x))[['item_id']].reset_index() pred_items = pred_bpr_ctb.groupby('user_id').agg(lambda x: list(x))[['item_id']].reset_index().rename(columns={'item_id': 'preds'}) true_pred_items = true_items.merge(pred_items, how='left') true_pred_items = true_pred_items.dropna(subset=['preds']) In\u00a0[21]: Copied! <pre># \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\nprint('recall@k',round(recall(true_pred_items, k=20),3))\nprint('precision@k',round(precision(true_pred_items, k=20),3))\nprint('mrr@k',round(mrr(true_pred_items, k=20),3))\n</pre> # \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 print('recall@k',round(recall(true_pred_items, k=20),3)) print('precision@k',round(precision(true_pred_items, k=20),3)) print('mrr@k',round(mrr(true_pred_items, k=20),3)) <pre>recall@k 0.072\nprecision@k 0.072\nmrr@k 0.051\n</pre>"},{"location":"portfolio/course_recsys/prob_hybrid.html","title":"\u0413\u0438\u0431\u0440\u0438\u0434\u043d\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438\u00b6","text":""},{"location":"portfolio/course_recsys/prob_hybrid.html#1","title":"1 | \u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/course_recsys/prob_hybrid.html","title":"\u0426\u0435\u043b\u044c:\u00b6","text":"<ul> <li>\u0412 \u044d\u0442\u043e\u043c \u0437\u0430\u0434\u0430\u043d\u0438\u0438 \u0432\u044b \u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0438\u0442\u0435 \u0440\u0430\u0431\u043e\u0442\u0443 \u0441 \u0434\u0432\u0443\u0445\u0443\u0440\u043e\u0432\u043d\u0435\u0432\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u044c\u044e, \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u0442\u0435 \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0442\u044c (1) \u043d\u043e\u0432\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, (2) \u0430 \u0442\u0430\u043a\u0436\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f, (3) \u043f\u043e\u0434\u0431\u0435\u0440\u0435\u0442\u0435 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b.</li> <li>\u0412 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0435 \u0434\u043e\u0440\u0430\u0431\u043e\u0442\u043e\u043a \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0438, \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0435\u043d\u043d\u043e\u0439 \u043d\u0430 \u0437\u0430\u043d\u044f\u0442\u0438\u0438, \u0434\u043e\u043b\u0436\u043d\u043e \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c\u0441\u044f.</li> </ul>"},{"location":"portfolio/course_recsys/prob_hybrid.html","title":"\u041f\u043e\u0448\u0430\u0433\u043e\u0432\u0430\u044f \u0438\u043d\u0441\u0442\u0440\u0443\u043a\u0446\u0438\u044f:\u00b6","text":"<ul> <li><p>\u041f\u0440\u043e\u0434\u043e\u043b\u0436\u0438\u043c \u0440\u0430\u0431\u043e\u0442\u0443 \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u043e\u043d\u043b\u0430\u0439\u043d-\u043a\u0438\u043d\u043e\u0442\u0435\u0430\u0442\u0440\u0430 KION: https://github.com/irsafilo/KION_DATASET/tree/main</p> </li> <li><p>\u0417\u0430\u0434\u0430\u0447\u0430 - \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0434\u0432\u0443\u0445\u0443\u0440\u043e\u0432\u043d\u0435\u0432\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438, \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0435\u043d\u043d\u043e\u0439 \u043d\u0430 \u0437\u0430\u043d\u044f\u0442\u0438\u0438, \u043c\u0438\u043d\u0438\u043c\u0443\u043c \u043d\u0430 10% \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0435 precision@20. \u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u0442\u0441\u044f \u043f\u0440\u043e\u0434\u0435\u043b\u0430\u0442\u044c \u0445\u043e\u0442\u044f \u0431\u044b \u0434\u0432\u0430 \u0438\u0437 \u0442\u0440\u0435\u0445 \u0448\u0430\u0433\u043e\u0432:</p> <ul> <li><p>(\u043c\u043e\u0434\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432) \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u043e\u0434\u043d\u0443 \u0438\u043b\u0438 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f (\u043c\u043e\u0436\u043d\u043e \u0431\u0440\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 implicit \u0438\u043b\u0438 \u0434\u0440\u0443\u0433\u0438\u0445 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a); \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439 \u044d\u0442\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043e\u0442\u043e\u0431\u0440\u0430\u0442\u044c \u043e\u0431\u044a\u0435\u043a\u0442\u044b-\u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u044b \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0432\u0442\u043e\u0440\u043e\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f;</p> </li> <li><p>\u0444\u0438\u0447\u0430-\u0438\u043d\u0436\u0438\u043d\u0438\u0440\u0438\u043d\u0433: \u043f\u043e\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 \u0438\u043c\u0435\u044e\u0449\u0438\u043c\u0438\u0441\u044f \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a\u0430\u043c\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0438/\u0438\u043b\u0438 \u0441\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043d\u043e\u0432\u044b\u0435 \u0444\u0438\u0447\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u043e\u0441\u0442\u0438 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432/\u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439; \u043f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c \u0443\u0447\u0435\u0441\u0442\u044c \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u0443\u044e \u0441\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0449\u0443\u044e, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u043e\u0441\u0442\u044c \u0437\u0430 \u0440\u0430\u0437\u043d\u044b\u0435 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043f\u0440\u043e\u043c\u0435\u0436\u0443\u0442\u043a\u0438;</p> </li> <li><p>\u043f\u043e\u0434\u043e\u0431\u0440\u0430\u0442\u044c \u043b\u0443\u0447\u0448\u0435\u0435 train-val-test \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435; \u043f\u043e\u0434\u043e\u0431\u0440\u0430\u0442\u044c \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0438 \u0432\u0442\u043e\u0440\u043e\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u0435\u0439.</p> </li> </ul> </li> <li><p>\u041e\u0446\u0435\u043d\u0438\u0442\u044c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0430\u043c precision@20, recall@20, mrr@20</p> </li> </ul>"},{"location":"portfolio/course_recsys/prob_hybrid.html#2","title":"2 | \u0427\u0442\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<p><code>readData</code> : \u043a\u043b\u0430\u0441\u0441 \u0434\u043b\u044f \u0447\u0442\u0435\u043d\u0438\u0435 \u0438 \u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445</p>"},{"location":"portfolio/course_recsys/prob_hybrid.html#3","title":"3 | \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<p><code>dataPreprocess</code> \u043a\u043b\u0430\u0441\u0441 \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445</p> <ul> <li>\u0414\u043b\u044f \u0442\u0435\u043a\u0443\u0449\u0435\u0433\u043e \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0430 \u043c\u044b \u043d\u0435 \u043c\u0435\u043d\u044f\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443</li> <li>\u043a\u043b\u0430\u0441\u0441 \u0438\u043d\u0438\u0446\u0438\u0438\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u0442 \u043a\u044e\u0440\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0439 \u043a\u043b\u0430\u0441\u0441 \u0433\u0434\u0435 \u0445\u0440\u043e\u043d\u044f\u0442\u0441\u044f \u0434\u0430\u043d\u043d\u044b\u0435</li> </ul>"},{"location":"portfolio/course_recsys/prob_hybrid.html#4","title":"4 | \u0412\u044b\u0431\u043e\u0440 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432\u00b6","text":"<p><code>genCandidate</code> : \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0434\u043b\u044f \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u043f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u044b\u0445 \u043a\u0430\u0434\u0438\u0434\u0430\u0442\u043e\u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0434\u043b\u044f \u0444\u0438\u0447 \u043c\u043e\u0434\u0435\u043b\u0438 \u0432\u0442\u043e\u0440\u043e\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f</p>"},{"location":"portfolio/course_recsys/prob_hybrid.html#5","title":"5 | \u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u043e\u0442\u0431\u043e\u0440\u0430\u00b6","text":""},{"location":"portfolio/course_recsys/prob_hybrid.html#1","title":"1) \u0411\u0435\u0437 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432\u00b6","text":"<p><code>catClassifier</code> : \u043a\u043b\u0430\u0441\u0441 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0432\u0442\u043e\u0440\u043e\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f</p> <ul> <li>\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0438 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u043c \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435 \u043d\u0435 \u044f\u0432\u043b\u044f\u043b\u0438\u0441\u044c \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u043c\u0438, \u0441\u043e\u043e\u0442\u0432\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043c\u043e\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u0443\u043b\u0443\u0447\u0438\u0442\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0432 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044e \u044d\u0442\u0438\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432</li> </ul>"},{"location":"portfolio/course_recsys/prob_hybrid.html#2","title":"2) \u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u0413\u0438\u043f\u0435\u0440\u0430\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432\u00b6","text":"<p>\u041c\u044b \u043c\u043e\u0436\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>optuna</code> \u0434\u043b\u044f \u043f\u043e\u0434\u0431\u043e\u0440\u0430 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043f\u0430\u0440\u0430\u043c\u0435\u0440\u0442\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u0443\u0434\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043d\u0430\u0445\u043e\u0434\u044f\u0442\u0441\u044f \u0432 <code>param</code> \u0438 \u0438\u043c\u0435\u044e\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 <code>trial.suggest_</code></p> <ul> <li><code>subsample</code></li> <li><code>depth</code></li> <li><code>n_estimators</code></li> <li><code>learning_rate</code></li> </ul>"},{"location":"portfolio/course_recsys/prob_hybrid.html#6","title":"6 | \u0424\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438\u00b6","text":"<p>\u041c\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0438 <code>train</code> \u0438 <code>val</code> \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u0443\u0434\u0443\u0442 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u0440\u0435\u043a\u043e\u043c\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u043d\u0430 <code>test</code></p> <ul> <li>\u0444\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f 100 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432 \u0442\u043e\u0432\u0430\u0440\u043e\u0432 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</li> </ul>"},{"location":"portfolio/course_recsys/prob_hybrid.html","title":"\u0428\u0430\u0433\u0438 \u0434\u043b\u044f \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<p>\u0412\u044b\u0431\u043e\u0440 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432</p> <ul> <li>\u0418\u0437\u043d\u043e\u0447\u0430\u043b\u044c\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u043e\u0441\u044c 30 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f, \u043a\u0430\u0447\u0435\u0441\u0442\u043e \u043e\u0431\u043e\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0432\u0430\u043b\u043e\u0441\u044c \u043f\u0440\u0438 \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u0438 \u044d\u0442\u043e\u0433\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440, \u0432 \u044d\u0442\u043e\u043c \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u043e\u0441\u044c 100 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432 (positive sample).</li> <li>\u041f\u0440\u043e\u0431\u043e\u0432\u0430\u043b\u043e\u0441\u044c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432 \u043f\u043e\u0434\u0431\u043e\u0440\u0430 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430 \u0440\u0435\u0433\u0443\u0440\u0435\u043b\u0438\u0437\u0430\u0446\u0438\u0438, \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u043c \u043d\u043e\u0443\u0442\u0430\u0431\u0443\u043a\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u043e\u0441\u044c 0.01, \u043f\u0440\u0438 \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u0438 \u044d\u0442\u043e\u0433\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0438 \u0441\u0442\u0430\u043b\u043e \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043e \u0445\u0443\u0436\u0435. \u0410\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u043e \u0435\u0441\u043b\u0438 \u043e\u043d \u0431\u044b\u043b \u0441\u043b\u0438\u0447\u043a\u043e\u043c \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u0439, \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0438 \u0442\u043e\u0436\u0435 \u043f\u0430\u0434\u0430\u043b\u043e.</li> <li>\u0422\u0430\u043a \u0436\u0435 \u0431\u044b\u043b\u0438 \u043f\u043e\u043f\u044b\u0442\u043a\u0438 \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0438 \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0432 <code>factors</code> \u043e\u0442 50 \u0434\u043e 75, \u043d\u043e \u044d\u0442\u043e \u043d\u0435 \u043f\u043e\u0432\u043b\u0438\u044f\u043b\u043e \u043d\u0430 \u0438\u0442\u043e\u0433\u043e\u0432\u0443\u044e \u043c\u0435\u0442\u0440\u0438\u043a\u0443</li> </ul> <p>\u0424\u0438\u0447\u0438 \u0438\u043d\u0436\u0438\u043d\u0438\u0440\u0438\u043d\u0433</p> <p>\u0412 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0435 \u0432\u0441\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u0438\u0441\u044c, \u0434\u043b\u044f \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0435\u0439, \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 <code>release_year</code>,<code>directors</code> \u0431\u044b\u043b\u0438 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u044b \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 2\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f \u0447\u0442\u043e \u0443\u043b\u0443\u0447\u0448\u0438\u043b\u043e \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0438</p> <ul> <li>\u041f\u0440\u0438\u0440\u043e\u0441\u0442 \u043e\u0442 \u044d\u0442\u043e\u0433\u043e \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u044d\u0442\u0438\u0445 \u0444\u0438\u0447 \u0431\u044b\u043b \u0432 \u0440\u0430\u0439\u043e\u043d\u0435 0.01</li> </ul> <p>\u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u0413\u0438\u043f\u0435\u0440\u0430\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0438 2\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f</p> <p>\u041c\u044b \u043c\u043e\u0436\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>optuna</code> \u0434\u043b\u044f \u043f\u043e\u0434\u0431\u043e\u0440\u0430 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043f\u0430\u0440\u0430\u043c\u0435\u0440\u0442\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u0443\u0434\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043d\u0430\u0445\u043e\u0434\u044f\u0442\u0441\u044f \u0432 <code>param</code> \u0438 \u0438\u043c\u0435\u044e\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 <code>trial.suggest_</code></p> <ul> <li><code>subsample</code></li> <li><code>depth</code></li> <li><code>n_estimators</code></li> <li><code>learning_rate</code></li> </ul> <p>\u041f\u0440\u0438\u0440\u043e\u0441\u0442 \u043c\u0435\u0442\u0440\u043f\u0438\u043a\u0438 \u043e\u0442 \u044d\u0442\u043e\u0439 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0442\u0430\u043a \u0436\u0435 \u0441\u043e\u0441\u0442\u0430\u0432\u0438\u043b \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e 0.01</p> <p>\u0414\u043e \u0438 \u043f\u043e\u0441\u043b\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438</p> <p>metric| \u0438\u0441\u0445\u043e\u0434\u043d\u0430\u044f | \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 | -: | - | - | | recall | recall@k - | recall@k 0.072 | precision | \u0431\u044b\u043b\u043e 0.0468 | precision@k 0.072 | mrr | mrr@k - | mrr@k 0.051 |</p>"},{"location":"portfolio/course_recsys/prob_lightfm.html","title":"Prob lightfm","text":"In\u00a0[1]: Copied! <pre>!pip install lightfm --quiet\n</pre> !pip install lightfm --quiet <pre>     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 316.4/316.4 kB 5.8 MB/s eta 0:00:00\r\n  Preparing metadata (setup.py) ... done\r\n  Building wheel for lightfm (setup.py) ... done\r\n</pre> In\u00a0[2]: Copied! <pre>from lightfm import LightFM\nfrom lightfm.data import Dataset as LFMDataset\nimport numpy as np\nimport pandas as pd\nimport scipy.sparse as sparse\nfrom scipy.sparse import csr_matrix, diags\nfrom scipy.sparse.linalg import svds\nfrom tqdm import tqdm\n</pre> from lightfm import LightFM from lightfm.data import Dataset as LFMDataset import numpy as np import pandas as pd import scipy.sparse as sparse from scipy.sparse import csr_matrix, diags from scipy.sparse.linalg import svds from tqdm import tqdm In\u00a0[3]: Copied! <pre>path_items = '/kaggle/input/mts-library/items.csv'\npath_mts_lib = '/kaggle/input/mts-library/mts_lib.csv'\npath_users = '/kaggle/input/mts-library/users.csv'\n</pre> path_items = '/kaggle/input/mts-library/items.csv' path_mts_lib = '/kaggle/input/mts-library/mts_lib.csv' path_users = '/kaggle/input/mts-library/users.csv' In\u00a0[4]: Copied! <pre># \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043f\u043e\u0434\u0441\u0447\u0435\u0442\u0430 precision \n\n#       y_pred  item_id\n#  idx [a,b,c]  [a,f,d]\n\ndef precision(df: pd.DataFrame, \n              pred_col='y_pred', \n              true_col='item_id', \n              k=20) -&gt; float:\n    \n    precision_values = []\n    for _, row in df.iterrows():\n      num_relevant = len(set(row[true_col]) &amp; set(row[pred_col][:k]))\n      num_true = min(k, len(row[true_col]))\n      precision_values.append(num_relevant / num_true)\n    return np.mean(precision_values)\n</pre> # \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043f\u043e\u0434\u0441\u0447\u0435\u0442\u0430 precision   #       y_pred  item_id #  idx [a,b,c]  [a,f,d]  def precision(df: pd.DataFrame,                pred_col='y_pred',                true_col='item_id',                k=20) -&gt; float:          precision_values = []     for _, row in df.iterrows():       num_relevant = len(set(row[true_col]) &amp; set(row[pred_col][:k]))       num_true = min(k, len(row[true_col]))       precision_values.append(num_relevant / num_true)     return np.mean(precision_values) In\u00a0[5]: Copied! <pre>df = pd.read_csv(path_mts_lib)\ndf.head()\n</pre> df = pd.read_csv(path_mts_lib) df.head() Out[5]: user_id item_id progress rating start_date 0 126706 14433 80 NaN 2018-01-01 1 127290 140952 58 NaN 2018-01-01 2 66991 198453 89 NaN 2018-01-01 3 46791 83486 23 5.0 2018-01-01 4 79313 188770 88 5.0 2018-01-01 <p>\u0412\u043e\u0437\u043c\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u0434\u043b\u044f 2018 \u0433\u043e\u0434</p> In\u00a0[6]: Copied! <pre>df['start_date'] = df['start_date'].astype('datetime64[ns]')\ndf = df[(df['start_date'] &gt; '2018-01-01') &amp; (df['start_date'] &lt; '2019-01-01')]\nprint(df['start_date'].min())\nprint(df['start_date'].max())\n</pre> df['start_date'] = df['start_date'].astype('datetime64[ns]') df = df[(df['start_date'] &gt; '2018-01-01') &amp; (df['start_date'] &lt; '2019-01-01')] print(df['start_date'].min()) print(df['start_date'].max()) <pre>2018-01-02 00:00:00\n2018-12-31 00:00:00\n</pre> In\u00a0[7]: Copied! <pre># \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043e\u0432\nu_features = pd.read_csv(path_users)\ni_features = pd.read_csv(path_items)\ni_features.rename(columns={'id': 'item_id'}, inplace=True)\n</pre> # \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043e\u0432 u_features = pd.read_csv(path_users) i_features = pd.read_csv(path_items) i_features.rename(columns={'id': 'item_id'}, inplace=True) In\u00a0[8]: Copied! <pre>display(u_features.head())\n</pre> display(u_features.head()) user_id age sex 0 1 45_54 NaN 1 2 18_24 0.0 2 3 65_inf 0.0 3 4 18_24 0.0 4 5 35_44 0.0 In\u00a0[9]: Copied! <pre>#  \u041f\u0440\u0438\u0437\u043d\u0430\u0446\u0438 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \ndisplay(i_features.head())\n</pre> #  \u041f\u0440\u0438\u0437\u043d\u0430\u0446\u0438 \u0444\u0438\u043b\u044c\u043c\u043e\u0432  display(i_features.head()) item_id title genres authors year 0 128115 \u0412\u043e\u0440\u043e\u043d-\u0447\u0435\u043b\u043e\u0431\u0438\u0442\u0447\u0438\u043a \u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u0434\u0435\u0442\u0441\u043a\u0438\u0435 \u043a\u043d\u0438\u0433\u0438,\u0421\u043a\u0430\u0437\u043a\u0438,\u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d 1886 1 210979 \u0421\u043a\u0440\u0438\u043f\u043a\u0430 \u0420\u043e\u0442\u0448\u0438\u043b\u044c\u0434\u0430 \u041a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430,\u041b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430,\u0420\u0443\u0441\u0441\u043a\u0430\u044f ... \u0410\u043d\u0442\u043e\u043d \u0427\u0435\u0445\u043e\u0432 1894 2 95632 \u0418\u0441\u043f\u043e\u0440\u0447\u0435\u043d\u043d\u044b\u0435 \u0434\u0435\u0442\u0438 \u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430,\u041a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430,\u041b\u0438\u0442\u0435\u0440\u0430\u0442... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d 1869 3 247906 \u0421\u0442\u0440\u0430\u043d\u043d\u044b\u0439 \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u041f\u044c\u0435\u0441\u044b \u0438 \u0434\u0440\u0430\u043c\u0430\u0442\u0443\u0440\u0433\u0438\u044f,\u041b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430 \u041c\u0438\u0445\u0430\u0438\u043b \u041b\u0435\u0440\u043c\u043e\u043d\u0442\u043e\u0432 1831 4 294280 \u0413\u043e\u0441\u043f\u043e\u0434\u0430 \u0442\u0430\u0448\u043a\u0435\u043d\u0442\u0446\u044b \u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430,\u041a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430,\u041b\u0438\u0442\u0435\u0440\u0430\u0442... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d 1873 In\u00a0[10]: Copied! <pre># \u0443\u0434\u0430\u043b\u044f\u0435\u043c \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b, \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u043f\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438\ndf = df.sort_values('start_date').drop_duplicates(subset=['user_id', 'item_id'], \n                                                  keep='last')\n</pre> # \u0443\u0434\u0430\u043b\u044f\u0435\u043c \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b, \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u043f\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 df = df.sort_values('start_date').drop_duplicates(subset=['user_id', 'item_id'],                                                    keep='last') In\u00a0[11]: Copied! <pre># \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u0443 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 30% \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u0430\ndf = df[df['progress'] &gt; 30]\n\n# \u041e\u0442\u0444\u0438\u043b\u044c\u0442\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \ndef filter_data(df, user_count=20, item_count=30):\n    item_counts = df.groupby('item_id')['user_id'].count()\n    pop_items = item_counts[item_counts &gt;= user_count]\n    df_implicit = df[df['item_id'].isin(pop_items.index)]\n\n    user_counts = df.groupby('user_id')['item_id'].count()\n    pop_users = user_counts[user_counts &gt;= item_count]\n    df = df[df['user_id'].isin(pop_users.index)].copy()\n    return df\n\ndf = filter_data(df,\n                 user_count=20,\n                 item_count=20)\ndf.iloc[1,:]\n</pre> # \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u0443 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 30% \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u0430 df = df[df['progress'] &gt; 30]  # \u041e\u0442\u0444\u0438\u043b\u044c\u0442\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435  def filter_data(df, user_count=20, item_count=30):     item_counts = df.groupby('item_id')['user_id'].count()     pop_items = item_counts[item_counts &gt;= user_count]     df_implicit = df[df['item_id'].isin(pop_items.index)]      user_counts = df.groupby('user_id')['item_id'].count()     pop_users = user_counts[user_counts &gt;= item_count]     df = df[df['user_id'].isin(pop_users.index)].copy()     return df  df = filter_data(df,                  user_count=20,                  item_count=20) df.iloc[1,:] Out[11]: <pre>user_id                     85673\nitem_id                    210979\nprogress                      100\nrating                        5.0\nstart_date    2018-01-02 00:00:00\nName: 3536, dtype: object</pre> In\u00a0[12]: Copied! <pre># \u041b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430; \u0432 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u0444\u0438\u043b\u044c\u043c\u044b \u0442\u043e\u043b\u044c\u043a\u043e \u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445 \u0444\u0438\u0447\u0435\u0439\ni_features = i_features[i_features['item_id'].isin(df['item_id'])].copy()\nu_features = u_features[u_features['user_id'].isin(df['user_id'])].copy()\nprint(i_features.shape,u_features.shape)\n</pre> # \u041b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430; \u0432 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u0444\u0438\u043b\u044c\u043c\u044b \u0442\u043e\u043b\u044c\u043a\u043e \u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445 \u0444\u0438\u0447\u0435\u0439 i_features = i_features[i_features['item_id'].isin(df['item_id'])].copy() u_features = u_features[u_features['user_id'].isin(df['user_id'])].copy() print(i_features.shape,u_features.shape) <pre>(17512, 5) (1482, 3)\n</pre> In\u00a0[13]: Copied! <pre># \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043d\u043e\u0432\u044b\u0439 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0434\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043e\u0432\nuser_idx = df.user_id.astype('category').cat.codes\nitem_idx = df.item_id.astype('category').cat.codes\nuser2id = dict(zip(df.user_id, user_idx))\nitem2id = dict(zip(df.item_id, item_idx))\n\n# \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c ids\ndf.user_id = df.user_id.map(user2id)\ndf.item_id = df.item_id.map(item2id)\ni_features.item_id = i_features.item_id.map(item2id)\nu_features.user_id = u_features.user_id.map(user2id)\n</pre> # \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043d\u043e\u0432\u044b\u0439 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u0434\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043e\u0432 user_idx = df.user_id.astype('category').cat.codes item_idx = df.item_id.astype('category').cat.codes user2id = dict(zip(df.user_id, user_idx)) item2id = dict(zip(df.item_id, item_idx))  # \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c ids df.user_id = df.user_id.map(user2id) df.item_id = df.item_id.map(item2id) i_features.item_id = i_features.item_id.map(item2id) u_features.user_id = u_features.user_id.map(user2id) In\u00a0[14]: Copied! <pre>df['user_id'] = df['user_id'].apply(lambda x: 'user ' + str(x))\ndf['item_id'] = df['item_id'].apply(lambda x: 'item ' + str(x))\ni_features['item_id'] = i_features['item_id'].apply(lambda x: 'item ' + str(x))\nu_features['user_id'] = u_features['user_id'].apply(lambda x: 'user ' + str(x))\ndf.head()\n</pre> df['user_id'] = df['user_id'].apply(lambda x: 'user ' + str(x)) df['item_id'] = df['item_id'].apply(lambda x: 'item ' + str(x)) i_features['item_id'] = i_features['item_id'].apply(lambda x: 'item ' + str(x)) u_features['user_id'] = u_features['user_id'].apply(lambda x: 'user ' + str(x)) df.head() Out[14]: user_id item_id progress rating start_date 3555 user 921 item 2116 100 5.0 2018-01-02 3536 user 848 item 11499 100 5.0 2018-01-02 3577 user 848 item 6289 100 5.0 2018-01-02 3570 user 606 item 6907 92 NaN 2018-01-02 3567 user 921 item 7631 100 5.0 2018-01-02 In\u00a0[15]: Copied! <pre># \u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u0434\u0432\u0435 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \ndef train_test_split(X, user_col, time_col):\n  full_history = X.sort_values([user_col, time_col]).groupby(user_col)\n  test = full_history.tail(1)\n  train = full_history.head(-1)\n  return train, test\n\ntrain, test = train_test_split(df, 'user_id', 'start_date')\n</pre> # \u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u0434\u0432\u0435 \u0432\u044b\u0431\u043e\u0440\u043a\u0438  def train_test_split(X, user_col, time_col):   full_history = X.sort_values([user_col, time_col]).groupby(user_col)   test = full_history.tail(1)   train = full_history.head(-1)   return train, test  train, test = train_test_split(df, 'user_id', 'start_date') In\u00a0[16]: Copied! <pre>print('train',train.shape)\nprint('test',test.shape)\nprint('train tmin ',train['start_date'].min(),' tmax ',train['start_date'].max())\nprint('test tmin ',test['start_date'].min(),' tmax ',test['start_date'].max())\n</pre> print('train',train.shape) print('test',test.shape) print('train tmin ',train['start_date'].min(),' tmax ',train['start_date'].max()) print('test tmin ',test['start_date'].min(),' tmax ',test['start_date'].max()) <pre>train (77164, 5)\ntest (1565, 5)\ntrain tmin  2018-01-02 00:00:00  tmax  2018-12-31 00:00:00\ntest tmin  2018-01-30 00:00:00  tmax  2018-12-31 00:00:00\n</pre> In\u00a0[17]: Copied! <pre># \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u043f\u043e\u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u0442\u044c \u043f\u043e\u0441\u043b\u0435 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438\nlen(train['item_id'].unique())\n</pre> # \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u043f\u043e\u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u0442\u044c \u043f\u043e\u0441\u043b\u0435 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438 len(train['item_id'].unique()) Out[17]: <pre>17148</pre> In\u00a0[18]: Copied! <pre># user features \nu_features.set_index('user_id', inplace=True)\n\n# merge features into one column; merging column name with column value \nu_features_list = u_features.apply(\n    lambda feature_values: [f'{feature}_{feature_values[feature]}' for feature in feature_values.index if not pd.isna(feature_values[feature])],\n    axis=1)\nu_features_list = u_features_list.rename('features')\nu_features_list\n</pre> # user features  u_features.set_index('user_id', inplace=True)  # merge features into one column; merging column name with column value  u_features_list = u_features.apply(     lambda feature_values: [f'{feature}_{feature_values[feature]}' for feature in feature_values.index if not pd.isna(feature_values[feature])],     axis=1) u_features_list = u_features_list.rename('features') u_features_list Out[18]: <pre>user_id\nuser 0        [age_35_44, sex_0.0]\nuser 1        [age_18_24, sex_0.0]\nuser 2        [age_18_24, sex_0.0]\nuser 3       [age_65_inf, sex_1.0]\nuser 4        [age_55_64, sex_0.0]\n                     ...          \nuser 1560     [age_18_24, sex_0.0]\nuser 1561     [age_18_24, sex_1.0]\nuser 1562     [age_45_54, sex_1.0]\nuser 1563     [age_55_64, sex_1.0]\nuser 1564     [age_35_44, sex_0.0]\nName: features, Length: 1482, dtype: object</pre> In\u00a0[19]: Copied! <pre># all unique combinations for user features\nuser_tags = set(u_features_list.explode().dropna().values)\nuser_tags\n</pre> # all unique combinations for user features user_tags = set(u_features_list.explode().dropna().values) user_tags Out[19]: <pre>{'age_18_24',\n 'age_25_34',\n 'age_35_44',\n 'age_45_54',\n 'age_55_64',\n 'age_65_inf',\n 'sex_0.0',\n 'sex_1.0'}</pre> In\u00a0[20]: Copied! <pre># i_features['item_id'].value_counts()\ni_features_lfm = i_features.copy()\ni_features_lfm.set_index('item_id', inplace=True) # each row represents a unique item feature\n\n# from interactions add the number of reads of the particular item\ni_features_lfm['reads'] = df.groupby('item_id')['user_id'].count()\n\n# column genres contains genres separated by ,\ni_features_lfm['genres'] = i_features_lfm['genres'].str.lower().str.split(',')\ni_features_lfm['genres'] = i_features_lfm['genres'].apply(lambda x: x if isinstance(x, list) else [])\ni_features_lfm.head()\n</pre> # i_features['item_id'].value_counts() i_features_lfm = i_features.copy() i_features_lfm.set_index('item_id', inplace=True) # each row represents a unique item feature  # from interactions add the number of reads of the particular item i_features_lfm['reads'] = df.groupby('item_id')['user_id'].count()  # column genres contains genres separated by , i_features_lfm['genres'] = i_features_lfm['genres'].str.lower().str.split(',') i_features_lfm['genres'] = i_features_lfm['genres'].apply(lambda x: x if isinstance(x, list) else []) i_features_lfm.head() Out[20]: title genres authors year reads item_id item 7037 \u0412\u043e\u0440\u043e\u043d-\u0447\u0435\u043b\u043e\u0431\u0438\u0442\u0447\u0438\u043a [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u0434\u0435\u0442\u0441\u043a\u0438\u0435 \u043a\u043d\u0438\u0433\u0438, \u0441\u043a\u0430\u0437\u043a\u0438, \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f ... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d 1886 1 item 11499 \u0421\u043a\u0440\u0438\u043f\u043a\u0430 \u0420\u043e\u0442\u0448\u0438\u043b\u044c\u0434\u0430 [\u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430, \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430, \u0440\u0443\u0441\u0441\u043a... \u0410\u043d\u0442\u043e\u043d \u0427\u0435\u0445\u043e\u0432 1894 1 item 7296 \u0421\u043e\u0441\u0435\u0434\u0438 [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u0434\u0435\u0442\u0441\u043a\u0438\u0435 \u043a\u043d\u0438\u0433\u0438, \u0441\u043a\u0430\u0437\u043a\u0438, \u0440\u0443\u0441\u0441\u043a\u0430\u044f \u043a\u043b\u0430... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d NaN 1 item 4125 \u042f\u0440\u043c\u0430\u0440\u043a\u0430 \u0442\u0449\u0435\u0441\u043b\u0430\u0432\u0438\u044f [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430, \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u0441\u0442\u0430\u0440\u0438\u043d\u043d\u0430\u044f \u043b\u0438\u0442... \u0423\u0438\u043b\u044c\u044f\u043c \u0422\u0435\u043a\u043a\u0435\u0440\u0435\u0439 1848 3 item 15550 \u0425\u0438\u0442\u0440\u043e\u0441\u0442\u044c [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430, \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430] \u0413\u0438 \u0434\u0435 \u041c\u043e\u043f\u0430\u0441\u0441\u0430\u043d NaN 2 In\u00a0[21]: Copied! <pre># count the number of genre references in column genres\ngenres_count = i_features_lfm[['genres', 'reads']].explode('genres').groupby('genres')['reads'].sum()\n</pre> # count the number of genre references in column genres genres_count = i_features_lfm[['genres', 'reads']].explode('genres').groupby('genres')['reads'].sum() In\u00a0[22]: Copied! <pre># the top genres in the user/item interactions\ngenres_count.sort_values(ascending=False)\n</pre> # the top genres in the user/item interactions genres_count.sort_values(ascending=False) Out[22]: <pre>genres\n\u043b\u044e\u0431\u043e\u0432\u043d\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438         18595\n\u043f\u043e\u043f\u0430\u0434\u0430\u043d\u0446\u044b                12132\n\u0433\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438       9038\n\u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0432\u044b     8018\n\u043c\u0430\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0430\u043a\u0430\u0434\u0435\u043c\u0438\u0438       6674\n                         ...  \n\u0440\u0430\u0441\u0442\u0435\u043d\u0438\u0435\u0432\u043e\u0434\u0441\u0442\u0432\u043e              1\n\u043a\u043e\u043c\u043c\u0435\u0440\u0447\u0435\u0441\u043a\u043e\u0435 \u043f\u0440\u0430\u0432\u043e           1\n\u043c\u043e\u043b\u0438\u0442\u0432\u044b \u0432 \u0438\u0441\u043b\u0430\u043c\u0435             1\n\u043c\u0435\u0436\u0434\u0443\u043d\u0430\u0440\u043e\u0434\u043d\u043e\u0435 \u043f\u0440\u0430\u0432\u043e          1\n\u0430\u0441\u0442\u0440\u043e\u043d\u043e\u043c\u0438\u044f                   1\nName: reads, Length: 455, dtype: int64</pre> In\u00a0[23]: Copied! <pre># item_tags : top n genres by read count\nn = 100\nitem_tags = genres_count.sort_values(ascending=False)[:n].index\nitem_tags[:30]\n</pre> # item_tags : top n genres by read count n = 100 item_tags = genres_count.sort_values(ascending=False)[:n].index item_tags[:30] Out[23]: <pre>Index(['\u043b\u044e\u0431\u043e\u0432\u043d\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438', '\u043f\u043e\u043f\u0430\u0434\u0430\u043d\u0446\u044b', '\u0433\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438',\n       '\u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0432\u044b', '\u043c\u0430\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0430\u043a\u0430\u0434\u0435\u043c\u0438\u0438', '\u0431\u043e\u0435\u0432\u0430\u044f \u0444\u0430\u043d\u0442\u0430\u0441\u0442\u0438\u043a\u0430',\n       '\u043a\u043d\u0438\u0433\u0438 \u043f\u0440\u043e \u0432\u043e\u043b\u0448\u0435\u0431\u043d\u0438\u043a\u043e\u0432', '\u0431\u043e\u0435\u0432\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438', '\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u043b\u044e\u0431\u043e\u0432\u043d\u044b\u0435 \u0440\u043e\u043c\u0430\u043d\u044b',\n       '\u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043b\u044e\u0431\u043e\u0432\u043d\u044b\u0435 \u0440\u043e\u043c\u0430\u043d\u044b', '\u0438\u0440\u043e\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0432\u044b',\n       '\u044e\u043c\u043e\u0440\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438', '\u043e\u0441\u0442\u0440\u043e\u0441\u044e\u0436\u0435\u0442\u043d\u044b\u0435 \u043b\u044e\u0431\u043e\u0432\u043d\u044b\u0435 \u0440\u043e\u043c\u0430\u043d\u044b',\n       '\u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0444\u0430\u043d\u0442\u0430\u0441\u0442\u0438\u043a\u0430', '\u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0440\u0443\u0441\u0441\u043a\u0430\u044f \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430',\n       '\u043a\u043e\u0440\u043e\u0442\u043a\u0438\u0435 \u043b\u044e\u0431\u043e\u0432\u043d\u044b\u0435 \u0440\u043e\u043c\u0430\u043d\u044b', '\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0432\u044b',\n       '\u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438', '\u0442\u0440\u0438\u043b\u043b\u0435\u0440\u044b', '\u0433\u043e\u0440\u043e\u0434\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438',\n       '\u043b\u044e\u0431\u043e\u0432\u043d\u043e-\u0444\u0430\u043d\u0442\u0430\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0440\u043e\u043c\u0430\u043d\u044b', '\u0438\u0441\u0442\u043e\u0440\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0444\u0430\u043d\u0442\u0430\u0441\u0442\u0438\u043a\u0430',\n       '\u043d\u0430\u0443\u0447\u043d\u0430\u044f \u0444\u0430\u043d\u0442\u0430\u0441\u0442\u0438\u043a\u0430', '\u044d\u0440\u043e\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0440\u043e\u043c\u0430\u043d\u044b', '\u043f\u043e\u043b\u0438\u0446\u0435\u0439\u0441\u043a\u0438\u0435 \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0432\u044b',\n       '\u0433\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0444\u0430\u043d\u0442\u0430\u0441\u0442\u0438\u043a\u0430', '\u043c\u0438\u0441\u0442\u0438\u043a\u0430', '\u0444\u044d\u043d\u0442\u0435\u0437\u0438 \u043f\u0440\u043e \u0434\u0440\u0430\u043a\u043e\u043d\u043e\u0432',\n       '\u0441\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u0430\u044f \u0444\u0430\u043d\u0442\u0430\u0441\u0442\u0438\u043a\u0430', '\u0438\u0441\u0442\u043e\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0432\u044b'],\n      dtype='object', name='genres')</pre> In\u00a0[24]: Copied! <pre># filter the column genres list to include only the top 50 genres \ndef filter_genres(genres_list, valid_genres=None):\n    if not genres_list:\n        return []\n    return [genre for genre in genres_list if genre in valid_genres]\n\ni_features_lfm['features'] = i_features_lfm['genres'].apply(filter_genres, valid_genres=set(item_tags))\ni_features_lfm.head()\n</pre> # filter the column genres list to include only the top 50 genres  def filter_genres(genres_list, valid_genres=None):     if not genres_list:         return []     return [genre for genre in genres_list if genre in valid_genres]  i_features_lfm['features'] = i_features_lfm['genres'].apply(filter_genres, valid_genres=set(item_tags)) i_features_lfm.head() Out[24]: title genres authors year reads features item_id item 7037 \u0412\u043e\u0440\u043e\u043d-\u0447\u0435\u043b\u043e\u0431\u0438\u0442\u0447\u0438\u043a [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u0434\u0435\u0442\u0441\u043a\u0438\u0435 \u043a\u043d\u0438\u0433\u0438, \u0441\u043a\u0430\u0437\u043a\u0438, \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f ... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d 1886 1 [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u0434\u0435\u0442\u0441\u043a\u0438\u0435 \u043a\u043d\u0438\u0433\u0438, \u0441\u043a\u0430\u0437\u043a\u0438, \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f ... item 11499 \u0421\u043a\u0440\u0438\u043f\u043a\u0430 \u0420\u043e\u0442\u0448\u0438\u043b\u044c\u0434\u0430 [\u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430, \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430, \u0440\u0443\u0441\u0441\u043a... \u0410\u043d\u0442\u043e\u043d \u0427\u0435\u0445\u043e\u0432 1894 1 [\u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430, \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430, \u0440\u0443\u0441\u0441\u043a... item 7296 \u0421\u043e\u0441\u0435\u0434\u0438 [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u0434\u0435\u0442\u0441\u043a\u0438\u0435 \u043a\u043d\u0438\u0433\u0438, \u0441\u043a\u0430\u0437\u043a\u0438, \u0440\u0443\u0441\u0441\u043a\u0430\u044f \u043a\u043b\u0430... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d NaN 1 [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u0434\u0435\u0442\u0441\u043a\u0438\u0435 \u043a\u043d\u0438\u0433\u0438, \u0441\u043a\u0430\u0437\u043a\u0438, \u0440\u0443\u0441\u0441\u043a\u0430\u044f \u043a\u043b\u0430... item 4125 \u042f\u0440\u043c\u0430\u0440\u043a\u0430 \u0442\u0449\u0435\u0441\u043b\u0430\u0432\u0438\u044f [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430, \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u0441\u0442\u0430\u0440\u0438\u043d\u043d\u0430\u044f \u043b\u0438\u0442... \u0423\u0438\u043b\u044c\u044f\u043c \u0422\u0435\u043a\u043a\u0435\u0440\u0435\u0439 1848 3 [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430, \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430, \u043b\u0438\u0442\u0435... item 15550 \u0425\u0438\u0442\u0440\u043e\u0441\u0442\u044c [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430, \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430] \u0413\u0438 \u0434\u0435 \u041c\u043e\u043f\u0430\u0441\u0441\u0430\u043d NaN 2 [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430, \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430] In\u00a0[25]: Copied! <pre>i_features_list = i_features_lfm['features']\ni_features_list\n</pre> i_features_list = i_features_lfm['features'] i_features_list Out[25]: <pre>item_id\nitem 7037     [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u0434\u0435\u0442\u0441\u043a\u0438\u0435 \u043a\u043d\u0438\u0433\u0438, \u0441\u043a\u0430\u0437\u043a\u0438, \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f ...\nitem 11499    [\u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430, \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430, \u0440\u0443\u0441\u0441\u043a...\nitem 7296     [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u0434\u0435\u0442\u0441\u043a\u0438\u0435 \u043a\u043d\u0438\u0433\u0438, \u0441\u043a\u0430\u0437\u043a\u0438, \u0440\u0443\u0441\u0441\u043a\u0430\u044f \u043a\u043b\u0430...\nitem 4125     [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430, \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430, \u043b\u0438\u0442\u0435...\nitem 15550            [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430, \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430]\n                                    ...                        \nitem 14903    [\u044d\u0440\u043e\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0440\u043e\u043c\u0430\u043d\u044b, \u043a\u043e\u0440\u043e\u0442\u043a\u0438\u0435 \u043b\u044e\u0431\u043e\u0432\u043d\u044b\u0435 \u0440\u043e\u043c\u0430\u043d\u044b,...\nitem 12570                                [\u043f\u043e\u043b\u0438\u0442\u043e\u043b\u043e\u0433\u0438\u044f, \u0433\u0430\u0437\u0435\u0442\u044b]\nitem 3146                                  [\u0436\u0443\u0440\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u0438\u0437\u0434\u0430\u043d\u0438\u044f]\nitem 16184                                [\u043f\u043e\u043b\u0438\u0442\u043e\u043b\u043e\u0433\u0438\u044f, \u0433\u0430\u0437\u0435\u0442\u044b]\nitem 3477                                              [\u0433\u0430\u0437\u0435\u0442\u044b]\nName: features, Length: 17512, dtype: object</pre> In\u00a0[26]: Copied! <pre>print(user_tags)\nprint(item_tags[:10])\n</pre> print(user_tags) print(item_tags[:10]) <pre>{'sex_0.0', 'age_35_44', 'age_25_34', 'age_55_64', 'age_45_54', 'age_65_inf', 'age_18_24', 'sex_1.0'}\nIndex(['\u043b\u044e\u0431\u043e\u0432\u043d\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438', '\u043f\u043e\u043f\u0430\u0434\u0430\u043d\u0446\u044b', '\u0433\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438',\n       '\u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0432\u044b', '\u043c\u0430\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0430\u043a\u0430\u0434\u0435\u043c\u0438\u0438', '\u0431\u043e\u0435\u0432\u0430\u044f \u0444\u0430\u043d\u0442\u0430\u0441\u0442\u0438\u043a\u0430',\n       '\u043a\u043d\u0438\u0433\u0438 \u043f\u0440\u043e \u0432\u043e\u043b\u0448\u0435\u0431\u043d\u0438\u043a\u043e\u0432', '\u0431\u043e\u0435\u0432\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438', '\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u043b\u044e\u0431\u043e\u0432\u043d\u044b\u0435 \u0440\u043e\u043c\u0430\u043d\u044b',\n       '\u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043b\u044e\u0431\u043e\u0432\u043d\u044b\u0435 \u0440\u043e\u043c\u0430\u043d\u044b'],\n      dtype='object', name='genres')\n</pre> In\u00a0[27]: Copied! <pre>'''\n\nCreate LightFM dataset \n\n'''\n\nlfm_dataset = LFMDataset()\n\n# unique value in interactions (user,items)\nlfm_dataset.fit_partial(users=df['user_id'].unique(), \n                        items=df['item_id'].unique())\n\nlfm_dataset.fit_partial(user_features=user_tags, \n                        item_features=item_tags)\n\n# \u043c\u0430\u043f\u043f\u0435\u0440\u044b \u0434\u043b\u044f id LightFM \u0438 id \u0434\u0430\u043d\u043d\u044b\u0445\nuser_mapping, item_mapping = lfm_dataset.mapping()[0], lfm_dataset.mapping()[2]\ninv_user_mapping = {value: key for key, value in user_mapping.items()}\ninv_item_mapping = {value: key for key, value in item_mapping.items()}\n\n'''\n\nBuild features\n\n'''\n\nsparse_i_features = lfm_dataset.build_item_features([[row.item_id, row.features] for row in i_features_list.reset_index().itertuples()])\nsparse_u_features = lfm_dataset.build_user_features([[row.user_id, row.features] for row in u_features_list.reset_index().itertuples()])\n(interactions, weights) = lfm_dataset.build_interactions([(row.user_id, row.item_id, row.progress) for row in train.itertuples()])\n\nprint('interactions',interactions.shape) \nprint('interactions data',interactions.data[:10])\nprint('interactions weight', weights.data[:10])\n\n\n'''\n\nTrain Model\n\n'''\n\nlightfm = LightFM(no_components=20, \n                  loss='warp')\n\nlightfm.fit(interactions, \n            user_features=sparse_u_features, \n            item_features=sparse_i_features, \n            epochs=40, num_threads=8)\n</pre> '''  Create LightFM dataset   '''  lfm_dataset = LFMDataset()  # unique value in interactions (user,items) lfm_dataset.fit_partial(users=df['user_id'].unique(),                          items=df['item_id'].unique())  lfm_dataset.fit_partial(user_features=user_tags,                          item_features=item_tags)  # \u043c\u0430\u043f\u043f\u0435\u0440\u044b \u0434\u043b\u044f id LightFM \u0438 id \u0434\u0430\u043d\u043d\u044b\u0445 user_mapping, item_mapping = lfm_dataset.mapping()[0], lfm_dataset.mapping()[2] inv_user_mapping = {value: key for key, value in user_mapping.items()} inv_item_mapping = {value: key for key, value in item_mapping.items()}  '''  Build features  '''  sparse_i_features = lfm_dataset.build_item_features([[row.item_id, row.features] for row in i_features_list.reset_index().itertuples()]) sparse_u_features = lfm_dataset.build_user_features([[row.user_id, row.features] for row in u_features_list.reset_index().itertuples()]) (interactions, weights) = lfm_dataset.build_interactions([(row.user_id, row.item_id, row.progress) for row in train.itertuples()])  print('interactions',interactions.shape)  print('interactions data',interactions.data[:10]) print('interactions weight', weights.data[:10])   '''  Train Model  '''  lightfm = LightFM(no_components=20,                    loss='warp')  lightfm.fit(interactions,              user_features=sparse_u_features,              item_features=sparse_i_features,              epochs=40, num_threads=8) <pre>interactions (1565, 17512)\ninteractions data [1 1 1 1 1 1 1 1 1 1]\ninteractions weight [ 64. 100.  35.  42. 100. 100. 100. 100. 100. 100.]\n</pre> Out[27]: <pre>&lt;lightfm.lightfm.LightFM at 0x7c479fc3b790&gt;</pre> In\u00a0[28]: Copied! <pre>def inference(model,\n              user_id,\n              top_k=10,\n              user_features=None,\n              item_features=None):\n\n    # user_ids -&gt; LightFM id \n    pred = model.predict(user_ids=user_id,  # user_id LightFM\n                           item_ids=sorted(item_mapping.values()), # \u0412\u0441\u0435 item_id LightFM\n                           user_features=sparse_u_features, \n                           item_features=sparse_i_features)\n    \n    k = top_k\n    ids = np.argpartition(pred, - k)[- k:] \n    rel = pred[ids]\n    res = pd.DataFrame(zip(ids, rel), \n                       columns=['y_pred', 'relevance'])\n    # res['item_id'] = res['item_id'].map(inv_item_mapping)\n    res['user_id'] = [user_id for i in range(0,res.shape[0])]\n    return res\n</pre> def inference(model,               user_id,               top_k=10,               user_features=None,               item_features=None):      # user_ids -&gt; LightFM id      pred = model.predict(user_ids=user_id,  # user_id LightFM                            item_ids=sorted(item_mapping.values()), # \u0412\u0441\u0435 item_id LightFM                            user_features=sparse_u_features,                             item_features=sparse_i_features)          k = top_k     ids = np.argpartition(pred, - k)[- k:]      rel = pred[ids]     res = pd.DataFrame(zip(ids, rel),                         columns=['y_pred', 'relevance'])     # res['item_id'] = res['item_id'].map(inv_item_mapping)     res['user_id'] = [user_id for i in range(0,res.shape[0])]     return res In\u00a0[29]: Copied! <pre>lst_inference = []\n\n#  \u0414\u043b\u044f \u0432\u0441\u0435\u0445 user_id \u0438\u0437 train (user X)\nfor user in tqdm(sorted(user_mapping.values())):\n    lst_inference.append(inference(model=lightfm,\n                          user_id=user,\n                          top_k=20))\n\ndf_inference = pd.concat(lst_inference)\n</pre> lst_inference = []  #  \u0414\u043b\u044f \u0432\u0441\u0435\u0445 user_id \u0438\u0437 train (user X) for user in tqdm(sorted(user_mapping.values())):     lst_inference.append(inference(model=lightfm,                           user_id=user,                           top_k=20))  df_inference = pd.concat(lst_inference) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1565/1565 [00:07&lt;00:00, 196.68it/s]\n</pre> In\u00a0[30]: Copied! <pre>df_inference.head()\n</pre> df_inference.head() Out[30]: y_pred relevance user_id 0 10 -47.367077 0 1 1542 -47.349598 0 2 6356 -47.328232 0 3 930 -47.339104 0 4 2938 -47.288143 0 In\u00a0[31]: Copied! <pre># \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u0442 LightFM item_id -&gt; df item_id\ndef map_predictions(x):\n    return inv_item_mapping[x['y_pred']]\n\n# \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u0442 LightFM user_id -&gt; df user_id\ndef map_users(x):\n    return inv_user_mapping[x['user_id']]\n\ndf_inference['y_pred'] = df_inference.apply(map_predictions,axis=1)\ndf_inference['user_id'] = df_inference.apply(map_users,axis=1)\n\n# aggregations for [user_id]\nagg_inf = df_inference.groupby('user_id')['y_pred'].apply(lambda x: list(x)).to_frame()\nagg_train = train.groupby('user_id')['item_id'].apply(lambda x: list(x)).to_frame()\nmerged_inf = agg_inf.merge(agg_train,left_index=True,right_index=True)\nmerged_inf.head()\n</pre> # \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u0442 LightFM item_id -&gt; df item_id def map_predictions(x):     return inv_item_mapping[x['y_pred']]  # \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u0442 LightFM user_id -&gt; df user_id def map_users(x):     return inv_user_mapping[x['user_id']]  df_inference['y_pred'] = df_inference.apply(map_predictions,axis=1) df_inference['user_id'] = df_inference.apply(map_users,axis=1)  # aggregations for [user_id] agg_inf = df_inference.groupby('user_id')['y_pred'].apply(lambda x: list(x)).to_frame() agg_train = train.groupby('user_id')['item_id'].apply(lambda x: list(x)).to_frame() merged_inf = agg_inf.merge(agg_train,left_index=True,right_index=True) merged_inf.head() Out[31]: y_pred item_id user_id user 0 [item 1729, item 10171, item 3252, item 9847, ... [item 8391, item 11677, item 9991, item 13581,... user 1 [item 260, item 10469, item 2469, item 3398, i... [item 15894, item 10185, item 7475, item 13534... user 10 [item 15456, item 9481, item 4596, item 8656, ... [item 13448, item 2236, item 5657, item 14069,... user 100 [item 14037, item 13055, item 8543, item 14616... [item 11373, item 7555, item 8133, item 2662, ... user 1000 [item 17039, item 13021, item 8513, item 669, ... [item 9829, item 5744, item 10201, item 7491, ... In\u00a0[32]: Copied! <pre># \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c precision \nprint('precision k@20:',round(precision(merged_inf,\n                                        pred_col='y_pred',\n                                        true_col='item_id',\n                                        k=20),4))\n</pre> # \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c precision  print('precision k@20:',round(precision(merged_inf,                                         pred_col='y_pred',                                         true_col='item_id',                                         k=20),4)) <pre>precision k@20: 0.1853\n</pre> In\u00a0[33]: Copied! <pre>'''\n\nCreate LightFM dataset \n\n'''\n\nlfm_dataset = LFMDataset()\n\nlfm_dataset.fit_partial(users=df['user_id'].unique(), \n                        items=df['item_id'].unique())\n\nlfm_dataset.fit_partial(user_features=user_tags, \n                        item_features=item_tags)\n\nuser_mapping, item_mapping = lfm_dataset.mapping()[0], lfm_dataset.mapping()[2]\ninv_user_mapping = {value: key for key, value in user_mapping.items()}\ninv_item_mapping = {value: key for key, value in item_mapping.items()}\n\nprint('interactions shape')\nprint(lfm_dataset.interactions_shape())\n\n'''\n\nBuild features (Interactions Only)\n\n'''\n\n# train - interactions dataset \n(interactions, weights) = lfm_dataset.build_interactions([(row.user_id, row.item_id, row.progress) for row in train.itertuples()])\n\nprint('interactions',interactions.shape) \nprint('interactions data',interactions.data[:10])\nprint('interactions weight', weights.data[:10])\n\n\n'''\n\nTrain Model\n\n'''\n\nlightfm_inter = LightFM(no_components=20, \n                        loss='warp')\n\nlightfm_inter.fit(interactions, \n                  epochs=40, \n                  num_threads=8)\n</pre> '''  Create LightFM dataset   '''  lfm_dataset = LFMDataset()  lfm_dataset.fit_partial(users=df['user_id'].unique(),                          items=df['item_id'].unique())  lfm_dataset.fit_partial(user_features=user_tags,                          item_features=item_tags)  user_mapping, item_mapping = lfm_dataset.mapping()[0], lfm_dataset.mapping()[2] inv_user_mapping = {value: key for key, value in user_mapping.items()} inv_item_mapping = {value: key for key, value in item_mapping.items()}  print('interactions shape') print(lfm_dataset.interactions_shape())  '''  Build features (Interactions Only)  '''  # train - interactions dataset  (interactions, weights) = lfm_dataset.build_interactions([(row.user_id, row.item_id, row.progress) for row in train.itertuples()])  print('interactions',interactions.shape)  print('interactions data',interactions.data[:10]) print('interactions weight', weights.data[:10])   '''  Train Model  '''  lightfm_inter = LightFM(no_components=20,                          loss='warp')  lightfm_inter.fit(interactions,                    epochs=40,                    num_threads=8) <pre>interactions shape\n(1565, 17512)\ninteractions (1565, 17512)\ninteractions data [1 1 1 1 1 1 1 1 1 1]\ninteractions weight [ 64. 100.  35.  42. 100. 100. 100. 100. 100. 100.]\n</pre> Out[33]: <pre>&lt;lightfm.lightfm.LightFM at 0x7c479ae5fa00&gt;</pre> In\u00a0[34]: Copied! <pre>def inference(model,\n              user_id,\n              top_k=10,\n              user_features=None,\n              item_features=None):\n\n    # user_ids -&gt; LightFM id \n    pred = model.predict(user_ids=user_id,  # user_id LightFM\n                         item_ids=sorted(item_mapping.values())) # \u0412\u0441\u0435 item_id LightFM\n    \n    k = top_k\n    ids = np.argpartition(pred, - k)[- k:] \n    rel = pred[ids]\n    res = pd.DataFrame(zip(ids, rel), \n                       columns=['y_pred', 'relevance'])\n    # res['item_id'] = res['item_id'].map(inv_item_mapping)\n    res['user_id'] = [user_id for i in range(0,res.shape[0])]\n    return res\n</pre> def inference(model,               user_id,               top_k=10,               user_features=None,               item_features=None):      # user_ids -&gt; LightFM id      pred = model.predict(user_ids=user_id,  # user_id LightFM                          item_ids=sorted(item_mapping.values())) # \u0412\u0441\u0435 item_id LightFM          k = top_k     ids = np.argpartition(pred, - k)[- k:]      rel = pred[ids]     res = pd.DataFrame(zip(ids, rel),                         columns=['y_pred', 'relevance'])     # res['item_id'] = res['item_id'].map(inv_item_mapping)     res['user_id'] = [user_id for i in range(0,res.shape[0])]     return res In\u00a0[35]: Copied! <pre>lst_inference = []\nfor user in tqdm(sorted(user_mapping.values())):\n    lst_inference.append(inference(model=lightfm_inter,\n                          user_id=user,\n                          top_k=20))\n\ndf_inference_inter = pd.concat(lst_inference)\n</pre> lst_inference = [] for user in tqdm(sorted(user_mapping.values())):     lst_inference.append(inference(model=lightfm_inter,                           user_id=user,                           top_k=20))  df_inference_inter = pd.concat(lst_inference) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1565/1565 [00:06&lt;00:00, 248.41it/s]\n</pre> In\u00a0[36]: Copied! <pre>df_inference_inter.head()\n</pre> df_inference_inter.head() Out[36]: y_pred relevance user_id 0 2956 1.709743 0 1 111 1.717322 0 2 61 1.762205 0 3 272 1.861350 0 4 3001 1.842448 0 In\u00a0[37]: Copied! <pre>def map_predictions(x):\n    return inv_item_mapping[x['y_pred']]\n\ndef map_users(x):\n    return inv_user_mapping[x['user_id']]\n\ndf_inference_inter['y_pred'] = df_inference_inter.apply(map_predictions,axis=1)\ndf_inference_inter['user_id'] = df_inference_inter.apply(map_users,axis=1)\ndf_inference_inter.head()\n</pre> def map_predictions(x):     return inv_item_mapping[x['y_pred']]  def map_users(x):     return inv_user_mapping[x['user_id']]  df_inference_inter['y_pred'] = df_inference_inter.apply(map_predictions,axis=1) df_inference_inter['user_id'] = df_inference_inter.apply(map_users,axis=1) df_inference_inter.head() Out[37]: y_pred relevance user_id 0 item 16997 1.709743 user 921 1 item 16554 1.717322 user 921 2 item 1573 1.762205 user 921 3 item 2298 1.861350 user 921 4 item 12542 1.842448 user 921 In\u00a0[38]: Copied! <pre>agg_inf_inter = df_inference_inter.groupby('user_id')['y_pred'].apply(lambda x: list(x)).to_frame()\nagg_train = train.groupby('user_id')['item_id'].apply(lambda x: list(x)).to_frame()\nmerged_inf_inter = agg_inf_inter.merge(agg_train,left_index=True,right_index=True)\n</pre> agg_inf_inter = df_inference_inter.groupby('user_id')['y_pred'].apply(lambda x: list(x)).to_frame() agg_train = train.groupby('user_id')['item_id'].apply(lambda x: list(x)).to_frame() merged_inf_inter = agg_inf_inter.merge(agg_train,left_index=True,right_index=True) In\u00a0[39]: Copied! <pre>print('precision k@20:',round(precision(merged_inf_inter,\n                                        pred_col='y_pred',\n                                        true_col='item_id',\n                                        k=20),4))\n</pre> print('precision k@20:',round(precision(merged_inf_inter,                                         pred_col='y_pred',                                         true_col='item_id',                                         k=20),4)) <pre>precision k@20: 0.4533\n</pre> In\u00a0[40]: Copied! <pre>'''\n\nCreate LightFM dataset \n\n'''\n\nlfm_dataset = LFMDataset()\n\nlfm_dataset.fit_partial(users=df['user_id'].unique(), \n                        items=df['item_id'].unique())\n\nlfm_dataset.fit_partial(user_features=user_tags, \n                        item_features=item_tags)\n\nuser_mapping, item_mapping = lfm_dataset.mapping()[0], lfm_dataset.mapping()[2]\ninv_user_mapping = {value: key for key, value in user_mapping.items()}\ninv_item_mapping = {value: key for key, value in item_mapping.items()}\n\nnum_users, num_items = lfm_dataset.interactions_shape()\n\n'''\n\nBuild features\n\n'''\n\nsparse_i_features = lfm_dataset.build_item_features([[row.item_id, row.features] for row in i_features_list.reset_index().itertuples()])\nsparse_u_features = lfm_dataset.build_user_features([[row.user_id, row.features] for row in u_features_list.reset_index().itertuples()])\n\n\n'''\n\nTrain Model\n\n'''\n\ndummy = csr_matrix(np.zeros((num_users, num_items)))\n\nlightfm_feat = LightFM(no_components=20, \n                      loss='warp')\n\nlightfm_feat.fit(interactions=dummy, \n                    user_features=sparse_u_features, \n                    item_features=sparse_i_features, \n                    epochs=40)\n</pre> '''  Create LightFM dataset   '''  lfm_dataset = LFMDataset()  lfm_dataset.fit_partial(users=df['user_id'].unique(),                          items=df['item_id'].unique())  lfm_dataset.fit_partial(user_features=user_tags,                          item_features=item_tags)  user_mapping, item_mapping = lfm_dataset.mapping()[0], lfm_dataset.mapping()[2] inv_user_mapping = {value: key for key, value in user_mapping.items()} inv_item_mapping = {value: key for key, value in item_mapping.items()}  num_users, num_items = lfm_dataset.interactions_shape()  '''  Build features  '''  sparse_i_features = lfm_dataset.build_item_features([[row.item_id, row.features] for row in i_features_list.reset_index().itertuples()]) sparse_u_features = lfm_dataset.build_user_features([[row.user_id, row.features] for row in u_features_list.reset_index().itertuples()])   '''  Train Model  '''  dummy = csr_matrix(np.zeros((num_users, num_items)))  lightfm_feat = LightFM(no_components=20,                        loss='warp')  lightfm_feat.fit(interactions=dummy,                      user_features=sparse_u_features,                      item_features=sparse_i_features,                      epochs=40) Out[40]: <pre>&lt;lightfm.lightfm.LightFM at 0x7c479ae5fd30&gt;</pre> In\u00a0[41]: Copied! <pre>def inference(model,\n              user_id,\n              top_k=10,\n              user_features=None,\n              item_features=None):\n\n    # user_ids -&gt; LightFM id \n    pred = model.predict(user_ids=user_id,  # user_id LightFM\n                           item_ids=sorted(item_mapping.values()),  # \u0412\u0441\u0435 item_id LightFM\n                           user_features=sparse_u_features, \n                           item_features=sparse_i_features,num_threads=16)\n    \n    k = top_k\n    ids = np.argpartition(pred, - k)[- k:] \n    rel = pred[ids]\n    res = pd.DataFrame(zip(ids, rel), \n                       columns=['y_pred', 'relevance'])\n    # res['item_id'] = res['item_id'].map(inv_item_mapping)\n    res['user_id'] = [user_id for i in range(0,res.shape[0])]\n    return res\n</pre> def inference(model,               user_id,               top_k=10,               user_features=None,               item_features=None):      # user_ids -&gt; LightFM id      pred = model.predict(user_ids=user_id,  # user_id LightFM                            item_ids=sorted(item_mapping.values()),  # \u0412\u0441\u0435 item_id LightFM                            user_features=sparse_u_features,                             item_features=sparse_i_features,num_threads=16)          k = top_k     ids = np.argpartition(pred, - k)[- k:]      rel = pred[ids]     res = pd.DataFrame(zip(ids, rel),                         columns=['y_pred', 'relevance'])     # res['item_id'] = res['item_id'].map(inv_item_mapping)     res['user_id'] = [user_id for i in range(0,res.shape[0])]     return res In\u00a0[42]: Copied! <pre>lst_inference = []\nfor user in tqdm(sorted(user_mapping.values())):\n    lst_inference.append(inference(model=lightfm_feat,\n                          user_id=user,\n                          top_k=20))\n\ndf_inference_feat = pd.concat(lst_inference)\n</pre> lst_inference = [] for user in tqdm(sorted(user_mapping.values())):     lst_inference.append(inference(model=lightfm_feat,                           user_id=user,                           top_k=20))  df_inference_feat = pd.concat(lst_inference) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1565/1565 [00:06&lt;00:00, 253.49it/s]\n</pre> In\u00a0[43]: Copied! <pre>def map_predictions(x):\n    return inv_item_mapping[x['y_pred']]\n\ndef map_users(x):\n    return inv_user_mapping[x['user_id']]\n\ndf_inference_feat['y_pred'] = df_inference_feat.apply(map_predictions,axis=1)\ndf_inference_feat['user_id'] = df_inference_feat.apply(map_users,axis=1)\n\n# aggregations for [user_id]\nagg_inf_feat = df_inference_feat.groupby('user_id')['y_pred'].apply(lambda x: list(x)).to_frame()\nagg_train = train.groupby('user_id')['item_id'].apply(lambda x: list(x)).to_frame()\nmerged_inf_feat = agg_inf_feat.merge(agg_train,left_index=True,right_index=True)\nmerged_inf_feat.head()\n</pre> def map_predictions(x):     return inv_item_mapping[x['y_pred']]  def map_users(x):     return inv_user_mapping[x['user_id']]  df_inference_feat['y_pred'] = df_inference_feat.apply(map_predictions,axis=1) df_inference_feat['user_id'] = df_inference_feat.apply(map_users,axis=1)  # aggregations for [user_id] agg_inf_feat = df_inference_feat.groupby('user_id')['y_pred'].apply(lambda x: list(x)).to_frame() agg_train = train.groupby('user_id')['item_id'].apply(lambda x: list(x)).to_frame() merged_inf_feat = agg_inf_feat.merge(agg_train,left_index=True,right_index=True) merged_inf_feat.head() Out[43]: y_pred item_id user_id user 0 [item 12733, item 17326, item 16302, item 1238... [item 8391, item 11677, item 9991, item 13581,... user 1 [item 13967, item 13915, item 8750, item 9356,... [item 15894, item 10185, item 7475, item 13534... user 10 [item 16041, item 3718, item 4034, item 4323, ... [item 13448, item 2236, item 5657, item 14069,... user 100 [item 11072, item 13483, item 5651, item 15967... [item 11373, item 7555, item 8133, item 2662, ... user 1000 [item 12384, item 13038, item 17507, item 3081... [item 9829, item 5744, item 10201, item 7491, ... In\u00a0[44]: Copied! <pre>print('precision k@20:',round(precision(merged_inf_feat,\n                                        pred_col='y_pred',\n                                        true_col='item_id',\n                                        k=20),4))\n</pre> print('precision k@20:',round(precision(merged_inf_feat,                                         pred_col='y_pred',                                         true_col='item_id',                                         k=20),4)) <pre>precision k@20: 0.0008\n</pre>"},{"location":"portfolio/course_recsys/prob_lightfm.html","title":"\u041a\u043e\u043d\u0442\u0435\u043d\u0442\u043d\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439\u00b6","text":""},{"location":"portfolio/course_recsys/prob_lightfm.html#1","title":"1 | \u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":"<ul> <li>\u041f\u0440\u043e\u0434\u043e\u043b\u0436\u0438\u043c \u0440\u0430\u0431\u043e\u0442\u0443 \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 mts-library: https://www.kaggle.com/datasets/sharthz23/mts-library</li> <li>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 LightFM \u0442\u043e\u043b\u044c\u043a\u043e \u043d\u0430 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f\u0445 \u0438 \u0442\u043e\u043b\u044c\u043a\u043e \u043d\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u0430\u0439\u0442\u0435\u043c\u043e\u0432.</li> <li>\u0421\u0440\u0430\u0432\u043d\u0438\u0442\u0435 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439, \u0430 \u0442\u0430\u043a\u0436\u0435 \u043c\u043e\u0434\u0435\u043b\u0438, \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0435\u043d\u043d\u043e\u0439 \u043d\u0430 \u0437\u0430\u043d\u044f\u0442\u0438\u0438, \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0435 precision@20</li> <li>\u041d\u0430\u043f\u0438\u0448\u0438\u0442\u0435 \u0432\u044b\u0432\u043e\u0434\u044b \u043e \u043f\u0440\u043e\u0434\u0435\u043b\u0430\u043d\u043d\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u0435: \u0432 \u043a\u0430\u043a\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0438 \u043e\u043a\u0430\u0437\u0430\u043b\u043e\u0441\u044c \u043b\u0443\u0447\u0448\u0435, \u043f\u043e\u0447\u0435\u043c\u0443.</li> </ul>"},{"location":"portfolio/course_recsys/prob_lightfm.html#2","title":"2 | \u0427\u0442\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<p>\u0412\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u0442\u0435\u0439 \u0438 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043e\u0432</p>"},{"location":"portfolio/course_recsys/prob_lightfm.html#4","title":"4 | \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430\u00b6","text":""},{"location":"portfolio/course_recsys/prob_lightfm.html","title":"\u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\u00b6","text":"<p>\u041f\u043e\u0434\u0433\u043e\u0442\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c \u0444\u0438\u0447\u0438 \u0434\u043b\u044f <code>LightFM</code></p>"},{"location":"portfolio/course_recsys/prob_lightfm.html#a","title":"(a) \u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u00b6","text":"<p>\u0423 \u043d\u0430\u0441 \u0442\u043e\u043b\u044c\u043a\u043e \u0434\u0432\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</p>"},{"location":"portfolio/course_recsys/prob_lightfm.html#b","title":"(b) \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043e\u0432\u00b6","text":"<p>\u0418\u043c\u0435\u044f <code>genres</code>, \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0443 \u043f\u043e \u0436\u0430\u043d\u0440\u0430\u043c \u0438 \u043e\u0441\u0442\u0430\u0432\u0438\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u043e\u043f 100 \u0436\u0430\u043d\u0440\u043e\u0432</p>"},{"location":"portfolio/course_recsys/prob_lightfm.html#5-1","title":"5 | \u041c\u043e\u0434\u0435\u043b\u044c 1 : \u0412\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439 &amp; \u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\u00b6","text":"<p>\u0414\u043b\u044f \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u043f\u043e\u0434\u0445\u043e\u0434\u0430, \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043f\u043e\u0434\u0445\u043e\u0434 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0434\u0435\u043b\u0430\u043b\u0438 \u043d\u0430 \u0437\u0430\u043d\u044f\u0442\u0438\u0438 \u0433\u0434\u0435 \u043c\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u0438 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u0438 \u0444\u0438\u0447\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0438 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u0430</p>"},{"location":"portfolio/course_recsys/prob_lightfm.html#lightfm-dataset","title":"LightFM dataset\u00b6","text":""},{"location":"portfolio/course_recsys/prob_lightfm.html#6-2","title":"6 | \u041c\u043e\u0434\u0435\u043b\u044c 2 : \u0422\u043e\u043b\u044c\u043a\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439\u00b6","text":"<p>\u0414\u043b\u044f \u0432\u0442\u043e\u0440\u043e\u0433\u043e \u043f\u043e\u0434\u0445\u043e\u0434\u0430 \u043c\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438</p>"},{"location":"portfolio/course_recsys/prob_lightfm.html#lightfm-dataset","title":"LightFM dataset\u00b6","text":""},{"location":"portfolio/course_recsys/prob_lightfm.html#7-3","title":"7 | \u041c\u043e\u0434\u0435\u043b\u044c 3 : \u0422\u043e\u043b\u044c\u043a\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\u00b6","text":"<p>\u0414\u043b\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0433\u043e \u043f\u043e\u0434\u0445\u043e\u0434\u0430 \u043c\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0438 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043e\u0432</p>"},{"location":"portfolio/course_recsys/prob_lightfm.html#lightfm-dataset","title":"LightFM dataset\u00b6","text":""},{"location":"portfolio/course_recsys/prob_mf.html","title":"Prob mf","text":"In\u00a0[1]: Copied! <pre>!pip install replay-rec --quiet\n!pip install implicit --quiet\n!pip install lightfm --quiet\n!pip install apyori --quiet\n</pre> !pip install replay-rec --quiet !pip install implicit --quiet !pip install lightfm --quiet !pip install apyori --quiet <pre>     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 196.8/196.8 kB 8.7 MB/s eta 0:00:00\r\n  Preparing metadata (setup.py) ... done\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n  Preparing metadata (pyproject.toml) ... done\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 235.9/235.9 kB 14.7 MB/s eta 0:00:00\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 390.6/390.6 kB 24.2 MB/s eta 0:00:00\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 31.0/31.0 MB 4.2 MB/s eta 0:00:00\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 290.5/290.5 kB 16.3 MB/s eta 0:00:00\r\n  Building wheel for fixed-install-nmslib (setup.py) ... done\r\n  Building wheel for hnswlib (pyproject.toml) ... done\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\ndistributed 2024.8.0 requires dask==2024.8.0, but you have dask 2024.12.1 which is incompatible.\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8.9/8.9 MB 95.1 MB/s eta 0:00:00\r\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 316.4/316.4 kB 14.6 MB/s eta 0:00:00\r\n  Preparing metadata (setup.py) ... done\r\n  Building wheel for lightfm (setup.py) ... done\r\n  Preparing metadata (setup.py) ... done\r\n  Building wheel for apyori (setup.py) ... done\r\n</pre> In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.metrics import mean_absolute_error\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse.linalg import svds\nimport scipy.sparse as sparse\nimport scipy\nfrom tqdm import tqdm\nimport warnings; warnings.filterwarnings('ignore')\n\nfrom replay.metrics import Experiment\nimport implicit # \u0434\u043b\u044f ALS\nfrom datasets import load_dataset\nimport matplotlib.pyplot as plt\n</pre> import numpy as np import pandas as pd import os from sklearn.metrics import mean_absolute_error from scipy.sparse import csr_matrix from scipy.sparse.linalg import svds import scipy.sparse as sparse import scipy from tqdm import tqdm import warnings; warnings.filterwarnings('ignore')  from replay.metrics import Experiment import implicit # \u0434\u043b\u044f ALS from datasets import load_dataset import matplotlib.pyplot as plt In\u00a0[3]: Copied! <pre>dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \n                       \"raw_review_Software\", \n                       trust_remote_code=True)\ndataset_df = dataset['full'].to_pandas()\n\n# \u041a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c datetime \u0432 ms\ndataset_df['timestamp'] = pd.to_datetime(dataset_df['timestamp'], unit='ms')\n\ndataset_df.drop(['title','text','images','helpful_vote','verified_purchase'],axis=1,inplace=True)\ndataset_df = dataset_df.rename(columns={'parent_asin':'item_id'})\n</pre> dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\",                         \"raw_review_Software\",                         trust_remote_code=True) dataset_df = dataset['full'].to_pandas()  # \u041a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c datetime \u0432 ms dataset_df['timestamp'] = pd.to_datetime(dataset_df['timestamp'], unit='ms')  dataset_df.drop(['title','text','images','helpful_vote','verified_purchase'],axis=1,inplace=True) dataset_df = dataset_df.rename(columns={'parent_asin':'item_id'}) <p>\u0423\u0434\u0430\u043b\u044f\u0435\u043c \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u0434\u043b\u044f \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0438 \u0442\u0440\u0435\u0445 \u043a\u043e\u043b\u043e\u043d\u043e\u043a <code>user_id</code>, <code>item_id</code>, <code>timestamp</code></p> In\u00a0[4]: Copied! <pre># \u043f\u043e\u0434\u0441\u0447\u0435\u0442 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \ndataset_df[['user_id','item_id','timestamp']].value_counts()\n</pre> # \u043f\u043e\u0434\u0441\u0447\u0435\u0442 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432  dataset_df[['user_id','item_id','timestamp']].value_counts() Out[4]: <pre>user_id                       item_id     timestamp              \nAGALPU5ARZEK75CGKYELK232AHGA  B06XP4F49R  2019-01-23 22:01:29.630    27\nAE3QHFBC2YC4YRQJZWSP2Z7LUVNA  B00BJ4ETBW  2015-04-25 07:45:40.000    11\nAHK7CJWVLIXTRDTPJ5ZEEL56UCVA  B01LXOU5PM  2018-09-26 00:17:44.554    10\nAGCHXQUQ54YTXOGYRWFSV4MQFMQA  B00O109QX2  2018-04-08 15:37:17.347    10\nAGNGFKMPODAP7XWAGIC42WCSGBMQ  B00IG2DOKM  2016-08-18 13:08:27.000    10\n                                                                     ..\nAFEUXIC24P7FP4T6M6M5CZO33NKQ  B00DVKOYBM  2015-09-07 20:33:11.000     1\nAFEUXHN4I3PIA6WYZQT2SEJUSXWA  B00XGNNN52  2017-07-29 18:00:39.829     1\n                              B0081JPTXK  2013-04-03 18:31:55.000     1\n                              B007TBAQCK  2016-07-23 16:24:17.000     1\nAHZZZYHANWL2OW5PGXDOBUTJXCTA  B07RCG9SRL  2020-04-23 04:02:34.597     1\nName: count, Length: 4829120, dtype: int64</pre> In\u00a0[5]: Copied! <pre># \u0443\u0434\u0430\u043b\u044f\u0435\u043c \u044d\u0442\u0438 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b (\u043e\u0434\u043d\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e \u043f\u0440\u0435\u043e\u0431\u0440\u0435\u043b \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0442\u043e\u0432\u0430\u0440\u043e\u0432)\ndataset_df.drop_duplicates(subset=['user_id','item_id','timestamp'],inplace=True)\n</pre> # \u0443\u0434\u0430\u043b\u044f\u0435\u043c \u044d\u0442\u0438 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b (\u043e\u0434\u043d\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e \u043f\u0440\u0435\u043e\u0431\u0440\u0435\u043b \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0442\u043e\u0432\u0430\u0440\u043e\u0432) dataset_df.drop_duplicates(subset=['user_id','item_id','timestamp'],inplace=True) In\u00a0[6]: Copied! <pre>meta = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \n                    \"raw_meta_Software\",\n                    trust_remote_code=True)\nldf = meta[\"full\"].to_pandas()\nldf = ldf.rename(columns={'parent_asin':'item_id'})\nldf.drop(['features','description','images','videos','details',\n          'bought_together','subtitle','author','categories'],axis=1,inplace=True)\nldf = ldf[~(ldf['price'] == 'None')]\nldf['price'] = ldf['price'].astype('float')\nldf.head()\n</pre> meta = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\",                      \"raw_meta_Software\",                     trust_remote_code=True) ldf = meta[\"full\"].to_pandas() ldf = ldf.rename(columns={'parent_asin':'item_id'}) ldf.drop(['features','description','images','videos','details',           'bought_together','subtitle','author','categories'],axis=1,inplace=True) ldf = ldf[~(ldf['price'] == 'None')] ldf['price'] = ldf['price'].astype('float') ldf.head() Out[6]: main_category title average_rating rating_number price store item_id 0 Appstore for Android Accupressure Guide 3.6 NaN 0.00 mAppsguru B00VRPSGEO 1 Appstore for Android Ankylosaurus Fights Back - Smithsonian's Prehi... 4.0 NaN 2.99 Oceanhouse Media, Inc B00NWQXXHQ 2 Appstore for Android Mahjong 2015 3.1 NaN 0.00 sophiathach B00RFKP6AC 3 Appstore for Android Jewels Brick Breakout 4.2 NaN 0.00 Bad Chicken B00SP2QU0E 4 Appstore for Android Traffic Police: Off-Road Cub 3.3 NaN 0.00 Dast 2 For Metro B01DZIT64O In\u00a0[7]: Copied! <pre># \u0421\u0440\u0435\u0434\u043d\u0435\u0435 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0430 \u0442\u043e\u0432\u0430\u0440\u0430\nldf.average_rating.hist(bins=50)\n</pre> # \u0421\u0440\u0435\u0434\u043d\u0435\u0435 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0430 \u0442\u043e\u0432\u0430\u0440\u0430 ldf.average_rating.hist(bins=50) Out[7]: <pre>&lt;Axes: &gt;</pre> In\u00a0[8]: Copied! <pre># \u0421\u0440\u0435\u0434\u043d\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0435\u0442\u0438\u043d\u0433\u043e\u0432\nldf.rating_number.value_counts().hist(bins=3000)\nplt.xlim(0,100)\n</pre> # \u0421\u0440\u0435\u0434\u043d\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0435\u0442\u0438\u043d\u0433\u043e\u0432 ldf.rating_number.value_counts().hist(bins=3000) plt.xlim(0,100) Out[8]: <pre>(0.0, 100.0)</pre> In\u00a0[9]: Copied! <pre>df = dataset_df.copy()\n</pre> df = dataset_df.copy() In\u00a0[10]: Copied! <pre># main category\nldf['main_category'].value_counts()\n</pre> # main category ldf['main_category'].value_counts() Out[10]: <pre>main_category\nAppstore for Android    67849\nSoftware                 2922\nHome Audio &amp; Theater        1\nComputers                   1\nName: count, dtype: int64</pre> In\u00a0[11]: Copied! <pre>ldf['price'].hist(bins=1000)\nplt.xlim(0,100)\n</pre> ldf['price'].hist(bins=1000) plt.xlim(0,100) Out[11]: <pre>(0.0, 100.0)</pre> In\u00a0[12]: Copied! <pre>'''\n\n\u0424\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u044f \u043f\u043e \u0444\u0438\u0447\u0430\u043c [ldf]\n\n'''\n\nprint('\u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439: ',df['user_id'].nunique())\nprint('\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 ',df.shape[0])\nprint('\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u043e\u0432\u0430\u0440\u043e\u0432 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 ',ldf.shape[0])\n# \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u043e\u0432\u0430\u0440\u043e\u0432 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\n\n# \u0421\u0440\u0435\u0434\u043d\u0438\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0442\u043e\u0432\u0430\u043d\u0430 &gt; 3.5\n# \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432 &gt; 50 \u0440\u0430\u0437\nldf = ldf[(ldf['average_rating'] &gt;= 4.0) &amp; (ldf['rating_number'] &gt; 500)]\n\ndf = df[df['item_id'].isin(ldf['item_id'])]\nprint('\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u043f\u043e\u0441\u043b\u0435 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438 ',df.shape[0])\nprint('\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u043e\u0432\u0430\u0440\u043e\u0432 \u043f\u043e\u0441\u043b\u0435 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438 ',ldf.shape[0])\n</pre> '''  \u0424\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u044f \u043f\u043e \u0444\u0438\u0447\u0430\u043c [ldf]  '''  print('\u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439: ',df['user_id'].nunique()) print('\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 ',df.shape[0]) print('\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u043e\u0432\u0430\u0440\u043e\u0432 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 ',ldf.shape[0]) # \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u043e\u0432\u0430\u0440\u043e\u0432 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435  # \u0421\u0440\u0435\u0434\u043d\u0438\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0442\u043e\u0432\u0430\u043d\u0430 &gt; 3.5 # \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432 &gt; 50 \u0440\u0430\u0437 ldf = ldf[(ldf['average_rating'] &gt;= 4.0) &amp; (ldf['rating_number'] &gt; 500)]  df = df[df['item_id'].isin(ldf['item_id'])] print('\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u043f\u043e\u0441\u043b\u0435 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438 ',df.shape[0]) print('\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u043e\u0432\u0430\u0440\u043e\u0432 \u043f\u043e\u0441\u043b\u0435 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438 ',ldf.shape[0]) <pre>\u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439:  2589466\n\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435  4829120\n\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u043e\u0432\u0430\u0440\u043e\u0432 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435  70985\n\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u043f\u043e\u0441\u043b\u0435 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438  1545391\n\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u043e\u0432\u0430\u0440\u043e\u0432 \u043f\u043e\u0441\u043b\u0435 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438  1487\n</pre> In\u00a0[13]: Copied! <pre>print('\u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439: ',df['user_id'].nunique())\n</pre> print('\u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439: ',df['user_id'].nunique()) <pre>\u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439:  1133626\n</pre> In\u00a0[14]: Copied! <pre>'''\n\n\u0424\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438 \u043f\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f\u043c [df]\n\n'''\n\n# \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u0434\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\nuser_counts = df['user_id'].value_counts()\n\n# \u041f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438 \u0443 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0431\u043e\u043b\u044c\u0448\u0435 n \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439\nuser_count_limit_users = user_counts[user_counts &gt; 10].index\ndf = df[df['user_id'].isin(user_count_limit_users)]\nprint('\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u043f\u043e\u0441\u043b\u0435 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438 ',df.shape[0])\n\n# \u0422\u043e\u0432\u0430\u0440\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0439\u0445 \u043f\u043e\u043a\u0443\u043f\u0430\u043b\u0438 \u043c\u0435\u043b\u044c\u0448\u0435 n \u0440\u0430\u0437\nitem_counts = df['item_id'].value_counts()\ndf = df[~df['item_id'].isin(item_counts[item_counts &lt; 30].index)]\nprint('\u0420\u0430\u0437\u043c\u0435\u0440 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u043f\u043e\u0441\u043b\u0435 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438 \u0442\u043e\u0432\u0430\u0440\u043e\u0432: ',df.shape[0])\n</pre> '''  \u0424\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438 \u043f\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f\u043c [df]  '''  # \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u0434\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f user_counts = df['user_id'].value_counts()  # \u041f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438 \u0443 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0431\u043e\u043b\u044c\u0448\u0435 n \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 user_count_limit_users = user_counts[user_counts &gt; 10].index df = df[df['user_id'].isin(user_count_limit_users)] print('\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u043f\u043e\u0441\u043b\u0435 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438 ',df.shape[0])  # \u0422\u043e\u0432\u0430\u0440\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0439\u0445 \u043f\u043e\u043a\u0443\u043f\u0430\u043b\u0438 \u043c\u0435\u043b\u044c\u0448\u0435 n \u0440\u0430\u0437 item_counts = df['item_id'].value_counts() df = df[~df['item_id'].isin(item_counts[item_counts &lt; 30].index)] print('\u0420\u0430\u0437\u043c\u0435\u0440 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u043f\u043e\u0441\u043b\u0435 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438 \u0442\u043e\u0432\u0430\u0440\u043e\u0432: ',df.shape[0]) <pre>\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u043f\u043e\u0441\u043b\u0435 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438  42234\n\u0420\u0430\u0437\u043c\u0435\u0440 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u043f\u043e\u0441\u043b\u0435 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438 \u0442\u043e\u0432\u0430\u0440\u043e\u0432:  35520\n</pre> In\u00a0[15]: Copied! <pre>df['rating'].value_counts(normalize=True)*100\n</pre> df['rating'].value_counts(normalize=True)*100 Out[15]: <pre>rating\n5.0    60.526464\n4.0    12.722410\n3.0    11.627252\n1.0    11.221847\n2.0     3.902027\nName: proportion, dtype: float64</pre> In\u00a0[16]: Copied! <pre>'''\n\nReset user_id, item_id index to start from 0\n\n'''\n\nall_users = df['user_id'].unique().tolist()  # all unique users in ratings data\nall_items = df['item_id'].unique().tolist() # all unique movies in ratings data\n\nn_users = df['user_id'].nunique() # number of unique users\nn_items = df['item_id'].nunique() # number of unique movies\n\nuser_id2idx = dict(zip(all_users, range(n_users)))\nitem_id2idx = dict(zip(all_items, range(n_items)))\n\ndf['user_id'] = df['user_id'].map(user_id2idx) # redefine user id  (for better interpretation)\ndf['item_id'] = df['item_id'].map(item_id2idx) # redefine movie id (for better interpretation)\n</pre> '''  Reset user_id, item_id index to start from 0  '''  all_users = df['user_id'].unique().tolist()  # all unique users in ratings data all_items = df['item_id'].unique().tolist() # all unique movies in ratings data  n_users = df['user_id'].nunique() # number of unique users n_items = df['item_id'].nunique() # number of unique movies  user_id2idx = dict(zip(all_users, range(n_users))) item_id2idx = dict(zip(all_items, range(n_items)))  df['user_id'] = df['user_id'].map(user_id2idx) # redefine user id  (for better interpretation) df['item_id'] = df['item_id'].map(item_id2idx) # redefine movie id (for better interpretation) In\u00a0[17]: Copied! <pre>print(all_items[:5]) # all unique items found in user/item interaction dataset\nprint(n_items,'unique items found in user/item interaction dataset after filtration')\nprint(ldf.shape[0],'unique items found in metadata')\nprint(df.shape[0],'number of interactions')\n</pre> print(all_items[:5]) # all unique items found in user/item interaction dataset print(n_items,'unique items found in user/item interaction dataset after filtration') print(ldf.shape[0],'unique items found in metadata') print(df.shape[0],'number of interactions') <pre>['B075VFR5DB', 'B0117U0G3M', 'B012GOSOD2', 'B00LV4D70O', 'B0094BB4TW']\n223 unique items found in user/item interaction dataset after filtration\n1487 unique items found in metadata\n35520 number of interactions\n</pre> In\u00a0[18]: Copied! <pre># convert item_id \nldf = ldf[ldf['item_id'].isin(all_items)]\nldf['item_id'] = ldf['item_id'].map(item_id2idx)\n</pre> # convert item_id  ldf = ldf[ldf['item_id'].isin(all_items)] ldf['item_id'] = ldf['item_id'].map(item_id2idx) In\u00a0[19]: Copied! <pre>USER_COL = 'user_id'\nITEM_COL = 'item_id'\nTIMESTAMP = 'timestamp'\n\ndef train_test_split(\n    clickstream_df,\n    test_quantile=0.9\n):\n    \"\"\"\n    Split clickstream by date.\n    \"\"\"\n    clickstream_df = clickstream_df.sort_values([USER_COL, TIMESTAMP])\n    test_timepoint = clickstream_df[TIMESTAMP].quantile(\n    q=test_quantile, interpolation='nearest'\n    )\n    test = clickstream_df.query(f'{TIMESTAMP} &gt;= @test_timepoint')\n    train = clickstream_df.drop(test.index)\n\n    test = test[test[USER_COL].isin(train[USER_COL])]\n    test = test[test[ITEM_COL].isin(train[ITEM_COL])]\n\n    test_full_history = test.sort_values([USER_COL, TIMESTAMP]).groupby(USER_COL)\n    last_item = test_full_history.tail(1)\n    test_history = test_full_history.head(-1)\n\n    test = pd.concat([train, test_history])\n\n    test.reset_index(drop=True, inplace=True)\n    train.reset_index(drop=True, inplace=True)\n    return train, test, last_item\n\ntrain,test,last_item = train_test_split(df)\n\nprint('Training ',train.shape[0])\nprint('Test ',test.shape[0])\nprint('Last Item ',last_item.shape[0])\n</pre> USER_COL = 'user_id' ITEM_COL = 'item_id' TIMESTAMP = 'timestamp'  def train_test_split(     clickstream_df,     test_quantile=0.9 ):     \"\"\"     Split clickstream by date.     \"\"\"     clickstream_df = clickstream_df.sort_values([USER_COL, TIMESTAMP])     test_timepoint = clickstream_df[TIMESTAMP].quantile(     q=test_quantile, interpolation='nearest'     )     test = clickstream_df.query(f'{TIMESTAMP} &gt;= @test_timepoint')     train = clickstream_df.drop(test.index)      test = test[test[USER_COL].isin(train[USER_COL])]     test = test[test[ITEM_COL].isin(train[ITEM_COL])]      test_full_history = test.sort_values([USER_COL, TIMESTAMP]).groupby(USER_COL)     last_item = test_full_history.tail(1)     test_history = test_full_history.head(-1)      test = pd.concat([train, test_history])      test.reset_index(drop=True, inplace=True)     train.reset_index(drop=True, inplace=True)     return train, test, last_item  train,test,last_item = train_test_split(df)  print('Training ',train.shape[0]) print('Test ',test.shape[0]) print('Last Item ',last_item.shape[0]) <pre>Training  31967\nTest  33102\nLast Item  507\n</pre> In\u00a0[20]: Copied! <pre>'''\n\nMost popular items (top 10)\n\n'''\n\nfrom collections import Counter\n\ncount_items = Counter(train['item_id'])\ncount_items = [*count_items.items()]\ncount_items.sort(key=lambda x: x[1], reverse=True)\n\npred_items = [k for k, v in count_items[:10]]\npred_counter = test.copy()\npred_counter['item_id'] = [pred_items] * len(pred_counter)\npred_counter = pred_counter.drop_duplicates(subset='user_id').explode('item_id')\npred_counter = pred_counter[['user_id','item_id','rating']]\npred_counter.head()\n</pre> '''  Most popular items (top 10)  '''  from collections import Counter  count_items = Counter(train['item_id']) count_items = [*count_items.items()] count_items.sort(key=lambda x: x[1], reverse=True)  pred_items = [k for k, v in count_items[:10]] pred_counter = test.copy() pred_counter['item_id'] = [pred_items] * len(pred_counter) pred_counter = pred_counter.drop_duplicates(subset='user_id').explode('item_id') pred_counter = pred_counter[['user_id','item_id','rating']] pred_counter.head() Out[20]: user_id item_id rating 0 0 36 5.0 0 0 35 5.0 0 0 4 5.0 0 0 55 5.0 0 0 60 5.0 In\u00a0[21]: Copied! <pre>from collections import Counter\n\ncount_items = Counter(train[train.rating == 5]['item_id'])\ncount_items = [*count_items.items()]\ncount_items.sort(key=lambda x: x[1], reverse=True)\n\npred_items = [k for k, v in count_items[:10]]\npred_highest = test.copy()\npred_highest['item_id'] = [pred_items] * len(pred_highest)\npred_highest = pred_highest.drop_duplicates(subset='user_id').explode('item_id')\npred_highest = pred_highest[['user_id','item_id','rating']]\npred_highest.head()\n</pre> from collections import Counter  count_items = Counter(train[train.rating == 5]['item_id']) count_items = [*count_items.items()] count_items.sort(key=lambda x: x[1], reverse=True)  pred_items = [k for k, v in count_items[:10]] pred_highest = test.copy() pred_highest['item_id'] = [pred_items] * len(pred_highest) pred_highest = pred_highest.drop_duplicates(subset='user_id').explode('item_id') pred_highest = pred_highest[['user_id','item_id','rating']] pred_highest.head() Out[21]: user_id item_id rating 0 0 55 5.0 0 0 36 5.0 0 0 4 5.0 0 0 98 5.0 0 0 71 5.0 In\u00a0[22]: Copied! <pre># \u0412\u0441\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u0441 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u043c 3+\nldf = train[train.rating &gt; 2].copy()\nhighest_ratings_user = ldf.groupby('user_id')['item_id'].apply(lambda r: ' '.join([str(A) for A in r]))\nhighest_ratings_user.head()\n</pre> # \u0412\u0441\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u0441 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u043c 3+ ldf = train[train.rating &gt; 2].copy() highest_ratings_user = ldf.groupby('user_id')['item_id'].apply(lambda r: ' '.join([str(A) for A in r])) highest_ratings_user.head() Out[22]: <pre>user_id\n0                                  11 9 6 5 4 3 2\n1                      18 17 16 15 14 13 8 3 4 12\n2        17 29 28 27 26 25 16 24 23 22 21 4 20 19\n3                                     22 24 32 30\n4    29 49 48 47 46 45 44 42 41 40 39 38 37 36 14\nName: item_id, dtype: object</pre> In\u00a0[23]: Copied! <pre>import apyori\n\n# \u0441\u0442\u0440\u043e\u0438\u043c \u043f\u0440\u0430\u0432\u0438\u043b\u0430 \u0434\u043b\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0435\u0439 \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432 \u0444\u0438\u043b\u044c\u043c\u043e\u0432\nassociation_rules = apyori.apriori(highest_ratings_user.apply(lambda r: r.split(' ')), \n                                   min_support=0.02, \n                                   min_confidence=0.1, \n                                   min_lift=2, \n                                   min_length=2)\n</pre> import apyori  # \u0441\u0442\u0440\u043e\u0438\u043c \u043f\u0440\u0430\u0432\u0438\u043b\u0430 \u0434\u043b\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0435\u0439 \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 association_rules = apyori.apriori(highest_ratings_user.apply(lambda r: r.split(' ')),                                     min_support=0.02,                                     min_confidence=0.1,                                     min_lift=2,                                     min_length=2) In\u00a0[24]: Copied! <pre># \u0432\u044b\u0432\u043e\u0434\u0438\u043c \u043f\u0440\u0430\u0432\u0438\u043b\u0430\nasr_df = pd.DataFrame(columns = ['from', 'to', 'confidence', 'support', 'lift'])\nfor item in association_rules:\n    pair = item[0] \n    items = [x for x in pair]\n    asr_df.loc[len(asr_df), :] =  ' '.join(list(item[2][0][0])), \\\n                                  ' '.join(list(item[2][0][1])),\\\n                                  item[2][0][2], item[1], item[2][0][3]\n\n# \u043f\u0440\u0430\u0432\u0438\u043b\u0430 (\u043e\u0442)\nprint(asr_df.shape)\nasr_df.head()\n</pre> # \u0432\u044b\u0432\u043e\u0434\u0438\u043c \u043f\u0440\u0430\u0432\u0438\u043b\u0430 asr_df = pd.DataFrame(columns = ['from', 'to', 'confidence', 'support', 'lift']) for item in association_rules:     pair = item[0]      items = [x for x in pair]     asr_df.loc[len(asr_df), :] =  ' '.join(list(item[2][0][0])), \\                                   ' '.join(list(item[2][0][1])),\\                                   item[2][0][2], item[1], item[2][0][3]  # \u043f\u0440\u0430\u0432\u0438\u043b\u0430 (\u043e\u0442) print(asr_df.shape) asr_df.head() <pre>(1486, 5)\n</pre> Out[24]: from to confidence support lift 0 100 157 0.431034 0.028879 3.941537 1 100 36 0.614943 0.041201 2.042207 2 100 41 0.333333 0.022333 2.208333 3 100 55 0.678161 0.045437 2.41589 4 100 61 0.304598 0.020408 3.282325 In\u00a0[25]: Copied! <pre># \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0432\u044b\u0432\u043e\u0434\u0438\u0442 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u0438 \u043c\u0435\u0442\u0440\u0438\u043a\u0443 lift\ndef get_recom_aprio(x):\n    \n    result = []; lift= []; ii = 0\n    for iii,row in asr_df.iterrows():\n        watched = set(x['history'].split(' ')) # watched films\n        froms = set(row['from'].split(' ')) # watched film rules\n        overlap = set(x['history'].split(' ')) &amp; set(row['from'].split(' '))\n\n        if len(overlap) &gt; 0:\n            to_watch = [int(i) for i in row['to'].split(' ')]\n            to_lifts = [row['lift']] * len(to_watch)\n            result.extend(to_watch)\n            lift.extend(to_lifts)\n            ii+=1\n\n        if(ii==10):\n            break\n\n    return result, lift \n\ndf_highest_ratings_user = highest_ratings_user.to_frame()\ndf_highest_ratings_user.columns = ['history']\ndf_highest_ratings_user[['item_id','rating']] = df_highest_ratings_user.apply(get_recom_aprio,axis=1,result_type='expand')\n</pre> # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0432\u044b\u0432\u043e\u0434\u0438\u0442 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u0438 \u043c\u0435\u0442\u0440\u0438\u043a\u0443 lift def get_recom_aprio(x):          result = []; lift= []; ii = 0     for iii,row in asr_df.iterrows():         watched = set(x['history'].split(' ')) # watched films         froms = set(row['from'].split(' ')) # watched film rules         overlap = set(x['history'].split(' ')) &amp; set(row['from'].split(' '))          if len(overlap) &gt; 0:             to_watch = [int(i) for i in row['to'].split(' ')]             to_lifts = [row['lift']] * len(to_watch)             result.extend(to_watch)             lift.extend(to_lifts)             ii+=1          if(ii==10):             break      return result, lift   df_highest_ratings_user = highest_ratings_user.to_frame() df_highest_ratings_user.columns = ['history'] df_highest_ratings_user[['item_id','rating']] = df_highest_ratings_user.apply(get_recom_aprio,axis=1,result_type='expand') In\u00a0[26]: Copied! <pre># history : \u0438\u0441\u0442\u043e\u0440\u0438\u044f \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432\n# item_id : \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\ndf_highest_ratings_user.head()\n</pre> # history : \u0438\u0441\u0442\u043e\u0440\u0438\u044f \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432 # item_id : \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f df_highest_ratings_user.head() Out[26]: history item_id rating user_id 0 11 9 6 5 4 3 2 [8, 71, 6, 66, 66, 3, 23, 23, 71, 4, 71, 35, 36] [4.899327708033203, 2.109130967156173, 7.78656... 1 18 17 16 15 14 13 8 3 4 12 [3, 15, 17, 23, 29, 4, 16, 3, 71, 3] [3.2042398665227205, 2.5517071972488337, 2.030... 2 17 29 28 27 26 25 16 24 23 22 21 4 20 19 [3, 22, 27, 48, 27, 28, 5, 6, 66, 7] [2.211219357649202, 5.536115916955017, 4.82478... 3 22 24 32 30 [27] [8.403523489932885] 4 29 49 48 47 46 45 44 42 41 40 39 38 37 36 14 [16, 3, 71, 43, 56, 98, 98, 40, 41, 42] [2.005609955120359, 2.142369678198331, 2.67433... In\u00a0[27]: Copied! <pre>df_apriori = df_highest_ratings_user.explode(['item_id','rating'])\ndf_apriori = df_apriori.reset_index() \ndf_apriori.drop(['history'],axis=1,inplace=True)\ndf_apriori = df_apriori.dropna(subset=['item_id'],axis=0)\ndf_apriori['item_id'] = df_apriori['item_id'].astype(int)\ndf_apriori.head()\n</pre> df_apriori = df_highest_ratings_user.explode(['item_id','rating']) df_apriori = df_apriori.reset_index()  df_apriori.drop(['history'],axis=1,inplace=True) df_apriori = df_apriori.dropna(subset=['item_id'],axis=0) df_apriori['item_id'] = df_apriori['item_id'].astype(int) df_apriori.head() Out[27]: user_id item_id rating 0 0 8 4.899328 1 0 71 2.109131 2 0 6 7.786568 3 0 66 4.463851 4 0 66 4.544918 In\u00a0[28]: Copied! <pre>class BaseFactorizationModel:\n    def __init__(self,\n                 random_state=0, \n                 user_col='user_id', \n                 item_col='item_id', \n                 rating_col='rating'):\n        \n        self.random_state = np.random.RandomState(random_state)\n        self.user_col = user_col\n        self.item_col = item_col\n        self.rating_col = rating_col\n        self.user_matrix = None\n        self.item_matrix = None\n\n    # sparse matrix of rating matrix\n    def get_rating_matrix(self,df):\n        \n        rating = list(df[self.rating_col])\n        rows = df[self.user_col].astype('category').cat.codes\n        cols = df[self.item_col].astype('category').cat.codes\n        df_sparse = sparse.csr_matrix((rating, (rows, cols)))\n        return df_sparse\n\n    \n    \"\"\"\n    \n    When we receive the matrix with scores, for each user\n    sort and get the top k \n    \n    \"\"\"\n    \n    def predict(self, \n                scores,             # (user,film) score matrix\n                rating_matrix=None, # (user,film) rating matrix\n                filter_seen=False, \n                k=10):\n        \n        if filter_seen:\n            scores = np.multiply(scores,\n            np.invert(rating_matrix.astype(bool))\n            )\n\n        ind_part = np.argpartition(scores, -k + 1)[:, -k:].copy()  \n        scores_not_sorted = np.take_along_axis(scores, ind_part, axis=1)\n        ind_sorted = np.argsort(scores_not_sorted, axis=1) # \n        scores_sorted = np.sort(scores_not_sorted, axis=1) \n        indices = np.take_along_axis(ind_part, ind_sorted, axis=1)\n\n        preds = pd.DataFrame({\n            self.user_col: range(scores.shape[0]), # each user \n            self.item_col: np.flip(indices, axis=1).tolist(), # movieId index \n            self.rating_col: np.flip(scores_sorted, axis=1).tolist() # movieId score\n            })\n        \n        preds = preds.explode([self.item_col, self.rating_col])\n\n        return preds\n</pre> class BaseFactorizationModel:     def __init__(self,                  random_state=0,                   user_col='user_id',                   item_col='item_id',                   rating_col='rating'):                  self.random_state = np.random.RandomState(random_state)         self.user_col = user_col         self.item_col = item_col         self.rating_col = rating_col         self.user_matrix = None         self.item_matrix = None      # sparse matrix of rating matrix     def get_rating_matrix(self,df):                  rating = list(df[self.rating_col])         rows = df[self.user_col].astype('category').cat.codes         cols = df[self.item_col].astype('category').cat.codes         df_sparse = sparse.csr_matrix((rating, (rows, cols)))         return df_sparse           \"\"\"          When we receive the matrix with scores, for each user     sort and get the top k           \"\"\"          def predict(self,                  scores,             # (user,film) score matrix                 rating_matrix=None, # (user,film) rating matrix                 filter_seen=False,                  k=10):                  if filter_seen:             scores = np.multiply(scores,             np.invert(rating_matrix.astype(bool))             )          ind_part = np.argpartition(scores, -k + 1)[:, -k:].copy()           scores_not_sorted = np.take_along_axis(scores, ind_part, axis=1)         ind_sorted = np.argsort(scores_not_sorted, axis=1) #          scores_sorted = np.sort(scores_not_sorted, axis=1)          indices = np.take_along_axis(ind_part, ind_sorted, axis=1)          preds = pd.DataFrame({             self.user_col: range(scores.shape[0]), # each user              self.item_col: np.flip(indices, axis=1).tolist(), # movieId index              self.rating_col: np.flip(scores_sorted, axis=1).tolist() # movieId score             })                  preds = preds.explode([self.item_col, self.rating_col])          return preds  In\u00a0[29]: Copied! <pre>class SVD(BaseFactorizationModel):\n    \n    def __init__(self, \n                 random_state=0, \n                 user_col='user_id', \n                 item_col='item_id',\n                 n_factors=10): # hyperparameter\n        super().__init__(random_state, user_col, item_col)\n        self.n_factors = n_factors\n\n    \"\"\"\n    \n    Calculate the scores for each user\n    \n    \"\"\"\n        \n    def fit(self, data):\n        \n        # user,item rating matrix\n        self.rating_matrix = self.get_rating_matrix(data) \n        csr_rating_matrix = self.rating_matrix\n\n        # svd decomposition of user_id vs item_id grating matrix\n        user_matrix, singular_values, item_matrix = svds(A=csr_rating_matrix, \n                                                         k=self.n_factors)\n    \n        user_matrix = user_matrix * np.sqrt(singular_values)  # (unique users,k latent features)\n        item_matrix = item_matrix.T * np.sqrt(singular_values) # (unique films, k latent features)\n        self.scores = user_matrix @ item_matrix.T  # score matrix for each user &amp; film (unique users, unique films)\n        \n        self.user_matrix = user_matrix\n        self.item_matrix = item_matrix\n</pre> class SVD(BaseFactorizationModel):          def __init__(self,                   random_state=0,                   user_col='user_id',                   item_col='item_id',                  n_factors=10): # hyperparameter         super().__init__(random_state, user_col, item_col)         self.n_factors = n_factors      \"\"\"          Calculate the scores for each user          \"\"\"              def fit(self, data):                  # user,item rating matrix         self.rating_matrix = self.get_rating_matrix(data)          csr_rating_matrix = self.rating_matrix          # svd decomposition of user_id vs item_id grating matrix         user_matrix, singular_values, item_matrix = svds(A=csr_rating_matrix,                                                           k=self.n_factors)              user_matrix = user_matrix * np.sqrt(singular_values)  # (unique users,k latent features)         item_matrix = item_matrix.T * np.sqrt(singular_values) # (unique films, k latent features)         self.scores = user_matrix @ item_matrix.T  # score matrix for each user &amp; film (unique users, unique films)                  self.user_matrix = user_matrix         self.item_matrix = item_matrix In\u00a0[30]: Copied! <pre># \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\nsvd_model = SVD()\nsvd_model.fit(train)\n\nprint(svd_model.scores.shape) \nprint(svd_model.user_matrix.shape)\nprint(svd_model.item_matrix.shape)\n\n# \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\npreds_svd = svd_model.predict(svd_model.scores,        # user movie scores from svd decomposition\n                              svd_model.rating_matrix) # user movie ratings \npreds_svd.head()\n</pre> # \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c svd_model = SVD() svd_model.fit(train)  print(svd_model.scores.shape)  print(svd_model.user_matrix.shape) print(svd_model.item_matrix.shape)  # \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c preds_svd = svd_model.predict(svd_model.scores,        # user movie scores from svd decomposition                               svd_model.rating_matrix) # user movie ratings  preds_svd.head() <pre>(2637, 223)\n(2637, 10)\n(223, 10)\n</pre> Out[30]: user_id item_id rating 0 0 23 2.524573 0 0 4 2.4697 0 0 3 1.522159 0 0 66 1.360593 0 0 13 0.965334 In\u00a0[31]: Copied! <pre># train[train['user_id'] == ids] # train\n# preds_svd[preds_svd['user_id'] == ids] # predictions\n</pre> # train[train['user_id'] == ids] # train # preds_svd[preds_svd['user_id'] == ids] # predictions In\u00a0[32]: Copied! <pre># create the user movie rating matrix \nbase_model = BaseFactorizationModel()\nrating_matrix = base_model.get_rating_matrix(train) # sparse rating matrix\n\nials_model = implicit.als.AlternatingLeastSquares(factors=20, \n                                                  regularization=0.01, \n                                                  iterations=50, use_gpu=False)\n# ials_model.fit((train_sparse).astype('double'))\nials_model.fit(rating_matrix)\n\nuser_vecs = ials_model.user_factors\nitem_vecs = ials_model.item_factors\nprint(user_vecs.shape, item_vecs.shape)\n\n# iALS scores\nscores = user_vecs.dot(item_vecs.T)\n\npreds_ials = base_model.predict(scores, rating_matrix)\npreds_ials\n</pre> # create the user movie rating matrix  base_model = BaseFactorizationModel() rating_matrix = base_model.get_rating_matrix(train) # sparse rating matrix  ials_model = implicit.als.AlternatingLeastSquares(factors=20,                                                    regularization=0.01,                                                    iterations=50, use_gpu=False) # ials_model.fit((train_sparse).astype('double')) ials_model.fit(rating_matrix)  user_vecs = ials_model.user_factors item_vecs = ials_model.item_factors print(user_vecs.shape, item_vecs.shape)  # iALS scores scores = user_vecs.dot(item_vecs.T)  preds_ials = base_model.predict(scores, rating_matrix) preds_ials <pre>(2637, 20) (223, 20)\n</pre> Out[32]: user_id item_id rating 0 0 23 0.994244 0 0 7 0.810566 0 0 66 0.810353 0 0 6 0.782333 0 0 5 0.742521 ... ... ... ... 2636 2636 9 0.731683 2636 2636 105 0.727632 2636 2636 101 0.706921 2636 2636 55 0.646084 2636 2636 67 0.628524 <p>26370 rows \u00d7 3 columns</p> In\u00a0[33]: Copied! <pre>from replay.metrics import HitRate, NDCG, MAP, Coverage, MRR\n\n\nK = [10]\nmetrics = Experiment(\n    [\n        NDCG(K),\n        MRR(K),\n        Coverage(K),\n        HitRate(K),\n    ],\n    train,\n    test,\n    query_column='user_id', \n    item_column= 'item_id',\n    rating_column='rating'\n)\n\nmetrics.add_result('\u041f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435',pred_counter)\nmetrics.add_result('\u041f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435 \u0432\u044b\u0441\u043e\u043a\u0438\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433',pred_highest)\nmetrics.add_result('Apriori',df_apriori)\nmetrics.add_result(\"iALS\", preds_ials)\nmetrics.add_result('SVD', preds_svd)\nmetrics.results\n</pre> from replay.metrics import HitRate, NDCG, MAP, Coverage, MRR   K = [10] metrics = Experiment(     [         NDCG(K),         MRR(K),         Coverage(K),         HitRate(K),     ],     train,     test,     query_column='user_id',      item_column= 'item_id',     rating_column='rating' )  metrics.add_result('\u041f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435',pred_counter) metrics.add_result('\u041f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435 \u0432\u044b\u0441\u043e\u043a\u0438\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433',pred_highest) metrics.add_result('Apriori',df_apriori) metrics.add_result(\"iALS\", preds_ials) metrics.add_result('SVD', preds_svd) metrics.results Out[33]: NDCG@10 MRR@10 Coverage@10 HitRate@10 \u041f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435 0.279331 0.448207 0.044843 0.826318 \u041f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435 \u0432\u044b\u0441\u043e\u043a\u0438\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 0.275444 0.451899 0.044843 0.878650 Apriori 0.292923 0.465507 0.237668 0.799014 iALS 0.182182 0.346100 0.739910 0.613955 SVD 0.212864 0.399959 0.421525 0.658324"},{"location":"portfolio/course_recsys/prob_mf.html","title":"\u041c\u043e\u0434\u0435\u043b\u0438 \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u043e\u0439 \u0444\u0430\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u0438\u00b6","text":""},{"location":"portfolio/course_recsys/prob_mf.html#1","title":"1 | \u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":"<ul> <li>\u0412\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u043b\u044e\u0431\u043e\u0439 \u043f\u043e\u043d\u0440\u0430\u0432\u0438\u0432\u0448\u0438\u0439\u0441\u044f \u0432\u0430\u043c \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e\u043c \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432 \u0431\u043e\u043b\u0435\u0435 1\u041c \u043f\u043e \u0441\u0441\u044b\u043b\u043a\u0435: https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023</li> <li>\u041f\u0440\u043e\u0432\u0435\u0434\u0438\u0442\u0435 \u0431\u0430\u0437\u043e\u0432\u044b\u0439 EDA - \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0442\u043e\u0432\u0430\u0440\u043e\u0432 \u0438 \u0442.\u0434. \u041f\u0440\u0438 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u0444\u0438\u043b\u044c\u0442\u0440\u0443\u0439\u0442\u0435 \u0434\u0430\u043d\u043d\u044b\u0435.</li> <li>\u0421\u0434\u0435\u043b\u0430\u0439\u0442\u0435 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 train \u0438 test \u043c\u0435\u0442\u043e\u0434\u043e\u043c leave-one-out.</li> <li>\u041d\u0430 \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u0438\u0438 \u0432\u0430\u0448\u0435\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u043f\u043e\u0441\u0442\u0440\u043e\u0439\u0442\u0435 3-4 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0438. \u041c\u043e\u0436\u043d\u043e \u0432\u043a\u043b\u044e\u0447\u0438\u0442\u044c \u043f\u0440\u043e\u0441\u0442\u043e\u0439 \u0431\u0435\u0439\u0437\u043b\u0430\u0439\u043d \u0442\u0438\u043f\u0430 \u043c\u043e\u0434\u0435\u043b\u0438, \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u044e\u0449\u0435\u0439 \u0441\u0430\u043c\u044b\u0435 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435 \u0442\u043e\u0432\u0430\u0440\u044b, \u0430 \u0442\u0430\u043a\u0436\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443 implicit \u0438 \u0434\u0440\u0443\u0433\u0438\u0435.</li> <li>\u041e\u0446\u0435\u043d\u0438\u0442\u0435 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439 \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0430\u043c HR@10, MRR@10, NDCG@10, coverage.</li> <li>\u041d\u0430\u043f\u0438\u0448\u0438\u0442\u0435 \u0432\u044b\u0432\u043e\u0434\u044b \u043e \u043f\u0440\u043e\u0434\u0435\u043b\u0430\u043d\u043d\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u0435: \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u0430\u043a\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043e\u043a\u0430\u0437\u0430\u043b\u043e\u0441\u044c \u043b\u0443\u0447\u0448\u0435, \u043f\u043e\u0447\u0435\u043c\u0443.</li> </ul>"},{"location":"portfolio/course_recsys/prob_mf.html#2","title":"2 | \u0427\u0442\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":""},{"location":"portfolio/course_recsys/prob_mf.html","title":"\u0414\u0430\u0442\u0430\u0441\u0435\u0442 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438\u00b6","text":"<p>\u0414\u043b\u044f \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430, \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043f\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0438\u044e \u043f\u043e\u043a\u0443\u043f\u0430\u0442\u0435\u043b\u044f \u0441 \u0442\u043e\u0432\u0430\u0440\u0430\u043c\u0438 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043d\u043e\u0433\u043e \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0435\u043d\u0438\u044f</p>"},{"location":"portfolio/course_recsys/prob_mf.html","title":"\u0414\u0430\u0442\u0430\u0441\u0435\u0442 \u0442\u043e\u0432\u0430\u0440\u043e\u0432\u00b6","text":"<ul> <li><code>parent_asin</code> : \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 (\u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u043d\u043e\u0435 \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0435\u043d\u0438\u0435) \u0442\u043e\u0432\u0430\u0440\u0430</li> <li><code>user_id</code> : \u0418\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u041f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c</li> </ul>"},{"location":"portfolio/course_recsys/prob_mf.html#3","title":"3 | \u0420\u0430\u0437\u0432\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437\u00b6","text":""},{"location":"portfolio/course_recsys/prob_mf.html","title":"\u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439\u00b6","text":"<p>\u0420\u0435\u0439\u0442\u0438\u043d\u0433\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u0442\u0430\u0432\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438</p>"},{"location":"portfolio/course_recsys/prob_mf.html","title":"\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0435\u0439\u0438\u0442\u0438\u043d\u0433\u043e\u0432\u00b6","text":"<p>\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432 \u0443 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</p>"},{"location":"portfolio/course_recsys/prob_mf.html#4","title":"4 | \u041f\u0440\u0435\u043f\u043e\u0434\u0440\u0430\u0431\u043e\u0442\u043a\u0430\u00b6","text":""},{"location":"portfolio/course_recsys/prob_mf.html#a","title":"a) \u0424\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0443 \u0444\u0438\u0447\u0435\u0439\u00b6","text":"<p>\u041e\u0442\u0444\u0438\u043b\u044c\u0442\u0440\u0443\u0435\u043c \u043f\u043e \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0444\u0438\u0447\u0435\u0439:</p> <ul> <li><code>average_price</code> : \u0421\u0440\u0435\u0434\u043d\u0438\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0442\u043e\u0432\u0430\u0440\u0430</li> <li><code>rating_number</code> : \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0442\u0437\u044b\u0432\u043e\u0432</li> </ul> <p>\u041e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0432\u0437\u0430\u043c\u0438\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0441 \u044d\u0442\u0438\u043c\u0438 \u0442\u043e\u0432\u0430\u0440\u0430\u043c\u0438, \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0435 \u0443\u0434\u0430\u043b\u044f\u0435\u043c</p> <ul> <li>\u0426\u0435\u043b\u044c \u043d\u0430\u0448\u0435\u0439 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438, \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0438\u0442\u044c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044e \u0442\u043e\u0432\u0430\u0440\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0445\u043e\u0440\u043e\u0448\u043e \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u043b\u0438 \u0438 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0446\u0435\u043d\u043e\u043a \u0442\u043e\u0436\u0435 \u043c\u043d\u043e\u0433\u043e</li> </ul>"},{"location":"portfolio/course_recsys/prob_mf.html#b","title":"b) \u0424\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f\u043c\u00b6","text":"<p>\u0424\u0438\u043b\u044c\u0440\u0443\u0435\u043c \u043f\u043e \u0434\u0430\u043d\u043d\u044b\u043c \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u0442\u043e\u0432\u0430\u0440\u043e\u043c:</p> <ul> <li>\u0444\u0438\u043b\u044c\u0442\u0440\u0443\u0435\u043c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0443 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043c\u0430\u043b\u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0432\u0438\u0438</li> <li>\u0444\u0438\u043b\u044c\u0442\u0440\u0443\u0435\u043c \u0442\u043e\u0432\u0430\u0440\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043c\u0430\u043b\u043e \u043f\u043e\u043a\u0443\u043f\u0430\u043b\u0438</li> </ul>"},{"location":"portfolio/course_recsys/prob_mf.html","title":"\u0420\u0430\u0441\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432\u00b6","text":""},{"location":"portfolio/course_recsys/prob_mf.html#ids","title":"\u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c IDs\u00b6","text":"<p>\u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u044b \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\u0445, \u0447\u0442\u043e\u0431\u044b \u043e\u043d\u0438 \u043d\u0430\u0447\u0438\u043d\u0430\u043b\u0438\u0441\u044c \u0441 0</p>"},{"location":"portfolio/course_recsys/prob_mf.html","title":"\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438\u00b6","text":"<p>\u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u0434\u0432\u0435 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438, <code>train</code> \u0438 <code>test</code>, \u0431\u0443\u0434\u0435\u043c \u0438\u043c\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043f\u0435\u0440\u0432\u0443\u044e \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0438 \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438</p>"},{"location":"portfolio/course_recsys/prob_mf.html#4","title":"4 | \u041c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/course_recsys/prob_mf.html","title":"\u0430) \u0421\u0430\u043c\u044b\u0435 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435\u00b6","text":"<p>\u0418\u0437 \u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u0432\u044b\u0431\u0435\u0440\u0435\u043c \u0442\u043e\u043f n \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u043e\u0432\u0430\u0440\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u0443\u0434\u0435\u043c \u0440\u0435\u043a\u043e\u043c\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u0442\u044c \u0432\u0441\u0435\u043c, \u0441\u0442\u0440\u043e\u0438\u043c \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u044d\u0432\u0440\u0438\u0441\u0442\u0438\u043a</p>"},{"location":"portfolio/course_recsys/prob_mf.html#b","title":"b) \u0421\u0430\u043c\u044b\u0435 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435 \u0441 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u043c \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u043c\u00b6","text":"<p>\u0422\u0430\u043a \u0436\u0435 \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0442\u0430\u043a\u043e\u0439 \u0436\u0435 \u0432\u0430\u0440\u0438\u0430\u043d\u0442 \u043d\u043e \u0441 \u0432\u044b\u0441\u043e\u043a\u0438\u043c\u0438 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0430\u043c\u0438</p>"},{"location":"portfolio/course_recsys/prob_mf.html#c-apriori","title":"c) Apriori\u00b6","text":"<p>\u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043f\u043e\u0434\u0445\u043e\u0434 <code>apriori</code>, \u0431\u0443\u0434\u0435\u043c \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u0442\u044c \u0444\u0438\u043b\u044c\u043c\u044b \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043f\u0430\u0442\u0435\u0440\u043d\u043e\u0432 \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432 \u0432\u043e \u0443 \u0432\u0441\u0435\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439. \u041f\u043e\u0441\u0442\u0440\u043e\u0438\u0432 \u043f\u0440\u0430\u0432\u0438\u043b\u0430 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432, \u043d\u0430\u0445\u043e\u0434\u0438\u043c \u043f\u0435\u0440\u0435\u0441\u0435\u0447\u0435\u043d\u0438\u0435 \u043c\u0435\u0436\u0434\u0443 \u043f\u0440\u043e\u0441\u0442\u043e\u0442\u0440\u043e\u0432 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0438 \u0432 \u043f\u0440\u0430\u0432\u0438\u043b\u0435 \u043e\u0442 <code>from</code>. Ec\u043b\u0438 \u043f\u0435\u0440\u0435\u0441\u0435\u0447\u0435\u043d\u0438\u0435 \u0435\u0441\u0442\u044c, \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u043c \u0432\u0441\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u0432 \u043f\u0440\u0430\u0432\u0438\u043b\u0435 <code>to</code></p>"},{"location":"portfolio/course_recsys/prob_mf.html#d-svd-single-value-decomposition","title":"d) SVD (Single Value Decomposition)\u00b6","text":"<p>\u0414\u043b\u044f <code>SVD</code> \u0440\u0430\u0437\u043b\u043e\u0436\u0435\u043d\u0438\u044f, \u0441\u043e\u0437\u0434\u0430\u043b\u0438\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0438 <code>user_id</code> \u0438 <code>item_id</code>. \u0420\u0430\u0437\u043b\u043e\u0436\u0438\u043c \u0435\u0435 \u043d\u0430 \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u0442\u043e\u0432\u0430\u0440\u043e\u0432, \u0437\u0430\u0442\u0435\u043c \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c <code>scores</code></p> <p>\u0414\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f, \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0431\u0435\u0440\u0435\u043c \u0442\u043e\u043f n \u044d\u0442\u0438\u0445 <code>score</code>\u0438\u0437 \u043e\u0442\u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e \u0432\u0435\u043a\u0442\u043e\u0440\u0430</p>"},{"location":"portfolio/course_recsys/prob_mf.html#e-ials-alternating-least-squares","title":"e) iALS (Alternating Least Squares)\u00b6","text":"<p>\u0410\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u043e \u0441 <code>SVD</code>, \u0432 <code>iALS</code>, \u0434\u0435\u043b\u0430\u0435\u043c \u0434\u0435\u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u044e \u043c\u0430\u0440\u0442\u0438\u0446\u044b \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432 \u0442\u043e\u0432\u0430\u0440\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c\u0438</p>"},{"location":"portfolio/course_recsys/prob_mf.html#5","title":"5 | \u041e\u0446\u0435\u043d\u043a\u0430 \u043c\u0435\u0442\u0440\u0438\u043a\u00b6","text":""},{"location":"portfolio/course_recsys/prob_mf.html","title":"\u041e\u0446\u0435\u043d\u043a\u0430 \u043c\u0435\u0442\u0440\u0438\u043a\u00b6","text":"<p>\u0412 \u043a\u043e\u043d\u0446\u0435 \u043c\u044b \u0441\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0435\u043c \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0445\u043e\u0440\u043e\u0448\u043e \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u0435\u043b\u0430\u044e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f</p>"},{"location":"portfolio/course_recsys/prob_mf.html","title":"\u0412\u044b\u0432\u043e\u0434\u044b\u00b6","text":"<p>\u0412\u0441\u043f\u043e\u043c\u043d\u0438\u043c \u0447\u0442\u043e \u043d\u0430\u0448\u0438 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043e\u0446\u0435\u043d\u043a\u0438 \u043e\u0437\u043d\u0430\u0447\u0430\u044e\u0442:</p> <ul> <li><p><code>NDCG</code> \u0438\u0437\u043c\u0435\u0440\u044f\u0435\u0442 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432. \u0427\u0435\u043c \u0432\u044b\u0448\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435, \u0442\u0435\u043c \u043b\u0443\u0447\u0448\u0435 \u043c\u043e\u0434\u0435\u043b\u044c \u0440\u0430\u043d\u0436\u0438\u0440\u0443\u0435\u0442 \u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u044b\u0435 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u044b.</p> </li> <li><p><code>MRR</code> \u0438\u0437\u043c\u0435\u0440\u044f\u0435\u0442 \u0441\u0440\u0435\u0434\u043d\u044e\u044e \u043e\u0431\u0440\u0430\u0442\u043d\u0443\u044e \u043f\u043e\u0437\u0438\u0446\u0438\u044e \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u043e\u0433\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u0430 \u0432 \u0441\u043f\u0438\u0441\u043a\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439. \u0412\u044b\u0441\u043e\u043a\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0442 \u043d\u0430 \u0442\u043e, \u0447\u0442\u043e \u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u044b\u0435 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u044b \u043d\u0430\u0445\u043e\u0434\u044f\u0442\u0441\u044f \u0431\u043b\u0438\u0436\u0435 \u043a \u043d\u0430\u0447\u0430\u043b\u0443 \u0441\u043f\u0438\u0441\u043a\u0430.</p> </li> <li><p><code>Coverage</code> \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u0434\u043e\u043b\u044e \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432, \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u044c\u044e, \u043f\u043e \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044e \u0441 \u043e\u0431\u0449\u0438\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e\u043c \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432. \u0412\u044b\u0441\u043e\u043a\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u043d\u0430 \u0445\u043e\u0440\u043e\u0448\u0435\u0435 \u0440\u0430\u0437\u043d\u043e\u043e\u0431\u0440\u0430\u0437\u0438\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439.</p> </li> <li><p><code>Hitrate</code> \u0438\u0437\u043c\u0435\u0440\u044f\u0435\u0442 \u0434\u043e\u043b\u044e \u0441\u043b\u0443\u0447\u0430\u0435\u0432, \u043a\u043e\u0433\u0434\u0430 \u0445\u043e\u0442\u044f \u0431\u044b \u043e\u0434\u0438\u043d \u0438\u0437 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0434\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u044b\u043c. \u0412\u044b\u0441\u043e\u043a\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0442 \u043d\u0430 \u0442\u043e, \u0434\u043e\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0434\u043b\u044f \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u044c \u043c\u043e\u0436\u0435\u0442 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u0442\u044c \u043a\u0430\u043a \u043c\u0438\u043d\u0438\u043c\u0443\u043c \u043e\u0434\u0438\u043d \u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u044b\u0439 \u044d\u043b\u0435\u043c\u0435\u043d\u0442</p> </li> </ul> <p>\u0412\u0430\u0436\u043d\u043e \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u0442\u044c, \u043a\u0430\u043a\u0438\u0435 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u043a\u0440\u0438\u0442\u0438\u0447\u043d\u044b \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u0447\u0438:</p> <ul> <li>\u0415\u0441\u043b\u0438 \u0432\u0430\u0436\u0435\u043d \u0440\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u0441\u043f\u0438\u0441\u043e\u043a, \u0441\u0442\u043e\u0438\u0442 \u043e\u0431\u0440\u0430\u0442\u0438\u0442\u044c \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043d\u0430 \u0442\u043e \u0447\u0442\u043e \u044d\u0432\u0440\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043c\u043e\u0436\u0435\u043b\u0438 \u043a\u0430\u043a \u0438 <code>apriori</code> \u043f\u043e\u043a\u0430\u0437\u0430\u043b\u0438 \u0441\u0435\u0431\u044f \u043b\u0443\u0447\u0448\u0435 \u0447\u0435\u043c \u0431\u043e\u043b\u0435\u0435 \u043f\u0440\u043e\u0434\u0432\u0438\u043d\u0443\u0442\u044b\u0435 \u043a\u0430\u043a <code>ALS</code>;</li> <li>\u0415\u0441\u043b\u0438 \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u0440\u0430\u0437\u043d\u043e\u043e\u0431\u0440\u0430\u0437\u0438\u0435 \u0434\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439, \u0442\u043e \u043c\u044b \u0432\u0438\u0434\u0438\u043c \u0447\u0442\u043e \u044d\u0432\u0440\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0442\u0443\u0442 \u043c\u0430\u043b\u043e \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u044b, \u0442\u0430\u043a \u043a\u0430\u043a \u043c\u044b \u0441\u0430\u043c\u0438 \u043e\u0442\u043e\u0431\u0440\u0430\u043b\u0438 \u043c\u0430\u043b\u0443\u044e \u0433\u0440\u0443\u043f\u043f\u0443 \u0444\u0438\u043b\u044c\u043c\u043e\u043c. <code>apriori</code>, \u0434\u0430\u043d\u0442 \u043b\u0443\u0447\u0448\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442, \u043d\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0440\u0430\u0432\u0438\u043b \u043f\u043e \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u0442\u044c \u0442\u043e\u0436\u0435 \u043e\u0433\u0440\u0430\u043d\u0438\u0441\u0438\u043d\u043e \u0438 \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u0432\u044b\u0431\u0440\u0430\u043d\u043d\u044b\u0445 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432. \u0411\u043e\u043b\u0435\u0435 \u043f\u0440\u0434\u0432\u0438\u043d\u0443\u0442\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u043a\u0430\u043a <code>ALS</code> \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u044e \u0431\u043e\u043b\u0435\u0435 \u0440\u0430\u0437\u043d\u043e\u043e\u0431\u0440\u0430\u0437\u043d\u044b\u0435 \u0444\u0438\u043b\u044c\u043c\u044b.</li> </ul> <p>\u0412 \u0446\u0435\u043b\u043e\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0430 \u0443 \u043d\u0430\u0441 \u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u0431\u043e\u043b\u044c\u0448\u0430\u044f, \u0438 \u043f\u043e\u0445\u043e\u0436\u0435 \u0447\u0442\u043e \u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u0441\u043b\u043e\u0436\u043d\u0430\u044f \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430 \u0438 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u043e\u0441\u0442\u044c \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0438\u0433\u0440\u0430\u0435\u0442 \u0432\u0430\u0436\u043d\u0443\u044e \u0440\u043e\u043b\u044c, \u0447\u0442\u043e \u0434\u0430\u0435\u0442 \u043d\u0430\u043c \u043b\u0443\u0447\u0448\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0434\u043b\u044f \u0431\u043e\u043b\u0435\u0435 \u043f\u0440\u043e\u0441\u0442\u044b\u0445 \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u0432.</p> <p>\u0414\u043b\u044f \u0444\u043e\u0440\u043c\u0430\u0440\u043e\u0432\u0430\u0433\u0438\u044f \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u043c\u043e\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u0431\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>SVD</code> \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0431\u043e\u043b\u0435\u0435 \u0440\u0430\u0437\u043d\u043e\u043e\u0431\u0440\u0430\u0437\u043d\u043e\u0433\u043e \u043a\u043e\u043d\u0442\u0435\u043d\u0442\u0430 \u0434\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0445\u043e\u0442\u044f \u0442\u0430\u043a\u043e\u0433\u043e \u043a\u043e\u043d\u0442\u0435\u043d\u0442\u0430 (\u0432\u044b\u0431\u043e\u0440\u043e\u0447\u043d\u044b\u0435 \u043a\u043b\u0438\u0435\u043d\u0442\u044b), \u0430 <code>apriori</code> \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043a\u0430\u043a \u0433\u043b\u0430\u043d\u044b\u043c \u043c\u0435\u0442\u043e\u0434\u043e\u043c \u0434\u043b\u044f \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438.</p>"},{"location":"portfolio/course_recsys/prob_neumf.html","title":"Prob neumf","text":"In\u00a0[1]: Copied! <pre>import numpy as np \nimport pandas as pd \nimport scipy.sparse as sp\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom dataclasses import dataclass\nimport os\nimport time\nimport torch\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torch.backends.cudnn as cudnn\n\n@dataclass\nclass Config:\n\n    train_rating = '/kaggle/input/ratings-test/ml-1m.train.rating'\n    test_negative = '/kaggle/input/ratings-test/ml-1m.test.negative'\n    model_path = '/kaggle/working/model/'\n    out = True\n    \n    model : str\n    batch_size : int = 256\n    factor_num : int = 32\n    num_layers : int = 3\n    test_num_ng : int = 99\n    num_ng : int = 4\n    dropout : float = 0.0\n    lr : float = 0.001\n    epochs : int = 20\n    top_k = 10\n    gpu = \"0\"\n    \nconfig = Config(model='NeuMF-end')\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu\ncudnn.benchmark = True\n</pre> import numpy as np  import pandas as pd  import scipy.sparse as sp import torch.nn as nn import torch.nn.functional as F from dataclasses import dataclass import os import time import torch import torch.optim as optim import torch.utils.data as data import torch.backends.cudnn as cudnn  @dataclass class Config:      train_rating = '/kaggle/input/ratings-test/ml-1m.train.rating'     test_negative = '/kaggle/input/ratings-test/ml-1m.test.negative'     model_path = '/kaggle/working/model/'     out = True          model : str     batch_size : int = 256     factor_num : int = 32     num_layers : int = 3     test_num_ng : int = 99     num_ng : int = 4     dropout : float = 0.0     lr : float = 0.001     epochs : int = 20     top_k = 10     gpu = \"0\"      config = Config(model='NeuMF-end') os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu cudnn.benchmark = True In\u00a0[2]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F \n\nclass NCF(nn.Module):\n\tdef __init__(self, user_num, item_num, factor_num, num_layers,\n\t\t\t\t\tdropout, model, GMF_model=None, MLP_model=None):\n\t\tsuper(NCF, self).__init__()\n\t\t\"\"\"\n\t\tuser_num: number of users;\n\t\titem_num: number of items;\n\t\tfactor_num: number of predictive factors;\n\t\tnum_layers: the number of layers in MLP model;\n\t\tdropout: dropout rate between fully connected layers;\n\t\tmodel: 'MLP', 'GMF', 'NeuMF-end', and 'NeuMF-pre';\n\t\tGMF_model: pre-trained GMF weights;\n\t\tMLP_model: pre-trained MLP weights.\n\t\t\"\"\"\t\t\n\t\tself.dropout = dropout\n\t\tself.model = model\n\t\tself.GMF_model = GMF_model\n\t\tself.MLP_model = MLP_model\n\n\t\tself.embed_user_GMF = nn.Embedding(user_num, factor_num)\n\t\tself.embed_item_GMF = nn.Embedding(item_num, factor_num)\n\t\tself.embed_user_MLP = nn.Embedding(\n\t\t\t\tuser_num, factor_num * (2 ** (num_layers - 1)))\n\t\tself.embed_item_MLP = nn.Embedding(\n\t\t\t\titem_num, factor_num * (2 ** (num_layers - 1)))\n\n\t\tMLP_modules = []\n\t\tfor i in range(num_layers):\n\t\t\tinput_size = factor_num * (2 ** (num_layers - i))\n\t\t\tMLP_modules.append(nn.Dropout(p=self.dropout))\n\t\t\tMLP_modules.append(nn.Linear(input_size, input_size//2))\n\t\t\tMLP_modules.append(nn.ReLU())\n\t\tself.MLP_layers = nn.Sequential(*MLP_modules)\n\n\t\tif self.model in ['MLP', 'GMF']:\n\t\t\tpredict_size = factor_num \n\t\telse:\n\t\t\tpredict_size = factor_num * 2\n\t\tself.predict_layer = nn.Linear(predict_size, 1)\n\t\tself._init_weight_()\n\n\tdef _init_weight_(self):\n\t\t\n\t\tif not self.model == 'NeuMF-pre':\n\t\t\tnn.init.normal_(self.embed_user_GMF.weight, std=0.01)\n\t\t\tnn.init.normal_(self.embed_user_MLP.weight, std=0.01)\n\t\t\tnn.init.normal_(self.embed_item_GMF.weight, std=0.01)\n\t\t\tnn.init.normal_(self.embed_item_MLP.weight, std=0.01)\n\n\t\t\tfor m in self.MLP_layers:\n\t\t\t\tif isinstance(m, nn.Linear):\n\t\t\t\t\tnn.init.xavier_uniform_(m.weight)\n\t\t\tnn.init.kaiming_uniform_(self.predict_layer.weight, \n\t\t\t\t\t\t\t\t\ta=1, nonlinearity='sigmoid')\n\n\t\t\tfor m in self.modules():\n\t\t\t\tif isinstance(m, nn.Linear) and m.bias is not None:\n\t\t\t\t\tm.bias.data.zero_()\n\t\telse:\n\t\t\t# embedding layers\n\t\t\tself.embed_user_GMF.weight.data.copy_(\n\t\t\t\t\t\t\tself.GMF_model.embed_user_GMF.weight)\n\t\t\tself.embed_item_GMF.weight.data.copy_(\n\t\t\t\t\t\t\tself.GMF_model.embed_item_GMF.weight)\n\t\t\tself.embed_user_MLP.weight.data.copy_(\n\t\t\t\t\t\t\tself.MLP_model.embed_user_MLP.weight)\n\t\t\tself.embed_item_MLP.weight.data.copy_(\n\t\t\t\t\t\t\tself.MLP_model.embed_item_MLP.weight)\n\n\t\t\t# mlp layers\n\t\t\tfor (m1, m2) in zip(\n\t\t\t\tself.MLP_layers, self.MLP_model.MLP_layers):\n\t\t\t\tif isinstance(m1, nn.Linear) and isinstance(m2, nn.Linear):\n\t\t\t\t\tm1.weight.data.copy_(m2.weight)\n\t\t\t\t\tm1.bias.data.copy_(m2.bias)\n\n\t\t\t# predict layers\n\t\t\tpredict_weight = torch.cat([\n\t\t\t\tself.GMF_model.predict_layer.weight, \n\t\t\t\tself.MLP_model.predict_layer.weight], dim=1)\n\t\t\tprecit_bias = self.GMF_model.predict_layer.bias + \\\n\t\t\t\t\t\tself.MLP_model.predict_layer.bias\n\n\t\t\tself.predict_layer.weight.data.copy_(0.5 * predict_weight)\n\t\t\tself.predict_layer.bias.data.copy_(0.5 * precit_bias)\n\n\tdef forward(self, user, item):\n\t\tif not self.model == 'MLP':\n\t\t\tembed_user_GMF = self.embed_user_GMF(user)\n\t\t\tembed_item_GMF = self.embed_item_GMF(item)\n\t\t\toutput_GMF = embed_user_GMF * embed_item_GMF\n\t\tif not self.model == 'GMF':\n\t\t\tembed_user_MLP = self.embed_user_MLP(user)\n\t\t\tembed_item_MLP = self.embed_item_MLP(item)\n\t\t\tinteraction = torch.cat((embed_user_MLP, embed_item_MLP), -1)\n\t\t\toutput_MLP = self.MLP_layers(interaction)\n\n\t\tif self.model == 'GMF':\n\t\t\tconcat = output_GMF\n\t\telif self.model == 'MLP':\n\t\t\tconcat = output_MLP\n\t\telse:\n\t\t\tconcat = torch.cat((output_GMF, output_MLP), -1)\n\n\t\tprediction = self.predict_layer(concat)\n\t\treturn prediction.view(-1)\n</pre> import torch import torch.nn as nn import torch.nn.functional as F   class NCF(nn.Module): \tdef __init__(self, user_num, item_num, factor_num, num_layers, \t\t\t\t\tdropout, model, GMF_model=None, MLP_model=None): \t\tsuper(NCF, self).__init__() \t\t\"\"\" \t\tuser_num: number of users; \t\titem_num: number of items; \t\tfactor_num: number of predictive factors; \t\tnum_layers: the number of layers in MLP model; \t\tdropout: dropout rate between fully connected layers; \t\tmodel: 'MLP', 'GMF', 'NeuMF-end', and 'NeuMF-pre'; \t\tGMF_model: pre-trained GMF weights; \t\tMLP_model: pre-trained MLP weights. \t\t\"\"\"\t\t \t\tself.dropout = dropout \t\tself.model = model \t\tself.GMF_model = GMF_model \t\tself.MLP_model = MLP_model  \t\tself.embed_user_GMF = nn.Embedding(user_num, factor_num) \t\tself.embed_item_GMF = nn.Embedding(item_num, factor_num) \t\tself.embed_user_MLP = nn.Embedding( \t\t\t\tuser_num, factor_num * (2 ** (num_layers - 1))) \t\tself.embed_item_MLP = nn.Embedding( \t\t\t\titem_num, factor_num * (2 ** (num_layers - 1)))  \t\tMLP_modules = [] \t\tfor i in range(num_layers): \t\t\tinput_size = factor_num * (2 ** (num_layers - i)) \t\t\tMLP_modules.append(nn.Dropout(p=self.dropout)) \t\t\tMLP_modules.append(nn.Linear(input_size, input_size//2)) \t\t\tMLP_modules.append(nn.ReLU()) \t\tself.MLP_layers = nn.Sequential(*MLP_modules)  \t\tif self.model in ['MLP', 'GMF']: \t\t\tpredict_size = factor_num  \t\telse: \t\t\tpredict_size = factor_num * 2 \t\tself.predict_layer = nn.Linear(predict_size, 1) \t\tself._init_weight_()  \tdef _init_weight_(self): \t\t \t\tif not self.model == 'NeuMF-pre': \t\t\tnn.init.normal_(self.embed_user_GMF.weight, std=0.01) \t\t\tnn.init.normal_(self.embed_user_MLP.weight, std=0.01) \t\t\tnn.init.normal_(self.embed_item_GMF.weight, std=0.01) \t\t\tnn.init.normal_(self.embed_item_MLP.weight, std=0.01)  \t\t\tfor m in self.MLP_layers: \t\t\t\tif isinstance(m, nn.Linear): \t\t\t\t\tnn.init.xavier_uniform_(m.weight) \t\t\tnn.init.kaiming_uniform_(self.predict_layer.weight,  \t\t\t\t\t\t\t\t\ta=1, nonlinearity='sigmoid')  \t\t\tfor m in self.modules(): \t\t\t\tif isinstance(m, nn.Linear) and m.bias is not None: \t\t\t\t\tm.bias.data.zero_() \t\telse: \t\t\t# embedding layers \t\t\tself.embed_user_GMF.weight.data.copy_( \t\t\t\t\t\t\tself.GMF_model.embed_user_GMF.weight) \t\t\tself.embed_item_GMF.weight.data.copy_( \t\t\t\t\t\t\tself.GMF_model.embed_item_GMF.weight) \t\t\tself.embed_user_MLP.weight.data.copy_( \t\t\t\t\t\t\tself.MLP_model.embed_user_MLP.weight) \t\t\tself.embed_item_MLP.weight.data.copy_( \t\t\t\t\t\t\tself.MLP_model.embed_item_MLP.weight)  \t\t\t# mlp layers \t\t\tfor (m1, m2) in zip( \t\t\t\tself.MLP_layers, self.MLP_model.MLP_layers): \t\t\t\tif isinstance(m1, nn.Linear) and isinstance(m2, nn.Linear): \t\t\t\t\tm1.weight.data.copy_(m2.weight) \t\t\t\t\tm1.bias.data.copy_(m2.bias)  \t\t\t# predict layers \t\t\tpredict_weight = torch.cat([ \t\t\t\tself.GMF_model.predict_layer.weight,  \t\t\t\tself.MLP_model.predict_layer.weight], dim=1) \t\t\tprecit_bias = self.GMF_model.predict_layer.bias + \\ \t\t\t\t\t\tself.MLP_model.predict_layer.bias  \t\t\tself.predict_layer.weight.data.copy_(0.5 * predict_weight) \t\t\tself.predict_layer.bias.data.copy_(0.5 * precit_bias)  \tdef forward(self, user, item): \t\tif not self.model == 'MLP': \t\t\tembed_user_GMF = self.embed_user_GMF(user) \t\t\tembed_item_GMF = self.embed_item_GMF(item) \t\t\toutput_GMF = embed_user_GMF * embed_item_GMF \t\tif not self.model == 'GMF': \t\t\tembed_user_MLP = self.embed_user_MLP(user) \t\t\tembed_item_MLP = self.embed_item_MLP(item) \t\t\tinteraction = torch.cat((embed_user_MLP, embed_item_MLP), -1) \t\t\toutput_MLP = self.MLP_layers(interaction)  \t\tif self.model == 'GMF': \t\t\tconcat = output_GMF \t\telif self.model == 'MLP': \t\t\tconcat = output_MLP \t\telse: \t\t\tconcat = torch.cat((output_GMF, output_MLP), -1)  \t\tprediction = self.predict_layer(concat) \t\treturn prediction.view(-1) In\u00a0[3]: Copied! <pre>def hit(gt_item, pred_items):\n\tif gt_item in pred_items:\n\t\treturn 1\n\treturn 0\n\ndef ndcg(gt_item, pred_items):\n\tif gt_item in pred_items:\n\t\tindex = pred_items.index(gt_item)\n\t\treturn np.reciprocal(np.log2(index+2))\n\treturn 0\n\ndef metrics(model, test_loader, top_k):\n\tHR, NDCG = [], []\n\n\tfor user, item, label in test_loader:\n\t\tuser = user.cuda()\n\t\titem = item.cuda()\n\n\t\tpredictions = model(user, item)\n\t\t_, indices = torch.topk(predictions, top_k)\n\t\trecommends = torch.take(\n\t\t\t\titem, indices).cpu().numpy().tolist()\n\n\t\tgt_item = item[0].item()\n\t\tHR.append(hit(gt_item, recommends))\n\t\tNDCG.append(ndcg(gt_item, recommends))\n\n\treturn np.mean(HR), np.mean(NDCG)\n</pre> def hit(gt_item, pred_items): \tif gt_item in pred_items: \t\treturn 1 \treturn 0  def ndcg(gt_item, pred_items): \tif gt_item in pred_items: \t\tindex = pred_items.index(gt_item) \t\treturn np.reciprocal(np.log2(index+2)) \treturn 0  def metrics(model, test_loader, top_k): \tHR, NDCG = [], []  \tfor user, item, label in test_loader: \t\tuser = user.cuda() \t\titem = item.cuda()  \t\tpredictions = model(user, item) \t\t_, indices = torch.topk(predictions, top_k) \t\trecommends = torch.take( \t\t\t\titem, indices).cpu().numpy().tolist()  \t\tgt_item = item[0].item() \t\tHR.append(hit(gt_item, recommends)) \t\tNDCG.append(ndcg(gt_item, recommends))  \treturn np.mean(HR), np.mean(NDCG) In\u00a0[4]: Copied! <pre>def load_all(test_num=100):\n\n\t# load training data (positive samples only)\n\ttrain_data = pd.read_csv(\n\t\tconfig.train_rating, \n\t\tsep='\\t', header=None, names=['user', 'item'], \n\t\tusecols=[0, 1], dtype={0: np.int32, 1: np.int32})\n\n\tuser_num = train_data['user'].max() + 1\n\titem_num = train_data['item'].max() + 1\n\ttrain_data = train_data.values.tolist()\n\n\t# load user/film rating combinations as a dok matrix\n\ttrain_mat = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n\tfor x in train_data:\n\t\ttrain_mat[x[0], x[1]] = 1.0\n\n\t# load test data (positive &amp; 99 negative samples) \n\ttest_data = []\n\twith open(config.test_negative, 'r') as fd:\n\t\tline = fd.readline()\n\t\twhile line != None and line != '':\n\t\t\tarr = line.split('\\t')\n\t\t\tu = eval(arr[0])[0]\n\t\t\ttest_data.append([u, eval(arr[0])[1]])\n\t\t\tfor i in arr[1:]:\n\t\t\t\ttest_data.append([u, int(i)])\n\t\t\tline = fd.readline()\n\treturn train_data, test_data, user_num, item_num, train_mat\n</pre> def load_all(test_num=100):  \t# load training data (positive samples only) \ttrain_data = pd.read_csv( \t\tconfig.train_rating,  \t\tsep='\\t', header=None, names=['user', 'item'],  \t\tusecols=[0, 1], dtype={0: np.int32, 1: np.int32})  \tuser_num = train_data['user'].max() + 1 \titem_num = train_data['item'].max() + 1 \ttrain_data = train_data.values.tolist()  \t# load user/film rating combinations as a dok matrix \ttrain_mat = sp.dok_matrix((user_num, item_num), dtype=np.float32) \tfor x in train_data: \t\ttrain_mat[x[0], x[1]] = 1.0  \t# load test data (positive &amp; 99 negative samples)  \ttest_data = [] \twith open(config.test_negative, 'r') as fd: \t\tline = fd.readline() \t\twhile line != None and line != '': \t\t\tarr = line.split('\\t') \t\t\tu = eval(arr[0])[0] \t\t\ttest_data.append([u, eval(arr[0])[1]]) \t\t\tfor i in arr[1:]: \t\t\t\ttest_data.append([u, int(i)]) \t\t\tline = fd.readline() \treturn train_data, test_data, user_num, item_num, train_mat In\u00a0[5]: Copied! <pre>class NCFData(data.Dataset):\n    \n\tdef __init__(self,\n\t\t\t\tfeatures, \n\t\t\t\tnum_item, \n\t\t\t\ttrain_mat=None, \n\t\t\t\tnum_ng=0, \n\t\t\t\tis_training=None):\n\t\t\t\t\n\t\tsuper(NCFData, self).__init__()\n\t\tself.features_ps = features\n\t\tself.num_item = num_item\n\t\tself.train_mat = train_mat\n\t\tself.num_ng = num_ng\n\t\tself.is_training = is_training\n\t\tself.labels = [0 for _ in range(len(features))]\n\n\t# add negative samples to the positive samples (train)\n\tdef ng_sample(self):\n\t\tassert self.is_training, 'no need to sampling when testing'\n\n\t\tself.features_ng = []\n\t\tfor x in self.features_ps:\n\t\t\tu = x[0]\n\t\t\tfor t in range(self.num_ng):\n\t\t\t\tj = np.random.randint(self.num_item)\n\t\t\t\twhile (u, j) in self.train_mat:\n\t\t\t\t\tj = np.random.randint(self.num_item)\n\t\t\t\tself.features_ng.append([u, j])\n\n\t\tlabels_ps = [1 for _ in range(len(self.features_ps))]\n\t\tlabels_ng = [0 for _ in range(len(self.features_ng))]\n\n\t\tself.features_fill = self.features_ps + self.features_ng\n\t\tself.labels_fill = labels_ps + labels_ng\n\n\tdef __len__(self):\n\t\treturn (self.num_ng + 1) * len(self.labels)\n\n\t# get items during training\n\tdef __getitem__(self, idx):\n\t\t\n\t\tfeatures = self.features_fill if self.is_training \\\n\t\t\t\t\telse self.features_ps\n\t\tlabels = self.labels_fill if self.is_training \\\n\t\t\t\t\telse self.labels\n\n\t\tuser = features[idx][0]\n\t\titem = features[idx][1]\n\t\tlabel = labels[idx]\n\t\treturn user, item ,label\n</pre> class NCFData(data.Dataset):      \tdef __init__(self, \t\t\t\tfeatures,  \t\t\t\tnum_item,  \t\t\t\ttrain_mat=None,  \t\t\t\tnum_ng=0,  \t\t\t\tis_training=None): \t\t\t\t \t\tsuper(NCFData, self).__init__() \t\tself.features_ps = features \t\tself.num_item = num_item \t\tself.train_mat = train_mat \t\tself.num_ng = num_ng \t\tself.is_training = is_training \t\tself.labels = [0 for _ in range(len(features))]  \t# add negative samples to the positive samples (train) \tdef ng_sample(self): \t\tassert self.is_training, 'no need to sampling when testing'  \t\tself.features_ng = [] \t\tfor x in self.features_ps: \t\t\tu = x[0] \t\t\tfor t in range(self.num_ng): \t\t\t\tj = np.random.randint(self.num_item) \t\t\t\twhile (u, j) in self.train_mat: \t\t\t\t\tj = np.random.randint(self.num_item) \t\t\t\tself.features_ng.append([u, j])  \t\tlabels_ps = [1 for _ in range(len(self.features_ps))] \t\tlabels_ng = [0 for _ in range(len(self.features_ng))]  \t\tself.features_fill = self.features_ps + self.features_ng \t\tself.labels_fill = labels_ps + labels_ng  \tdef __len__(self): \t\treturn (self.num_ng + 1) * len(self.labels)  \t# get items during training \tdef __getitem__(self, idx): \t\t \t\tfeatures = self.features_fill if self.is_training \\ \t\t\t\t\telse self.features_ps \t\tlabels = self.labels_fill if self.is_training \\ \t\t\t\t\telse self.labels  \t\tuser = features[idx][0] \t\titem = features[idx][1] \t\tlabel = labels[idx] \t\treturn user, item ,label In\u00a0[6]: Copied! <pre>train_data, test_data, user_num ,item_num, train_mat = load_all()\n\n# construct the train and test datasets\ntrain_dataset = NCFData(train_data,\n\t\t\t\t\t\titem_num, \n\t\t\t\t\t\ttrain_mat, \n\t\t\t\t\t\tconfig.num_ng,\n\t\t\t\t\t\tTrue)\n\t\t\t\t\t\t\ntest_dataset = NCFData(test_data, \n\t\t\t\t\t\titem_num,\n\t\t\t\t\t\ttrain_mat,\n\t\t\t\t\t\t0, \n\t\t\t\t\t\tFalse)\n\ntrain_loader = data.DataLoader(train_dataset,\n\t\t\t\t\t\t\t\tbatch_size=config.batch_size, \n\t\t\t\t\t\t\t\tshuffle=True, \n\t\t\t\t\t\t\t\tnum_workers=4)\n\t\t\t\t\t\t\t\t\ntest_loader = data.DataLoader(test_dataset,\n\t\t\t\t\t\t\t\tbatch_size=config.test_num_ng+1, \n\t\t\t\t\t\t\t\tshuffle=False, \n\t\t\t\t\t\t\t\tnum_workers=0)\n</pre> train_data, test_data, user_num ,item_num, train_mat = load_all()  # construct the train and test datasets train_dataset = NCFData(train_data, \t\t\t\t\t\titem_num,  \t\t\t\t\t\ttrain_mat,  \t\t\t\t\t\tconfig.num_ng, \t\t\t\t\t\tTrue) \t\t\t\t\t\t test_dataset = NCFData(test_data,  \t\t\t\t\t\titem_num, \t\t\t\t\t\ttrain_mat, \t\t\t\t\t\t0,  \t\t\t\t\t\tFalse)  train_loader = data.DataLoader(train_dataset, \t\t\t\t\t\t\t\tbatch_size=config.batch_size,  \t\t\t\t\t\t\t\tshuffle=True,  \t\t\t\t\t\t\t\tnum_workers=4) \t\t\t\t\t\t\t\t test_loader = data.DataLoader(test_dataset, \t\t\t\t\t\t\t\tbatch_size=config.test_num_ng+1,  \t\t\t\t\t\t\t\tshuffle=False,  \t\t\t\t\t\t\t\tnum_workers=0) In\u00a0[7]: Copied! <pre>if config.model == 'NeuMF-pre':\n\tassert os.path.exists(config.GMF_model_path), 'lack of GMF model'\n\tassert os.path.exists(config.MLP_model_path), 'lack of MLP model'\n\tGMF_model = torch.load(config.GMF_model_path)\n\tMLP_model = torch.load(config.MLP_model_path)\nelse:\n\tGMF_model = None\n\tMLP_model = None\n\nmodel = NCF(user_num, \n\t\t\titem_num, \n\t\t\tconfig.factor_num, \n\t\t\tconfig.num_layers, \n\t\t\tconfig.dropout, \n\t\t\tconfig.model, \n\t\t\tGMF_model, \n\t\t\tMLP_model)\n\t\t\t\nmodel.cuda()\nloss_function = nn.BCEWithLogitsLoss()\n\nif config.model == 'NeuMF-pre':\n\toptimizer = optim.SGD(model.parameters(), lr=config.lr)\nelse:\n\toptimizer = optim.Adam(model.parameters(), lr=config.lr)\n</pre> if config.model == 'NeuMF-pre': \tassert os.path.exists(config.GMF_model_path), 'lack of GMF model' \tassert os.path.exists(config.MLP_model_path), 'lack of MLP model' \tGMF_model = torch.load(config.GMF_model_path) \tMLP_model = torch.load(config.MLP_model_path) else: \tGMF_model = None \tMLP_model = None  model = NCF(user_num,  \t\t\titem_num,  \t\t\tconfig.factor_num,  \t\t\tconfig.num_layers,  \t\t\tconfig.dropout,  \t\t\tconfig.model,  \t\t\tGMF_model,  \t\t\tMLP_model) \t\t\t model.cuda() loss_function = nn.BCEWithLogitsLoss()  if config.model == 'NeuMF-pre': \toptimizer = optim.SGD(model.parameters(), lr=config.lr) else: \toptimizer = optim.Adam(model.parameters(), lr=config.lr) In\u00a0[8]: Copied! <pre>count, best_hr = 0, 0\nfor epoch in range(config.epochs):\n\tmodel.train() \n\tstart_time = time.time()\n\ttrain_loader.dataset.ng_sample()\n\n\tfor user, item, label in train_loader:\n\t\tuser = user.cuda()\n\t\titem = item.cuda()\n\t\tlabel = label.float().cuda()\n\n\t\tmodel.zero_grad()\n\t\tprediction = model(user, item)\n\t\tloss = loss_function(prediction, label)\n\t\tloss.backward()\n\t\toptimizer.step()\n\t\tcount += 1\n\n\tmodel.eval()\n\tHR, NDCG = metrics(model, test_loader, config.top_k)\n\n\telapsed_time = time.time() - start_time\n\tprint(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + \n\t\t\ttime.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n\tprint(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n\n\tif HR &gt; best_hr:\n\t\tbest_hr, best_ndcg, best_epoch = HR, NDCG, epoch\n\t\tif config.out:\n\t\t\tif not os.path.exists(config.model_path):\n\t\t\t\tos.mkdir(config.model_path)\n\t\t\ttorch.save(model, \n\t\t\t\t'{}{}.pth'.format(config.model_path, config.model))\n\nprint(\"End. Best epoch {:03d}: HR = {:.3f}, NDCG = {:.3f}\".format(\n\t\t\t\t\t\t\t\t\tbest_epoch, best_hr, best_ndcg))\n</pre> count, best_hr = 0, 0 for epoch in range(config.epochs): \tmodel.train()  \tstart_time = time.time() \ttrain_loader.dataset.ng_sample()  \tfor user, item, label in train_loader: \t\tuser = user.cuda() \t\titem = item.cuda() \t\tlabel = label.float().cuda()  \t\tmodel.zero_grad() \t\tprediction = model(user, item) \t\tloss = loss_function(prediction, label) \t\tloss.backward() \t\toptimizer.step() \t\tcount += 1  \tmodel.eval() \tHR, NDCG = metrics(model, test_loader, config.top_k)  \telapsed_time = time.time() - start_time \tprint(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" +  \t\t\ttime.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time))) \tprint(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))  \tif HR &gt; best_hr: \t\tbest_hr, best_ndcg, best_epoch = HR, NDCG, epoch \t\tif config.out: \t\t\tif not os.path.exists(config.model_path): \t\t\t\tos.mkdir(config.model_path) \t\t\ttorch.save(model,  \t\t\t\t'{}{}.pth'.format(config.model_path, config.model))  print(\"End. Best epoch {:03d}: HR = {:.3f}, NDCG = {:.3f}\".format( \t\t\t\t\t\t\t\t\tbest_epoch, best_hr, best_ndcg)) <pre>The time elapse of epoch 000 is: 00: 02: 01\nHR: 0.629\tNDCG: 0.366\nThe time elapse of epoch 001 is: 00: 02: 00\nHR: 0.670\tNDCG: 0.397\nThe time elapse of epoch 002 is: 00: 02: 01\nHR: 0.686\tNDCG: 0.411\nThe time elapse of epoch 003 is: 00: 02: 00\nHR: 0.685\tNDCG: 0.410\nThe time elapse of epoch 004 is: 00: 01: 59\nHR: 0.698\tNDCG: 0.419\nThe time elapse of epoch 005 is: 00: 02: 01\nHR: 0.702\tNDCG: 0.425\nThe time elapse of epoch 006 is: 00: 01: 59\nHR: 0.700\tNDCG: 0.423\nThe time elapse of epoch 007 is: 00: 02: 01\nHR: 0.698\tNDCG: 0.418\nThe time elapse of epoch 008 is: 00: 01: 59\nHR: 0.699\tNDCG: 0.422\nThe time elapse of epoch 009 is: 00: 02: 01\nHR: 0.699\tNDCG: 0.423\nThe time elapse of epoch 010 is: 00: 01: 59\nHR: 0.698\tNDCG: 0.422\nThe time elapse of epoch 011 is: 00: 02: 01\nHR: 0.691\tNDCG: 0.420\nThe time elapse of epoch 012 is: 00: 01: 59\nHR: 0.695\tNDCG: 0.420\nThe time elapse of epoch 013 is: 00: 02: 00\nHR: 0.691\tNDCG: 0.416\nThe time elapse of epoch 014 is: 00: 01: 58\nHR: 0.693\tNDCG: 0.418\nThe time elapse of epoch 015 is: 00: 01: 59\nHR: 0.688\tNDCG: 0.417\nThe time elapse of epoch 016 is: 00: 01: 58\nHR: 0.683\tNDCG: 0.413\nThe time elapse of epoch 017 is: 00: 02: 00\nHR: 0.678\tNDCG: 0.410\nThe time elapse of epoch 018 is: 00: 01: 59\nHR: 0.681\tNDCG: 0.414\nThe time elapse of epoch 019 is: 00: 01: 59\nHR: 0.676\tNDCG: 0.407\nEnd. Best epoch 005: HR = 0.702, NDCG = 0.425\n</pre>"},{"location":"portfolio/course_recsys/prob_neumf.html#neumf","title":"NeuMF\u00b6","text":""},{"location":"portfolio/course_recsys/prob_x5.html","title":"Prob x5","text":"In\u00a0[1]: Copied! <pre>from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom catboost import CatBoostRegressor\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> from sklearn.model_selection import train_test_split from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder from catboost import CatBoostRegressor from matplotlib import pyplot as plt import seaborn as sns import pandas as pd import numpy as np  import warnings warnings.filterwarnings('ignore') In\u00a0[2]: Copied! <pre>random_state = 47\nk = 0.1\n</pre> random_state = 47 k = 0.1 In\u00a0[3]: Copied! <pre># \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 uplift@k\ndef uplift_at_k(uplift_scores, y_true, treatment, k):\n    \n    n_top = int(len(uplift_scores) * k)\n    indices = np.argsort(-uplift_scores)[:n_top]\n    \n    treat_top = treatment.iloc[indices]\n    y_top = y_true.iloc[indices]\n    return y_top[treat_top == 1].mean() - y_top[treat_top == 0].mean()\n</pre> # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 uplift@k def uplift_at_k(uplift_scores, y_true, treatment, k):          n_top = int(len(uplift_scores) * k)     indices = np.argsort(-uplift_scores)[:n_top]          treat_top = treatment.iloc[indices]     y_top = y_true.iloc[indices]     return y_top[treat_top == 1].mean() - y_top[treat_top == 0].mean() In\u00a0[4]: Copied! <pre>models_results = {\n    'approach': [],\n    f'train_uplift@{k*100}%': [],\n    f'test_uplift@{k*100}%': []\n}\n</pre> models_results = {     'approach': [],     f'train_uplift@{k*100}%': [],     f'test_uplift@{k*100}%': [] } In\u00a0[5]: Copied! <pre>import pandas as pd\n\ndf_clients = pd.read_csv('/kaggle/input/fetch-x5/fetch_x5_clients.csv')\ndf_train = pd.read_csv('/kaggle/input/fetch-x5/fetch_x5_train.csv')\n\n# \u041a\u043b\u0438\u0435\u043d\u0442\u044b\ndf_clients.head()\n</pre> import pandas as pd  df_clients = pd.read_csv('/kaggle/input/fetch-x5/fetch_x5_clients.csv') df_train = pd.read_csv('/kaggle/input/fetch-x5/fetch_x5_train.csv')  # \u041a\u043b\u0438\u0435\u043d\u0442\u044b df_clients.head() Out[5]: client_id first_issue_date first_redeem_date age gender 0 000012768d 2017-08-05 15:40:48 2018-01-04 19:30:07 45 U 1 000036f903 2017-04-10 13:54:23 2017-04-23 12:37:56 72 F 2 000048b7a6 2018-12-15 13:33:11 NaN 68 F 3 000073194a 2017-05-23 12:56:14 2017-11-24 11:18:01 60 F 4 00007c7133 2017-05-22 16:17:08 2018-12-31 17:17:33 67 U In\u00a0[6]: Copied! <pre># \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0438\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u044f \u0432\u043e\u0437\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043d\u0430 \u043a\u043b\u0435\u0438\u043d\u0442\u0430\ndf_train.head()\n</pre> # \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0438\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u044f \u0432\u043e\u0437\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u043d\u0430 \u043a\u043b\u0435\u0438\u043d\u0442\u0430 df_train.head() Out[6]: client_id treatment_flg target 0 000012768d 0 1 1 000036f903 1 1 2 00010925a5 1 1 3 0001f552b0 1 1 4 00020e7b18 1 1 In\u00a0[7]: Copied! <pre>df_clients = df_clients[df_clients['client_id'].isin(df_train['client_id'])]\nprint(df_clients.shape, df_train.shape)\n</pre> df_clients = df_clients[df_clients['client_id'].isin(df_train['client_id'])] print(df_clients.shape, df_train.shape) <pre>(200039, 5) (200039, 3)\n</pre> In\u00a0[8]: Copied! <pre>print(f\"Dataset features shape: {df_clients.shape}\")\nprint(f\"Dataset train shape: {df_train.shape}\")\nprint(f\"Dataset target mean: {df_train.target.mean()}\")\nprint(f\"Dataset treatment mean: {df_train.treatment_flg.mean()}\")\n</pre> print(f\"Dataset features shape: {df_clients.shape}\") print(f\"Dataset train shape: {df_train.shape}\") print(f\"Dataset target mean: {df_train.target.mean()}\") print(f\"Dataset treatment mean: {df_train.treatment_flg.mean()}\") <pre>Dataset features shape: (200039, 5)\nDataset train shape: (200039, 3)\nDataset target mean: 0.6198891216212838\nDataset treatment mean: 0.4998075375301816\n</pre> In\u00a0[9]: Copied! <pre>df_clients = df_clients.dropna()\ndf_clients.shape\n</pre> df_clients = df_clients.dropna() df_clients.shape Out[9]: <pre>(182493, 5)</pre> In\u00a0[10]: Copied! <pre>df_features = df_train.merge(df_clients,on='client_id')\ndf_features.index = df_features['client_id']\ndel df_features['client_id']\n</pre> df_features = df_train.merge(df_clients,on='client_id') df_features.index = df_features['client_id'] del df_features['client_id'] In\u00a0[11]: Copied! <pre>from sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\ndf_features['gender'] = encoder.fit_transform(df_features['gender'])\n</pre> from sklearn.preprocessing import LabelEncoder  encoder = LabelEncoder() df_features['gender'] = encoder.fit_transform(df_features['gender']) In\u00a0[12]: Copied! <pre># \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c datetime \u0432 unix\ndf_features['first_issue_time'] = (pd.to_datetime(df_features['first_issue_date'])- pd.Timestamp('1970-01-01')) // pd.Timedelta('1s')\ndf_features['first_redeem_time'] = (pd.to_datetime(df_features['first_redeem_date']) - pd.Timestamp('1970-01-01')) // pd.Timedelta('1s')\ndf_features['issue_redeem_delay'] = df_features['first_redeem_time'] - df_features['first_issue_time']\ndf_features = df_features.drop(['first_issue_date', 'first_redeem_date'], axis=1)\n\ndf_features.head(2)\n</pre> # \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c datetime \u0432 unix df_features['first_issue_time'] = (pd.to_datetime(df_features['first_issue_date'])- pd.Timestamp('1970-01-01')) // pd.Timedelta('1s') df_features['first_redeem_time'] = (pd.to_datetime(df_features['first_redeem_date']) - pd.Timestamp('1970-01-01')) // pd.Timedelta('1s') df_features['issue_redeem_delay'] = df_features['first_redeem_time'] - df_features['first_issue_time'] df_features = df_features.drop(['first_issue_date', 'first_redeem_date'], axis=1)  df_features.head(2) Out[12]: treatment_flg target age gender first_issue_time first_redeem_time issue_redeem_delay client_id 000012768d 0 1 45 2 1501947648 1515094207 13146559 000036f903 1 1 72 0 1491832463 1492951076 1118613 In\u00a0[13]: Copied! <pre># \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u0435\u0441\u043b\u0438 \u043e\u043d\u0438 \u0435\u0441\u0442\u044c\ndf_features = df_features.drop_duplicates()\n</pre> # \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u0435\u0441\u043b\u0438 \u043e\u043d\u0438 \u0435\u0441\u0442\u044c df_features = df_features.drop_duplicates() In\u00a0[14]: Copied! <pre>from sklearn.model_selection import train_test_split\n\ny = \"target\"\nT = \"treatment_flg\"\nX = [\"age\",\"gender\",\"first_issue_time\",\"first_redeem_time\",\"issue_redeem_delay\"]\n\ntrain, test = train_test_split(df_features,\n                               test_size=0.3,\n                               random_state=47)\n</pre> from sklearn.model_selection import train_test_split  y = \"target\" T = \"treatment_flg\" X = [\"age\",\"gender\",\"first_issue_time\",\"first_redeem_time\",\"issue_redeem_delay\"]  train, test = train_test_split(df_features,                                test_size=0.3,                                random_state=47) In\u00a0[15]: Copied! <pre>print('train target ratio',round(train[y].mean(),3))\nprint('test target ratio',round(test[y].mean(),3))\n</pre> print('train target ratio',round(train[y].mean(),3)) print('test target ratio',round(test[y].mean(),3)) <pre>train target ratio 0.645\ntest target ratio 0.644\n</pre> In\u00a0[16]: Copied! <pre>from sklearn.linear_model import LogisticRegression\n\nnp.random.seed(123)\n\n# \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f\nm0 = CatBoostRegressor(iterations=100,depth=4,learning_rate=0.1,random_seed=42,verbose=0)\nm1 = CatBoostRegressor(iterations=100,depth=4,learning_rate=0.1,random_seed=42,verbose=0)\n\n# propensity \u043c\u043e\u0434\u0435\u043b\u044c\ng = LogisticRegression(solver=\"lbfgs\", penalty='none') \n</pre> from sklearn.linear_model import LogisticRegression  np.random.seed(123)  # \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f m0 = CatBoostRegressor(iterations=100,depth=4,learning_rate=0.1,random_seed=42,verbose=0) m1 = CatBoostRegressor(iterations=100,depth=4,learning_rate=0.1,random_seed=42,verbose=0)  # propensity \u043c\u043e\u0434\u0435\u043b\u044c g = LogisticRegression(solver=\"lbfgs\", penalty='none')  In\u00a0[17]: Copied! <pre>m0.fit(train.query(f\"{T}==0\")[X], train.query(f\"{T}==0\")[y])\nm1.fit(train.query(f\"{T}==1\")[X], train.query(f\"{T}==1\")[y])   \ng.fit(train[X], train[T]);\n\nd_train = np.where(train[T]==0,\n                   m1.predict(train[X]) - train[y],\n                   train[y] - m0.predict(train[X]))\n</pre> m0.fit(train.query(f\"{T}==0\")[X], train.query(f\"{T}==0\")[y]) m1.fit(train.query(f\"{T}==1\")[X], train.query(f\"{T}==1\")[y])    g.fit(train[X], train[T]);  d_train = np.where(train[T]==0,                    m1.predict(train[X]) - train[y],                    train[y] - m0.predict(train[X])) In\u00a0[18]: Copied! <pre># corrected labels for 2nd group of models\nd_train = np.where(train[T]==0,\n                   m1.predict(train[X]) - train[y],\n                   train[y] - m0.predict(train[X]))\n</pre> # corrected labels for 2nd group of models d_train = np.where(train[T]==0,                    m1.predict(train[X]) - train[y],                    train[y] - m0.predict(train[X])) In\u00a0[19]: Copied! <pre># second stage\nmx0 = CatBoostRegressor(iterations=100,depth=4,learning_rate=0.1,random_seed=42,verbose=0)\nmx1 = CatBoostRegressor(iterations=100,depth=4,learning_rate=0.1,random_seed=42,verbose=0)\n\nmx0.fit(train.query(f\"{T}==0\")[X], d_train[train[T]==0])\nmx1.fit(train.query(f\"{T}==1\")[X], d_train[train[T]==1]);\n</pre> # second stage mx0 = CatBoostRegressor(iterations=100,depth=4,learning_rate=0.1,random_seed=42,verbose=0) mx1 = CatBoostRegressor(iterations=100,depth=4,learning_rate=0.1,random_seed=42,verbose=0)  mx0.fit(train.query(f\"{T}==0\")[X], d_train[train[T]==0]) mx1.fit(train.query(f\"{T}==1\")[X], d_train[train[T]==1]); In\u00a0[20]: Copied! <pre>def ps_predict(df, t): \n    return g.predict_proba(df[X])[:, t]\n    \nx_cate_train = (ps_predict(train,1)*mx0.predict(train[X]) +\n                ps_predict(train,0)*mx1.predict(train[X]))\n\nx_cate_test = (ps_predict(test,1)*mx0.predict(test[X]) +\n                                ps_predict(test,0)*mx1.predict(test[X]))\nx_cate_test_res = test.assign(cate=x_cate_test)\n</pre> def ps_predict(df, t):      return g.predict_proba(df[X])[:, t]      x_cate_train = (ps_predict(train,1)*mx0.predict(train[X]) +                 ps_predict(train,0)*mx1.predict(train[X]))  x_cate_test = (ps_predict(test,1)*mx0.predict(test[X]) +                                 ps_predict(test,0)*mx1.predict(test[X])) x_cate_test_res = test.assign(cate=x_cate_test) In\u00a0[21]: Copied! <pre>ct_score_train = uplift_at_k(x_cate_train, train[y].copy(), train[T].copy(), k)\nct_score_test = uplift_at_k(x_cate_test, test[y].copy(), test[T].copy(), k)\nprint('train uplift @k=10%',round(ct_score_train,3))\nprint('test set uplift @k=10%',round(ct_score_test,3))\n</pre> ct_score_train = uplift_at_k(x_cate_train, train[y].copy(), train[T].copy(), k) ct_score_test = uplift_at_k(x_cate_test, test[y].copy(), test[T].copy(), k) print('train uplift @k=10%',round(ct_score_train,3)) print('test set uplift @k=10%',round(ct_score_test,3)) <pre>train uplift @k=10% 0.147\ntest set uplift @k=10% 0.109\n</pre> <p>\u0421\u0440\u0430\u0432\u043d\u0438\u043c \u043a\u0430\u043a \u043c\u043e\u0434\u0435\u043b\u044c \u0441\u0435\u0431\u044f \u043f\u043e\u043a\u0430\u0437\u0430\u043b\u0430 \u043f\u043e \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044e \u0432 \u0434\u0440\u0443\u0433\u0438\u043c\u0438 \u043c\u043e\u0434\u0435\u043b\u044f\u043c\u0438</p> In\u00a0[22]: Copied! <pre>import io\n\nstrs = \"\"\"\napproach\ttrain_uplift@10.0%\ttest_uplift@10.0%\n2\tTwo-Model Approach (T-Learner)\t0.146412\t0.114067\n0\tClassTransformation\t0.149799\t0.111760\n3\tTwo-Model Approach (ddr_control)\t0.121663\t0.108137\n4\tTwo-Model Approach (ddr_treatment)\t0.101789\t0.076922\n1\tS-Learner\t0.075230\t0.075230\n\"\"\"\n\nmodels_results = pd.read_csv(io.StringIO(strs),sep='\\t').to_dict('list')\nmodels_results['approach'].append('X Learner')\nmodels_results[f'train_uplift@{k*100}%'].append(ct_score_train)\nmodels_results[f'test_uplift@{k*100}%'].append(ct_score_test)\npd.DataFrame(models_results).sort_values(by='test_uplift@10.0%',ascending=False)\n</pre> import io  strs = \"\"\" approach\ttrain_uplift@10.0%\ttest_uplift@10.0% 2\tTwo-Model Approach (T-Learner)\t0.146412\t0.114067 0\tClassTransformation\t0.149799\t0.111760 3\tTwo-Model Approach (ddr_control)\t0.121663\t0.108137 4\tTwo-Model Approach (ddr_treatment)\t0.101789\t0.076922 1\tS-Learner\t0.075230\t0.075230 \"\"\"  models_results = pd.read_csv(io.StringIO(strs),sep='\\t').to_dict('list') models_results['approach'].append('X Learner') models_results[f'train_uplift@{k*100}%'].append(ct_score_train) models_results[f'test_uplift@{k*100}%'].append(ct_score_test) pd.DataFrame(models_results).sort_values(by='test_uplift@10.0%',ascending=False) Out[22]: approach train_uplift@10.0% test_uplift@10.0% 0 Two-Model Approach (T-Learner) 0.146412 0.114067 1 ClassTransformation 0.149799 0.111760 5 X Learner 0.147077 0.109125 2 Two-Model Approach (ddr_control) 0.121663 0.108137 3 Two-Model Approach (ddr_treatment) 0.101789 0.076922 4 S-Learner 0.075230 0.075230"},{"location":"portfolio/course_recsys/prob_x5.html#x-learner","title":"\u0420\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f X-Learner\u00b6","text":""},{"location":"portfolio/course_recsys/prob_x5.html#1","title":"1 | \u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":"<ul> <li>\u041f\u043e\u0432\u0442\u043e\u0440\u0438\u0442\u0435 \u043c\u0435\u0442\u043e\u0434 <code>X-learner</code> \u043f\u0435\u0440\u0435\u043a\u0440\u0435\u0441\u0442\u043d\u043e\u0439 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0435\u0439.</li> <li>\u041e\u0431\u0440\u0430\u0442\u0438\u0442\u0435 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435, \u0447\u0442\u043e \u0432\u0430\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u043c\u0435\u0442\u043e\u0434 \u0431\u0435\u0437 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u044b\u0445 \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u0432</li> <li>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435\u0441\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\u043c\u0438 <code>fetch_x5</code>, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u044b\u043b\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u044b \u0432 \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0430\u0446\u0438\u0438 \u043d\u0430 \u0432\u0435\u0431\u0438\u043d\u0430\u0440\u0435.</li> <li>\u0420\u0430\u0437\u0431\u0435\u0439\u0442\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 train, valid, test.</li> <li>\u041e\u0431\u0440\u0430\u0442\u0438\u0442\u0435 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435, \u0447\u0442\u043e \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043f\u0435\u0440\u0435\u043a\u0440\u0435\u0441\u0442\u043d\u043e\u0439 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u0432\u0430\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0433\u0440\u0443\u043f\u043f \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e \u0434\u0440\u0443\u0433 \u043e\u0442 \u0434\u0440\u0443\u0433\u0430.<ul> <li>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e \u0431\u0430\u0437\u043e\u0432\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b \u0438 \u0446\u0435\u043b\u0435\u0432\u043e\u0439.</li> <li>\u0421\u043a\u043e\u0440\u0440\u0435\u043a\u0442\u0438\u0440\u0443\u0439\u0442\u0435 \u0442\u0430\u0440\u0433\u0435\u0442.</li> <li>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u0441\u043a\u043e\u0440\u0440\u0435\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u0440\u0433\u0435\u0442\u0430\u0445.</li> <li>\u041e\u0446\u0435\u043d\u0438\u0442\u0435 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 X-learner \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u043f\u0440\u0438 \u0440\u0430\u0437\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430 g. \u0421\u0440\u0430\u0432\u043d\u0438\u0442\u0435 \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0435 uplift@10% \u0441 \u043c\u043e\u0434\u0435\u043b\u044f\u043c\u0438, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u043c\u0438 \u043d\u0430 \u0432\u0435\u0431\u0438\u043d\u0430\u0440\u0435.</li> </ul> </li> <li>\u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e: \u0437\u0430\u043c\u0435\u0440\u044c\u0442\u0435 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e \u0434\u0440\u0443\u0433\u0438\u043c \u043c\u0435\u0442\u0440\u0438\u043a\u0430\u043c, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u044b \u0437\u043d\u0430\u0435\u0442\u0435.</li> </ul>"},{"location":"portfolio/course_recsys/prob_x5.html#2","title":"2 | \u0427\u0438\u0442\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\u043c\u00b6","text":"<p>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 <code>fetch_x5_clients</code></p> <ul> <li><code>treatment_flg</code> : \u0432\u0435\u043a\u0442\u043e\u0440 \u0432\u043e\u0437\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u0441 \u043a\u043b\u0438\u0435\u043d\u0442\u043e\u043c, \u0434\u0430\u043d\u043d\u044b\u0435 \u0441\u043e\u0434\u0435\u0440\u0434\u0430\u0442 \u043a\u043b\u0438\u0435\u043d\u0442\u044b \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c\u0438 \u043c\u044b \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u043b\u0438 (treatment_flg=1) [target] \u0438 \u0442\u0435 \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c\u0438 \u043c\u044b \u043d\u0435 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u043b\u0438 (treatment_flg=0) [control]</li> <li><code>target</code> : \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0432\u043e\u0437\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f, \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 (target = 0), \u043f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u044b \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 (target = 1)</li> </ul>"},{"location":"portfolio/course_recsys/prob_x5.html#3","title":"3 | \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430\u00b6","text":"<ul> <li>\u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043d\u0430\u043c \u043d\u0430\u0434\u043e \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0432 \u0446\u0438\u0444\u0440\u043e\u0432\u044b\u0435</li> <li>\u041e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u0435\u043c datetime \u0444\u0438\u0447\u0438 \u0432 \u0446\u0438\u0444\u0440\u043e\u0432\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f; UNIX \u0432\u0440\u0435\u043c\u044f</li> </ul>"},{"location":"portfolio/course_recsys/prob_x5.html#4-traintest","title":"4 | Train/Test \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438\u00b6","text":"<p>\u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u0434\u0432\u0435 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0432 <code>y</code> \u0438 <code>T</code></p>"},{"location":"portfolio/course_recsys/prob_x5.html#5","title":"5 | \u0426\u0435\u043b\u0435\u0432\u044b\u0435 \u0432\u0435\u043a\u0442\u043e\u0440\u0430\u00b6","text":"<p>\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u0431\u0430\u043b\u0430\u043d\u0441 \u0447\u0435\u043b\u0435\u0432\u043e\u0433\u043e \u0432\u0435\u043a\u0442\u043e\u0440\u0430</p>"},{"location":"portfolio/course_recsys/prob_x5.html#6","title":"6 | \u041c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/course_recsys/prob_x5.html#x-learner-approach","title":"X-learner approach\u00b6","text":"<p>1) \u0421\u0442\u0440\u043e\u0438\u043c \u0434\u0432\u0435 \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u0430\u0445</p> <ul> <li><p>$ \\hat{M}_0(X) \\approx E[Y| T=0, X] $</p> </li> <li><p>$ \\hat{M}_1(X) \\approx E[Y| T=1, X] $</p> </li> </ul> <p>2) \u0420\u0430\u0441\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u0440\u0430\u0437\u043d\u043e\u0441\u0442\u044c (\u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u043e, Uplift) \u043c\u0435\u0436\u0434\u0443 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\u043c \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u043f\u0440\u0438 \u0432\u043e\u0437\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u043d\u0430 \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u0438 \u0431\u0435\u0437 \u0432\u043e\u0437\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f</p> <ul> <li><p>$ \\hat{\\tau}(X, T=0) = \\hat{M}_1(X, T=0) - Y_{T=0} $</p> </li> <li><p>$ \\hat{\\tau}(X, T=1) = Y_{T=1} - \\hat{M}_0(X, T=1) $</p> </li> </ul> <p>3) \u0421\u0442\u0440\u043e\u0438\u043c \u0434\u0432\u0435 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u0444\u0438\u0447\u0430\u0445 \u0441\u044d\u043c\u043f\u043b\u043e\u0432 \u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u0445 Uplift</p> <ul> <li><p>$ \\hat{M}_{\\tau 0}(X) \\approx E[\\hat{\\tau}(X)|T=0] $</p> </li> <li><p>$ \\hat{M}_{\\tau 1}(X) \\approx E[\\hat{\\tau}(X)|T=1] $</p> </li> </ul> <p>4) \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0434\u0432\u0443\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0441\u043a\u043b\u0430\u0434\u044b\u0432\u0430\u0435\u043c \u0441 \u0443\u0447\u0451\u0442\u043e\u043c \u0432\u0435\u0441\u0430 $\\hat{e}(x)$; propensity score model</p> <ul> <li>$ \\hat{\\tau(x)} = \\hat{M}_{\\tau 0}(X)\\hat{e}(x) +  \\hat{M}_{\\tau 1}(X)(1-\\hat{e}(x)) $</li> </ul>"},{"location":"portfolio/course_recsys/prob_x5.html#1","title":"1) \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<p>\u0421\u0442\u0440\u043e\u0438\u043c \u0434\u0432\u0435 \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u0430\u0445</p> <ul> <li>\u041a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u0430\u044f \u0433\u0440\u0443\u043f\u043f\u0430 <code>m0</code> (treatment_flg = 0)</li> <li>\u0426\u0435\u043b\u0435\u0432\u0430\u044f \u0433\u0440\u0443\u043f\u043f\u0430 <code>m1</code> (treatment_flg = 1)</li> </ul> <p>\u0422\u0430\u043a \u0436\u0435 \u043e\u0431\u0443\u0447\u0430\u0435\u043c propensity score \u043c\u043e\u0434\u0435\u043b\u044c, \u043e\u043d\u0430 \u0443 \u043d\u0430\u0441 \u043d\u0435 \u0431\u0443\u0434\u0435\u043c \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u043e\u0439</p>"},{"location":"portfolio/course_recsys/prob_x5.html#2","title":"2) \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u0438\u0440\u043e\u0432\u043a\u0430 \u0442\u0430\u0440\u0433\u0435\u0442\u043e\u0432\u00b6","text":"<p>\u0420\u0430\u0441\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u0440\u0430\u0437\u043d\u043e\u0441\u0442\u044c (\u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u043e, Uplift) \u043c\u0435\u0436\u0434\u0443 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\u043c \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u043f\u0440\u0438 \u0432\u043e\u0437\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 \u043d\u0430 \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u0438 \u0431\u0435\u0437 \u0432\u043e\u0437\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f</p>"},{"location":"portfolio/course_recsys/prob_x5.html#3","title":"3) \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u0441\u043a\u043e\u0440\u0440\u0435\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439\u00b6","text":"<p>Using the adjusted target variables ( for both the target and control groups, we train the main models</p>"},{"location":"portfolio/course_recsys/prob_x5.html#4","title":"4) \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0434\u0432\u0443\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0441\u043a\u043b\u0430\u0434\u044b\u0432\u0430\u0435\u043c \u0441 \u0443\u0447\u0451\u0442\u043e\u043c \u0432\u0435\u0441\u0430\u00b6","text":"<p>\u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0434\u0432\u0443\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0441\u043a\u043b\u0430\u0434\u044b\u0432\u0430\u0435\u043c \u0441 \u0443\u0447\u0451\u0442\u043e\u043c \u0432\u0435\u0441\u0430</p>"},{"location":"portfolio/course_recsys/prob_x5.html#7","title":"7 | \u041e\u0446\u0435\u043d\u043a\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043c\u0435\u0442\u0440\u0438\u043a\u0438\u00b6","text":""},{"location":"portfolio/course_recsys/prob_x5.html#upliftk-metric","title":"Uplift@k metric\u00b6","text":"<p>\u041e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u043c \u043c\u0435\u0442\u0440\u0438\u043a\u0443 <code>uplift_at_k</code> (k=0.1) \u0434\u043b\u044f \u0434\u0432\u0443\u0445 \u0432\u044b\u0431\u043e\u0440\u043e\u043a</p>"},{"location":"portfolio/course_recsys/recsys_lightfm.html","title":"Recsys lightfm","text":"In\u00a0[102]: Copied! <pre>!pip install lightfm -qqq\n</pre> !pip install lightfm -qqq In\u00a0[103]: Copied! <pre>from lightfm import LightFM\nfrom lightfm.data import Dataset as LFMDataset\nimport numpy as np\nimport pandas as pd\nimport scipy.sparse as sparse\nfrom scipy.sparse import csr_matrix, diags\nfrom scipy.sparse.linalg import svds\nfrom tqdm import tqdm\n</pre> from lightfm import LightFM from lightfm.data import Dataset as LFMDataset import numpy as np import pandas as pd import scipy.sparse as sparse from scipy.sparse import csr_matrix, diags from scipy.sparse.linalg import svds from tqdm import tqdm In\u00a0[104]: Copied! <pre>df = pd.read_csv('/kaggle/input/mtc-data/mts_lib.csv')\ndf.head()\n</pre> df = pd.read_csv('/kaggle/input/mtc-data/mts_lib.csv') df.head() Out[104]: user_id item_id progress rating start_date 0 126706 14433 80 NaN 2018-01-01 1 127290 140952 58 NaN 2018-01-01 2 66991 198453 89 NaN 2018-01-01 3 46791 83486 23 5.0 2018-01-01 4 79313 188770 88 5.0 2018-01-01 In\u00a0[105]: Copied! <pre>def get_data_info(data, \n                  user_id='user_id', \n                  item_id='item_id'):\n    print(f'\u0420\u0430\u0437\u043c\u0435\u0440 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 = {data.shape[0]} \\n\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 = {data[user_id].nunique()} \\n\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 = {data[item_id].nunique()}')\n</pre> def get_data_info(data,                    user_id='user_id',                    item_id='item_id'):     print(f'\u0420\u0430\u0437\u043c\u0435\u0440 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 = {data.shape[0]} \\n\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 = {data[user_id].nunique()} \\n\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 = {data[item_id].nunique()}') In\u00a0[106]: Copied! <pre># \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0432 \u0434\u0430\u0442\u0443\ndf.loc[:, 'start_date'] = pd.to_datetime(df['start_date'], \n                                         format=\"%Y-%m-%d\")\n\n# \u0443\u0434\u0430\u043b\u044f\u0435\u043c \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b, \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u043f\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438\ndf = df.sort_values('start_date').drop_duplicates(subset=['user_id', 'item_id'], \n                                                  keep='last')\n\nget_data_info(df)\n</pre> # \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0432 \u0434\u0430\u0442\u0443 df.loc[:, 'start_date'] = pd.to_datetime(df['start_date'],                                           format=\"%Y-%m-%d\")  # \u0443\u0434\u0430\u043b\u044f\u0435\u043c \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b, \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u043f\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 df = df.sort_values('start_date').drop_duplicates(subset=['user_id', 'item_id'],                                                    keep='last')  get_data_info(df) <pre>\u0420\u0430\u0437\u043c\u0435\u0440 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 = 1532998 \n\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 = 151600 \n\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 = 59599\n</pre> In\u00a0[107]: Copied! <pre>df.progress.value_counts()\n</pre> df.progress.value_counts() Out[107]: <pre>progress\n100    228230\n0      200915\n99      56710\n1       48356\n2       33917\n        ...  \n74       8427\n79       8426\n76       8407\n58       8316\n63       8267\nName: count, Length: 101, dtype: int64</pre> In\u00a0[108]: Copied! <pre># remove user processes less than 30\ndf = df[df['progress'] &gt; 30]\n</pre> # remove user processes less than 30 df = df[df['progress'] &gt; 30] In\u00a0[109]: Copied! <pre>def filter_data(df, user_count=5, item_count=5):\n\n    # select users who have selected [item_count] or more\n    user_counts = df.groupby('user_id')['item_id'].count()\n    pop_users = user_counts[user_counts &gt;= item_count]\n    df = df[df['user_id'].isin(pop_users.index)].copy()\n    return df\n\ndf = filter_data(df)\nget_data_info(df)\n</pre> def filter_data(df, user_count=5, item_count=5):      # select users who have selected [item_count] or more     user_counts = df.groupby('user_id')['item_id'].count()     pop_users = user_counts[user_counts &gt;= item_count]     df = df[df['user_id'].isin(pop_users.index)].copy()     return df  df = filter_data(df) get_data_info(df) <pre>\u0420\u0430\u0437\u043c\u0435\u0440 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 = 654819 \n\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 = 64955 \n\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 = 59485\n</pre> In\u00a0[110]: Copied! <pre>u_features = pd.read_csv('/kaggle/input/mtc-data/users.csv')\ni_features = pd.read_csv('/kaggle/input/mtc-data/items.csv')\ni_features.rename(columns={'id': 'item_id'}, inplace=True)\n</pre> u_features = pd.read_csv('/kaggle/input/mtc-data/users.csv') i_features = pd.read_csv('/kaggle/input/mtc-data/items.csv') i_features.rename(columns={'id': 'item_id'}, inplace=True) <p>\u0423\u0434\u043e\u0441\u0442\u043e\u0432\u0435\u0440\u0438\u043c\u0441\u044f \u0447\u0442\u043e \u043d\u043e\u0432\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u0442\u043e\u0432\u0430\u0440\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0435\u0441\u0442\u044c \u0432 <code>df</code></p> In\u00a0[111]: Copied! <pre># make sure the feature are present in rating dataframe df\ni_features = i_features[i_features['item_id'].isin(df['item_id'])].copy()\nu_features = u_features[u_features['user_id'].isin(df['user_id'])].copy()\n</pre> # make sure the feature are present in rating dataframe df i_features = i_features[i_features['item_id'].isin(df['item_id'])].copy() u_features = u_features[u_features['user_id'].isin(df['user_id'])].copy() <p>\u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043c\u0430\u043f\u043f\u0435\u0440\u044b \u0434\u043b\u044f <code>user2id</code> \u0438 <code>item2id</code></p> In\u00a0[112]: Copied! <pre>user_idx = df['user_id'].astype('category').cat.codes\nitem_idx = df['item_id'].astype('category').cat.codes\nuser2id = dict(zip(df['user_id'], user_idx))\nitem2id = dict(zip(df['item_id'], item_idx))\n</pre> user_idx = df['user_id'].astype('category').cat.codes item_idx = df['item_id'].astype('category').cat.codes user2id = dict(zip(df['user_id'], user_idx)) item2id = dict(zip(df['item_id'], item_idx)) In\u00a0[113]: Copied! <pre># user features\nu_features.head()\n</pre> # user features u_features.head() Out[113]: user_id age sex 0 1 45_54 NaN 2 3 65_inf 0.0 10 11 55_64 0.0 11 12 55_64 1.0 12 13 25_34 0.0 In\u00a0[114]: Copied! <pre># item features \ni_features.head()\n</pre> # item features  i_features.head() Out[114]: item_id title genres authors year 0 128115 \u0412\u043e\u0440\u043e\u043d-\u0447\u0435\u043b\u043e\u0431\u0438\u0442\u0447\u0438\u043a \u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u0434\u0435\u0442\u0441\u043a\u0438\u0435 \u043a\u043d\u0438\u0433\u0438,\u0421\u043a\u0430\u0437\u043a\u0438,\u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d 1886 1 210979 \u0421\u043a\u0440\u0438\u043f\u043a\u0430 \u0420\u043e\u0442\u0448\u0438\u043b\u044c\u0434\u0430 \u041a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430,\u041b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430,\u0420\u0443\u0441\u0441\u043a\u0430\u044f ... \u0410\u043d\u0442\u043e\u043d \u0427\u0435\u0445\u043e\u0432 1894 2 95632 \u0418\u0441\u043f\u043e\u0440\u0447\u0435\u043d\u043d\u044b\u0435 \u0434\u0435\u0442\u0438 \u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430,\u041a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430,\u041b\u0438\u0442\u0435\u0440\u0430\u0442... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d 1869 3 247906 \u0421\u0442\u0440\u0430\u043d\u043d\u044b\u0439 \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u041f\u044c\u0435\u0441\u044b \u0438 \u0434\u0440\u0430\u043c\u0430\u0442\u0443\u0440\u0433\u0438\u044f,\u041b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430 \u041c\u0438\u0445\u0430\u0438\u043b \u041b\u0435\u0440\u043c\u043e\u043d\u0442\u043e\u0432 1831 4 294280 \u0413\u043e\u0441\u043f\u043e\u0434\u0430 \u0442\u0430\u0448\u043a\u0435\u043d\u0442\u0446\u044b \u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430,\u041a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430,\u041b\u0438\u0442\u0435\u0440\u0430\u0442... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d 1873 In\u00a0[115]: Copied! <pre>u_features['age'].value_counts()\n</pre> u_features['age'].value_counts() Out[115]: <pre>age\n18_24     21396\n25_34     12207\n35_44      7422\n55_64      7256\n45_54      6186\n65_inf     4016\nName: count, dtype: int64</pre> In\u00a0[116]: Copied! <pre>def train_test_split(X, user_col, time_col):\n    full_history = X.sort_values([user_col, time_col]).groupby(user_col)\n    test = full_history.tail(1)\n    train = full_history.head(-1)\n    return train, test\n\ntrain, test = train_test_split(df, 'user_id', 'start_date')\n</pre> def train_test_split(X, user_col, time_col):     full_history = X.sort_values([user_col, time_col]).groupby(user_col)     test = full_history.tail(1)     train = full_history.head(-1)     return train, test  train, test = train_test_split(df, 'user_id', 'start_date') In\u00a0[117]: Copied! <pre>u_features.head()\n</pre> u_features.head() Out[117]: user_id age sex 0 1 45_54 NaN 2 3 65_inf 0.0 10 11 55_64 0.0 11 12 55_64 1.0 12 13 25_34 0.0 <p>\u0421\u0433\u0443\u043f\u043f\u0438\u0440\u0443\u0435\u043c \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0444\u0438\u0447\u0435\u0439 \u0438 \u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0438 \u043f\u043e\u043c\u0435\u0441\u0442\u0438\u043c \u0438\u0445 \u0432 \u043e\u0434\u0438\u043d \u0432\u0435\u043a\u0442\u043e\u0440 <code>u_features_list</code></p> In\u00a0[118]: Copied! <pre># set index as user_id\nu_features.set_index('user_id', inplace=True)\n\n# create feature name &amp; value \nu_features_list = u_features.apply(\n    lambda feature_values: [f'{feature}_{feature_values[feature]}' \n                            for feature in feature_values.index \n                            if not pd.isna(feature_values[feature])],\n    axis=1\n    )\nu_features_list = u_features_list.rename('features')\nu_features_list\n</pre> # set index as user_id u_features.set_index('user_id', inplace=True)  # create feature name &amp; value  u_features_list = u_features.apply(     lambda feature_values: [f'{feature}_{feature_values[feature]}'                              for feature in feature_values.index                              if not pd.isna(feature_values[feature])],     axis=1     ) u_features_list = u_features_list.rename('features') u_features_list Out[118]: <pre>user_id\n1                   [age_45_54]\n3         [age_65_inf, sex_0.0]\n11         [age_55_64, sex_0.0]\n12         [age_55_64, sex_1.0]\n13         [age_25_34, sex_0.0]\n                  ...          \n159603     [age_18_24, sex_1.0]\n159605     [age_18_24, sex_0.0]\n159606     [age_25_34, sex_0.0]\n159607              [age_25_34]\n159610     [age_35_44, sex_0.0]\nName: features, Length: 58529, dtype: object</pre> <p>\u0412\u0441\u0435 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u0432 \u044d\u0442\u043e\u043c \u0432\u0435\u043a\u0442\u043e\u0440\u0435</p> In\u00a0[119]: Copied! <pre># all unique feature name &amp; value \nuser_tags = set(u_features_list.explode().dropna().values)\nuser_tags\n</pre> # all unique feature name &amp; value  user_tags = set(u_features_list.explode().dropna().values) user_tags Out[119]: <pre>{'age_18_24',\n 'age_25_34',\n 'age_35_44',\n 'age_45_54',\n 'age_55_64',\n 'age_65_inf',\n 'sex_0.0',\n 'sex_1.0'}</pre> In\u00a0[120]: Copied! <pre># book genre\ni_features['genres']\n</pre> # book genre i_features['genres'] Out[120]: <pre>0        \u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u0434\u0435\u0442\u0441\u043a\u0438\u0435 \u043a\u043d\u0438\u0433\u0438,\u0421\u043a\u0430\u0437\u043a\u0438,\u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430...\n1        \u041a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430,\u041b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430,\u0420\u0443\u0441\u0441\u043a\u0430\u044f ...\n2        \u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430,\u041a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430,\u041b\u0438\u0442\u0435\u0440\u0430\u0442...\n3                   \u041f\u044c\u0435\u0441\u044b \u0438 \u0434\u0440\u0430\u043c\u0430\u0442\u0443\u0440\u0433\u0438\u044f,\u041b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430\n4        \u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430,\u041a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430,\u041b\u0438\u0442\u0435\u0440\u0430\u0442...\n                               ...                        \n59594                \u041f\u043e\u043b\u0438\u0442\u043e\u043b\u043e\u0433\u0438\u044f,\u041a\u043d\u0438\u0433\u0438 \u043f\u043e \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0435,\u0413\u0430\u0437\u0435\u0442\u044b\n59595                \u041f\u043e\u043b\u0438\u0442\u043e\u043b\u043e\u0433\u0438\u044f,\u041a\u043d\u0438\u0433\u0438 \u043f\u043e \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0435,\u0413\u0430\u0437\u0435\u0442\u044b\n59596                     \u041f\u043e\u043b\u0438\u0442\u043e\u043b\u043e\u0433\u0438\u044f,\u041e\u0431\u0449\u0430\u044f \u0438\u0441\u0442\u043e\u0440\u0438\u044f,\u0413\u0430\u0437\u0435\u0442\u044b\n59597                                   \u0416\u0443\u0440\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u0438\u0437\u0434\u0430\u043d\u0438\u044f\n59598    \u0416\u0443\u0440\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u0438\u0437\u0434\u0430\u043d\u0438\u044f,\u042d\u043d\u0446\u0438\u043a\u043b\u043e\u043f\u0435\u0434\u0438\u0438,\u041d\u0430\u0443\u0447\u043d\u0430\u044f \u0444\u0430\u043d\u0442\u0430\u0441...\nName: genres, Length: 59485, dtype: object</pre> In\u00a0[121]: Copied! <pre>i_features_lfm = i_features.copy()\ni_features_lfm.set_index('item_id', inplace=True)\ni_features_lfm.head()\n</pre> i_features_lfm = i_features.copy() i_features_lfm.set_index('item_id', inplace=True) i_features_lfm.head() Out[121]: title genres authors year item_id 128115 \u0412\u043e\u0440\u043e\u043d-\u0447\u0435\u043b\u043e\u0431\u0438\u0442\u0447\u0438\u043a \u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u0434\u0435\u0442\u0441\u043a\u0438\u0435 \u043a\u043d\u0438\u0433\u0438,\u0421\u043a\u0430\u0437\u043a\u0438,\u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d 1886 210979 \u0421\u043a\u0440\u0438\u043f\u043a\u0430 \u0420\u043e\u0442\u0448\u0438\u043b\u044c\u0434\u0430 \u041a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430,\u041b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430,\u0420\u0443\u0441\u0441\u043a\u0430\u044f ... \u0410\u043d\u0442\u043e\u043d \u0427\u0435\u0445\u043e\u0432 1894 95632 \u0418\u0441\u043f\u043e\u0440\u0447\u0435\u043d\u043d\u044b\u0435 \u0434\u0435\u0442\u0438 \u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430,\u041a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430,\u041b\u0438\u0442\u0435\u0440\u0430\u0442... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d 1869 247906 \u0421\u0442\u0440\u0430\u043d\u043d\u044b\u0439 \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u041f\u044c\u0435\u0441\u044b \u0438 \u0434\u0440\u0430\u043c\u0430\u0442\u0443\u0440\u0433\u0438\u044f,\u041b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430 \u041c\u0438\u0445\u0430\u0438\u043b \u041b\u0435\u0440\u043c\u043e\u043d\u0442\u043e\u0432 1831 294280 \u0413\u043e\u0441\u043f\u043e\u0434\u0430 \u0442\u0430\u0448\u043a\u0435\u043d\u0442\u0446\u044b \u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430,\u041a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430,\u041b\u0438\u0442\u0435\u0440\u0430\u0442... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d 1873 In\u00a0[122]: Copied! <pre># \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0432\u0437 \u043a\u043d\u0438\u0433\u0430 \u0431\u044b\u043b\u0430 \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u043d\u0430\ni_features_lfm['reads'] = df.groupby('item_id')['user_id'].count()\ni_features_lfm.head()\n</pre> # \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0432\u0437 \u043a\u043d\u0438\u0433\u0430 \u0431\u044b\u043b\u0430 \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u043d\u0430 i_features_lfm['reads'] = df.groupby('item_id')['user_id'].count() i_features_lfm.head() Out[122]: title genres authors year reads item_id 128115 \u0412\u043e\u0440\u043e\u043d-\u0447\u0435\u043b\u043e\u0431\u0438\u0442\u0447\u0438\u043a \u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u0434\u0435\u0442\u0441\u043a\u0438\u0435 \u043a\u043d\u0438\u0433\u0438,\u0421\u043a\u0430\u0437\u043a\u0438,\u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d 1886 11 210979 \u0421\u043a\u0440\u0438\u043f\u043a\u0430 \u0420\u043e\u0442\u0448\u0438\u043b\u044c\u0434\u0430 \u041a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430,\u041b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430,\u0420\u0443\u0441\u0441\u043a\u0430\u044f ... \u0410\u043d\u0442\u043e\u043d \u0427\u0435\u0445\u043e\u0432 1894 87 95632 \u0418\u0441\u043f\u043e\u0440\u0447\u0435\u043d\u043d\u044b\u0435 \u0434\u0435\u0442\u0438 \u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430,\u041a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430,\u041b\u0438\u0442\u0435\u0440\u0430\u0442... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d 1869 5 247906 \u0421\u0442\u0440\u0430\u043d\u043d\u044b\u0439 \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u041f\u044c\u0435\u0441\u044b \u0438 \u0434\u0440\u0430\u043c\u0430\u0442\u0443\u0440\u0433\u0438\u044f,\u041b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430 \u041c\u0438\u0445\u0430\u0438\u043b \u041b\u0435\u0440\u043c\u043e\u043d\u0442\u043e\u0432 1831 6 294280 \u0413\u043e\u0441\u043f\u043e\u0434\u0430 \u0442\u0430\u0448\u043a\u0435\u043d\u0442\u0446\u044b \u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430,\u041a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430,\u041b\u0438\u0442\u0435\u0440\u0430\u0442... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d 1873 7 In\u00a0[123]: Copied! <pre># genre features (list)\ni_features_lfm['genres'] = i_features_lfm['genres'].str.lower().str.split(',')\ni_features_lfm['genres'] = i_features_lfm['genres'].apply(lambda x: x if isinstance(x, list) else [])\ni_features_lfm[['genres','reads']].head()\n</pre> # genre features (list) i_features_lfm['genres'] = i_features_lfm['genres'].str.lower().str.split(',') i_features_lfm['genres'] = i_features_lfm['genres'].apply(lambda x: x if isinstance(x, list) else []) i_features_lfm[['genres','reads']].head() Out[123]: genres reads item_id 128115 [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u0434\u0435\u0442\u0441\u043a\u0438\u0435 \u043a\u043d\u0438\u0433\u0438, \u0441\u043a\u0430\u0437\u043a\u0438, \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f ... 11 210979 [\u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430, \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430, \u0440\u0443\u0441\u0441\u043a... 87 95632 [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430, \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430, \u043b\u0438\u0442\u0435... 5 247906 [\u043f\u044c\u0435\u0441\u044b \u0438 \u0434\u0440\u0430\u043c\u0430\u0442\u0443\u0440\u0433\u0438\u044f, \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430] 6 294280 [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430, \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430, \u043b\u0438\u0442\u0435... 7 In\u00a0[124]: Copied! <pre>i_features_lfm[['genres','reads']].explode('genres')\n</pre> i_features_lfm[['genres','reads']].explode('genres') Out[124]: genres reads item_id 128115 \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u0434\u0435\u0442\u0441\u043a\u0438\u0435 \u043a\u043d\u0438\u0433\u0438 11 128115 \u0441\u043a\u0430\u0437\u043a\u0438 11 128115 \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430 11 128115 \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430 11 128115 \u0440\u0443\u0441\u0441\u043a\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430 11 ... ... ... 125582 \u0433\u0430\u0437\u0435\u0442\u044b 10 33188 \u0436\u0443\u0440\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u0438\u0437\u0434\u0430\u043d\u0438\u044f 5 65317 \u0436\u0443\u0440\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u0438\u0437\u0434\u0430\u043d\u0438\u044f 4 65317 \u044d\u043d\u0446\u0438\u043a\u043b\u043e\u043f\u0435\u0434\u0438\u0438 4 65317 \u043d\u0430\u0443\u0447\u043d\u0430\u044f \u0444\u0430\u043d\u0442\u0430\u0441\u0442\u0438\u043a\u0430 4 <p>126510 rows \u00d7 2 columns</p> In\u00a0[125]: Copied! <pre># genre based read count\ngenres_count = i_features_lfm[['genres','reads']].explode('genres').groupby('genres')['reads'].sum()\ngenres_count.sort_values(ascending=False)\n</pre> # genre based read count genres_count = i_features_lfm[['genres','reads']].explode('genres').groupby('genres')['reads'].sum() genres_count.sort_values(ascending=False) Out[125]: <pre>genres\n\u043b\u044e\u0431\u043e\u0432\u043d\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438               72008\n\u043f\u043e\u043f\u0430\u0434\u0430\u043d\u0446\u044b                      52832\n\u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043b\u044e\u0431\u043e\u0432\u043d\u044b\u0435 \u0440\u043e\u043c\u0430\u043d\u044b    49248\n\u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0432\u044b          46660\n\u0433\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438            40794\n                               ...  \n\u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 7 \u043a\u043b\u0430\u0441\u0441                 3\n\u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0430           2\n\u0432\u043e\u0437\u0434\u0443\u0448\u043d\u044b\u0439 \u0442\u0440\u0430\u043d\u0441\u043f\u043e\u0440\u0442                2\n\u043d\u0430\u0443\u0447\u043d\u043e-\u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0436\u0443\u0440\u043d\u0430\u043b\u044b        2\n\u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u043a\u0430 3 \u043a\u043b\u0430\u0441\u0441                 1\nName: reads, Length: 640, dtype: int64</pre> In\u00a0[126]: Copied! <pre># top 50 genres by read ammount; get their index in dataframe \nitem_tags = genres_count.sort_values(ascending=False)[:50].index\nlist(item_tags[:10])\n</pre> # top 50 genres by read ammount; get their index in dataframe  item_tags = genres_count.sort_values(ascending=False)[:50].index list(item_tags[:10]) Out[126]: <pre>['\u043b\u044e\u0431\u043e\u0432\u043d\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438',\n '\u043f\u043e\u043f\u0430\u0434\u0430\u043d\u0446\u044b',\n '\u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043b\u044e\u0431\u043e\u0432\u043d\u044b\u0435 \u0440\u043e\u043c\u0430\u043d\u044b',\n '\u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0432\u044b',\n '\u0433\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438',\n '\u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0440\u0443\u0441\u0441\u043a\u0430\u044f \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430',\n '\u0431\u043e\u0435\u0432\u0430\u044f \u0444\u0430\u043d\u0442\u0430\u0441\u0442\u0438\u043a\u0430',\n '\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u043b\u044e\u0431\u043e\u0432\u043d\u044b\u0435 \u0440\u043e\u043c\u0430\u043d\u044b',\n '\u0431\u043e\u0435\u0432\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438',\n '\u044d\u0440\u043e\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0440\u043e\u043c\u0430\u043d\u044b']</pre> In\u00a0[127]: Copied! <pre># filter item dataframe; select only top 50 genres \ndef filter_genres(genres_list, valid_genres=None):\n    if not genres_list:\n        return []\n    return [genre for genre in genres_list if genre in valid_genres]\n\n# filter genres\ni_features_lfm['features'] = i_features_lfm['genres'].apply(filter_genres, \n                                                            valid_genres=set(item_tags))\ni_features_list = i_features_lfm['features']\ni_features_lfm.head()\n</pre> # filter item dataframe; select only top 50 genres  def filter_genres(genres_list, valid_genres=None):     if not genres_list:         return []     return [genre for genre in genres_list if genre in valid_genres]  # filter genres i_features_lfm['features'] = i_features_lfm['genres'].apply(filter_genres,                                                              valid_genres=set(item_tags)) i_features_list = i_features_lfm['features'] i_features_lfm.head() Out[127]: title genres authors year reads features item_id 128115 \u0412\u043e\u0440\u043e\u043d-\u0447\u0435\u043b\u043e\u0431\u0438\u0442\u0447\u0438\u043a [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u0434\u0435\u0442\u0441\u043a\u0438\u0435 \u043a\u043d\u0438\u0433\u0438, \u0441\u043a\u0430\u0437\u043a\u0438, \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f ... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d 1886 11 [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430, \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430, \u0440\u0443\u0441\u0441... 210979 \u0421\u043a\u0440\u0438\u043f\u043a\u0430 \u0420\u043e\u0442\u0448\u0438\u043b\u044c\u0434\u0430 [\u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430, \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430, \u0440\u0443\u0441\u0441\u043a... \u0410\u043d\u0442\u043e\u043d \u0427\u0435\u0445\u043e\u0432 1894 87 [\u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430, \u0440\u0443\u0441\u0441\u043a\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430] 95632 \u0418\u0441\u043f\u043e\u0440\u0447\u0435\u043d\u043d\u044b\u0435 \u0434\u0435\u0442\u0438 [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430, \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430, \u043b\u0438\u0442\u0435... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d 1869 5 [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430, \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430, \u0440\u0443\u0441\u0441... 247906 \u0421\u0442\u0440\u0430\u043d\u043d\u044b\u0439 \u0447\u0435\u043b\u043e\u0432\u0435\u043a [\u043f\u044c\u0435\u0441\u044b \u0438 \u0434\u0440\u0430\u043c\u0430\u0442\u0443\u0440\u0433\u0438\u044f, \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430] \u041c\u0438\u0445\u0430\u0438\u043b \u041b\u0435\u0440\u043c\u043e\u043d\u0442\u043e\u0432 1831 6 [\u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430] 294280 \u0413\u043e\u0441\u043f\u043e\u0434\u0430 \u0442\u0430\u0448\u043a\u0435\u043d\u0442\u0446\u044b [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430, \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430, \u043b\u0438\u0442\u0435... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d 1873 7 [\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430, \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430, \u0440\u0443\u0441\u0441... In\u00a0[128]: Copied! <pre>\"\"\"\n\n(A) \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0432\u0437\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439 \n\n- LightFM \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0441 sparse \u0444\u043e\u0440\u043c\u0430\u0442\u0435\n- \u0414\u0430\u0442\u0430\u0441\u0435 \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u0442 \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 sparse \u0444\u043e\u0440\u043c\u0430\u0442 \n- \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439 - \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c (\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439,\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432)\n\n\"\"\"\n\nlfm_dataset = LFMDataset()\n</pre> \"\"\"  (A) \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0432\u0437\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439   - LightFM \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0441 sparse \u0444\u043e\u0440\u043c\u0430\u0442\u0435 - \u0414\u0430\u0442\u0430\u0441\u0435 \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u0442 \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 sparse \u0444\u043e\u0440\u043c\u0430\u0442  - \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439 - \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c (\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439,\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432)  \"\"\"  lfm_dataset = LFMDataset() In\u00a0[129]: Copied! <pre># \u0421\u043f\u0438\u0441\u043e\u043a \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043e\u0432\nprint('user_id',df['user_id'].unique()[:5]) # (1)\nprint('item_id',df['item_id'].unique()[:5]) # (2)\n</pre> # \u0421\u043f\u0438\u0441\u043e\u043a \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043e\u0432 print('user_id',df['user_id'].unique()[:5]) # (1) print('item_id',df['item_id'].unique()[:5]) # (2) <pre>user_id [47427 99355 55263 58868 40184]\nitem_id [ 46915 249281  80651 164458 128111]\n</pre> In\u00a0[130]: Copied! <pre># (4) \u0441\u043f\u0438\u0441\u043e\u043a \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0444\u0438\u0447\u0438 \u043f\u0440\u0435\u04340-\u0434\u0431\u044c \u0437\u0449\u0436\u044e\u043c\u0435\u0442\u043e\u0432\nlist(item_tags[:10])\n</pre> # (4) \u0441\u043f\u0438\u0441\u043e\u043a \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0444\u0438\u0447\u0438 \u043f\u0440\u0435\u04340-\u0434\u0431\u044c \u0437\u0449\u0436\u044e\u043c\u0435\u0442\u043e\u0432 list(item_tags[:10]) Out[130]: <pre>['\u043b\u044e\u0431\u043e\u0432\u043d\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438',\n '\u043f\u043e\u043f\u0430\u0434\u0430\u043d\u0446\u044b',\n '\u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043b\u044e\u0431\u043e\u0432\u043d\u044b\u0435 \u0440\u043e\u043c\u0430\u043d\u044b',\n '\u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0432\u044b',\n '\u0433\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438',\n '\u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0440\u0443\u0441\u0441\u043a\u0430\u044f \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430',\n '\u0431\u043e\u0435\u0432\u0430\u044f \u0444\u0430\u043d\u0442\u0430\u0441\u0442\u0438\u043a\u0430',\n '\u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u043b\u044e\u0431\u043e\u0432\u043d\u044b\u0435 \u0440\u043e\u043c\u0430\u043d\u044b',\n '\u0431\u043e\u0435\u0432\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438',\n '\u044d\u0440\u043e\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0440\u043e\u043c\u0430\u043d\u044b']</pre> In\u00a0[131]: Copied! <pre># set unique users &amp; items \nlfm_dataset.fit_partial(users=df['user_id'].unique(),  # (1)\n                        items=df['item_id'].unique())  # (2)\n\n# user features &amp; item features\nlfm_dataset.fit_partial(user_features=user_tags, # (3)\n                        item_features=item_tags) # (4)\n\n# \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439\nlfm_dataset.interactions_shape()\n</pre> # set unique users &amp; items  lfm_dataset.fit_partial(users=df['user_id'].unique(),  # (1)                         items=df['item_id'].unique())  # (2)  # user features &amp; item features lfm_dataset.fit_partial(user_features=user_tags, # (3)                         item_features=item_tags) # (4)  # \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439 lfm_dataset.interactions_shape() Out[131]: <pre>(64955, 59485)</pre> In\u00a0[132]: Copied! <pre>\"\"\"\n\nMappers\n\n\"\"\"\n\nuser_mapping = lfm_dataset.mapping()[0]\nitem_mapping = lfm_dataset.mapping()[2]\n\nprint('user mapping')\nfor ii,(i,j) in enumerate(user_mapping.items()):\n    if(ii&lt;5):\n        print(i,j)\n        \nprint('\\nitem mapping')\nfor ii,(i,j) in enumerate(item_mapping.items()):\n    if(ii&lt;5):\n        print(i,j)\n        \ninv_user_mapping = {value: key for key, value in user_mapping.items()}\ninv_item_mapping = {value: key for key, value in item_mapping.items()}\n</pre> \"\"\"  Mappers  \"\"\"  user_mapping = lfm_dataset.mapping()[0] item_mapping = lfm_dataset.mapping()[2]  print('user mapping') for ii,(i,j) in enumerate(user_mapping.items()):     if(ii&lt;5):         print(i,j)          print('\\nitem mapping') for ii,(i,j) in enumerate(item_mapping.items()):     if(ii&lt;5):         print(i,j)          inv_user_mapping = {value: key for key, value in user_mapping.items()} inv_item_mapping = {value: key for key, value in item_mapping.items()} <pre>user mapping\n47427 0\n99355 1\n55263 2\n58868 3\n40184 4\n\nitem mapping\n46915 0\n249281 1\n80651 2\n164458 3\n128111 4\n</pre> In\u00a0[133]: Copied! <pre>'''\n\n(B) \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432\n\n'''\n\nsparse_i_features = lfm_dataset.build_item_features([[row.item_id, row.features] for row in i_features_list.reset_index().itertuples()])\nsparse_u_features = lfm_dataset.build_user_features([[row.user_id, row.features] for row in u_features_list.reset_index().itertuples()])\nsparse_i_features\n</pre> '''  (B) \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432  '''  sparse_i_features = lfm_dataset.build_item_features([[row.item_id, row.features] for row in i_features_list.reset_index().itertuples()]) sparse_u_features = lfm_dataset.build_user_features([[row.user_id, row.features] for row in u_features_list.reset_index().itertuples()]) sparse_i_features Out[133]: <pre>&lt;Compressed Sparse Row sparse matrix of dtype 'float32'\n\twith 131534 stored elements and shape (59485, 59535)&gt;</pre> In\u00a0[134]: Copied! <pre># check what the data contains\nsparse_i_features[51, :].nonzero(), sparse_i_features[51, :].data\n</pre> # check what the data contains sparse_i_features[51, :].nonzero(), sparse_i_features[51, :].data Out[134]: <pre>((array([0], dtype=int32), array([51], dtype=int32)),\n array([1.], dtype=float32))</pre> In\u00a0[135]: Copied! <pre>\"\"\"\n\n(\u0421) \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0438\u043d\u0442\u0435\u0440\u0430\u043a\u0446\u0438\u0438 \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\n\n- [user_id] [item_id] [progress] \u0438\u0437 \u0433\u043b\u0430\u0432\u043d\u043e\u0439 \u043c\u0430\u0442\u0440\u0438\u0446\u044b\n\n\"\"\"\n\n# \u0422\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \n(interactions, weights) = lfm_dataset.build_interactions([(row.user_id, row.item_id, row.progress) for row in train.itertuples()])\n</pre> \"\"\"  (\u0421) \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0438\u043d\u0442\u0435\u0440\u0430\u043a\u0446\u0438\u0438 \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442  - [user_id] [item_id] [progress] \u0438\u0437 \u0433\u043b\u0430\u0432\u043d\u043e\u0439 \u043c\u0430\u0442\u0440\u0438\u0446\u044b  \"\"\"  # \u0422\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c  (interactions, weights) = lfm_dataset.build_interactions([(row.user_id, row.item_id, row.progress) for row in train.itertuples()]) <p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u0435 \u043c\u0430\u0442\u0440\u0438\u0446 interactions \u0438 weights: \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0443 interactions \u043c\u043e\u0436\u043d\u043e \u043b\u0435\u0433\u043a\u043e \u043f\u0435\u0440\u0435\u0439\u0442\u0438 \u043e\u0442 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432 \u043a \u0431\u0438\u043d\u0430\u0440\u043d\u043e\u043c\u0443 \u0444\u0438\u0434\u0431\u0435\u043a\u0443. \u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c weights.</p> In\u00a0[136]: Copied! <pre>print(interactions.shape)\nprint(interactions.data) # interaction \nprint(weights.data) # progress\n</pre> print(interactions.shape) print(interactions.data) # interaction  print(weights.data) # progress <pre>(64955, 59485)\n[1 1 1 ... 1 1 1]\n[65. 78. 77. ... 85. 60. 33.]\n</pre> In\u00a0[137]: Copied! <pre>%%time\nlightfm = LightFM(no_components=50, \n                  loss='warp')\n\nlightfm.fit(interactions, \n            user_features=sparse_u_features, \n            item_features=sparse_i_features, \n            epochs=40, \n            num_threads=8)\n</pre> %%time lightfm = LightFM(no_components=50,                    loss='warp')  lightfm.fit(interactions,              user_features=sparse_u_features,              item_features=sparse_i_features,              epochs=40,              num_threads=8) <pre>CPU times: user 4min 1s, sys: 146 ms, total: 4min 1s\nWall time: 1min 4s\n</pre> Out[137]: <pre>&lt;lightfm.lightfm.LightFM at 0x7ba3aa7fc3d0&gt;</pre> In\u00a0[138]: Copied! <pre># merge on item_id, \ntrain[train.user_id == 2535].merge(i_features)['genres'].value_counts()\n</pre> # merge on item_id,  train[train.user_id == 2535].merge(i_features)['genres'].value_counts() Out[138]: <pre>genres\n\u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0440\u0443\u0441\u0441\u043a\u0430\u044f \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430                           19\n\u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430                         6\n\u041b\u0435\u0433\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430                                              2\n\u0421\u0430\u043c\u043e\u0440\u0430\u0437\u0432\u0438\u0442\u0438\u0435 / \u043b\u0438\u0447\u043d\u043e\u0441\u0442\u043d\u044b\u0439 \u0440\u043e\u0441\u0442,\u041e \u043f\u0441\u0438\u0445\u043e\u043b\u043e\u0433\u0438\u0438 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u043e     2\n\u041c\u0438\u0441\u0442\u0438\u043a\u0430,\u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430                 2\n\u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0432\u044b                                     1\n\u0418\u0441\u0442\u043e\u0440\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430,\u041a\u0443\u043b\u044c\u0442\u0443\u0440\u043e\u043b\u043e\u0433\u0438\u044f                     1\n\u041a\u043d\u0438\u0433\u0438 \u0434\u043b\u044f \u0434\u0435\u0442\u0435\u0439                                           1\n\u042e\u043c\u043e\u0440 \u0438 \u0441\u0430\u0442\u0438\u0440\u0430,\u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0440\u0443\u0441\u0441\u043a\u0430\u044f \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430              1\n\u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043f\u0443\u0431\u043b\u0438\u0446\u0438\u0441\u0442\u0438\u043a\u0430,\u0411\u0438\u043e\u0433\u0440\u0430\u0444\u0438\u0438 \u0438 \u043c\u0435\u043c\u0443\u0430\u0440\u044b               1\n\u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0440\u0443\u0441\u0441\u043a\u0430\u044f \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430,\u042e\u043c\u043e\u0440\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430       1\n\u041a\u043d\u0438\u0433\u0438 \u0434\u043b\u044f \u043f\u043e\u0434\u0440\u043e\u0441\u0442\u043a\u043e\u0432,\u0414\u0435\u0442\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430                        1\n\u041e\u0431\u0449\u0430\u044f \u043f\u0441\u0438\u0445\u043e\u043b\u043e\u0433\u0438\u044f,\u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0440\u0443\u0441\u0441\u043a\u0430\u044f \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430           1\n\u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430,\u041b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 20 \u0432\u0435\u043a\u0430                    1\nName: count, dtype: int64</pre> In\u00a0[139]: Copied! <pre>pred = lightfm.predict(user_ids=user_mapping[2535], \n                       item_ids=sorted(item_mapping.values()), \n                       user_features=sparse_u_features, \n                       item_features=sparse_i_features)\n</pre> pred = lightfm.predict(user_ids=user_mapping[2535],                         item_ids=sorted(item_mapping.values()),                         user_features=sparse_u_features,                         item_features=sparse_i_features) In\u00a0[140]: Copied! <pre># \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u043a\u043d\u0438\u0433\u0438\npred\n</pre> # \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u043a\u043d\u0438\u0433\u0438 pred Out[140]: <pre>array([-152.5988 , -152.75304, -154.45453, ..., -153.4577 , -154.3803 ,\n       -153.56342], dtype=float32)</pre> In\u00a0[141]: Copied! <pre>k = 10\nids = np.argpartition(pred,-k)[-k:]\nrel = pred[ids]\nres = pd.DataFrame(zip(ids, rel), \n                   columns=['item_id', 'relevance'])\nres['item_id'] = res['item_id'].map(inv_item_mapping)\n</pre> k = 10 ids = np.argpartition(pred,-k)[-k:] rel = pred[ids] res = pd.DataFrame(zip(ids, rel),                     columns=['item_id', 'relevance']) res['item_id'] = res['item_id'].map(inv_item_mapping) In\u00a0[142]: Copied! <pre>res.merge(i_features).head()\n</pre> res.merge(i_features).head() Out[142]: item_id relevance title genres authors year 0 99616 -147.307663 \u041a\u043e\u0433\u0434\u0430 \u0434\u044b\u0445\u0430\u043d\u0438\u0435 \u0440\u0430\u0441\u0442\u0432\u043e\u0440\u044f\u0435\u0442\u0441\u044f \u0432 \u0432\u043e\u0437\u0434\u0443\u0445\u0435. \u0418\u043d\u043e\u0433\u0434\u0430 \u0441... \u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043f\u0443\u0431\u043b\u0438\u0446\u0438\u0441\u0442\u0438\u043a\u0430,\u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f... \u041f\u043e\u043b \u041a\u0430\u043b\u0430\u043d\u0438\u0442\u0438 2016 1 237169 -147.230911 \u041f\u043e\u043d\u0430\u0435\u0445\u0430\u0432\u0448\u0430\u044f \u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0440\u0443\u0441\u0441\u043a\u0430\u044f \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 \u041d\u0430\u0440\u0438\u043d\u044d \u0410\u0431\u0433\u0430\u0440\u044f\u043d 2011 2 90225 -145.876343 \u0417\u0443\u043b\u0435\u0439\u0445\u0430 \u043e\u0442\u043a\u0440\u044b\u0432\u0430\u0435\u0442 \u0433\u043b\u0430\u0437\u0430 \u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0440\u0443\u0441\u0441\u043a\u0430\u044f \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 \u0413\u0443\u0437\u0435\u043b\u044c \u042f\u0445\u0438\u043d\u0430 2015 3 107876 -146.454849 \u0416\u0435\u043d\u0449\u0438\u043d\u044b \u043d\u0435\u043f\u0440\u0435\u043a\u043b\u043e\u043d\u043d\u043e\u0433\u043e \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0430 \u0438\u00a0\u0434\u0440.\u00a0\u0431\u0435\u0441\u043f\u0440\u0438\u043d\u0446\u042b... \u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0440\u0443\u0441\u0441\u043a\u0430\u044f \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430,\u042e\u043c\u043e\u0440\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f ... \u0410\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440 \u0426\u044b\u043f\u043a\u0438\u043d 2018 4 184129 -147.189774 \u041d\u0435 \u043e\u0433\u043b\u044f\u0434\u044b\u0432\u0430\u044e\u0449\u0438\u0439\u0441\u044f \u043d\u0438\u043a\u043e\u0433\u0434\u0430 \u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0440\u0443\u0441\u0441\u043a\u0430\u044f \u043b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 \u0422\u0430\u0442\u044c\u044f\u043d\u0430 \u0423\u0441\u0442\u0438\u043d\u043e\u0432\u0430 2011 In\u00a0[143]: Copied! <pre>item_id_lfm = item_mapping[55913]\nuser_id_lfm = user_mapping[2535]\n</pre> item_id_lfm = item_mapping[55913] user_id_lfm = user_mapping[2535] In\u00a0[144]: Copied! <pre># user\nu_biases, u_vectors = lightfm.get_user_representations()\nu_vectors.shape, u_biases.shape\n</pre> # user u_biases, u_vectors = lightfm.get_user_representations() u_vectors.shape, u_biases.shape Out[144]: <pre>((64963, 50), (64963,))</pre> In\u00a0[145]: Copied! <pre>i_biases, i_vectors = lightfm.get_item_representations()\ni_vectors.shape, i_biases.shape\n</pre> i_biases, i_vectors = lightfm.get_item_representations() i_vectors.shape, i_biases.shape Out[145]: <pre>((59535, 50), (59535,))</pre> In\u00a0[146]: Copied! <pre>user_vector = sparse_u_features[user_id_lfm] @ u_vectors\nitem_vector = sparse_i_features[item_id_lfm] @ i_vectors\nrel_ours = (user_vector @ item_vector.T + sparse_u_features[user_id_lfm] @ u_biases + sparse_i_features[item_id_lfm] @ i_biases).ravel()[0]\n</pre> user_vector = sparse_u_features[user_id_lfm] @ u_vectors item_vector = sparse_i_features[item_id_lfm] @ i_vectors rel_ours = (user_vector @ item_vector.T + sparse_u_features[user_id_lfm] @ u_biases + sparse_i_features[item_id_lfm] @ i_biases).ravel()[0] In\u00a0[147]: Copied! <pre>rel_ours\n</pre> rel_ours Out[147]: <pre>-147.31015</pre>"},{"location":"portfolio/course_recsys/recsys_lightfm.html","title":"\u041a\u043e\u043d\u0442\u0435\u043d\u0442\u043d\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439\u00b6","text":""},{"location":"portfolio/course_recsys/recsys_lightfm.html#1","title":"1 | \u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/course_recsys/recsys_lightfm.html#lightfm","title":"LightFM\u00b6","text":"<ul> <li><code>LightFM</code> \u044d\u0442\u043e \u0433\u0438\u0431\u0440\u0438\u0434\u043d\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0441\u043e\u0447\u0435\u0442\u0430\u0435\u0442 \u0432 \u0441\u0435\u0431\u0435 \u043c\u0435\u0442\u043e\u0434\u044b <code>\u043a\u043e\u043b\u043b\u0430\u0431\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u043e\u0439 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438</code> \u0438 \u043a\u043e\u043d\u0442\u0435\u043d\u0442\u043d\u043e\u0439 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438.</li> <li>\u042d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u043c\u043e\u0436\u043d\u043e \u043e\u0431\u0443\u0447\u0430\u0442\u044c \u043a\u0430\u043a \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0438\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u0442\u0430\u043a \u0438 \u0431\u0435\u0437.</li> <li>\u0415\u0441\u043b\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f, \u0442\u043e \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u0441\u043e\u0431\u043e\u0439 \u0441\u0443\u043c\u043c\u0443 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432 \u0438\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (\u0432\u043a\u043b\u044e\u0447\u0430\u044f id \u043a\u0430\u043a \u043f\u0440\u0438\u0437\u043d\u0430\u043a).</li> </ul>"},{"location":"portfolio/course_recsys/recsys_lightfm.html#2","title":"2 | \u0414\u0430\u043d\u043d\u044b\u0435\u00b6","text":""},{"location":"portfolio/course_recsys/recsys_lightfm.html#mtc","title":"MTC \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0430 \u043a\u043d\u0438\u0433\u00b6","text":"<ul> <li>\u041f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>mts</code> \u0434\u0430\u043d\u043d\u044b\u0435 \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u043d\u043d\u044b\u0445 \u043a\u043d\u0438\u0433.</li> <li>\u041a\u0430\u043a \u0438 \u043f\u0440\u043e\u0448\u043b\u044b\u0439 \u0440\u0430\u0437 \u043c\u044b \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c <code>mts_lib.csv</code> \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0443 \u043d\u0430\u0441 \u0445\u0440\u043e\u043d\u0438\u0442\u0441\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u043e \u0447\u0442\u0435\u043d\u0438\u0435 \u043a\u043d\u0438\u0433.</li> <li>\u0423 \u043d\u0430\u0441 \u0434\u0432\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043e\u0446\u0435\u043d\u043a\u0438 \u0442\u043e\u0432\u0430\u0440\u0430 <code>implicit</code>  \u0438 <code>explicit</code>, \u0441\u0435\u0433\u043e\u0434\u043d\u044f \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>progress</code></li> </ul>"},{"location":"portfolio/course_recsys/recsys_lightfm.html#3","title":"3 | \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430\u00b6","text":""},{"location":"portfolio/course_recsys/recsys_lightfm.html","title":"\u0424\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u044f\u00b6","text":"<ul> <li>\u0412\u043e\u0437\u043c\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0434\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0441 \u043f\u0440\u043e\u0433\u0440\u0435\u0441\u0441\u043e\u043c \u0447\u0442\u0435\u043d\u0438\u044f <code>\u043a\u043d\u0438\u0433 &gt; 30</code></li> <li>\u0411\u043e\u043b\u044c\u0448\u0430\u044f \u0447\u0430\u0441\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u043c\u0435\u0435\u0442 \u043f\u0440\u043e\u0433\u0440\u0435\u0441\u0441 0, \u0438 \u043d\u0435 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u044b\u043c</li> </ul>"},{"location":"portfolio/course_recsys/recsys_lightfm.html","title":"\u0427\u0442\u0435\u043d\u0438\u0435 \u0424\u0438\u0447 \u041f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439, \u0422\u043e\u0432\u0430\u0440\u0430\u00b6","text":"<p>\u041a \u0434\u0430\u043d\u043d\u044b\u043c <code>df</code> \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u043f\u043e\u0434\u0442\u044f\u043d\u0443\u0442\u044c \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u0443\u044e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435 (<code>user_id</code>) \u0438 \u0442\u043e\u0432\u0430\u0440\u0430 (<code>item_id</code>)</p>"},{"location":"portfolio/course_recsys/recsys_lightfm.html","title":"\u0424\u0438\u0447\u0438 \u041f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439\u00b6","text":"<p>\u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f <code>user_id</code></p> <ul> <li>\u0412\u043e\u0437\u0440\u043e\u0441\u0442\u043d\u043e\u0439 \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d <code>age</code></li> <li>\u041f\u043e\u043b <code>sex</code></li> </ul>"},{"location":"portfolio/course_recsys/recsys_lightfm.html","title":"\u0424\u0438\u0447\u0438 \u0442\u043e\u0432\u0430\u0440\u043e\u0432\u00b6","text":"<p>\u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0442\u043e\u0432\u0430\u0440\u0430 <code>item_id</code></p> <ul> <li>\u0422\u0438\u0442\u0443\u043b <code>title</code></li> <li>\u0416\u0430\u043d\u0440 <code>genres</code></li> <li>\u041f\u0438\u0441\u0430\u0442\u0435\u043b\u044c <code>authors</code></li> <li>\u0413\u043e\u0434 \u0432\u044b\u043f\u0443\u0441\u043a\u0430 <code>year</code></li> </ul>"},{"location":"portfolio/course_recsys/recsys_lightfm.html","title":"\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438\u00b6","text":"<p>\u0421\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u0430\u044f \u043f\u0440\u0430\u043a\u0442\u0438\u043a\u0430 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0448\u0438\u044e \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443</p>"},{"location":"portfolio/course_recsys/recsys_lightfm.html#4-lightfm","title":"4 | \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f LightFM\u00b6","text":""},{"location":"portfolio/course_recsys/recsys_lightfm.html#a","title":"(A) \u0412\u0435\u043a\u0442\u043e\u0440 \u041a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0438 \u0424\u0438\u0447\u0435\u0439 \u041f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u00b6","text":"<p>\u0412\u0441\u043f\u043e\u043c\u043d\u0438\u043c \u043d\u0430\u0448\u0443 \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0444\u0438\u0447\u0435\u0439 \u0434\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439</p>"},{"location":"portfolio/course_recsys/recsys_lightfm.html#b","title":"(B) \u0424\u0438\u0447\u0438 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043e\u0432\u00b6","text":"<p>\u0412\u043e\u0437\u043c\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u043e\u043f <code>50 \u0436\u0430\u043d\u0440\u043e\u0432</code></p>"},{"location":"portfolio/course_recsys/recsys_lightfm.html#5-lightfm","title":"5 | \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u0414\u0430\u0442\u0430\u0441\u0435\u0442 LightFM\u00b6","text":"<p>\u0414\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435, \u0443\u0441\u043b\u043e\u0432\u043d\u043e \u043d\u0443\u0436\u043d\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c 3 \u0448\u0430\u0433\u0430 \u043f\u0435\u0440\u0435\u0434 <code>predict</code></p> <ul> <li>(a) \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438 <code>.fit_partial()</code> \u0434\u043b\u044f LightFM \u0434\u0430\u0442\u0430\u0441\u0435\u0442</li> <li>(b) \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 <code>build_item_features()</code> \u0434\u043b\u044f LightFM \u0434\u0430\u0442\u0430\u0441\u0435\u0442</li> <li>(\u0441) \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0438\u043d\u0442\u0435\u0440\u0430\u043a\u0446\u0438\u0438 \u0432 LightFM \u0434\u0430\u0442\u0430\u0441\u0435\u0442 <code>build_interactions()</code></li> </ul>"},{"location":"portfolio/course_recsys/recsys_lightfm.html#6-lightfm","title":"6 | \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u041c\u043e\u0434\u0435\u043b\u044c LightFM\u00b6","text":"<p>\u0412 \u043c\u043e\u0434\u0435\u043b\u0438 \u0442\u0432\u0430 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430; <code>no_components</code> \u0438 <code>loss</code></p> <p>\u0414\u043b\u044f <code>fit</code> \u043d\u0443\u0436\u043d\u043e</p> <ul> <li>\u041d\u0443\u0436\u0435\u043d \u0438\u043d\u0442\u0435\u0440\u0430\u043a\u0446\u0438\u0438 (\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438 \u0447\u0435\u0440\u0435\u0437 <code>build_interactions()</code> (C)</li> <li>\u041d\u0443\u0436\u043d\u044b <code>user_features</code> \u0438 <code>item_features</code> (\u043f\u043e\u0441\u0442\u0440\u043e\u0438\u043b\u0438 \u0447\u0435\u0440\u0435\u0437 <code>build_item_features()</code> (B)</li> </ul>"},{"location":"portfolio/course_recsys/recsys_lightfm.html#7","title":"7 | \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u00b6","text":""},{"location":"portfolio/course_recsys/recsys_lightfm.html#a-predict","title":"(A) \u0427\u0435\u0440\u0435\u0437 \u043c\u0435\u0442\u043e\u0434 predict\u00b6","text":"<p>\u0414\u043b\u044f \u043f\u0440\u0438\u043c\u0435\u0440\u0430 \u0432\u043e\u0437\u043c\u0435\u043c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f <code>2535</code>, \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0435\u043c\u0441\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f <code>predict</code>. \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0436\u0430\u043d\u0440\u044b \u043a\u043d\u0438\u0433 \u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f.</p>"},{"location":"portfolio/course_recsys/recsys_lightfm.html#b","title":"(B) \u0420\u0443\u0447\u043d\u044b\u043c \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u043c\u00b6","text":"<p>\u0412\u0435\u0440\u043d\u0435\u043c\u0441\u044f \u043a \u0444\u043e\u0440\u043c\u0443\u043b\u0435:</p> <p>$$r_{ui} = &lt;q_u , p_i&gt; + b_u + b_i$$</p> <p>$$q_u = \\sum_{j \\in f_u} e^U_j$$ $$ e^U_j = w^U_j  e_j$$ $$b_u = \\sum_{j \\in f_u} b^U_j$$ $$ b^U_j = w^U_j  b_j$$</p> <p>\u0433\u0434\u0435 $q_u, p_i$ - \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0438 \u043e\u0431\u044a\u0435\u043a\u0442\u0430, \u044f\u0432\u043b\u044f\u044e\u0449\u0438\u0435\u0441\u044f \u0441\u0443\u043c\u043c\u043e\u0439 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432 \u0438 \u0438\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 $b_u, b_i$ - \u0441\u043c\u0435\u0449\u0435\u043d\u0438\u044f \u0434\u043b\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0438 \u043e\u0431\u044a\u0435\u043a\u0442\u0430</p>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition.html","title":"Recsys matrix decomposition","text":"In\u00a0[68]: Copied! <pre># !wget http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n# !unzip -o ml-latest-small.zip\n</pre> # !wget http://files.grouplens.org/datasets/movielens/ml-latest-small.zip # !unzip -o ml-latest-small.zip In\u00a0[26]: Copied! <pre># pip install replay-rec --quiet\n# pip install implicit -qqq\n</pre> # pip install replay-rec --quiet # pip install implicit -qqq <pre>Note: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[28]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.metrics import mean_absolute_error\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse.linalg import svds\nimport scipy.sparse as sparse\nimport scipy\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport warnings; warnings.filterwarnings('ignore')\n\nfrom replay.metrics import HitRate, NDCG, MAP, Experiment # \u043f\u043e\u0434\u0441\u0447\u0435\u0442 \u043c\u0435\u0442\u0440\u0438\u043a \nimport implicit # \u0434\u043b\u044f ALS\n</pre> import numpy as np import pandas as pd import os from sklearn.metrics import mean_absolute_error from scipy.sparse import csr_matrix from scipy.sparse.linalg import svds import scipy.sparse as sparse import scipy import matplotlib.pyplot as plt from tqdm import tqdm import warnings; warnings.filterwarnings('ignore')  from replay.metrics import HitRate, NDCG, MAP, Experiment # \u043f\u043e\u0434\u0441\u0447\u0435\u0442 \u043c\u0435\u0442\u0440\u0438\u043a  import implicit # \u0434\u043b\u044f ALS In\u00a0[29]: Copied! <pre># data source 1\nratings = pd.read_csv('./ml-latest-small/ratings.csv', parse_dates=['timestamp'])\nratings.head()\n</pre> # data source 1 ratings = pd.read_csv('./ml-latest-small/ratings.csv', parse_dates=['timestamp']) ratings.head() Out[29]: userId movieId rating timestamp 0 1 1 4.0 964982703 1 1 3 4.0 964981247 2 1 6 4.0 964982224 3 1 47 5.0 964983815 4 1 50 5.0 964982931 In\u00a0[30]: Copied! <pre>print(f'\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u044e\u0437\u0435\u0440\u043e\u0432: {ratings[\"userId\"].nunique()}')\nprint(f'\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432: {ratings[\"movieId\"].nunique()}')\n</pre> print(f'\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u044e\u0437\u0435\u0440\u043e\u0432: {ratings[\"userId\"].nunique()}') print(f'\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432: {ratings[\"movieId\"].nunique()}') <pre>\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u044e\u0437\u0435\u0440\u043e\u0432: 610\n\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432: 9724\n</pre> In\u00a0[31]: Copied! <pre>ratings['rating'].value_counts()\n</pre> ratings['rating'].value_counts() Out[31]: <pre>rating\n4.0    26818\n3.0    20047\n5.0    13211\n3.5    13136\n4.5     8551\n2.0     7551\n2.5     5550\n1.0     2811\n1.5     1791\n0.5     1370\nName: count, dtype: int64</pre> <ul> <li>\u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0437 \u043a\u0430\u0436\u0434\u044b\u0439 \u0444\u0438\u043b\u044c\u043c \u0431\u044b\u043b \u043e\u0446\u0435\u043d\u0435\u043d</li> <li>\u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u043c\u0435\u044e\u0442 \u0431\u043e\u043b\u044c\u0448\u0435 20 \u043e\u0446\u0435\u043d\u043e\u043a</li> <li>\u0418\u0437 <code>ratings</code> \u0444\u0438\u043b\u044c\u0442\u0440\u0443\u0435\u043c \u0442\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u043c\u0435\u044e\u0442 \u0431\u043e\u043b\u044c\u0448\u0435 20 \u043e\u0446\u0435\u043d\u043e\u043a</li> </ul> In\u00a0[34]: Copied! <pre>\"\"\"\n\nSelect subset of ratings data\n\n\"\"\"\n\n# count how many times each movie was rated\nitem_counts = ratings.groupby('movieId')['userId'].count() # number of ratings for each movie\npop_items = item_counts[item_counts &gt; 20] # select only movies with counts &gt; 20\nratings = ratings[ratings['movieId'].isin(pop_items.index)] # select subset of user ratings for movies that have counts &gt; 20\nratings\n</pre> \"\"\"  Select subset of ratings data  \"\"\"  # count how many times each movie was rated item_counts = ratings.groupby('movieId')['userId'].count() # number of ratings for each movie pop_items = item_counts[item_counts &gt; 20] # select only movies with counts &gt; 20 ratings = ratings[ratings['movieId'].isin(pop_items.index)] # select subset of user ratings for movies that have counts &gt; 20 ratings Out[34]: userId movieId rating timestamp 0 1 1 4.0 964982703 1 1 3 4.0 964981247 2 1 6 4.0 964982224 3 1 47 5.0 964983815 4 1 50 5.0 964982931 ... ... ... ... ... 100803 610 148626 4.0 1493847175 100808 610 152081 4.0 1493846503 100829 610 164179 5.0 1493845631 100830 610 166528 4.0 1493879365 100834 610 168252 5.0 1493846352 <p>66658 rows \u00d7 4 columns</p> <p>\u0441\u043e\u0437\u0434\u0430\u043b\u0438\u043c \u0441\u0432\u043e\u044e \u0435\u043d\u043d\u0443\u043c\u0435\u0440\u0430\u0446\u0438\u044e \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u0444\u0438\u043b\u044c\u043c\u043e\u0432</p> In\u00a0[35]: Copied! <pre>all_users = ratings['userId'].unique().tolist()  # all unique users in ratings data\nall_items = ratings['movieId'].unique().tolist() # all unique movies in ratings data\n\nn_users = ratings['userId'].nunique() # number of unique users\nn_items = ratings['movieId'].nunique() # number of unique movies\n\nuser_id2idx = dict(zip(all_users, range(n_users)))\nitem_id2idx = dict(zip(all_items, range(n_items)))\n\nratings['userId'] = ratings['userId'].map(user_id2idx) # redefine user id  (for better interpretation)\nratings['movieId'] = ratings['movieId'].map(item_id2idx) # redefine movie id (for better interpretation)\n</pre> all_users = ratings['userId'].unique().tolist()  # all unique users in ratings data all_items = ratings['movieId'].unique().tolist() # all unique movies in ratings data  n_users = ratings['userId'].nunique() # number of unique users n_items = ratings['movieId'].nunique() # number of unique movies  user_id2idx = dict(zip(all_users, range(n_users))) item_id2idx = dict(zip(all_items, range(n_items)))  ratings['userId'] = ratings['userId'].map(user_id2idx) # redefine user id  (for better interpretation) ratings['movieId'] = ratings['movieId'].map(item_id2idx) # redefine movie id (for better interpretation) In\u00a0[36]: Copied! <pre># group\nfull_history = ratings.sort_values(['userId', 'timestamp']).groupby('userId') # group by userid w/ sorted data\n\ntest = full_history.tail(1) # get last movie rating for each user\ntrain = full_history.head(-1) # get all other movie ratings for each user except for first\n\ntrain.shape, test.shape\n</pre> # group full_history = ratings.sort_values(['userId', 'timestamp']).groupby('userId') # group by userid w/ sorted data  test = full_history.tail(1) # get last movie rating for each user train = full_history.head(-1) # get all other movie ratings for each user except for first  train.shape, test.shape Out[36]: <pre>((66048, 4), (610, 4))</pre> In\u00a0[37]: Copied! <pre>train = train.reset_index()\ntest = test.reset_index()\n</pre> train = train.reset_index() test = test.reset_index() <p>\u041e\u0441\u0442\u0430\u0432\u0438\u043c \u0442\u043e\u043b\u043a\u043e \u0442\u0435 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0438 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u0434\u043b\u044f \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043e\u043d\u0438 \u043f\u043e\u0441\u0442\u0432\u0432\u0438\u043b\u0438 5</p> In\u00a0[38]: Copied! <pre>test = test[test['rating'] == 5] # lets select only users who give 5.0 ratings to movies\ntest.shape\n</pre> test = test[test['rating'] == 5] # lets select only users who give 5.0 ratings to movies test.shape Out[38]: <pre>(117, 5)</pre> In\u00a0[1]: Copied! <pre>class BaseFactorizationModel:\n    def __init__(self, random_state=0, user_col='userId', item_col='movieId', rating_col='rating'):\n        self.random_state = np.random.RandomState(random_state)\n        self.user_col = user_col\n        self.item_col = item_col\n        self.rating_col = rating_col\n        self.user_matrix = None\n        self.item_matrix = None\n\n    # matrix which we will decompose\n    def get_rating_matrix(self, data):\n        return pd.pivot_table(\n            data,\n            values=self.rating_col,\n            index=self.user_col,\n            columns=self.item_col,\n            fill_value=0\n            ).values\n\n    \n    \"\"\"\n    \n    When we receive the matrix with scores, for each user\n    sort and get the top k \n    \n    \"\"\"\n    \n    def predict(self, \n                scores,  # (user,film) score matrix\n                rating_matrix=None, # (user,film) rating matrix\n                filter_seen=True, \n                k=10):\n        \n        # filter out films that already have been seen \n        if filter_seen:\n            scores = np.multiply(scores,\n            np.invert(rating_matrix.astype(bool))\n            )\n\n        # scores index : userId column : filmId \n\n        # get indicies of top k scores (indicies : movieId) in user array\n        ind_part = np.argpartition(scores, -k + 1)[:, -k:].copy()  \n\n        # get the values of the top k scores \n        scores_not_sorted = np.take_along_axis(scores, ind_part, axis=1)\n\n        # indicies of scores from lowest to highest \n        ind_sorted = np.argsort(scores_not_sorted, axis=1) # \n\n        # scores from lowest to highest \n        scores_sorted = np.sort(scores_not_sorted, axis=1) \n\n        # get the indicies of the movieId with the highest scores\n        indices = np.take_along_axis(ind_part, ind_sorted, axis=1)\n\n        # for each user return the movies with the highest scores\n        preds = pd.DataFrame({\n            self.user_col: range(scores.shape[0]), # each user \n            self.item_col: np.flip(indices, axis=1).tolist(), # movieId index \n            self.rating_col: np.flip(scores_sorted, axis=1).tolist() # movieId score\n            })\n        \n        # convert arrays (user_col,item_col) into rows for each user \n        preds = preds.explode([self.item_col, self.rating_col])\n\n        return preds\n</pre> class BaseFactorizationModel:     def __init__(self, random_state=0, user_col='userId', item_col='movieId', rating_col='rating'):         self.random_state = np.random.RandomState(random_state)         self.user_col = user_col         self.item_col = item_col         self.rating_col = rating_col         self.user_matrix = None         self.item_matrix = None      # matrix which we will decompose     def get_rating_matrix(self, data):         return pd.pivot_table(             data,             values=self.rating_col,             index=self.user_col,             columns=self.item_col,             fill_value=0             ).values           \"\"\"          When we receive the matrix with scores, for each user     sort and get the top k           \"\"\"          def predict(self,                  scores,  # (user,film) score matrix                 rating_matrix=None, # (user,film) rating matrix                 filter_seen=True,                  k=10):                  # filter out films that already have been seen          if filter_seen:             scores = np.multiply(scores,             np.invert(rating_matrix.astype(bool))             )          # scores index : userId column : filmId           # get indicies of top k scores (indicies : movieId) in user array         ind_part = np.argpartition(scores, -k + 1)[:, -k:].copy()            # get the values of the top k scores          scores_not_sorted = np.take_along_axis(scores, ind_part, axis=1)          # indicies of scores from lowest to highest          ind_sorted = np.argsort(scores_not_sorted, axis=1) #           # scores from lowest to highest          scores_sorted = np.sort(scores_not_sorted, axis=1)           # get the indicies of the movieId with the highest scores         indices = np.take_along_axis(ind_part, ind_sorted, axis=1)          # for each user return the movies with the highest scores         preds = pd.DataFrame({             self.user_col: range(scores.shape[0]), # each user              self.item_col: np.flip(indices, axis=1).tolist(), # movieId index              self.rating_col: np.flip(scores_sorted, axis=1).tolist() # movieId score             })                  # convert arrays (user_col,item_col) into rows for each user          preds = preds.explode([self.item_col, self.rating_col])          return preds In\u00a0[41]: Copied! <pre>class SVD(BaseFactorizationModel):\n    \n    def __init__(self, \n                 random_state=0, \n                 user_col='userId', \n                 item_col='movieId',\n                 n_factors=20): # hyperparameter\n        super().__init__(random_state, user_col, item_col)\n        self.n_factors = n_factors\n\n    \"\"\"\n    \n    Calculate the scores for each user\n    \n    \"\"\"\n        \n    def fit(self, data):\n        \n        # user,movie rating matrix\n        self.rating_matrix = self.get_rating_matrix(data) # (unique users,unique films) ratings\n        csr_rating_matrix = csr_matrix(self.rating_matrix.astype(float))\n        \n        # svd decomposition \n        user_matrix, singular_values, item_matrix = svds(A=csr_rating_matrix, \n                                                         k=self.n_factors)\n    \n        user_matrix = user_matrix * np.sqrt(singular_values)  # (unique users,k latent features)\n        item_matrix = item_matrix.T * np.sqrt(singular_values) # (unique films, k latent features)\n        self.scores = user_matrix @ item_matrix.T  # score matrix for each user &amp; film (unique users, unique films)\n        \n        self.user_matrix = user_matrix\n        self.item_matrix = item_matrix\n</pre> class SVD(BaseFactorizationModel):          def __init__(self,                   random_state=0,                   user_col='userId',                   item_col='movieId',                  n_factors=20): # hyperparameter         super().__init__(random_state, user_col, item_col)         self.n_factors = n_factors      \"\"\"          Calculate the scores for each user          \"\"\"              def fit(self, data):                  # user,movie rating matrix         self.rating_matrix = self.get_rating_matrix(data) # (unique users,unique films) ratings         csr_rating_matrix = csr_matrix(self.rating_matrix.astype(float))                  # svd decomposition          user_matrix, singular_values, item_matrix = svds(A=csr_rating_matrix,                                                           k=self.n_factors)              user_matrix = user_matrix * np.sqrt(singular_values)  # (unique users,k latent features)         item_matrix = item_matrix.T * np.sqrt(singular_values) # (unique films, k latent features)         self.scores = user_matrix @ item_matrix.T  # score matrix for each user &amp; film (unique users, unique films)                  self.user_matrix = user_matrix         self.item_matrix = item_matrix In\u00a0[42]: Copied! <pre>svd_model = SVD()\nsvd_model.fit(train)\n</pre> svd_model = SVD() svd_model.fit(train) In\u00a0[43]: Copied! <pre>preds_svd = svd_model.predict(svd_model.scores,        # user movie scores from svd decomposition\n                              svd_model.rating_matrix) # user movie ratings \npreds_svd\n</pre> preds_svd = svd_model.predict(svd_model.scores,        # user movie scores from svd decomposition                               svd_model.rating_matrix) # user movie ratings  preds_svd Out[43]: userId movieId rating 0 0 676 3.941808 0 0 677 3.500389 0 0 588 3.180376 0 0 593 3.135802 0 0 1062 2.902454 ... ... ... ... 609 609 1158 3.092268 609 609 61 3.092064 609 609 1201 3.08826 609 609 717 3.053321 609 609 1062 3.044532 <p>6100 rows \u00d7 3 columns</p> In\u00a0[44]: Copied! <pre>class FunkSVD(BaseFactorizationModel):\n    def __init__(self, random_state = 0,\n                 user_col='userId', \n                 item_col='movieId',\n                 rating_col='rating', \n                 lr = 0.01,  # learning rate\n                 reg = 0.05, # coefficient of regularisation\n                 n_factors=20, # size of vectors\n                 n_epochs=5): # number of iterations\n        super().__init__(random_state, user_col, item_col, rating_col)\n        self.lr = lr\n        self.reg = reg\n        self.n_factors = n_factors\n        self.n_epochs = n_epochs\n\n\n    def fit(self, data):\n        \n        self.rating_matrix = self.get_rating_matrix(data)\n        n_users, n_items = self.rating_matrix.shape\n\n        \"\"\"\n        \n        Parameter Initialisation\n        \n        mu : \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u0430\u044f \u0441\u0440\u0435\u0434\u043d\u044f\u044f \u043e\u0446\u0435\u043d\u043a\u0430\n        bu : \u0441\u043c\u0435\u0449\u0435\u043d\u0438\u0435 \u043e\u0446\u0435\u043d\u043e\u043a \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f (\u043d\u0430 \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043e\u0442\u043b\u0438\u0447\u0430\u0435\u0442\u0441\u044f \u0441\u0440\u0435\u0434\u043d\u044f\u044f \u043e\u0446\u0435\u043d\u043a\u0430 \u043e\u0442 \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u043e\u0433\u043e)\n        bi : \u0441\u043c\u0435\u0449\u0435\u043d\u0438\u0435 \u043e\u0446\u0435\u043d\u043e\u043a \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 (\u043d\u0430 \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043e\u0442\u043b\u0438\u0447\u0430\u0435\u0442\u0441\u044f \u0441\u0440\u0435\u0434\u043d\u044f\u044f \u043e\u0446\u0435\u043d\u043a\u0430 \u043e\u0442 \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u043e\u0433\u043e)\n        \n        P, Q : \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u043e\u0431\u0443\u0447\u0430\u0442\u044c \n        \n        \"\"\"\n        mu = data[self.rating_col].mean() # global average rating\n        \n        # initialise shifts \n        # how much avg rating of user differs from global avg\n        bu = np.zeros(n_users)  \n        bi = np.zeros(n_items)\n        \n        # we need to learn the matricies for users and items\n        P = self.random_state.normal(size=(n_users, self.n_factors))\n        Q = self.random_state.normal(size=(n_items, self.n_factors))\n\n        # number of iterations we will be updating our weights (matrices)\n        for _ in tqdm(range(self.n_epochs)):\n            \n            errs = []\n            for _, row in data.iterrows():\n                \n                # Get ids and ratings\n                user_id = row[self.user_col]\n                item_id = row[self.item_col]\n                rating = row[self.rating_col]\n\n                # Predict current rating\n                # global avg + shift of user/item + scalar vector multiplication\n                \n                pred = mu + bu[user_id] + bi[item_id] + Q[item_id] @ P[user_id]\n\n                # calculate the difference b/w prediction and actual user ratings\n                err = rating - pred\n                errs.append(abs(err)) \n\n                \"\"\"\n                \n                Update Biases bu/bi\n                \n                \"\"\"\n\n                bu[user_id] += self.lr * (err - self.reg * bu[user_id])\n                bi[item_id] += self.lr * (err - self.reg * bi[item_id])\n                \n                \"\"\"\n                \n                Update latent factors\n                \n                \"\"\"\n\n                P[user_id] += self.lr * (err * Q[item_id] - self.reg * P[user_id])\n                Q[item_id] += self.lr * (err * P[user_id] - self.reg * Q[item_id])\n\n            print(round(np.mean(errs), 4))\n            \n        # restore all data\n        self.user_matrix = P # (userId,k)\n        self.item_matrix = Q # (itemId,k)\n        self.user_bias = bu  # (userId,)\n        self.item_bias = bi  # (itemId,)\n        self.mu = mu  # global doesn't change\n        \n        #             [ dont really neeed ]  \n        self.scores = mu + bu.reshape(-1, 1) + bi + P @ Q.T\n</pre> class FunkSVD(BaseFactorizationModel):     def __init__(self, random_state = 0,                  user_col='userId',                   item_col='movieId',                  rating_col='rating',                   lr = 0.01,  # learning rate                  reg = 0.05, # coefficient of regularisation                  n_factors=20, # size of vectors                  n_epochs=5): # number of iterations         super().__init__(random_state, user_col, item_col, rating_col)         self.lr = lr         self.reg = reg         self.n_factors = n_factors         self.n_epochs = n_epochs       def fit(self, data):                  self.rating_matrix = self.get_rating_matrix(data)         n_users, n_items = self.rating_matrix.shape          \"\"\"                  Parameter Initialisation                  mu : \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u0430\u044f \u0441\u0440\u0435\u0434\u043d\u044f\u044f \u043e\u0446\u0435\u043d\u043a\u0430         bu : \u0441\u043c\u0435\u0449\u0435\u043d\u0438\u0435 \u043e\u0446\u0435\u043d\u043e\u043a \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f (\u043d\u0430 \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043e\u0442\u043b\u0438\u0447\u0430\u0435\u0442\u0441\u044f \u0441\u0440\u0435\u0434\u043d\u044f\u044f \u043e\u0446\u0435\u043d\u043a\u0430 \u043e\u0442 \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u043e\u0433\u043e)         bi : \u0441\u043c\u0435\u0449\u0435\u043d\u0438\u0435 \u043e\u0446\u0435\u043d\u043e\u043a \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 (\u043d\u0430 \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043e\u0442\u043b\u0438\u0447\u0430\u0435\u0442\u0441\u044f \u0441\u0440\u0435\u0434\u043d\u044f\u044f \u043e\u0446\u0435\u043d\u043a\u0430 \u043e\u0442 \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u043e\u0433\u043e)                  P, Q : \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u043e\u0431\u0443\u0447\u0430\u0442\u044c                   \"\"\"         mu = data[self.rating_col].mean() # global average rating                  # initialise shifts          # how much avg rating of user differs from global avg         bu = np.zeros(n_users)           bi = np.zeros(n_items)                  # we need to learn the matricies for users and items         P = self.random_state.normal(size=(n_users, self.n_factors))         Q = self.random_state.normal(size=(n_items, self.n_factors))          # number of iterations we will be updating our weights (matrices)         for _ in tqdm(range(self.n_epochs)):                          errs = []             for _, row in data.iterrows():                                  # Get ids and ratings                 user_id = row[self.user_col]                 item_id = row[self.item_col]                 rating = row[self.rating_col]                  # Predict current rating                 # global avg + shift of user/item + scalar vector multiplication                                  pred = mu + bu[user_id] + bi[item_id] + Q[item_id] @ P[user_id]                  # calculate the difference b/w prediction and actual user ratings                 err = rating - pred                 errs.append(abs(err))                   \"\"\"                                  Update Biases bu/bi                                  \"\"\"                  bu[user_id] += self.lr * (err - self.reg * bu[user_id])                 bi[item_id] += self.lr * (err - self.reg * bi[item_id])                                  \"\"\"                                  Update latent factors                                  \"\"\"                  P[user_id] += self.lr * (err * Q[item_id] - self.reg * P[user_id])                 Q[item_id] += self.lr * (err * P[user_id] - self.reg * Q[item_id])              print(round(np.mean(errs), 4))                      # restore all data         self.user_matrix = P # (userId,k)         self.item_matrix = Q # (itemId,k)         self.user_bias = bu  # (userId,)         self.item_bias = bi  # (itemId,)         self.mu = mu  # global doesn't change                  #             [ dont really neeed ]           self.scores = mu + bu.reshape(-1, 1) + bi + P @ Q.T In\u00a0[45]: Copied! <pre>funk_model = FunkSVD()\nfunk_model.fit(train)\n</pre> funk_model = FunkSVD() funk_model.fit(train) <pre> 20%|\u2588\u2588        | 1/5 [00:06&lt;00:24,  6.10s/it]</pre> <pre>1.7948\n</pre> <pre> 40%|\u2588\u2588\u2588\u2588      | 2/5 [00:12&lt;00:18,  6.13s/it]</pre> <pre>0.8553\n</pre> <pre> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:18&lt;00:12,  6.11s/it]</pre> <pre>0.728\n</pre> <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:24&lt;00:06,  6.11s/it]</pre> <pre>0.6719\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:30&lt;00:00,  6.13s/it]</pre> <pre>0.6401\n</pre> <pre>\n</pre> In\u00a0[46]: Copied! <pre>preds_funk = funk_model.predict(funk_model.scores, \n                                funk_model.rating_matrix)\n</pre> preds_funk = funk_model.predict(funk_model.scores,                                  funk_model.rating_matrix) In\u00a0[47]: Copied! <pre>preds_funk\n</pre> preds_funk Out[47]: userId movieId rating 0 0 544 5.201155 0 0 677 5.169179 0 0 1042 5.163057 0 0 1016 5.158277 0 0 215 5.151932 ... ... ... ... 609 609 6 4.403011 609 609 208 4.38257 609 609 254 4.38018 609 609 270 4.372523 609 609 1107 4.366516 <p>6100 rows \u00d7 3 columns</p> In\u00a0[48]: Copied! <pre>k = funk_model.scores.shape[1] # number of movieId\nprint(k)\n</pre> k = funk_model.scores.shape[1] # number of movieId print(k) <pre>1235\n</pre> In\u00a0[49]: Copied! <pre>preds_funk = funk_model.predict(funk_model.scores, \n                                funk_model.rating_matrix,\n                                k=k)\ntest_pred = test.merge(preds_funk, on=['userId', 'movieId'])\n</pre> preds_funk = funk_model.predict(funk_model.scores,                                  funk_model.rating_matrix,                                 k=k) test_pred = test.merge(preds_funk, on=['userId', 'movieId']) In\u00a0[50]: Copied! <pre>test_pred\n</pre> test_pred Out[50]: index userId movieId rating_x timestamp rating_y 0 839 5 40 5.0 845556915 3.611139 1 4160 26 134 5.0 965151428 2.638579 2 4876 29 1014 5.0 1500370457 2.947382 3 5375 36 418 5.0 845927014 3.46253 4 5756 40 730 5.0 1459369130 1.824829 ... ... ... ... ... ... ... 112 89049 575 1178 5.0 1358151542 -0.153748 113 89226 577 522 5.0 1300996817 6.002605 114 89245 578 331 5.0 977364909 2.795876 115 89912 583 336 5.0 834988340 9.314807 116 90392 586 913 5.0 953142269 2.811585 <p>117 rows \u00d7 6 columns</p> In\u00a0[51]: Copied! <pre>mean_absolute_error(test_pred['rating_x'], test_pred['rating_y'])\n</pre> mean_absolute_error(test_pred['rating_x'], test_pred['rating_y']) Out[51]: <pre>1.67207526461794</pre> In\u00a0[52]: Copied! <pre># create the user movie rating matrix \nbase_model = BaseFactorizationModel()\nrating_matrix = base_model.get_rating_matrix(train)\ntrain_sparse = sparse.csr_matrix(rating_matrix)\n</pre> # create the user movie rating matrix  base_model = BaseFactorizationModel() rating_matrix = base_model.get_rating_matrix(train) train_sparse = sparse.csr_matrix(rating_matrix) In\u00a0[\u00a0]: Copied! <pre>\u041a\u0430\u043a \u0438 `funcALS`, \u043c\u0435\u0442\u043e\u0434 \u0438\u043c\u0435\u0435\u0442 \u0441\u043f\u043e\u0441\u043e\u0431 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438, \u043a\u043e\u043b\u0438\u0447\u0435\u0442\u0441\u0432\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438 \u0438 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u043c\u0430\u0442\u0440\u0438\u0446 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \n</pre> \u041a\u0430\u043a \u0438 `funcALS`, \u043c\u0435\u0442\u043e\u0434 \u0438\u043c\u0435\u0435\u0442 \u0441\u043f\u043e\u0441\u043e\u0431 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438, \u043a\u043e\u043b\u0438\u0447\u0435\u0442\u0441\u0432\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438 \u0438 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u043c\u0430\u0442\u0440\u0438\u0446 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438  In\u00a0[1]: Copied! <pre>ials_model = implicit.als.AlternatingLeastSquares(factors=20, \n                                                  regularization=0.1, \n                                                  iterations=50, \n                                                  use_gpu=False)\nials_model.fit((train_sparse).astype('double'))\n</pre> ials_model = implicit.als.AlternatingLeastSquares(factors=20,                                                    regularization=0.1,                                                    iterations=50,                                                    use_gpu=False) ials_model.fit((train_sparse).astype('double')) In\u00a0[54]: Copied! <pre>user_vecs = ials_model.user_factors\nitem_vecs = ials_model.item_factors\n\n# \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043f\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0430\u043c, \u0447\u0442\u043e \u043c\u044b \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0435\u043b\u044c\u043d\u043e \u043d\u0435 \u043f\u0435\u0440\u0435\u043f\u0443\u0442\u0430\u043b\u0438 users \u0438 items\nprint(train_sparse.shape)\nprint(user_vecs.shape, item_vecs.shape)\n</pre> user_vecs = ials_model.user_factors item_vecs = ials_model.item_factors  # \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043f\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0430\u043c, \u0447\u0442\u043e \u043c\u044b \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0435\u043b\u044c\u043d\u043e \u043d\u0435 \u043f\u0435\u0440\u0435\u043f\u0443\u0442\u0430\u043b\u0438 users \u0438 items print(train_sparse.shape) print(user_vecs.shape, item_vecs.shape) <pre>(610, 1235)\n(610, 20) (1235, 20)\n</pre> <p>\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043a\u0430\u043a \u0438 \u0440\u0430\u043d\u044c\u0448\u0435 \u0447\u0435\u0440\u0435\u0437 \u0441\u043a\u0430\u043b\u0430\u0440\u043d\u043e\u0435 \u043f\u0440\u043e\u0438\u0437\u0432\u0438\u0434\u0435\u043d\u0438\u0435 \u043c\u0430\u0442\u0440\u0438\u0446 <code>user_factors</code> \u0438 <code>item_factors</code></p> In\u00a0[55]: Copied! <pre>scores = user_vecs.dot(item_vecs.T)\nscores.shape\n</pre> scores = user_vecs.dot(item_vecs.T) scores.shape Out[55]: <pre>(610, 1235)</pre> In\u00a0[56]: Copied! <pre>user_vecs[0]\n</pre> user_vecs[0] Out[56]: <pre>array([ 1.7469772 ,  1.7536646 , -0.15816154,  0.3728279 ,  1.2622052 ,\n        0.77496713,  1.3928413 ,  1.2806208 , -0.45395368,  0.81654304,\n        0.812823  ,  1.0866945 , -0.90293694,  2.4070432 ,  1.5287678 ,\n        1.9461825 ,  0.14609063,  0.13025095,  1.2569972 , -1.7232617 ],\n      dtype=float32)</pre> In\u00a0[57]: Copied! <pre>item_vecs[0]\n</pre> item_vecs[0] Out[57]: <pre>array([-0.00182583,  0.13658507, -0.14382875,  0.05055814,  0.09778877,\n        0.10126482,  0.03720245,  0.09039041,  0.1098939 , -0.29790142,\n       -0.04846857, -0.01657198,  0.03136025, -0.08922204,  0.31706864,\n       -0.06648538, -0.03854659,  0.06695224,  0.0011029 , -0.06568323],\n      dtype=float32)</pre> In\u00a0[66]: Copied! <pre>preds_ials = base_model.predict(scores, rating_matrix)\n</pre> preds_ials = base_model.predict(scores, rating_matrix) In\u00a0[67]: Copied! <pre>K = [10]\nmetrics = Experiment(\n    [\n        NDCG(K),\n        MAP(K),\n        HitRate(K),\n    ],\n    test,\n    query_column='userId', item_column= 'movieId'\n)\n\nmetrics.add_result(\"SVD\", preds_svd)\nmetrics.results\n\nmetrics.add_result(\"FunkSVD\", preds_funk)\nmetrics.results\n\nmetrics.add_result(\"iALS\", preds_ials)\nmetrics.results\n</pre> K = [10] metrics = Experiment(     [         NDCG(K),         MAP(K),         HitRate(K),     ],     test,     query_column='userId', item_column= 'movieId' )  metrics.add_result(\"SVD\", preds_svd) metrics.results  metrics.add_result(\"FunkSVD\", preds_funk) metrics.results  metrics.add_result(\"iALS\", preds_ials) metrics.results Out[67]: NDCG@10 MAP@10 HitRate@10 SVD 0.057592 0.036226 0.128205 FunkSVD 0.002573 0.000950 0.008547 iALS 0.059676 0.042165 0.119658"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition.html","title":"\u041c\u0435\u0442\u043e\u0434\u044b \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u043e\u0439 \u0444\u0430\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u0438\u00b6","text":""},{"location":"portfolio/course_recsys/recsys_matrix_decomposition.html","title":"\u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 \u00b6","text":"<ul> <li>\u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>implicit</code> \u0434\u043b\u044f ALS</li> <li>\u041d\u0430\u043c \u0442\u0430\u043a \u0436\u0435 \u043d\u0443\u0436\u043d\u0430 \u0431\u0438\u0431\u0438\u043e\u0442\u0435\u043a\u0430 <code>replay-rec</code>, \u0438\u0437 \u043d\u0435\u0435 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>replay.metrics</code></li> </ul>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition.html","title":"\u0414\u0430\u0442\u0430\u0441\u0435\u0442 \u0440\u0435\u0438\u0442\u0438\u043d\u0433\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u00b6","text":"<p>\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043e\u0442 <code>GroupLens</code> $-$ <code>MovieLens</code>: \u042d\u0442\u043e \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0437 $27 000$ \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u0438 $138 000$ \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439, \u0441 \u043e\u0431\u0449\u0438\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e\u043c \u043e\u0446\u0435\u043d\u043e\u043a \u0432 $20$ \u043c\u0438\u043b\u043b\u0438\u043e\u043d\u043e\u0432.</p> <p>\u041d\u043e \u043c\u044b \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0443\u043c\u0435\u043d\u044c\u0448\u0435\u043d\u043d\u043e\u0439 \u0432\u0435\u0440\u0441\u0438\u0435\u0439 \u0434\u043b\u044f \u0431\u044b\u0441\u0442\u0440\u043e\u0442\u044b \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439: $9 000$ \u0444\u0438\u043b\u044c\u043c\u043e\u0432, $700$ \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439, $100 000$ \u043e\u0446\u0435\u043d\u043e\u043a. \u0421\u043a\u0430\u0447\u0430\u0442\u044c \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043c\u043e\u0436\u043d\u043e \u043f\u043e \u044d\u0442\u043e\u0439 \u0441\u0441\u044b\u043b\u043a\u0435</p>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition.html","title":"\u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0412\u044b\u0431\u043e\u0440\u043a\u0443\u00b6","text":"<ul> <li>\u0421\u0433\u0440\u0443\u043f\u0438\u0440\u0443\u0435\u043c \u0432\u0441\u0435 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0438 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e <code>userId</code></li> <li>\u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u044e\u044e \u043f\u043e \u0434\u0430\u0442\u0435 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0443 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0438 \u043a\u043b\u0430\u0434\u0435\u043c \u0435\u0435 \u0432 <code>test</code></li> <li>\u0432\u0441\u0435 \u043e\u0441\u0442\u0432\u043b\u044c\u043d\u043e\u0435 \u0443 \u043d\u0430\u0441 \u0438\u0434\u0435\u0442 \u0432 <code>train</code></li> </ul>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition.html","title":"\u0412\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0424\u0443\u043d\u043a\u0446\u0438\u044f\u00b6","text":"<p>\u0421\u043e\u0437\u0434\u0430\u043b\u0438\u043c \u0431\u0430\u0437\u043e\u0432\u044b\u0439 \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0439 \u043a\u043b\u0430\u0441\u0441 \u043e\u0442 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u043d\u0430\u0448\u0438 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u044b <code>SVD</code>, <code>FunkSVD</code> \u0431\u0443\u0434\u0443\u0442 \u043d\u0430\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u044c. <code>ALS</code> \u0442\u043e\u0436\u0435 \u0431\u0443\u0434\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0448\u0430\u0433\u0438 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0432 \u044d\u0442\u043e\u043c \u043a\u043b\u0430\u0441\u0441\u0435.</p> <p><code>BaseFactorizationModel</code></p> <ul> <li><code>get_rating_matrix</code> \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442 \u043d\u0430 \u0432\u0445\u043e\u0434 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443, \u043a\u0430\u043a \u0438 \u0440\u0430\u043d\u044c\u0448\u0435 \u043c\u044b \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u0441\u0432\u043e\u0434\u043d\u0443\u044e \u0442\u0430\u0431\u043b\u0438\u0446\u0443 \u0441 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c\u0438 \u0438 \u0444\u0438\u043b\u044c\u043c\u0430\u043c\u0438, \u0433\u0434\u0435 \u0447\u0438\u0441\u043b\u0430 \u044d\u0442\u043e \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0432\u0443\u044e\u0449\u0438\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u043f\u043e\u0441\u0442\u0430\u0432\u0438\u043b</li> <li><code>predict</code> \u0434\u0435\u043b\u0430\u0435\u0442 \u0447\u0442\u043e \u0438 \u0440\u0430\u043d\u044c\u0448\u0435, \u0441\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u0442 \u0438 \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u0442 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0442\u043e\u043f \u043a \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u0438 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 <code>movieId</code> \u0438 \u0435\u0433\u043e score \u0438\u0437 SVD</li> </ul>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition.html#svd","title":"SVD\u00b6","text":"<p><code>SVD</code> \u0438\u0437 <code>scipy</code></p> <ul> <li>\u0412 \u0431\u0430\u0437\u043e\u0432\u043e\u043c \u043c\u0435\u0442\u043e\u0434\u0435 1 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 <code>n_factors</code></li> <li>\u0432\u043a\u043b\u044e\u0447\u0430\u0435\u0442 \u043e\u0434\u0438\u043d \u043c\u0435\u0442\u043e\u0434 <code>fit</code>; \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u0442 \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0438 \u0444\u0438\u043b\u044c\u043c\u0430</li> </ul> <p>\u0421\u043c\u044b\u0441\u043b <code>SVD</code> \u0440\u0430\u0437\u043b\u043e\u0436\u0435\u043d\u0438\u044f:</p> <ul> <li>\u0418\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u043c\u0435\u0442\u043e\u0434\u044b \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0430\u043b\u0433\u0435\u0431\u0440\u044b \u0434\u043b\u044f \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u044d\u0442\u0438\u0445 \u043c\u0430\u0442\u0440\u0438\u0446 \u0438 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043f\u0440\u044f\u043c\u044b\u0445 \u043c\u0435\u0442\u043e\u0434\u043e\u0432, \u0442\u0430\u043a\u0438\u0445 \u043a\u0430\u043a QR-\u0440\u0430\u0437\u043b\u043e\u0436\u0435\u043d\u0438\u0435</li> <li>\u041c\u044b \u0444\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 (get_rating_matrix) \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 <code>movieId</code> \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f <code>userId</code> \u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0435</li> <li><code>SVD</code> \u0440\u0430\u0437\u043b\u0430\u0433\u0430\u0435\u0442 \u044d\u0442\u0443 \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0438\u0439 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043d\u0430 \u0442\u0440\u0438 \u043c\u0430\u0442\u0440\u0438\u0446\u044b (U,</li> <li>\u041e\u0441\u0442\u0430\u0432\u043b\u044f\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c k (latern features) \u0434\u043b\u044f \u043c\u0430\u0442\u0440\u0438\u0445 U (<code>userId</code>,k) \u0438 V (<code>movieID</code>,k) \u0438 \u03a3I (k,k)</li> <li>\u042d\u0442\u0430 \u0430\u043f\u0440\u043e\u043a\u0441\u0438\u043c\u0430\u0446\u0438\u0438 \u043c\u0430\u0442\u0440\u0438\u0446\u044b U \u0438 V \u0437\u0430\u0445\u0432\u0430\u0442\u044b\u0432\u0430\u044e\u0442 \u0441\u043a\u0440\u044b\u0442\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0449\u0438\u0435 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u0437\u0430\u043a\u043e\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445</li> <li>\u0415\u0441\u043b\u0438 \u043c\u044b \u043d\u0435 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c <code>n_factors</code> \u0442\u043e \u043c\u044b \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0442\u043e\u0447\u043d\u0443\u044e \u0440\u0430\u0437\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u043c\u0430\u0440\u0442\u0438\u0446\u044b \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0430 (get_rating_matrix)</li> </ul>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition.html#funksvd","title":"FunkSVD\u00b6","text":"<p>\u0422\u0435\u043f\u0435\u0440\u044c \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0441 \u043d\u0443\u043b\u044f \u043f\u043e\u0434\u0445\u043e\u0434 <code>FunkSVD</code></p> <p>\u0412 \u043e\u0442\u043b\u0438\u0447\u0438\u0438 \u043e\u0442 <code>SVD</code> \u043c\u043e\u0434\u0435\u043b\u044c \u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430</p> <ul> <li>\u041c\u044b \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u044b P, Q (U \u0438 Q \u0438\u0437 <code>SVD</code>)</li> </ul>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition.html#ials","title":"iALS\u00b6","text":"<p>\u041c\u0435\u0442\u043e\u0434 <code>ALS</code> \u043f\u043e\u0445\u043e\u0436 \u043d\u0430 <code>funcSVD</code>, \u043e\u043d\u0438 \u043e\u0431\u0430 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0438\u0442\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u043c\u0435\u0434\u043e\u0442\u044b \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f</p> <ul> <li>\u041d\u043e \u0432 <code>ALS</code> \u043c\u044b \u0444\u0438\u043a\u0441\u0438\u0440\u0443\u0435\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 <code>userId</code> \u0438 \u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u043c \u0432\u0435\u0441\u0430 <code>itemId</code></li> <li>\u041f\u043e\u0442\u043e\u043c \u043c\u044b \u0444\u0438\u043a\u0441\u0438\u0440\u0443\u0435\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 <code>itemId</code> \u0438 \u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u043c \u0432\u0435\u0441\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439</li> <li>\u041c\u0435\u0442\u043e\u0434 \u0445\u043e\u0440\u043e\u0448\u043e \u043f\u0430\u0440\u0430\u043b\u043b\u0435\u043b\u0438\u0442\u0441\u044f \u0438 \u043c\u043e\u0436\u0435\u043c \u043d\u0430\u0439\u0442\u0438 \u0440\u0435\u0448\u0435\u043d\u0438\u0435 \u044f\u0432\u043d\u044b\u043c \u0432\u0438\u0434\u0435 (\u043d\u0435 \u043c\u0435\u0442\u043e\u0434\u043e\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u043c \u0441\u043f\u0443\u0441\u043a\u043e\u043c)</li> <li>\u041f\u043e\u0432\u0442\u043e\u0440\u044f\u0435\u043c \u044d\u0442\u043e \u0434\u043e \u0441\u0445\u043b\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043d\u0430\u0438\u043c\u0435\u043d\u0448\u0438\u0445 \u043a\u0432\u0430\u0434\u0440\u0430\u0442\u043e\u0432 \u0432 \u043e\u0431\u043e\u0438\u0445 \u043f\u043e\u0434\u0437\u0430\u0434\u0430\u0447</li> </ul> <p>\u041e\u0442\u043b\u0438\u0447\u0438\u0435 <code>iALS</code> \u043e\u0442 <code>ALS</code></p> <ul> <li><p><code>iALS</code> \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0442\u044c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438\u043d\u043a\u0440\u0435\u043c\u0435\u043d\u0442\u0430\u043b\u044c\u043d\u043e; \u043c\u043e\u0434\u0435\u043b\u044c \u043c\u043e\u0436\u0435\u0442 \u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0442\u044c\u0441\u044f \u043f\u043e \u043c\u0435\u0440\u0435 \u043f\u043e\u0441\u0442\u0443\u043f\u043b\u0435\u043d\u0438\u044f \u043d\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0431\u0435\u0437 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0430\u0442\u044c \u0432\u0441\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u0441 \u043d\u0443\u043b\u044f.</p> </li> <li><p>\u042d\u0442\u043e \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e \u043f\u043e\u043b\u0435\u0437\u043d\u043e \u0432 \u0441\u0446\u0435\u043d\u0430\u0440\u0438\u044f\u0445, \u0433\u0434\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u0441\u0442\u0443\u043f\u0430\u044e\u0442 \u043d\u0435\u043f\u0440\u0435\u0440\u044b\u0432\u043d\u043e (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0432 \u0440\u0435\u0430\u043b\u044c\u043d\u043e\u043c \u0432\u0440\u0435\u043c\u0435\u043d\u0438)</p> </li> <li><p><code>ALS</code> \u043c\u0435\u043d\u0435\u0435 \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0434\u043b\u044f \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u0438 \u0434\u0438\u043d\u0430\u043c\u0438\u0447\u0435\u0441\u043a\u0438 \u0438\u0437\u043c\u0435\u043d\u044f\u044e\u0449\u0438\u0445\u0441\u044f \u043d\u0430\u0431\u043e\u0440\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445, \u0442\u0430\u043a \u043a\u0430\u043a \u0442\u0440\u0435\u0431\u0443\u0435\u0442 \u043f\u043e\u043b\u043d\u043e\u0433\u043e \u043f\u0435\u0440\u0435\u0441\u0447\u0435\u0442\u0430 \u043c\u0430\u0442\u0440\u0438\u0446 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u0442\u043e\u0432\u0430\u0440\u043e\u0432 \u043f\u0440\u0438 \u043a\u0430\u0436\u0434\u043e\u043c \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0438.</p> </li> <li><p><code>iALS</code> \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0431\u043e\u043b\u0435\u0435 \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0440\u0435\u0441\u0443\u0440\u0441\u044b, \u0442\u0430\u043a \u043a\u0430\u043a \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u0443\u0447\u0442\u0435\u043d\u044b \u0431\u0435\u0437 \u043f\u043e\u043b\u043d\u043e\u0433\u043e \u043f\u0435\u0440\u0435\u0441\u0447\u0435\u0442\u0430 \u043c\u043e\u0434\u0435\u043b\u0438. \u042d\u0442\u043e \u0434\u0435\u043b\u0430\u0435\u0442 \u0435\u0433\u043e \u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0449\u0438\u043c \u0434\u043b\u044f \u0441\u0438\u0441\u0442\u0435\u043c \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439 \u0441 \u043f\u043e\u0441\u0442\u043e\u044f\u043d\u043d\u044b\u043c \u043f\u043e\u0442\u043e\u043a\u043e\u043c \u043d\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445.</p> </li> </ul>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition.html","title":"\u041e\u0446\u0435\u043d\u043a\u0438 \u041c\u043e\u0434\u0435\u043b\u0435\u0439\u00b6","text":"<p>\u0412 \u043a\u043e\u043d\u0446\u0435 \u043e\u0446\u0435\u043d\u0438\u043c \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u0432\u0441\u0435\u0445 \u0442\u0440\u0435\u0445 \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u0432</p>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html","title":"Recsys matrix decomposition practice","text":"In\u00a0[220]: Copied! <pre>!pip install implicit -qqq\n</pre> !pip install implicit -qqq In\u00a0[221]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport os\nimport implicit\nfrom scipy.sparse import csr_matrix\nimport scipy.sparse as sparse\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set(style='whitegrid')\nfrom tqdm import tqdm\n</pre> import numpy as np import pandas as pd import os import implicit from scipy.sparse import csr_matrix import scipy.sparse as sparse import scipy import matplotlib.pyplot as plt import seaborn as sns; sns.set(style='whitegrid') from tqdm import tqdm In\u00a0[222]: Copied! <pre>USER_COL = 'user_id'  # \u043a\u043e\u043b\u043e\u043d\u043a\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \nITEM_COL = 'item_id'  # \u043a\u043e\u043b\u043e\u043d\u043a\u0430 \u0441 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u0430\u043c\u0438\nEXP_RATING_COL = 'rating' # \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u0430; \u043a\u043d\u0438\u0433\u0438\nIMP_RATING_COL = 'progress' # \nTIMESTAMP = 'start_date'\n</pre> USER_COL = 'user_id'  # \u043a\u043e\u043b\u043e\u043d\u043a\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439  ITEM_COL = 'item_id'  # \u043a\u043e\u043b\u043e\u043d\u043a\u0430 \u0441 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u0430\u043c\u0438 EXP_RATING_COL = 'rating' # \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u0430; \u043a\u043d\u0438\u0433\u0438 IMP_RATING_COL = 'progress' #  TIMESTAMP = 'start_date' In\u00a0[223]: Copied! <pre>df = pd.read_csv('/kaggle/input/mts-library/interactions.csv')\ndf.head()\n</pre> df = pd.read_csv('/kaggle/input/mts-library/interactions.csv') df.head() Out[223]: user_id item_id progress rating start_date 0 126706 14433 80 NaN 2018-01-01 1 127290 140952 58 NaN 2018-01-01 2 66991 198453 89 NaN 2018-01-01 3 46791 83486 23 5.0 2018-01-01 4 79313 188770 88 5.0 2018-01-01 In\u00a0[224]: Copied! <pre>df.dtypes \n</pre> df.dtypes  Out[224]: <pre>user_id         int64\nitem_id         int64\nprogress        int64\nrating        float64\nstart_date     object\ndtype: object</pre> <p>\u0412 \u043d\u0430\u0448\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445, \u0443 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0441\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u043d\u043e\u0433\u043e \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f <code>start_date</code></p> In\u00a0[225]: Copied! <pre># \u043a\u043e\u043d\u0432\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f [start_date]\ndf['start_date'] = df['start_date'].astype('datetime64[ns]')\n\n# \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u0434\u043b\u044f \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0438 [user_id] [item_id] ie. \u043e\u0434\u043d\u0430 \u0437\u0430\u043f\u0438\u0441\u044c \u0432 \u0434\u0435\u043d\u044c / \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\ndf = df.sort_values('start_date').drop_duplicates(subset=['user_id', 'item_id'], keep='last')\n</pre> # \u043a\u043e\u043d\u0432\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f [start_date] df['start_date'] = df['start_date'].astype('datetime64[ns]')  # \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b \u0434\u043b\u044f \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0438 [user_id] [item_id] ie. \u043e\u0434\u043d\u0430 \u0437\u0430\u043f\u0438\u0441\u044c \u0432 \u0434\u0435\u043d\u044c / \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c df = df.sort_values('start_date').drop_duplicates(subset=['user_id', 'item_id'], keep='last') In\u00a0[226]: Copied! <pre>df[USER_COL].value_counts()\n</pre> df[USER_COL].value_counts() Out[226]: <pre>user_id\n41297     2091\n150282    1352\n155199     850\n143558     832\n86301      822\n          ... \n47123        1\n217          1\n152642       1\n64702        1\n23916        1\nName: count, Length: 151600, dtype: int64</pre> In\u00a0[227]: Copied! <pre>df[ITEM_COL].value_counts()\n</pre> df[ITEM_COL].value_counts() Out[227]: <pre>item_id\n99357     6575\n316995    2706\n109201    2585\n241026    2300\n28889     2287\n          ... \n205629       4\n171910       3\n132600       3\n93290        3\n285283       2\nName: count, Length: 59599, dtype: int64</pre> <p>\u041f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0443\u044e\u0442 \u0441 \u043a\u043d\u0438\u0433\u0430\u043c\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u043c, \u0441\u043a\u043e\u0440\u0435\u0435 \u0432\u0441\u0435\u0433\u043e, \u043f\u043e\u043d\u0440\u0430\u0432\u044f\u0442\u0441\u044f, \u0438 \u043f\u0440\u0435\u0434\u043f\u043e\u0447\u0438\u0442\u0430\u044e\u0442 \u0441\u0442\u0430\u0432\u0438\u0442\u044c \u043e\u0446\u0435\u043d\u043a\u0438 \u043f\u043e\u043d\u0440\u0430\u0432\u0438\u0432\u0448\u0438\u043c\u0441\u044f, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0432 \u0440\u0435\u0430\u043b\u044c\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0447\u0430\u0441\u0442\u043e \u043d\u0430\u0431\u043b\u044e\u0434\u0430\u0435\u0442\u0441\u044f \u0441\u043c\u0435\u0449\u0435\u043d\u0438\u0435 \u0432 \u0441\u0442\u043e\u0440\u043e\u043d\u0443 \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u043e\u0446\u0435\u043d\u043e\u043a.</p> In\u00a0[228]: Copied! <pre>df[EXP_RATING_COL].value_counts(dropna=False)\n</pre> df[EXP_RATING_COL].value_counts(dropna=False) Out[228]: <pre>rating\nNaN    1247661\n5.0     214703\n4.0      40382\n3.0      18273\n1.0       6115\n2.0       5864\nName: count, dtype: int64</pre> In\u00a0[229]: Copied! <pre>ax = df['rating'].hist(bins=10, figsize=(5,3));\nax.grid(color='gray', linestyle='--', linewidth=0.5,alpha=0.2) \nax.tick_params(axis='both', labelsize=9)\nsns.despine(top=True,left=True,right=True)\nplt.show()\n</pre> ax = df['rating'].hist(bins=10, figsize=(5,3)); ax.grid(color='gray', linestyle='--', linewidth=0.5,alpha=0.2)  ax.tick_params(axis='both', labelsize=9) sns.despine(top=True,left=True,right=True) plt.show() <p>\u041d\u0430 \u0441\u0447\u0435\u0442 <code>progress</code>, \u043c\u044b \u0432\u0438\u0434\u0438\u043c \u0447\u0442\u043e \u043d\u0435\u0442 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432 \u0438 \u0431\u043e\u043b\u044c\u0448\u0438\u043d\u0441\u0442\u0432\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043b\u0438\u0431\u043e \u043d\u0435 \u043d\u0430\u0447\u0430\u043b\u0438 \u0447\u0438\u0442\u0430\u0442\u044c \u043b\u0438\u0431\u043e \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u043b\u0438 \u043a\u043d\u0438\u0433\u0443</p> In\u00a0[230]: Copied! <pre>df['progress'].value_counts(dropna=False)\n</pre> df['progress'].value_counts(dropna=False) Out[230]: <pre>progress\n100    228230\n0      200915\n99      56710\n1       48356\n2       33917\n        ...  \n74       8427\n79       8426\n76       8407\n58       8316\n63       8267\nName: count, Length: 101, dtype: int64</pre> In\u00a0[231]: Copied! <pre>ax = df['progress'].hist(bins=50, figsize=(5,3));\nax.grid(color='gray', linestyle='--', linewidth=0.5,alpha=0.2) \nax.tick_params(axis='both', labelsize=9)\nsns.despine(top=True,left=True,right=True)\nplt.show()\n</pre> ax = df['progress'].hist(bins=50, figsize=(5,3)); ax.grid(color='gray', linestyle='--', linewidth=0.5,alpha=0.2)  ax.tick_params(axis='both', labelsize=9) sns.despine(top=True,left=True,right=True) plt.show() In\u00a0[232]: Copied! <pre># \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043d\u0438\u0433 per \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\nuser_groups = df.groupby('user_id')['item_id'].count()\nax = user_groups[user_groups &lt; user_groups.quantile(0.99)].hist(bins=50,figsize=(5,3));\nax.grid(color='gray', linestyle='--', linewidth=0.5,alpha=0.2) \nax.tick_params(axis='both', labelsize=9)\nsns.despine(top=True,left=True,right=True)\nplt.show()\n</pre> # \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043d\u0438\u0433 per \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c user_groups = df.groupby('user_id')['item_id'].count() ax = user_groups[user_groups &lt; user_groups.quantile(0.99)].hist(bins=50,figsize=(5,3)); ax.grid(color='gray', linestyle='--', linewidth=0.5,alpha=0.2)  ax.tick_params(axis='both', labelsize=9) sns.despine(top=True,left=True,right=True) plt.show() In\u00a0[233]: Copied! <pre>item_groups = df.groupby('item_id')['user_id'].count()\nax = item_groups[item_groups &lt; item_groups.quantile(0.99)].hist(bins=50,figsize=(5,3));\nax.grid(color='gray', linestyle='--', linewidth=0.5,alpha=0.2) \nax.tick_params(axis='both', labelsize=9)\nsns.despine(top=True,left=True,right=True)\nplt.show()\n</pre> item_groups = df.groupby('item_id')['user_id'].count() ax = item_groups[item_groups &lt; item_groups.quantile(0.99)].hist(bins=50,figsize=(5,3)); ax.grid(color='gray', linestyle='--', linewidth=0.5,alpha=0.2)  ax.tick_params(axis='both', labelsize=9) sns.despine(top=True,left=True,right=True) plt.show() In\u00a0[234]: Copied! <pre>df_implicit = df.copy()\ndf_explicit = df.dropna()\n\nprint('implicit',df_implicit.shape)\nprint('explicit',df_explicit.shape)\n</pre> df_implicit = df.copy() df_explicit = df.dropna()  print('implicit',df_implicit.shape) print('explicit',df_explicit.shape) <pre>implicit (1532998, 5)\nexplicit (285337, 5)\n</pre> In\u00a0[235]: Copied! <pre>def filter_data(df, user_count=10, item_count=10):\n    \n    # book filter \n    item_counts = df.groupby(ITEM_COL)[USER_COL].count()\n    pop_items = item_counts[item_counts &gt; user_count]\n    df_implicit = df[df[ITEM_COL].isin(pop_items.index)]\n\n    # user filter \n    user_counts = df.groupby(USER_COL)[ITEM_COL].count()\n    pop_users = user_counts[user_counts &gt; item_count]\n    df = df[df[USER_COL].isin(pop_users.index)].copy()\n    return df\n</pre> def filter_data(df, user_count=10, item_count=10):          # book filter      item_counts = df.groupby(ITEM_COL)[USER_COL].count()     pop_items = item_counts[item_counts &gt; user_count]     df_implicit = df[df[ITEM_COL].isin(pop_items.index)]      # user filter      user_counts = df.groupby(USER_COL)[ITEM_COL].count()     pop_users = user_counts[user_counts &gt; item_count]     df = df[df[USER_COL].isin(pop_users.index)].copy()     return df In\u00a0[236]: Copied! <pre>df_implicit = filter_data(df_implicit, \n                          user_count=10, \n                          item_count=20)\ndf_implicit.shape[0]\n</pre> df_implicit = filter_data(df_implicit,                            user_count=10,                            item_count=20) df_implicit.shape[0] Out[236]: <pre>459065</pre> In\u00a0[237]: Copied! <pre>def train_test_split(\n    clickstream_df,\n    test_quantile=0.9):\n    \n    '''\n    \n    Split clickstream by date\n    \n    '''\n    \n    clickstream_df = clickstream_df.sort_values([USER_COL, TIMESTAMP])\n    \n    # test set timestamp\n    test_timepoint = clickstream_df[TIMESTAMP].quantile(\n                                    q=test_quantile, \n                                    interpolation='nearest')\n    \n\n    # train/test split by timestamp\n    test = clickstream_df.query(f'{TIMESTAMP} &gt;= @test_timepoint')    \n    train = clickstream_df.drop(test.index)\n\n    # test must contain users &amp; books found in training set\n    test = test[test[USER_COL].isin(train[USER_COL])]\n    test = test[test[ITEM_COL].isin(train[ITEM_COL])]\n    \n    # sort &amp; group by for each user\n    # test will be last user item read status; train will be all other\n    test_full_history = test.sort_values([USER_COL, TIMESTAMP]).groupby(USER_COL)\n    last_item = test_full_history.tail(1)\n    test_history = test_full_history.head(-1)\n\n    test = pd.concat([train, test_history])\n\n    test.reset_index(drop=True, inplace=True)\n    train.reset_index(drop=True, inplace=True)\n    \n    return train, test, last_item\n</pre> def train_test_split(     clickstream_df,     test_quantile=0.9):          '''          Split clickstream by date          '''          clickstream_df = clickstream_df.sort_values([USER_COL, TIMESTAMP])          # test set timestamp     test_timepoint = clickstream_df[TIMESTAMP].quantile(                                     q=test_quantile,                                      interpolation='nearest')           # train/test split by timestamp     test = clickstream_df.query(f'{TIMESTAMP} &gt;= @test_timepoint')         train = clickstream_df.drop(test.index)      # test must contain users &amp; books found in training set     test = test[test[USER_COL].isin(train[USER_COL])]     test = test[test[ITEM_COL].isin(train[ITEM_COL])]          # sort &amp; group by for each user     # test will be last user item read status; train will be all other     test_full_history = test.sort_values([USER_COL, TIMESTAMP]).groupby(USER_COL)     last_item = test_full_history.tail(1)     test_history = test_full_history.head(-1)      test = pd.concat([train, test_history])      test.reset_index(drop=True, inplace=True)     train.reset_index(drop=True, inplace=True)          return train, test, last_item In\u00a0[238]: Copied! <pre>train, test, last_item = train_test_split(df_implicit)\n\nprint(train.shape)\nprint(test.shape)\nprint(last_item.shape)\n</pre> train, test, last_item = train_test_split(df_implicit)  print(train.shape) print(test.shape) print(last_item.shape) <pre>(412826, 5)\n(449660, 5)\n(4559, 5)\n</pre> In\u00a0[239]: Copied! <pre>print('train start',train['start_date'].min())\nprint('train end',train['start_date'].max())\nprint('test start',test['start_date'].min())\nprint('test end',test['start_date'].max())\nprint('last item start',last_item['start_date'].min())\nprint('last item end',last_item['start_date'].max())\n</pre> print('train start',train['start_date'].min()) print('train end',train['start_date'].max()) print('test start',test['start_date'].min()) print('test end',test['start_date'].max()) print('last item start',last_item['start_date'].min()) print('last item end',last_item['start_date'].max()) <pre>train start 2018-01-01 00:00:00\ntrain end 2019-10-22 00:00:00\ntest start 2018-01-01 00:00:00\ntest end 2019-12-31 00:00:00\nlast item start 2019-10-23 00:00:00\nlast item end 2019-12-31 00:00:00\n</pre> In\u00a0[240]: Copied! <pre>last_item = last_item[last_item[IMP_RATING_COL] &gt;= 10]\nlast_item.shape\n</pre> last_item = last_item[last_item[IMP_RATING_COL] &gt;= 10] last_item.shape Out[240]: <pre>(3675, 5)</pre> In\u00a0[241]: Copied! <pre>def get_sparse_matrix(df, rating_col):\n    \n    \"\"\"\n\n    Create sparse matrix (like via pivot_table)\n    but directly into sparse_matrix format\n    \n    rating_col : value column (implicit/explicit)\n\n    \"\"\"\n    \n    rating = list(df[rating_col]) # rating\n    rows = df[USER_COL].astype('category').cat.codes # userid\n    cols = df[ITEM_COL].astype('category').cat.codes # bookid\n    df_sparse = sparse.csr_matrix((rating, (rows, cols)))\n    \n    return df_sparse, rows, cols\n\n\ndef get_scores(user_vecs, \n               item_vecs, \n               rating_matrix):\n    \n    \"\"\"\n    \n    Get the score matrix from the user &amp; item matrices\n    don't include the books that have already been read\n    \n    R = U_{d} V^{T}_{d}\n    \n    \"\"\"\n    \n    scores = user_vecs.dot(item_vecs.T)\n    scores = np.multiply(scores,\n        np.invert(rating_matrix.astype(bool))\n        )\n    return scores\n\ndef predict(scores, rows, cols, k=10):\n    \n    \"\"\"\n    \n    Using the score matrix from [get_scores] return the \n    sorted recommendations based on the highest top k scores\n    for each user\n    \n    \"\"\"\n    \n    id2user = dict(zip(rows, train.user_id))\n    id2item = dict(zip(cols, train.item_id))\n\n    ind_part = np.argpartition(scores, -k + 1)[:, -k:].copy()\n    scores_not_sorted = np.take_along_axis(scores, ind_part, axis=1)\n    ind_sorted = np.argsort(scores_not_sorted, axis=1)\n    indices = np.take_along_axis(ind_part, ind_sorted, axis=1)\n\n    preds = pd.DataFrame({\n        USER_COL: range(scores.shape[0]),\n        'preds': np.flip(indices, axis=1).tolist(),\n        })\n    \n    preds[USER_COL] = preds[USER_COL].map(id2user)\n    preds['preds'] = preds['preds'].map(lambda inds: [id2item[i] for i in inds])\n    return preds\n\n# metric hitrate\ndef hr(df: pd.DataFrame, \n       pred_col='preds', \n       true_col='item_id') -&gt; float:\n    \n    hr_values = []\n    for _, row in df.iterrows():\n      hr_values.append(int(row[true_col] in row[pred_col]))\n    return round(np.mean(hr_values), 4)\n</pre> def get_sparse_matrix(df, rating_col):          \"\"\"      Create sparse matrix (like via pivot_table)     but directly into sparse_matrix format          rating_col : value column (implicit/explicit)      \"\"\"          rating = list(df[rating_col]) # rating     rows = df[USER_COL].astype('category').cat.codes # userid     cols = df[ITEM_COL].astype('category').cat.codes # bookid     df_sparse = sparse.csr_matrix((rating, (rows, cols)))          return df_sparse, rows, cols   def get_scores(user_vecs,                 item_vecs,                 rating_matrix):          \"\"\"          Get the score matrix from the user &amp; item matrices     don't include the books that have already been read          R = U_{d} V^{T}_{d}          \"\"\"          scores = user_vecs.dot(item_vecs.T)     scores = np.multiply(scores,         np.invert(rating_matrix.astype(bool))         )     return scores  def predict(scores, rows, cols, k=10):          \"\"\"          Using the score matrix from [get_scores] return the      sorted recommendations based on the highest top k scores     for each user          \"\"\"          id2user = dict(zip(rows, train.user_id))     id2item = dict(zip(cols, train.item_id))      ind_part = np.argpartition(scores, -k + 1)[:, -k:].copy()     scores_not_sorted = np.take_along_axis(scores, ind_part, axis=1)     ind_sorted = np.argsort(scores_not_sorted, axis=1)     indices = np.take_along_axis(ind_part, ind_sorted, axis=1)      preds = pd.DataFrame({         USER_COL: range(scores.shape[0]),         'preds': np.flip(indices, axis=1).tolist(),         })          preds[USER_COL] = preds[USER_COL].map(id2user)     preds['preds'] = preds['preds'].map(lambda inds: [id2item[i] for i in inds])     return preds  # metric hitrate def hr(df: pd.DataFrame,         pred_col='preds',         true_col='item_id') -&gt; float:          hr_values = []     for _, row in df.iterrows():       hr_values.append(int(row[true_col] in row[pred_col]))     return round(np.mean(hr_values), 4) In\u00a0[243]: Copied! <pre># train_sparse : user, book progress sparse matrix\n# IMP_RATING_COL : 'progress'\ntrain_sparse, train_rows_imp, train_cols_imp = get_sparse_matrix(train, \n                                                                 IMP_RATING_COL)\n</pre> # train_sparse : user, book progress sparse matrix # IMP_RATING_COL : 'progress' train_sparse, train_rows_imp, train_cols_imp = get_sparse_matrix(train,                                                                   IMP_RATING_COL) <p>\u041d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0445\u0440\u0430\u043d\u0438\u0442\u044c \u0432\u0441\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430\u043c \u043d\u0435 \u0438\u043c\u0435\u0435\u0442 \u0441\u043c\u044b\u0441\u043b\u0430, \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u0430\u044f \u0434\u043e\u043b\u044f \u0443 \u043d\u0430\u0441 \u0432\u043e\u043e\u0431\u0449\u0435 \u0435\u0441\u0442\u044c</p> In\u00a0[245]: Copied! <pre># matrix decomposition of user item interaction matrix\nalgo = implicit.als.AlternatingLeastSquares(factors=50, \n                                            regularization=0.1, \n                                            iterations=50, \n                                            use_gpu=False)\nalgo.fit((train_sparse).astype('double'))\n</pre> # matrix decomposition of user item interaction matrix algo = implicit.als.AlternatingLeastSquares(factors=50,                                              regularization=0.1,                                              iterations=50,                                              use_gpu=False) algo.fit((train_sparse).astype('double')) <pre>  0%|          | 0/50 [00:00&lt;?, ?it/s]</pre> In\u00a0[246]: Copied! <pre>user_vecs = algo.user_factors; print(user_vecs.shape) # user matrix containing 50 features\nitem_vecs = algo.item_factors; print(item_vecs.shape) # item matrix containing 50 featyres \n</pre> user_vecs = algo.user_factors; print(user_vecs.shape) # user matrix containing 50 features item_vecs = algo.item_factors; print(item_vecs.shape) # item matrix containing 50 featyres  <pre>(7561, 50)\n(51358, 50)\n</pre> In\u00a0[247]: Copied! <pre># score matrix for implicit approach\nscores_imp = get_scores(user_vecs, \n                        item_vecs, \n                        train_sparse.todense())\n</pre> # score matrix for implicit approach scores_imp = get_scores(user_vecs,                          item_vecs,                          train_sparse.todense()) In\u00a0[248]: Copied! <pre># create recommendations \npred_imp = predict(scores_imp, \n                   train_rows_imp, \n                   train_cols_imp)\n\n# limit recommendations to only users \npred_imp = pred_imp.merge(last_item, how='right', on='user_id')\n</pre> # create recommendations  pred_imp = predict(scores_imp,                     train_rows_imp,                     train_cols_imp)  # limit recommendations to only users  pred_imp = pred_imp.merge(last_item, how='right', on='user_id') In\u00a0[271]: Copied! <pre>pred_imp.head()\n</pre> pred_imp.head() Out[271]: user_id preds item_id progress rating start_date 0 21 [173707, 126214, 161550, 152551, 134598, 68549... 227443 100 5.0 2019-12-19 1 43 [84620, 39160, 306076, 70926, 181053, 18541, 7... 161017 62 NaN 2019-12-26 2 77 [230045, 295007, 155266, 292066, 270415, 16985... 235407 69 NaN 2019-12-13 3 83 [152716, 186680, 85287, 249564, 192249, 153677... 312297 51 NaN 2019-11-11 4 219 [45897, 81342, 39160, 18541, 108080, 150442, 8... 3192 99 NaN 2019-12-10 In\u00a0[288]: Copied! <pre>hr(pred_imp)\n</pre> hr(pred_imp) Out[288]: <pre>0.0103</pre> In\u00a0[251]: Copied! <pre>df_explicit = filter_data(df_explicit, \n                          user_count=10, \n                          item_count=10)\ndf_explicit.shape[0]\n</pre> df_explicit = filter_data(df_explicit,                            user_count=10,                            item_count=10) df_explicit.shape[0] Out[251]: <pre>101373</pre> <p>\u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u0442\u0440\u0438 \u0447\u0430\u0441\u0442\u0438; \u043a\u0430\u043a \u0438 \u0440\u0430\u043d\u044c\u0448\u0435. \u041d\u043e \u0434\u043b\u044f explicit \u043f\u043e\u0434\u0445\u043e\u0434\u0430 \u043c\u044b \u043e\u0441\u0442\u0430\u0432\u0438\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0438 \u043a\u043d\u0438\u0433 4 \u0438 \u0432\u044b\u0448\u0435</p> In\u00a0[252]: Copied! <pre>train, test, last_item = train_test_split(df_explicit)\nlast_item = last_item[last_item[EXP_RATING_COL] &gt;= 4]\n</pre> train, test, last_item = train_test_split(df_explicit) last_item = last_item[last_item[EXP_RATING_COL] &gt;= 4] In\u00a0[253]: Copied! <pre># create training sparse matrix\ntrain_sparse, train_rows_exp, train_cols_exp = get_sparse_matrix(train, \n                                                                 EXP_RATING_COL)\n</pre> # create training sparse matrix train_sparse, train_rows_exp, train_cols_exp = get_sparse_matrix(train,                                                                   EXP_RATING_COL) In\u00a0[255]: Copied! <pre>algo = implicit.als.AlternatingLeastSquares(factors=50, \n                                            regularization=0.1, \n                                            iterations=50, \n                                            use_gpu=False)\nalgo.fit((train_sparse).astype('double'))\n</pre> algo = implicit.als.AlternatingLeastSquares(factors=50,                                              regularization=0.1,                                              iterations=50,                                              use_gpu=False) algo.fit((train_sparse).astype('double')) <pre>  0%|          | 0/50 [00:00&lt;?, ?it/s]</pre> In\u00a0[256]: Copied! <pre>user_vecs = algo.user_factors # user matrix  (50 features)\nitem_vecs = algo.item_factors  # item matrix (50 features)\n\n# \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043f\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0430\u043c, \u0447\u0442\u043e \u043c\u044b \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0435\u043b\u044c\u043d\u043e \u043d\u0435 \u043f\u0435\u0440\u0435\u043f\u0443\u0442\u0430\u043b\u0438 users \u0438 items\nprint(train_sparse.shape)\nprint(user_vecs.shape, item_vecs.shape)\n</pre> user_vecs = algo.user_factors # user matrix  (50 features) item_vecs = algo.item_factors  # item matrix (50 features)  # \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043f\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0430\u043c, \u0447\u0442\u043e \u043c\u044b \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0435\u043b\u044c\u043d\u043e \u043d\u0435 \u043f\u0435\u0440\u0435\u043f\u0443\u0442\u0430\u043b\u0438 users \u0438 items print(train_sparse.shape) print(user_vecs.shape, item_vecs.shape) <pre>(2514, 17943)\n(2514, 50) (17943, 50)\n</pre> In\u00a0[281]: Copied! <pre># Using score matrix get the score matrix\nscores_exp = get_scores(user_vecs, \n                        item_vecs, \n                        train_sparse.todense())\n\nscores_exp.shape # (2514 users, 17943 items)\n</pre> # Using score matrix get the score matrix scores_exp = get_scores(user_vecs,                          item_vecs,                          train_sparse.todense())  scores_exp.shape # (2514 users, 17943 items) Out[281]: <pre>(2514, 17943)</pre> In\u00a0[284]: Copied! <pre># Make a recommendation prediction for each user using score matrix\npred_exp = predict(scores_exp, \n                   train_rows_exp, \n                   train_cols_exp)\n\nprint(pred_exp.head())\nprint(pred_exp.shape)\n</pre> # Make a recommendation prediction for each user using score matrix pred_exp = predict(scores_exp,                     train_rows_exp,                     train_cols_exp)  print(pred_exp.head()) print(pred_exp.shape) <pre>   user_id                                              preds\n0       21  [261138, 187501, 39765, 23553, 88397, 45920, 2...\n1       83  [251645, 152716, 220122, 192249, 228394, 18668...\n2      156  [198447, 271811, 78934, 216356, 22675, 144406,...\n3      219  [77729, 77581, 75360, 43255, 113015, 222246, 2...\n4      359  [23553, 34917, 140991, 221178, 234400, 83902, ...\n(2514, 2)\n</pre> <p>\u0421\u0434\u0435\u043b\u0430\u0435\u043c \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u044e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c \u0438\u0437 <code>last_item</code> \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u043b\u0438 \u043a\u043d\u0438\u0433\u0438 (4+)</p> In\u00a0[287]: Copied! <pre># give recommendation to users who rated books 4+ only\npred_exp = pred_exp.merge(last_item, how='right', on='user_id')\npred_exp.shape\n</pre> # give recommendation to users who rated books 4+ only pred_exp = pred_exp.merge(last_item, how='right', on='user_id') pred_exp.shape Out[287]: <pre>(771, 6)</pre> In\u00a0[258]: Copied! <pre>hr(pred_exp)\n</pre> hr(pred_exp) Out[258]: <pre>0.0285</pre> <ul> <li>\u041f\u043e \u0438\u0434\u0435\u0439 \u0435\u0441\u043b\u0438 \u043c\u044b \u0445\u043e\u0442\u0438\u043c \u0441\u0440\u0430\u0432\u043d\u0438\u0442\u044c \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043c\u0435\u0436\u0434\u0443 \u0434\u0432\u0443\u043c\u044f \u043f\u043e\u0434\u0445\u043e\u0434\u0430\u043c\u0438, \u043d\u0430\u043c \u043b\u0443\u0447\u0448\u0435 \u0441\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0442\u044c \u043f\u043e\u0445\u043e\u0436\u0438\u0435 \u043f\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 (\u043e\u0434\u0438\u043d \u0438 \u0442\u043e\u0442 \u0436\u0435 \u043d\u0430\u0431\u043e\u0440 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439)</li> <li>explicit feedback \u0434\u0430\u0435\u0442 \u043d\u0430\u043c \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u043b\u0443\u0447\u0448\u0435 hitrate</li> </ul> <p>\u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c sparse \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432 \u043a\u043d\u0438\u0433 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0434\u043b\u044f \u0432\u0441\u0435\u0439 <code>test</code></p> In\u00a0[289]: Copied! <pre># create sparse matrix of user, book [rating] sparse matrix \ntest_sparse, test_rows_exp, test_cols_exp = get_sparse_matrix(test, \n                                                              EXP_RATING_COL)\n</pre> # create sparse matrix of user, book [rating] sparse matrix  test_sparse, test_rows_exp, test_cols_exp = get_sparse_matrix(test,                                                                EXP_RATING_COL) In\u00a0[260]: Copied! <pre>def get_scores_new_actions(item_vecs, \n                           updated_rating_matrix):\n   \n    \"\"\"\n    \n    [item_vecs] : \n    [updated_rating_matrix] : new rating matrix of user/item combination\n    \n    \"\"\"\n\n    scores = updated_rating_matrix.dot(item_vecs) @ item_vecs.T\n    scores = np.multiply(scores,\n        np.invert(updated_rating_matrix.astype(bool))\n        )\n    return scores\n</pre> def get_scores_new_actions(item_vecs,                             updated_rating_matrix):         \"\"\"          [item_vecs] :      [updated_rating_matrix] : new rating matrix of user/item combination          \"\"\"      scores = updated_rating_matrix.dot(item_vecs) @ item_vecs.T     scores = np.multiply(scores,         np.invert(updated_rating_matrix.astype(bool))         )     return scores <p>\u0411\u0443\u0434\u0435\u043c \u043f\u0435\u0440\u0435\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 <code>item_vecs</code> (\u0440\u0430\u0437\u043b\u043e\u0436\u0435\u043d\u043d\u0443\u044e \u043c\u0430\u0442\u0440\u0438\u0443 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043e\u0432)</p> In\u00a0[290]: Copied! <pre>item_vecs.shape\n</pre> item_vecs.shape Out[290]: <pre>(17943, 50)</pre> In\u00a0[291]: Copied! <pre># update existing scores using new use rating sparse matrix &amp; existing item matrix\nscores_exp_updated = get_scores_new_actions(item_vecs, \n                                            test_sparse.todense())\n\n# update recommendations\npred_exp_updated = predict(scores_exp_updated, \n                           test_rows_exp, \n                           test_cols_exp)\n\npred_exp_updated = pred_exp_updated.merge(last_item, \n                                          how='right', \n                                          on='user_id')\npred_exp_updated.head()\n</pre> # update existing scores using new use rating sparse matrix &amp; existing item matrix scores_exp_updated = get_scores_new_actions(item_vecs,                                              test_sparse.todense())  # update recommendations pred_exp_updated = predict(scores_exp_updated,                             test_rows_exp,                             test_cols_exp)  pred_exp_updated = pred_exp_updated.merge(last_item,                                            how='right',                                            on='user_id') pred_exp_updated.head() Out[291]: user_id preds item_id progress rating start_date 0 21 [199064, 11614, 148773, 285903, 299349, 196161... 227443 100 5.0 2019-12-19 1 156 [78934, 271811, 198447, 216356, 22675, 265425,... 101867 100 5.0 2019-12-26 2 219 [251645, 77581, 77729, 43255, 75360, 113015, 2... 109265 99 5.0 2019-11-03 3 359 [23553, 34917, 140991, 221178, 234400, 83902, ... 256193 100 5.0 2019-11-26 4 474 [210268, 65830, 113403, 6943, 90671, 179716, 2... 235407 100 5.0 2019-12-25 <p>\u041a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u0443\u043b\u0443\u0447\u0448\u0438\u043b\u043e\u0441\u044c; \u043a\u0430 \u0441\u0447\u0435\u0442 \u0442\u043e\u0433\u043e \u0447\u0442\u043e \u043c\u044b \u0434\u043e\u0431\u0430\u0432\u0438\u043b\u0438 \u043d\u043e\u0432\u044b\u0435 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f</p> In\u00a0[292]: Copied! <pre>hr(pred_exp_updated)\n</pre> hr(pred_exp_updated) Out[292]: <pre>0.048</pre> <p>\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u0438\u0437 \u0442\u0440\u0435\u0439\u043d\u0430.</p> In\u00a0[293]: Copied! <pre>scores_exp = get_scores_new_actions(item_vecs, \n                                    train_sparse.todense())\npred_exp = predict(scores_exp, train_rows_exp, train_cols_exp)\npred_exp = pred_exp.merge(last_item, how='right', on='user_id')\nhr(pred_exp)\n</pre> scores_exp = get_scores_new_actions(item_vecs,                                      train_sparse.todense()) pred_exp = predict(scores_exp, train_rows_exp, train_cols_exp) pred_exp = pred_exp.merge(last_item, how='right', on='user_id') hr(pred_exp) Out[293]: <pre>0.0272</pre> In\u00a0[294]: Copied! <pre># book information data\nitems = pd.read_csv('/kaggle/input/mts-library/items.csv')\nitems.head()\n</pre> # book information data items = pd.read_csv('/kaggle/input/mts-library/items.csv') items.head() Out[294]: id title genres authors year 0 128115 \u0412\u043e\u0440\u043e\u043d-\u0447\u0435\u043b\u043e\u0431\u0438\u0442\u0447\u0438\u043a \u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435 \u0434\u0435\u0442\u0441\u043a\u0438\u0435 \u043a\u043d\u0438\u0433\u0438,\u0421\u043a\u0430\u0437\u043a\u0438,\u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d 1886 1 210979 \u0421\u043a\u0440\u0438\u043f\u043a\u0430 \u0420\u043e\u0442\u0448\u0438\u043b\u044c\u0434\u0430 \u041a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430,\u041b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430,\u0420\u0443\u0441\u0441\u043a\u0430\u044f ... \u0410\u043d\u0442\u043e\u043d \u0427\u0435\u0445\u043e\u0432 1894 2 95632 \u0418\u0441\u043f\u043e\u0440\u0447\u0435\u043d\u043d\u044b\u0435 \u0434\u0435\u0442\u0438 \u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430,\u041a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430,\u041b\u0438\u0442\u0435\u0440\u0430\u0442... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d 1869 3 247906 \u0421\u0442\u0440\u0430\u043d\u043d\u044b\u0439 \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u041f\u044c\u0435\u0441\u044b \u0438 \u0434\u0440\u0430\u043c\u0430\u0442\u0443\u0440\u0433\u0438\u044f,\u041b\u0438\u0442\u0435\u0440\u0430\u0442\u0443\u0440\u0430 19 \u0432\u0435\u043a\u0430 \u041c\u0438\u0445\u0430\u0438\u043b \u041b\u0435\u0440\u043c\u043e\u043d\u0442\u043e\u0432 1831 4 294280 \u0413\u043e\u0441\u043f\u043e\u0434\u0430 \u0442\u0430\u0448\u043a\u0435\u043d\u0442\u0446\u044b \u0417\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u043a\u0430,\u041a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0437\u0430,\u041b\u0438\u0442\u0435\u0440\u0430\u0442... \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0430\u043b\u0442\u044b\u043a\u043e\u0432-\u0429\u0435\u0434\u0440\u0438\u043d 1873 In\u00a0[313]: Copied! <pre># mappers to actual user_id and item_id from the dataset\n# we used ordered user and item identifiers from 0\n\nid2user = dict(zip(test_rows_exp,test.user_id))\nid2item = dict(zip(test_cols_exp,test.item_id))\nprint(id2item[0],id2user[0])\n</pre> # mappers to actual user_id and item_id from the dataset # we used ordered user and item identifiers from 0  id2user = dict(zip(test_rows_exp,test.user_id)) id2item = dict(zip(test_cols_exp,test.item_id)) print(id2item[0],id2user[0]) <pre>13 21\n</pre> <p>\u041c\u0435\u0442\u043e\u0434 <code>recommend</code> \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 <code>implicit</code> \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043d\u0430\u043c \u0434\u0435\u043b\u0430\u0442\u044c \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u044e \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</p> In\u00a0[296]: Copied! <pre># method realised in library implicit\nrecommendations = algo.recommend(0, test_sparse[0])\nrecommendations # (index array,scores array)\n</pre> # method realised in library implicit recommendations = algo.recommend(0, test_sparse[0]) recommendations # (index array,scores array) Out[296]: <pre>(array([14547, 10450,  2246,  1322,  4866,  2586,   124,  1238, 11283,\n         7814], dtype=int32),\n array([0.12248802, 0.11428122, 0.11298379, 0.11208673, 0.11028358,\n        0.1084349 , 0.1075625 , 0.10495262, 0.10148995, 0.10100657],\n       dtype=float32))</pre> <p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</p> In\u00a0[304]: Copied! <pre># recommendations for user id=0\npd.DataFrame([id2item[i] for i in recommendations[0]]).merge(items, left_on=0, right_on='id')\n</pre> # recommendations for user id=0 pd.DataFrame([id2item[i] for i in recommendations[0]]).merge(items, left_on=0, right_on='id') Out[304]: 0 id title genres authors year 0 261138 261138 \u041b\u0435\u0442\u043e\u0441 \u0413\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u0411\u043e\u0435\u0432\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438 \u0410\u043b\u0435\u043a\u0441\u0435\u0439 \u041f\u0435\u0445\u043e\u0432 2014 1 187501 187501 \u0420\u0430\u0437\u0440\u0443\u0448\u0438\u0442\u0435\u043b\u044c \u0431\u043e\u0436\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0445 \u0437\u0430\u043c\u044b\u0441\u043b\u043e\u0432 \u0413\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041f\u043e\u043f\u0430\u0434\u0430\u043d\u0446\u044b,\u0411\u043e\u0435\u0432\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438 \u0412\u043b\u0430\u0434\u0438\u043c\u0438\u0440 \u0421\u0443\u0445\u0438\u043d\u0438\u043d 2017 2 39765 39765 \u041f\u0435\u0440\u0432\u044b\u0435 \u0441\u043f\u043e\u043b\u043e\u0445\u0438 \u0432\u043e\u0439\u043d\u044b \u0413\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041f\u043e\u043f\u0430\u0434\u0430\u043d\u0446\u044b,\u0411\u043e\u0435\u0432\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438 \u0412\u043b\u0430\u0434\u0438\u043c\u0438\u0440 \u0421\u0443\u0445\u0438\u043d\u0438\u043d 2018 3 23553 23553 \u0412\u0441\u044f \u043f\u0440\u0430\u0432\u0434\u0430 \u043e \u043d\u0430\u0441 \u0414\u0435\u0442\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u0413\u043e\u0440\u043e\u0434\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041a\u043d\u0438\u0433\u0438 \u043f\u0440... \u041c\u0430\u043a\u0441 \u0424\u0440\u0430\u0439 2015 4 88397 88397 \u041f\u0435\u0448\u043a\u0430 \u0432 \u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u0438\u0433\u0440\u0435 \u0413\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041f\u043e\u043f\u0430\u0434\u0430\u043d\u0446\u044b,\u0411\u043e\u0435\u0432\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438 \u0412\u043b\u0430\u0434\u0438\u043c\u0438\u0440 \u0421\u0443\u0445\u0438\u043d\u0438\u043d 2017 5 45920 45920 \u0412\u0442\u043e\u0440\u0430\u044f \u0436\u0438\u0437\u043d\u044c \u043c\u0430\u0439\u043e\u0440\u0430 \u0413\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041f\u043e\u043f\u0430\u0434\u0430\u043d\u0446\u044b,\u0411\u043e\u0435\u0432\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438 \u0412\u043b\u0430\u0434\u0438\u043c\u0438\u0440 \u0421\u0443\u0445\u0438\u043d\u0438\u043d 2017 6 2028 2028 \u0421\u043a\u043e\u0440\u043f\u0438\u043e\u043d \u0415\u0433\u043e \u0412\u0435\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0413\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041f\u043e\u043f\u0430\u0434\u0430\u043d\u0446\u044b,\u0411\u043e\u0435\u0432\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438 \u0412\u043b\u0430\u0434\u0438\u043c\u0438\u0440 \u0421\u0443\u0445\u0438\u043d\u0438\u043d 2018 7 22066 22066 \u0417\u0430\u043b\u043e\u0436\u043d\u0438\u043a \u0434\u043e\u043b\u0433\u0430 \u0438 \u0447\u0435\u0441\u0442\u0438 \u0413\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041f\u043e\u043f\u0430\u0434\u0430\u043d\u0446\u044b,\u0411\u043e\u0435\u0432\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438 \u0412\u043b\u0430\u0434\u0438\u043c\u0438\u0440 \u0421\u0443\u0445\u0438\u043d\u0438\u043d 2018 8 202547 202547 \u0427\u0443\u0436\u0430\u044f \u0441\u0438\u043b\u0430 \u0413\u043e\u0440\u043e\u0434\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041a\u043d\u0438\u0433\u0438 \u043f\u0440\u043e \u0432\u043e\u043b\u0448\u0435\u0431\u043d\u0438\u043a\u043e\u0432 \u0410\u043d\u0434\u0440\u0435\u0439 \u0412\u0430\u0441\u0438\u043b\u044c\u0435\u0432 2017 9 140991 140991 \u041b\u0430\u0431\u0438\u0440\u0438\u043d\u0442 \u041c\u0451\u043d\u0438\u043d\u0430 (\u0441\u0431\u043e\u0440\u043d\u0438\u043a) \u0414\u0435\u0442\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u0413\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u0413\u043e\u0440\u043e\u0434\u0441... \u041c\u0430\u043a\u0441 \u0424\u0440\u0430\u0439 2000 <p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0438\u0441\u0442\u043e\u0440\u0438\u044e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</p> In\u00a0[306]: Copied! <pre># user read history\nuser_history = test[test[USER_COL] == 21][[ITEM_COL]] # user idx=0, id=21\nuser_history.merge(items, left_on='item_id', right_on='id')\n</pre> # user read history user_history = test[test[USER_COL] == 21][[ITEM_COL]] # user idx=0, id=21 user_history.merge(items, left_on='item_id', right_on='id') Out[306]: item_id id title genres authors year 0 193711 193711 \u0418\u0441\u0442\u043e\u0440\u0438\u044f \u0411\u0435\u0440\u043d\u0430\u0440\u0434\u044b \u0438 \u0422\u0430\u0439\u0440\u044b \u043d\u0430 \u0410\u0440\u0445\u0430\u043d\u0435 \u041b\u044e\u0431\u043e\u0432\u043d\u043e-\u0444\u0430\u043d\u0442\u0430\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0440\u043e\u043c\u0430\u043d\u044b,\u041d\u0430\u0443\u0447\u043d\u0430\u044f \u0444\u0430\u043d\u0442\u0430\u0441\u0442\u0438... \u0412\u0435\u0440\u043e\u043d\u0438\u043a\u0430 \u041c\u0435\u043b\u0430\u043d 2015 1 32399 32399 \u0421\u043f\u0435\u043a\u0442\u0440 \u0421\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u0430\u044f \u0444\u0430\u043d\u0442\u0430\u0441\u0442\u0438\u043a\u0430,\u041a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0444\u0430\u043d\u0442\u0430\u0441\u0442\u0438\u043a\u0430,\u041d... \u0421\u0435\u0440\u0433\u0435\u0439 \u041b\u0443\u043a\u044c\u044f\u043d\u0435\u043d\u043a\u043e 2001-02 2 155015 155015 \u0412\u044b\u0438\u0433\u0440\u0430\u0442\u044c \u0436\u0438\u0437\u043d\u044c. \u0421\u043a\u0430\u0437\u043a\u0438 \u0438\u0437 \u0441\u0443\u043d\u0434\u0443\u043a\u0430 \u042d\u0442\u043d\u043e\u0433\u0440\u0430\u0444\u0438\u044f,\u041a\u043d\u0438\u0433\u0438 \u043e \u043f\u0443\u0442\u0435\u0448\u0435\u0441\u0442\u0432\u0438\u044f\u0445 \u0412\u0438\u0442\u0430\u043b\u0438\u0439 \u0421\u0443\u043d\u0434\u0430\u043a\u043e\u0432 2018 3 121640 121640 \u0417\u043e\u043b\u043e\u0442\u044b\u0435 \u043a\u043e\u0441\u0442\u0440\u044b \u0413\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u0411\u043e\u0435\u0432\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438 \u0410\u043b\u0435\u043a\u0441\u0435\u0439 \u041f\u0435\u0445\u043e\u0432 2012 4 136668 136668 \u0421\u0438\u043d\u0435\u0435 \u043f\u043b\u0430\u043c\u044f \u0413\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u0411\u043e\u0435\u0432\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041a\u043d\u0438\u0433\u0438 \u043f\u0440\u043e \u0432... \u0410\u043b\u0435\u043a\u0441\u0435\u0439 \u041f\u0435\u0445\u043e\u0432 2015 5 11951 11951 \u0422\u0430\u043b\u043e\u0440\u0438\u0441 \u0413\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u0411\u043e\u0435\u0432\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041a\u043d\u0438\u0433\u0438 \u043f\u0440\u043e \u0432... \u0410\u043b\u0435\u043a\u0441\u0435\u0439 \u041f\u0435\u0445\u043e\u0432 2019 6 204089 204089 \u0417\u043c\u0435\u0439\u043a\u0430 \u0413\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u0411\u043e\u0435\u0432\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438 \u0410\u043b\u0435\u043a\u0441\u0435\u0439 \u041f\u0435\u0445\u043e\u0432 2005 7 128170 128170 \u0418\u043d\u0441\u0442\u0438\u043d\u043a\u0442 \u0437\u043b\u0430. \u0422\u0435\u043d\u044c \u042d\u0440\u043e\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u0413\u043e\u0440\u043e\u0434\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041b\u044e\u0431\u043e\u0432\u043d\u043e\u0435... \u041c\u0430\u0440\u0438\u043d\u0430 \u0421\u0443\u0440\u0436\u0435\u0432\u0441\u043a\u0430\u044f 2017 8 31873 31873 \u0418\u043d\u0441\u0442\u0438\u043d\u043a\u0442 \u0417\u043b\u0430. \u0412\u0435\u0440\u0448\u0438\u0442\u0435\u043b\u044c \u042d\u0440\u043e\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u0413\u043e\u0440\u043e\u0434\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041b\u044e\u0431\u043e\u0432\u043d\u043e\u0435... \u041c\u0430\u0440\u0438\u043d\u0430 \u0421\u0443\u0440\u0436\u0435\u0432\u0441\u043a\u0430\u044f 2016 9 131575 131575 \u0418\u043d\u0441\u0442\u0438\u043d\u043a\u0442 \u0417\u043b\u0430. \u0412\u043e\u0437\u0440\u043e\u0436\u0434\u0435\u043d\u043d\u0430\u044f \u042d\u0440\u043e\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u0413\u043e\u0440\u043e\u0434\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041b\u044e\u0431\u043e\u0432\u043d\u043e\u0435... \u041c\u0430\u0440\u0438\u043d\u0430 \u0421\u0443\u0440\u0436\u0435\u0432\u0441\u043a\u0430\u044f 2016 10 257435 257435 \u0417\u0430\u0447\u0435\u043c \u0446\u0432\u0435\u0442\u0435\u0442 \u043b\u043e\u0440\u0438 \u042d\u0440\u043e\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041b\u044e\u0431\u043e\u0432\u043d\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438 \u041c\u0430\u0440\u0438\u043d\u0430 \u0421\u0443\u0440\u0436\u0435\u0432\u0441\u043a\u0430\u044f 2017 11 261889 261889 \u0414\u043b\u044f \u043a\u043e\u0433\u043e \u0446\u0432\u0435\u0442\u0435\u0442 \u043b\u043e\u0440\u0438 \u042d\u0440\u043e\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041b\u044e\u0431\u043e\u0432\u043d\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438 \u041c\u0430\u0440\u0438\u043d\u0430 \u0421\u0443\u0440\u0436\u0435\u0432\u0441\u043a\u0430\u044f 2018 12 303208 303208 \u041e\u0442\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u043d\u0435 \u043c\u0435\u043d\u044f. \u0421\u0435\u0440\u0434\u0446\u0435 \u041e\u0445\u0445\u0430\u0440\u043e\u043d\u0430 \u041b\u044e\u0431\u043e\u0432\u043d\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438 \u041c\u0430\u0440\u0438\u043d\u0430 \u0421\u0443\u0440\u0436\u0435\u0432\u0441\u043a\u0430\u044f 2018 13 192161 192161 \u041f\u0440\u043e\u043d\u0438\u043a\u043d\u043e\u0432\u0435\u043d\u0438\u0435 \u0413\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041b\u044e\u0431\u043e\u0432\u043d\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438 \u041c\u0430\u0440\u0438\u043d\u0430 \u0421\u0443\u0440\u0436\u0435\u0432\u0441\u043a\u0430\u044f 2019 14 117269 117269 \u0412\u0435\u0442\u0435\u0440 \u0421\u0435\u0432\u0435\u0440\u0430. \u0420\u0438\u0432\u0435\u0440\u0441\u0442\u0435\u0439\u043d \u0413\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041b\u044e\u0431\u043e\u0432\u043d\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438 \u041c\u0430\u0440\u0438\u043d\u0430 \u0421\u0443\u0440\u0436\u0435\u0432\u0441\u043a\u0430\u044f 2017 15 132276 132276 \u0412\u0435\u0442\u0435\u0440 \u0421\u0435\u0432\u0435\u0440\u0430. \u0410\u043b\u0430\u0440\u0430\u043d\u0438\u044f \u0413\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041b\u044e\u0431\u043e\u0432\u043d\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438 \u041c\u0430\u0440\u0438\u043d\u0430 \u0421\u0443\u0440\u0436\u0435\u0432\u0441\u043a\u0430\u044f 2018 16 124256 124256 \u041a\u043e\u0440\u043e\u043b\u0435\u0432\u0441\u043a\u0430\u044f \u043a\u0440\u043e\u0432\u044c. \u0421\u043e\u0440\u0432\u0430\u043d\u043d\u044b\u0439 \u0432\u0435\u043d\u0435\u0446 \u0413\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041a\u043d\u0438\u0433\u0438 \u043f\u0440\u043e \u0432\u043e\u043b\u0448\u0435\u0431\u043d\u0438\u043a\u043e\u0432,\u041b\u044e\u0431\u043e... \u0418\u0440\u0438\u043d\u0430 \u041a\u043e\u0442\u043e\u0432\u0430 2016 17 186710 186710 \u041a\u043e\u0440\u043e\u043b\u0435\u0432\u0441\u043a\u0430\u044f \u043a\u0440\u043e\u0432\u044c. \u0421\u043a\u0440\u044b\u0442\u043e\u0435 \u043f\u043b\u0430\u043c\u044f \u0413\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041b\u044e\u0431\u043e\u0432\u043d\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438 \u0418\u0440\u0438\u043d\u0430 \u041a\u043e\u0442\u043e\u0432\u0430 2016 18 264910 264910 \u041a\u043e\u0440\u043e\u043b\u0435\u0432\u0441\u043a\u0430\u044f \u043a\u0440\u043e\u0432\u044c. \u0421\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0435 \u0441\u0443\u0434\u044c\u0431\u044b \u0413\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041a\u043d\u0438\u0433\u0438 \u043f\u0440\u043e \u0432\u043e\u043b\u0448\u0435\u0431\u043d\u0438\u043a\u043e\u0432,\u041b\u044e\u0431\u043e... \u0418\u0440\u0438\u043d\u0430 \u041a\u043e\u0442\u043e\u0432\u0430 2017 19 163270 163270 \u041a\u043e\u0440\u043e\u043b\u0435\u0432\u0441\u043a\u0430\u044f \u043a\u0440\u043e\u0432\u044c. \u0422\u0435\u043c\u043d\u043e\u0435 \u043d\u0430\u0441\u043b\u0435\u0434\u0438\u0435 \u0413\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041b\u044e\u0431\u043e\u0432\u043d\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438 \u0418\u0440\u0438\u043d\u0430 \u041a\u043e\u0442\u043e\u0432\u0430 2018 20 171916 171916 \u041a\u043e\u0440\u043e\u043b\u0435\u0432\u0441\u043a\u0430\u044f \u043a\u0440\u043e\u0432\u044c. \u041e\u0433\u043d\u0435\u043d\u043d\u044b\u0439 \u043f\u0443\u0442\u044c \u0413\u0435\u0440\u043e\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438,\u041b\u044e\u0431\u043e\u0432\u043d\u043e\u0435 \u0444\u044d\u043d\u0442\u0435\u0437\u0438 \u0418\u0440\u0438\u043d\u0430 \u041a\u043e\u0442\u043e\u0432\u0430 2018"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html#practical-example-of-matrix-factorisation","title":"Practical example of Matrix Factorisation\u00b6","text":"<ul> <li>\u0421\u043e\u0433\u043e\u0434\u043d\u044f \u043f\u0440\u0438\u0441\u0442\u0443\u043f\u0438\u043c \u043a \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0447\u0430\u0441\u0442\u0438 \u043f\u043e \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044e \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043c\u0435\u0442\u043e\u0434\u044b \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u043b\u043e\u0436\u0435\u043d\u0438\u044f</li> <li>\u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043c\u0435\u0442\u043e\u0434 \u0440\u0430\u0437\u043b\u043e\u0436\u0435\u043d\u0438\u044f <code>iALS</code></li> <li>\u0414\u0430\u043d\u043d\u044b\u0435 \u0431\u0443\u0434\u0443\u0442 \u0438\u0437 MTS; \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u0447\u0442\u0435\u043d\u0438\u044f \u044d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u043d\u044b\u0445 \u043a\u043d\u0438\u0433, \u043f\u0440\u0435\u0441\u0441\u044b \u0438 \u043f\u0440\u043e\u0441\u043b\u0443\u0448\u0438\u0432\u0430\u043d\u0438\u044f \u0430\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433, \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u043e \u0434\u043b\u044f \u0430\u0431\u043e\u043d\u0435\u043d\u0442\u043e\u0432 \u0432\u0441\u0435\u0445 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u0445 \u043e\u043f\u0435\u0440\u0430\u0442\u043e\u0440\u043e\u0432, \u043f\u0440\u043e\u0434\u0443\u043a\u0442 \u044d\u043a\u043e\u0441\u0438\u0441\u0442\u0435\u043c\u044b \u041c\u0422\u0421</li> </ul> <p>\u0417\u0430\u0442\u0440\u043e\u043d\u0435\u043c 4 \u043f\u0443\u043d\u043a\u0442\u0430:</p> <ul> <li>(1) Implicit feedback \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u0435 \u043f\u043e\u0434\u0445\u043e\u0434</li> <li>(2) Explicit feedback \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u0435 \u043f\u043e\u0434\u0445\u043e\u0434</li> <li>(3) Online (\u041e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 \u0442\u0435\u043a\u0443\u0449\u0435\u0439 score \u043c\u0430\u0442\u0440\u0438\u0446\u044b)</li> <li>(4) \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</li> </ul>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html","title":"\u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0443\u043b\u0438 \u00b6","text":""},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html","title":"\u041e\u0431\u0449\u0438\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u00b6","text":"<p>\u041c\u043e\u0436\u0435\u043c \u0441\u0440\u0430\u0437\u0443 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u0441\u044f \u0441 \u043e\u0431\u0449\u0438\u043c\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u0443\u0434\u0443\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0441\u044f \u0432 \u043d\u043e\u0443\u0442\u0435</p>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html","title":"\u0427\u0442\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<p>\u0414\u043b\u044f \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0440\u0435\u043a\u043e\u043c\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438, \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439</p>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html#exploratory-data-analysis","title":"Exploratory Data Analysis\u00b6","text":"<p>\u041a\u0430\u043a \u0432\u0441\u0435\u0433\u0434\u0430, \u0434\u0430\u0432\u0430\u0439 \u0438\u0441\u043b\u0435\u0434\u0443\u0435\u043c \u043d\u0430\u0448\u0438 \u0434\u0430\u043d\u043d\u044b\u0435</p>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html#user_id-item_id","title":"user_id &amp; item_id\u00b6","text":"<p>\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 <code>user_id</code> \u0438 \u043a\u043d\u0438\u0433 <code>item_id</code> \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435</p>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html#user-feedback","title":"User Feedback\u00b6","text":"<p>\u0423 \u043d\u0430\u0441 \u0434\u0432\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0441 feedback \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u0445;</p> <ul> <li><code>rating</code> (\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u043e\u0441\u0442\u0430\u0432\u0438\u043b \u043e\u0442\u0437\u044b\u0432) [explicit]</li> <li><code>progress</code> (\u0434\u043e\u043b\u044f \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u043d\u043d\u043e\u0439 \u043a\u043d\u0438\u0433\u0438) [implicit]</li> </ul> <p>Explicit</p> <ul> <li>\u043c\u0430\u043b\u043e, \u0438\u043d\u043e\u0433\u0434\u0430 \u043d\u0435\u0434\u043e\u0441\u0442\u0443\u043f\u0435\u043d</li> <li>\u044f\u0432\u043d\u043e \u043e\u0442\u0440\u0430\u0436\u0430\u0435\u0442 \u043f\u0440\u0435\u0434\u043f\u043e\u0447\u0442\u0435\u043d\u0438\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</li> </ul> <p>Implicit</p> <ul> <li>\u043c\u043d\u043e\u0433\u043e</li> <li>\u0440\u0430\u0437\u043d\u043e\u0440\u043e\u0434\u043d\u044b\u0439</li> <li>\u0447\u0430\u0441\u0442\u043e \u043d\u0435 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u043e\u0433\u043e \u043e\u043f\u044b\u0442\u0430/\u0435\u0433\u043e \u043d\u0430\u0434\u043e \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043f\u0440\u0430\u0432\u0438\u043b</li> <li>\u043a\u043e\u0441\u0432\u0435\u043d\u043d\u043e \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u043f\u0440\u0435\u0434\u043f\u043e\u0447\u0442\u0435\u043d\u0438\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439</li> </ul>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html","title":"\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0440\u043e\u0447\u0442\u0435\u043d\u0438\u0439 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439\u00b6","text":""},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html","title":"\u041a\u043e\u043b\u0438\u0447\u0435\u0442\u0441\u0432\u043e \u043f\u0440\u043e\u0447\u0442\u0435\u043d\u0438\u0439 \u043a\u0430\u0436\u0434\u043e\u0439 \u043a\u043d\u0438\u0433\u0438\u00b6","text":""},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html","title":"\u041f\u043e\u0434\u0433\u043e\u0442\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0438\u00b6","text":"<ul> <li>\u041c\u044b \u0443\u0432\u0438\u0434\u0438\u043b\u0438 \u0447\u0442\u043e \u0434\u043b\u044f implicit feedback <code>progress</code> \u0443 \u043d\u0430\u0441 \u043d\u0435\u0442 \u043f\u0440\u043e\u0431\u043b\u0435\u043c \u0441 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0430\u043c\u0438</li> <li>\u0410 \u0434\u043b\u044f \u043a\u043e\u043b\u043e\u043d\u043a\u0438 explicit feedback <code>rating</code> \u0443 \u043d\u0430\u0441 \u043d\u0430\u043c\u043d\u043e\u0433\u043e \u043c\u0435\u043d\u044c\u0448\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0442\u0430\u043a \u043a\u0430\u043a \u0443 \u043d\u0430\u0441 \u043c\u043d\u043e\u0433\u043e \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432,</li> <li>\u0420\u0430\u0437\u043e\u0431\u044a\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 \u0434\u0432\u0435 \u0447\u0430\u0441\u0442\u0438 <code>df_implicit</code> \u0438 <code>df_explicit</code></li> </ul>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html","title":"\u0424\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<p>\u0424\u0438\u043b\u044c\u0442\u0440\u0443\u0435\u043c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439, \u0443 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043c\u0435\u043d\u044c\u0448\u0435 10 \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u043d\u043d\u044b\u0445 \u043a\u043d\u0438\u0433 \u0438 \u043a\u043d\u0438\u0433\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u0440\u043e\u0447\u043b\u0438 \u043c\u0435\u043d\u0435\u0435 20 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439.</p>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html#train-test-split","title":"Train-test-split\u00b6","text":"<p>\u0420\u0430\u0437\u0434\u0435\u043b \u043d\u0430 <code>train</code> / <code>test</code> \u0438\u043c\u0435\u0435\u0442 \u0441\u0432\u043e\u0438 \u043d\u044e\u0430\u043d\u0441\u044b</p> <ul> <li><code>train</code> \u0431\u0443\u0434\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0441\u044f \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f (\u0440\u0430\u0437\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u043c\u0430\u0442\u0440\u0438\u0446\u044b)</li> <li><code>test</code> \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u043d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e <code>train</code> \u043d\u043e \u0438 \u0432\u0441\u0435 \u043a\u0440\u043e\u043c\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0432\u0441\u0435\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439</li> <li><code>test</code> \u0431\u0443\u0434\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0441\u044f \u0432 \u0440\u0430\u0437\u0434\u0435\u043b\u0435 <code>online prediction</code>, \u0432 \u043d\u0435\u043c \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438/\u043f\u0440\u0435\u0434\u043c\u0435\u0442\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u043f\u0430\u043b\u0438 \u0438 \u0432 <code>train</code></li> </ul>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html#last_time","title":"\u0424\u0438\u043b\u044c\u0442\u0440\u0443\u0435\u043c <code>last_time</code>\u00b6","text":"<p>\u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0438\u0437 <code>last_time</code> \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u043b\u0438 \u0431\u043e\u043b\u044c\u0448\u0435 10% \u043a\u043d\u0438\u0433\u0438</p>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html","title":"\u0412\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0424\u0443\u043d\u043a\u0446\u0438\u0438\u00b6","text":"<ul> <li><code>get_sparse_matrix</code> : \u0421\u043e\u0437\u0434\u0430\u0435\u0442 \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u0438 <code>rating_col</code> \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043e\u0432</li> <li><code>get_scores</code> : \u0421\u043a\u0430\u043b\u044f\u0440\u043d\u043e\u0435 \u043f\u0440\u043e\u0438\u0437\u0432\u0438\u0434\u0435\u043d\u0438\u0435 <code>user_id</code> \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0438 <code>item_id</code> \u043c\u0430\u0442\u0440\u0438\u0446\u0443 : score matrix</li> <li><code>predict</code> : Top k \u0440\u0435\u043a\u043e\u043c\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u043f\u043e \u0438\u0442\u043e\u0433\u0443 score matrix</li> <li><code>hr</code> : \u041c\u0435\u0442\u0440\u0438\u043a\u0430 hitrate</li> </ul>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html#a-implicit","title":"(a) Implicit\u00b6","text":"<p>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0434\u043e\u043b\u044e \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u043d\u043d\u044b\u0445 \u043a\u043d\u0438\u0433 \u0434\u043b\u044f <code>rating</code></p>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html#als-decomposition","title":"ALS Decomposition\u00b6","text":"<ul> <li>\u0420\u0430\u0437\u043b\u043e\u0436\u0438\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u043e\u0433\u043e \u043f\u0440\u043e\u0433\u0440\u0435\u0441\u0441\u0430 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u043a\u043d\u0438\u0433 <code>train_sparse</code></li> <li>\u0412\u043e\u0437\u043c\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e 50 \u0444\u0430\u043a\u0442\u043e\u0440\u043e\u0432 \u043a\u0430\u043a \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440</li> </ul>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html#b-explicit","title":"(b) Explicit\u00b6","text":"<p>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0438 \u0442\u043e\u0432\u0430\u0440\u0430</p>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html#als-decomposition","title":"ALS Decomposition\u00b6","text":"<ul> <li>\u0420\u0430\u0437\u043b\u043e\u0436\u0438\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u043e\u0433\u043e \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432 \u043a\u043d\u0438\u0433 <code>train_sparse</code></li> <li>\u0412\u043e\u0437\u043c\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e 50 \u0444\u0430\u043a\u0442\u043e\u0440\u043e\u0432 \u043a\u0430\u043a \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440</li> </ul>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html#c-online-prediction","title":"(c) Online Prediction \u00b6","text":"<ul> <li>\u041f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438 \u0441\u0434\u0435\u043b\u0430\u043b\u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u0438 \u043c\u044b \u0445\u043e\u0442\u0438\u043c \u0434\u0430\u0442\u044c \u0438\u043c \u043d\u043e\u0432\u044b\u0435 \u0440\u0435\u043a\u043e\u043c\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438</li> <li>\u041c\u044b \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u0438\u043b\u0438 \u0432 \u0440\u0430\u0437\u0434\u0435\u043b\u0435 Train-test-split \u0432\u044b\u0431\u043e\u0440\u043a\u0443 <code>test</code> (\u0432 \u043d\u0435\u0439 \u0443 \u043d\u0430\u0441 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442\u0441\u044f <code>train</code> \u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439 \u0442\u0435\u043a\u0443\u0449\u0438\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0447\u0442\u043e \u0438 \u0432 <code>train</code>)</li> <li>\u041d\u043e \u0445\u043e\u0442\u0438\u043c \u0431\u0435\u0437 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 (\u0441\u043d\u043e\u0432\u043e \u0440\u0430\u0437\u043b\u043e\u0436\u0438\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 '\u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432')</li> <li>\u041c\u044b \u043c\u043e\u0436\u0435\u043c \u0434\u0435\u043b\u0430\u0442\u044c $ \\hat{R} = R V_{d} V^{T}_{d}$ \u0442\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0442\u0435\u043a\u0443\u0449\u0438\u044e \u043c\u0430\u0442\u0440\u0438\u0446\u0443</li> </ul>"},{"location":"portfolio/course_recsys/recsys_matrix_decomposition_practice.html#d","title":"(d) \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u00b6","text":"<p>\u0422\u0435\u043f\u0435\u0440\u044c \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u043a \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0440\u0435\u043a\u043e\u043c\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</p>"},{"location":"portfolio/course_recsys/recsys_nlp.html","title":"Recsys nlp","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport warnings; warnings.filterwarnings('ignore')\n\nmovies = pd.read_csv('/kaggle/input/tmdb-movie-metadata/tmdb_5000_movies.csv')\n</pre> import pandas as pd import numpy as np import warnings; warnings.filterwarnings('ignore')  movies = pd.read_csv('/kaggle/input/tmdb-movie-metadata/tmdb_5000_movies.csv') <p>\u0414\u0430\u0442\u0430\u0441\u0435\u0442 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0435 \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a\u0438:</p> <ul> <li><code>budget</code> - \u0431\u044e\u0434\u0436\u0435\u0442, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u044b\u043b \u0441\u043d\u044f\u0442 \u0444\u0438\u043b\u044c\u043c.</li> <li><code>genre</code> - \u0416\u0430\u043d\u0440 \u0444\u0438\u043b\u044c\u043c\u0430, \u0431\u043e\u0435\u0432\u0438\u043a, \u043a\u043e\u043c\u0435\u0434\u0438\u044f, \u0442\u0440\u0438\u043b\u043b\u0435\u0440 \u0438 \u0442.\u0434.</li> <li><code>homepage</code> - \u0421\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u0434\u043e\u043c\u0430\u0448\u043d\u044e\u044e \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u0443 \u0444\u0438\u043b\u044c\u043c\u0430.</li> <li><code>id</code> - \u0424\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u044d\u0442\u043e \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u0444\u0438\u043b\u044c\u043c\u0430.</li> <li><code>keywords</code> - \u041a\u043b\u044e\u0447\u0435\u0432\u044b\u0435 \u0441\u043b\u043e\u0432\u0430 \u0438\u043b\u0438 \u0442\u0435\u0433\u0438, \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0435 \u0441 \u0444\u0438\u043b\u044c\u043c\u043e\u043c.</li> <li><code>original_language</code> - \u042f\u0437\u044b\u043a, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0431\u044b\u043b \u0441\u043d\u044f\u0442 \u0444\u0438\u043b\u044c\u043c.</li> <li><code>original_title</code> - \u041d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0444\u0438\u043b\u044c\u043c\u0430 \u0434\u043e \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0430 \u0438\u043b\u0438 \u0430\u0434\u0430\u043f\u0442\u0430\u0446\u0438\u0438.</li> <li><code>overview</code> - \u041a\u0440\u0430\u0442\u043a\u043e\u0435 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0444\u0438\u043b\u044c\u043c\u0430.</li> <li><code>popularity</code> - \u0427\u0438\u0441\u043b\u043e\u0432\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435, \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0449\u0435\u0435 \u043d\u0430 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u043e\u0441\u0442\u044c \u0444\u0438\u043b\u044c\u043c\u0430.</li> <li><code>production_companies</code> - \u041f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0441\u0442\u0432\u0435\u043d\u043d\u0430\u044f \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u044f \u0444\u0438\u043b\u044c\u043c\u0430.</li> <li><code>production_countries</code> - \u0421\u0442\u0440\u0430\u043d\u0430, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0431\u044b\u043b \u0441\u043d\u044f\u0442 \u0444\u0438\u043b\u044c\u043c.</li> <li><code>release_date</code> - \u0414\u0430\u0442\u0430 \u0432\u044b\u0445\u043e\u0434\u0430 \u0444\u0438\u043b\u044c\u043c\u0430 \u0432 \u043f\u0440\u043e\u043a\u0430\u0442.</li> <li><code>revenue</code> - \u041c\u0438\u0440\u043e\u0432\u043e\u0439 \u0434\u043e\u0445\u043e\u0434, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0439 \u0444\u0438\u043b\u044c\u043c\u043e\u043c.</li> <li><code>runtime</code> - \u0412\u0440\u0435\u043c\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0444\u0438\u043b\u044c\u043c\u0430 \u0432 \u043c\u0438\u043d\u0443\u0442\u0430\u0445.</li> <li><code>status</code> - \"Released\" \u0438\u043b\u0438 \"Rumored\".</li> <li><code>tagline</code> - \u0417\u0430\u0433\u043e\u043b\u043e\u0432\u043e\u043a \u0444\u0438\u043b\u044c\u043c\u0430.</li> <li><code>title</code> - \u041d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0444\u0438\u043b\u044c\u043c\u0430.</li> <li><code>vote_average</code> - \u0441\u0440\u0435\u0434\u043d\u0438\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0439 \u0444\u0438\u043b\u044c\u043c\u043e\u043c.</li> <li><code>vote_count</code> - \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0430\u0431\u0440\u0430\u043d\u043d\u044b\u0445 \u0433\u043e\u043b\u043e\u0441\u043e\u0432.</li> </ul> <p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0441\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u0435</p> In\u00a0[2]: Copied! <pre>movies.head(5)\n</pre> movies.head(5) Out[2]: budget genres homepage id keywords original_language original_title overview popularity production_companies production_countries release_date revenue runtime spoken_languages status tagline title vote_average vote_count 0 237000000 [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam... http://www.avatarmovie.com/ 19995 [{\"id\": 1463, \"name\": \"culture clash\"}, {\"id\":... en Avatar In the 22nd century, a paraplegic Marine is di... 150.437577 [{\"name\": \"Ingenious Film Partners\", \"id\": 289... [{\"iso_3166_1\": \"US\", \"name\": \"United States o... 2009-12-10 2787965087 162.0 [{\"iso_639_1\": \"en\", \"name\": \"English\"}, {\"iso... Released Enter the World of Pandora. Avatar 7.2 11800 1 300000000 [{\"id\": 12, \"name\": \"Adventure\"}, {\"id\": 14, \"... http://disney.go.com/disneypictures/pirates/ 285 [{\"id\": 270, \"name\": \"ocean\"}, {\"id\": 726, \"na... en Pirates of the Caribbean: At World's End Captain Barbossa, long believed to be dead, ha... 139.082615 [{\"name\": \"Walt Disney Pictures\", \"id\": 2}, {\"... [{\"iso_3166_1\": \"US\", \"name\": \"United States o... 2007-05-19 961000000 169.0 [{\"iso_639_1\": \"en\", \"name\": \"English\"}] Released At the end of the world, the adventure begins. Pirates of the Caribbean: At World's End 6.9 4500 2 245000000 [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam... http://www.sonypictures.com/movies/spectre/ 206647 [{\"id\": 470, \"name\": \"spy\"}, {\"id\": 818, \"name... en Spectre A cryptic message from Bond\u2019s past sends him o... 107.376788 [{\"name\": \"Columbia Pictures\", \"id\": 5}, {\"nam... [{\"iso_3166_1\": \"GB\", \"name\": \"United Kingdom\"... 2015-10-26 880674609 148.0 [{\"iso_639_1\": \"fr\", \"name\": \"Fran\\u00e7ais\"},... Released A Plan No One Escapes Spectre 6.3 4466 3 250000000 [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 80, \"nam... http://www.thedarkknightrises.com/ 49026 [{\"id\": 849, \"name\": \"dc comics\"}, {\"id\": 853,... en The Dark Knight Rises Following the death of District Attorney Harve... 112.312950 [{\"name\": \"Legendary Pictures\", \"id\": 923}, {\"... [{\"iso_3166_1\": \"US\", \"name\": \"United States o... 2012-07-16 1084939099 165.0 [{\"iso_639_1\": \"en\", \"name\": \"English\"}] Released The Legend Ends The Dark Knight Rises 7.6 9106 4 260000000 [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam... http://movies.disney.com/john-carter 49529 [{\"id\": 818, \"name\": \"based on novel\"}, {\"id\":... en John Carter John Carter is a war-weary, former military ca... 43.926995 [{\"name\": \"Walt Disney Pictures\", \"id\": 2}] [{\"iso_3166_1\": \"US\", \"name\": \"United States o... 2012-03-07 284139100 132.0 [{\"iso_639_1\": \"en\", \"name\": \"English\"}] Released Lost in our world, found in another. John Carter 6.1 2124 In\u00a0[3]: Copied! <pre>C= movies['vote_average'].mean()\nC.round(2)\n</pre> C= movies['vote_average'].mean() C.round(2) Out[3]: <pre>6.09</pre> <ul> <li>\u0418\u0442\u0430\u043a, \u0441\u0440\u0435\u0434\u043d\u044f\u044f \u043e\u0446\u0435\u043d\u043a\u0430 \u0432\u0441\u0435\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u0441\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u043e\u043a\u043e\u043b\u043e 6 \u043f\u043e 10-\u0431\u0430\u043b\u043b\u044c\u043d\u043e\u0439 \u0448\u043a\u0430\u043b\u0435</li> <li>\u0421\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0448\u0430\u0433 - \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0449\u0435\u0433\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f m (\u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0433\u043e\u043b\u043e\u0441\u043e\u0432, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0433\u043e \u0434\u043b\u044f \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f \u0432 \u0442\u0430\u0431\u043b\u0438\u0446\u0443)</li> <li>\u041c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c 90-\u0439 \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u0438\u043b\u044c \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0442\u0441\u0435\u0447\u043a\u0438. \u0414\u0440\u0443\u0433\u0438\u043c\u0438 \u0441\u043b\u043e\u0432\u0430\u043c\u0438, \u0447\u0442\u043e\u0431\u044b \u0444\u0438\u043b\u044c\u043c \u043f\u043e\u043f\u0430\u043b \u0432 \u0447\u0430\u0440\u0442, \u043e\u043d \u0434\u043e\u043b\u0436\u0435\u043d \u043d\u0430\u0431\u0440\u0430\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u0435 \u0433\u043e\u043b\u043e\u0441\u043e\u0432, \u0447\u0435\u043c \u0445\u043e\u0442\u044f \u0431\u044b 90 % \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u0432 \u0441\u043f\u0438\u0441\u043a\u0435.</li> </ul> In\u00a0[4]: Copied! <pre>m= movies['vote_count'].quantile(0.9)\nm.round(2)\n</pre> m= movies['vote_count'].quantile(0.9) m.round(2) Out[4]: <pre>1838.4</pre> <p>\u0422\u0435\u043f\u0435\u0440\u044c \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u043e\u0442\u0444\u0438\u043b\u044c\u0442\u0440\u043e\u0432\u0430\u0442\u044c \u0444\u0438\u043b\u044c\u043c\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u043f\u0430\u0434\u0430\u044e\u0442 \u0432 \u0442\u0430\u0431\u043b\u0438\u0446\u0443</p> In\u00a0[5]: Copied! <pre>q_movies = movies.copy().loc[movies['vote_count'] &gt;= m]\nq_movies.shape\n</pre> q_movies = movies.copy().loc[movies['vote_count'] &gt;= m] q_movies.shape Out[5]: <pre>(481, 20)</pre> <ul> <li>\u041c\u044b \u0432\u0438\u0434\u0438\u043c, \u0447\u0442\u043e \u0432 \u044d\u0442\u043e\u043c \u0441\u043f\u0438\u0441\u043a\u0435 481 \u0444\u0438\u043b\u044c\u043c.</li> <li>\u0422\u0435\u043f\u0435\u0440\u044c \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0440\u0430\u0441\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043d\u0430\u0448\u0443 \u043c\u0435\u0442\u0440\u0438\u043a\u0443 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0444\u0438\u043b\u044c\u043c\u0430, \u043e\u0442\u0432\u0435\u0447\u0430\u044e\u0449\u0435\u0433\u043e \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u044f\u043c. \u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043c\u044b \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e weighted_rating() \u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u043d\u043e\u0432\u0443\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u044e score</li> </ul> In\u00a0[6]: Copied! <pre>def weighted_rating(x, m=m, C=C):\n    v = x['vote_count']\n    R = x['vote_average']\n    # \u0420\u0430\u0441\u0447\u0435\u0442 \u043f\u043e \u0444\u043e\u0440\u043c\u0443\u043b\u0435 IMDB\n    return (v/(v+m) * R) + (m/(m+v) * C)\n</pre> def weighted_rating(x, m=m, C=C):     v = x['vote_count']     R = x['vote_average']     # \u0420\u0430\u0441\u0447\u0435\u0442 \u043f\u043e \u0444\u043e\u0440\u043c\u0443\u043b\u0435 IMDB     return (v/(v+m) * R) + (m/(m+v) * C) In\u00a0[7]: Copied! <pre># \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u043d\u043e\u0432\u0443\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u044e 'score' \u0438 \u0440\u0430\u0441\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e `weighted_rating()`.\nq_movies['score'] = q_movies.apply(weighted_rating, axis=1)\n</pre> # \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u043d\u043e\u0432\u0443\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u044e 'score' \u0438 \u0440\u0430\u0441\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e `weighted_rating()`. q_movies['score'] = q_movies.apply(weighted_rating, axis=1) <ul> <li>\u041d\u0430\u043a\u043e\u043d\u0435\u0446, \u043e\u0442\u0441\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c DataFrame \u043f\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0443 \u043e\u0446\u0435\u043d\u043a\u0438 \u0438 \u0432\u044b\u0432\u0435\u0434\u0435\u043c \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0433\u043e\u043b\u043e\u0441\u043e\u0432, \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0433\u043e\u043b\u043e\u0441\u043e\u0432 \u0438 \u0432\u0437\u0432\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0438\u043b\u0438 \u043e\u0446\u0435\u043d\u043a\u0443 10 \u043b\u0443\u0447\u0448\u0438\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432</li> </ul> In\u00a0[8]: Copied! <pre>#\u0421\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0430 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0430, \u0440\u0430\u0441\u0441\u0447\u0438\u0442\u0430\u043d\u043d\u043e\u0433\u043e \u0432\u044b\u0448\u0435\nq_movies = q_movies.sort_values('score', ascending=False)\n\n#\u0412\u044b\u0432\u043e\u0434 15 \u043b\u0443\u0447\u0448\u0438\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432\nq_movies[['title', 'vote_count', 'vote_average', 'score']].head(10)\n</pre> #\u0421\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0430 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0430, \u0440\u0430\u0441\u0441\u0447\u0438\u0442\u0430\u043d\u043d\u043e\u0433\u043e \u0432\u044b\u0448\u0435 q_movies = q_movies.sort_values('score', ascending=False)  #\u0412\u044b\u0432\u043e\u0434 15 \u043b\u0443\u0447\u0448\u0438\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 q_movies[['title', 'vote_count', 'vote_average', 'score']].head(10) Out[8]: title vote_count vote_average score 1881 The Shawshank Redemption 8205 8.5 8.059258 662 Fight Club 9413 8.3 7.939256 65 The Dark Knight 12002 8.2 7.920020 3232 Pulp Fiction 8428 8.3 7.904645 96 Inception 13752 8.1 7.863239 3337 The Godfather 5893 8.4 7.851236 95 Interstellar 10867 8.1 7.809479 809 Forrest Gump 7927 8.2 7.803188 329 The Lord of the Rings: The Return of the King 8064 8.1 7.727243 1990 The Empire Strikes Back 5879 8.2 7.697884 In\u00a0[9]: Copied! <pre>pop = movies.sort_values('popularity', ascending=False)\nldf = pop[['title','popularity']].head(6)\nldf.style\\\n    .bar(align='mid',\n         color=['#d65f5f','#d65f5f'])\n</pre> pop = movies.sort_values('popularity', ascending=False) ldf = pop[['title','popularity']].head(6) ldf.style\\     .bar(align='mid',          color=['#d65f5f','#d65f5f']) Out[9]: title popularity 546 Minions 875.581305 95 Interstellar 724.247784 788 Deadpool 514.569956 94 Guardians of the Galaxy 481.098624 127 Mad Max: Fury Road 434.278564 28 Jurassic World 418.708552 <ul> <li>\u0421\u043b\u0435\u0434\u0443\u0435\u0442 \u0438\u043c\u0435\u0442\u044c \u0432 \u0432\u0438\u0434\u0443, \u0447\u0442\u043e \u044d\u0442\u0438 \u0434\u0435\u043c\u043e\u0433\u0440\u0430\u0444\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u043e\u0431\u0449\u0443\u044e \u0442\u0430\u0431\u043b\u0438\u0446\u0443 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u043c\u044b\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439.</li> <li>\u041e\u043d\u0438 \u043d\u0435 \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u044e\u0442 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u044b \u0438 \u0432\u043a\u0443\u0441\u044b \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</li> <li>\u0414\u0430\u043b\u0435\u0435 \u043c\u044b \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b NLP \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0441\u0438\u0441\u0442\u0435\u043c, \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u043c \u0447\u0442\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u0447\u0442\u043e \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u043b \u0444\u0438\u043b\u044c\u043c <code>The Dark Knight Rises</code>, \u0438 \u0442\u0435\u043f\u0435\u0440\u044c \u043c\u044b \u0445\u043e\u0442\u0438\u043c \u043f\u043e\u0441\u043e\u0432\u0435\u0442\u043e\u0432\u0430\u0442\u044c \u043f\u043e\u0445\u043e\u0434\u0438\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u044d\u0442\u043e\u043c\u0443 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044e. \u0418\u043c\u0435\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043e\u0431 \u043e\u0431\u0437\u043e\u0440\u0430\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432<code>overview</code>, \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u043d\u0430\u0439\u0442\u0438 \u043f\u043e\u0445\u043e\u0434\u0438\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u043f\u043e \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u044e, \u0442\u0435. \u0435\u0441\u043b\u0438 \u0432 \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u043c \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0435 \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u0431\u0443\u0434\u0443\u0442 \u043f\u043e\u0445\u043e\u0436\u0438\u043c\u0438, \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0438\u0445 \u0440\u0435\u043a\u0435\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u0442\u044c</li> </ul> In\u00a0[10]: Copied! <pre>movies['overview'].head(5)\n</pre> movies['overview'].head(5) Out[10]: <pre>0    In the 22nd century, a paraplegic Marine is di...\n1    Captain Barbossa, long believed to be dead, ha...\n2    A cryptic message from Bond\u2019s past sends him o...\n3    Following the death of District Attorney Harve...\n4    John Carter is a war-weary, former military ca...\nName: overview, dtype: object</pre> <ul> <li>\u0422\u0435\u043f\u0435\u0440\u044c \u043c\u044b \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u043c \u0432\u0435\u043a\u0442\u043e\u0440\u044b Term Frequency-Inverse Document Frequency (TF-IDF) \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043e\u0431\u0437\u043e\u0440\u0430</li> <li>\u0421\u0443\u0442\u044c \u043f\u043e\u0434\u0445\u043e\u0434\u0430 \u0432 \u0442\u043e\u043c \u0447\u0442\u043e \u043c\u044b \u043f\u0440\u043e\u0431\u0438\u0433\u0430\u044f \u043f\u043e \u0432\u0441\u0435\u043c \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u043c, \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u0432\u0441\u0435 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u0441\u043b\u043e\u0432\u0430/\u0442\u043e\u043a\u0435\u043d\u044b \u0438 \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0438\u0445 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u043e \u0432\u0441\u0435\u0445 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0445; <code>TF</code></li> <li>\u041e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u044d\u0442\u043e\u0433\u043e \u043c\u0435\u0442\u043e\u0434\u0430 \u044d\u0442\u043e \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0442\u0435\u0440\u043c\u0438\u043d \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u0442 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u0441\u043b\u043e\u0432\u0430 (\u0432 \u043a\u0430\u043a\u0438\u0445 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0445 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0432\u0441\u0435\u0445 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0445); <code>IDF</code></li> </ul> In\u00a0[11]: Copied! <pre>from sklearn.feature_extraction.text import TfidfVectorizer\n\n#\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u043e\u0431\u044a\u0435\u043a\u0442 \u0432\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0442\u043e\u0440\u0430 TF-IDF. \u0423\u0434\u0430\u043b\u0438\u043c \u0432\u0441\u0435 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u0435 \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430, \u0442\u0430\u043a\u0438\u0435 \u043a\u0430\u043a 'the', 'a'\ntfidf = TfidfVectorizer(stop_words='english')\n\n#\u0417\u0430\u043c\u0435\u043d\u0438\u043c NaN \u043f\u0443\u0441\u0442\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u043e\u0439\nmovies['overview'] = movies['overview'].fillna('')\n\n#\u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u0442\u0440\u0435\u0431\u0443\u0435\u043c\u0443\u044e \u043c\u0430\u0442\u0440\u0438\u0446\u0443 TF-IDF \u043f\u0443\u0442\u0435\u043c \u043f\u043e\u0434\u0433\u043e\u043d\u043a\u0438 \u0438 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445\ntfidf_matrix = tfidf.fit_transform(movies['overview'])\n\nprint('tfidf matrix size:',tfidf_matrix.shape)\n</pre> from sklearn.feature_extraction.text import TfidfVectorizer  #\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u043e\u0431\u044a\u0435\u043a\u0442 \u0432\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0442\u043e\u0440\u0430 TF-IDF. \u0423\u0434\u0430\u043b\u0438\u043c \u0432\u0441\u0435 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u0435 \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430, \u0442\u0430\u043a\u0438\u0435 \u043a\u0430\u043a 'the', 'a' tfidf = TfidfVectorizer(stop_words='english')  #\u0417\u0430\u043c\u0435\u043d\u0438\u043c NaN \u043f\u0443\u0441\u0442\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u043e\u0439 movies['overview'] = movies['overview'].fillna('')  #\u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u0442\u0440\u0435\u0431\u0443\u0435\u043c\u0443\u044e \u043c\u0430\u0442\u0440\u0438\u0446\u0443 TF-IDF \u043f\u0443\u0442\u0435\u043c \u043f\u043e\u0434\u0433\u043e\u043d\u043a\u0438 \u0438 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445 tfidf_matrix = tfidf.fit_transform(movies['overview'])  print('tfidf matrix size:',tfidf_matrix.shape) <pre>tfidf matrix size: (4803, 20978)\n</pre> <p>\u041c\u044b \u0432\u0438\u0434\u0438\u043c, \u0447\u0442\u043e \u0434\u043b\u044f \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u044f 4800 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u0432 \u043d\u0430\u0448\u0435\u043c \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0431\u044b\u043b\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u043e \u0431\u043e\u043b\u0435\u0435 20 000 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0441\u043b\u043e\u0432.</p> <p>\u0422\u0435\u043f\u0435\u0440\u044c, \u0438\u043c\u0435\u044f \u043d\u0430 \u0440\u0443\u043a\u0430\u0445 \u044d\u0442\u0438 \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u0432\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0439 \u0432\u0441\u0435\u0445 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432, \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c \u043e\u0446\u0435\u043d\u043a\u0443 \u0441\u0445\u043e\u0434\u0441\u0442\u0432\u0430. \u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438, \u0442\u0430\u043a\u0438\u0435 \u043a\u0430\u043a \u0415\u0432\u043a\u043b\u0438\u0434\u043e\u0432\u0430, \u041f\u0438\u0440\u0441\u043e\u043d\u0430 \u0438\u043b\u0438 \u043a\u043e\u0441\u0438\u043d\u0443\u0441\u043d\u0430\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0430. \u0420\u0430\u0437\u043d\u044b\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0445\u043e\u0440\u043e\u0448\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u044e\u0442 \u0432 \u0440\u0430\u0437\u043d\u044b\u0445 \u0441\u0446\u0435\u043d\u0430\u0440\u0438\u044f\u0445, \u0438 \u0447\u0430\u0441\u0442\u043e \u0431\u044b\u0432\u0430\u0435\u0442 \u043f\u043e\u043b\u0435\u0437\u043d\u043e \u043f\u043e\u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c.</p> <p>\u041c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>\u043a\u043e\u0441\u0438\u043d\u0443\u0441\u043e\u0438\u0434\u0430\u043b\u044c\u043d\u043e\u0435 \u0441\u0445\u043e\u0434\u0441\u0442\u0432\u043e</code> \u043c\u0435\u0436\u0434\u0443 \u0434\u0432\u0443\u043c\u044f \u0444\u0438\u043b\u044c\u043c\u0430\u043c\u0438. \u041c\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u043a\u043e\u0441\u0438\u043d\u0443\u0441\u043e\u0438\u0434\u0430\u043b\u044c\u043d\u0443\u044e \u043c\u0435\u0442\u0440\u0438\u043a\u0443, \u043f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u043e\u043d\u043e \u043d\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u0430\u043c\u043f\u043b\u0438\u0442\u0443\u0434\u044b \u0438 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043b\u0435\u0433\u043a\u043e \u0438 \u0431\u044b\u0441\u0442\u0440\u043e \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442\u0441\u044f.</p> <p>\u041f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u043c\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u0438 \u0432\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0442\u043e\u0440 TF-IDF, \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \u0442\u043e\u0447\u0435\u0447\u043d\u043e\u0433\u043e \u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u043d\u0438\u044f \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0434\u0430\u0441\u0442 \u043d\u0430\u043c \u043e\u0446\u0435\u043d\u043a\u0443 \u043a\u043e\u0441\u0438\u043d\u0443\u0441\u043d\u043e\u0433\u043e \u0441\u0445\u043e\u0434\u0441\u0442\u0432\u0430. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>linear_kernel</code> \u043e\u0442 sklearn \u0432\u043c\u0435\u0441\u0442\u043e <code>cosine_similarity</code>, \u0442\u0430\u043a \u043a\u0430\u043a \u044d\u0442\u043e \u0431\u044b\u0441\u0442\u0440\u0435\u0435.</p> In\u00a0[12]: Copied! <pre>from sklearn.metrics.pairwise import linear_kernel\n\ncosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n</pre> from sklearn.metrics.pairwise import linear_kernel  cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix) In\u00a0[13]: Copied! <pre>indices = pd.Series(movies.index,\n                    index=movies['title']).drop_duplicates()\n</pre> indices = pd.Series(movies.index,                     index=movies['title']).drop_duplicates() In\u00a0[14]: Copied! <pre>def get_recommendations(title, cosine_sim=cosine_sim, scores=cosine_sim):\n    \n    # \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u0438\u043d\u0434\u0435\u043a\u0441\u0430 \u0444\u0438\u043b\u044c\u043c\u0430, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0433\u043e \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044e\n    idx = indices[title]\n\n    # \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043e\u0446\u0435\u043d\u043e\u043a \u043f\u0430\u0440\u043d\u043e\u0433\u043e \u0441\u0445\u043e\u0434\u0441\u0442\u0432\u0430 \u0432\u0441\u0435\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u0441 \u044d\u0442\u0438\u043c \u0444\u0438\u043b\u044c\u043c\u043e\u043c\n    sim_scores = list(enumerate(cosine_sim[idx]))\n\n    # \u0421\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0430 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u043f\u043e \u0441\u0442\u0435\u043f\u0435\u043d\u0438 \u0441\u0445\u043e\u0434\u0441\u0442\u0432\u0430\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    \n    #  10 \u0441\u0430\u043c\u044b\u0445 \u043f\u043e\u0445\u043e\u0436\u0438\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432\n    sim_scores = sim_scores[1:11]\n    \n    idx = []; vals = []\n    for i in sim_scores:\n        idx.append(i[0])\n        vals.append(i[1])\n        \n    pd_scores = pd.Series(vals,index=idx,name='similarity')\n    \n    # Get the movie indices\n    movie_indices = [i[0] for i in sim_scores]\n    merged = pd.concat([movies['title'].iloc[movie_indices],pd_scores],axis=1)\n    \n    \n    \n    # Return the top 10 most similar movies\n    return merged.style\\\n    .bar(align='mid',\n         color=['#d65f5f','#d65f5f'])\n</pre> def get_recommendations(title, cosine_sim=cosine_sim, scores=cosine_sim):          # \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u0438\u043d\u0434\u0435\u043a\u0441\u0430 \u0444\u0438\u043b\u044c\u043c\u0430, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0433\u043e \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044e     idx = indices[title]      # \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043e\u0446\u0435\u043d\u043e\u043a \u043f\u0430\u0440\u043d\u043e\u0433\u043e \u0441\u0445\u043e\u0434\u0441\u0442\u0432\u0430 \u0432\u0441\u0435\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u0441 \u044d\u0442\u0438\u043c \u0444\u0438\u043b\u044c\u043c\u043e\u043c     sim_scores = list(enumerate(cosine_sim[idx]))      # \u0421\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0430 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u043f\u043e \u0441\u0442\u0435\u043f\u0435\u043d\u0438 \u0441\u0445\u043e\u0434\u0441\u0442\u0432\u0430     sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)          #  10 \u0441\u0430\u043c\u044b\u0445 \u043f\u043e\u0445\u043e\u0436\u0438\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432     sim_scores = sim_scores[1:11]          idx = []; vals = []     for i in sim_scores:         idx.append(i[0])         vals.append(i[1])              pd_scores = pd.Series(vals,index=idx,name='similarity')          # Get the movie indices     movie_indices = [i[0] for i in sim_scores]     merged = pd.concat([movies['title'].iloc[movie_indices],pd_scores],axis=1)                    # Return the top 10 most similar movies     return merged.style\\     .bar(align='mid',          color=['#d65f5f','#d65f5f']) In\u00a0[15]: Copied! <pre>get_recommendations('The Dark Knight Rises')\n</pre> get_recommendations('The Dark Knight Rises') Out[15]: title similarity 65 The Dark Knight 0.301512 299 Batman Forever 0.298570 428 Batman Returns 0.287851 1359 Batman 0.264461 3854 Batman: The Dark Knight Returns, Part 2 0.185450 119 Batman Begins 0.167996 2507 Slow Burn 0.166829 9 Batman v Superman: Dawn of Justice 0.133740 1181 JFK 0.132197 210 Batman &amp; Robin 0.130455 In\u00a0[16]: Copied! <pre>get_recommendations('The Avengers')\n</pre> get_recommendations('The Avengers') Out[16]: title similarity 7 Avengers: Age of Ultron 0.146374 3144 Plastic 0.122791 1715 Timecop 0.110385 4124 This Thing of Ours 0.107529 3311 Thank You for Smoking 0.106203 3033 The Corruptor 0.097598 588 Wall Street: Money Never Sleeps 0.094084 2136 Team America: World Police 0.092244 1468 The Fountain 0.086643 1286 Snowpiercer 0.086189 In\u00a0[17]: Copied! <pre>from transformers import AutoTokenizer, AutoModel\nimport torch\nimport numpy as np\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load pre-trained LaBSE model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/LaBSE\")\nmodel = AutoModel.from_pretrained(\"sentence-transformers/LaBSE\").to(device)\n</pre> from transformers import AutoTokenizer, AutoModel import torch import numpy as np  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") print(f\"Using device: {device}\")  # Load pre-trained LaBSE model and tokenizer tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/LaBSE\") model = AutoModel.from_pretrained(\"sentence-transformers/LaBSE\").to(device) <pre>Using device: cuda\n</pre> <ul> <li><code>get_labse_vectors_batch</code> \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430 \u0432 <code>overview</code> \u0441\u043e\u043f\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0442\u043e\u043a\u0435\u043d\u0443 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0432\u0443\u044e\u0449\u0438\u0439 \u0432\u0435\u043a\u0442\u043e\u0440 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0438\u0437 \u043e\u0434\u043d\u043e\u0433\u043e \u0438\u0437 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0432\u0445 \u0441\u043b\u043e\u0435\u0432 \u043c\u043e\u0434\u0435\u043b\u0438. \u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430 \u043c\u044b \u0443\u0441\u0440\u0435\u0434\u043d\u044f\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0432\u0441\u0435\u0445 \u0441\u043b\u043e\u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0435\u0441\u0442\u044c \u0432 \u0441\u043b\u043e\u0432\u0430\u0440\u0435.</li> <li>\u0420\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432 \u0432 \u044d\u0442\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 768</li> </ul> In\u00a0[18]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\ndef get_labse_vectors_batch(texts, model, tokenizer, device, batch_size=32):\n    embeddings = []\n\n    num_batches = (len(texts) + batch_size - 1) // batch_size\n\n    for i in tqdm(range(0, len(texts), batch_size), total=num_batches, desc=\"Generating LaBSE vectors\"):\n        batch = texts[i:i+batch_size]\n        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n        embeddings.append(batch_embeddings)\n\n    return np.vstack(embeddings)\n\n\nmovies['overview'] = movies['overview'].fillna('')\noverviews = movies['overview'].tolist()\n\nlabse_vectors = get_labse_vectors_batch(overviews, model, tokenizer, device)\nlabse_vectors.shape\n</pre> import numpy as np import pandas as pd from tqdm import tqdm  def get_labse_vectors_batch(texts, model, tokenizer, device, batch_size=32):     embeddings = []      num_batches = (len(texts) + batch_size - 1) // batch_size      for i in tqdm(range(0, len(texts), batch_size), total=num_batches, desc=\"Generating LaBSE vectors\"):         batch = texts[i:i+batch_size]         inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)         inputs = {k: v.to(device) for k, v in inputs.items()}          with torch.no_grad():             outputs = model(**inputs)          batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()         embeddings.append(batch_embeddings)      return np.vstack(embeddings)   movies['overview'] = movies['overview'].fillna('') overviews = movies['overview'].tolist()  labse_vectors = get_labse_vectors_batch(overviews, model, tokenizer, device) labse_vectors.shape <pre>Generating LaBSE vectors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 151/151 [00:24&lt;00:00,  6.08it/s]\n</pre> Out[18]: <pre>(4803, 768)</pre> In\u00a0[19]: Copied! <pre>labse_vectors.shape\n</pre> labse_vectors.shape Out[19]: <pre>(4803, 768)</pre> In\u00a0[20]: Copied! <pre>from sklearn.metrics.pairwise import linear_kernel\n\nlabse_cosine_sim = linear_kernel(labse_vectors, labse_vectors)\n\nindices = pd.Series(movies.index, index=movies['title']).drop_duplicates()\n</pre> from sklearn.metrics.pairwise import linear_kernel  labse_cosine_sim = linear_kernel(labse_vectors, labse_vectors)  indices = pd.Series(movies.index, index=movies['title']).drop_duplicates() In\u00a0[21]: Copied! <pre>get_recommendations('The Dark Knight Rises', labse_cosine_sim)\n</pre> get_recommendations('The Dark Knight Rises', labse_cosine_sim) Out[21]: title similarity 1253 Kiss of Death 480.728943 65 The Dark Knight 458.324799 1422 The X Files: I Want to Believe 455.618408 600 Killer Elite 453.634796 1209 The Rainmaker 450.117798 982 Run All Night 446.992432 1830 Ride Along 446.656616 987 Dream House 446.387756 3805 Purple Violets 446.120972 210 Batman &amp; Robin 444.635986 In\u00a0[22]: Copied! <pre>from gensim.models import FastText\nimport gensim.downloader as api\nimport numpy as np\n\nmodel = api.load('fasttext-wiki-news-subwords-300')\n</pre> from gensim.models import FastText import gensim.downloader as api import numpy as np  model = api.load('fasttext-wiki-news-subwords-300') <pre>[==================================================] 100.0% 958.5/958.4MB downloaded\n</pre> <p><code>get_weighted_fasttext_vectors_batch</code> \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u0442 \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0432\u0441\u0435\u0445 \u0441\u043b\u043e\u0432 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430, \u0442\u0430\u043a\u0436\u0435 \u0447\u0430\u0441\u0442\u043e \u043c\u044b \u0441\u043e\u0432\u043c\u0435\u0449\u0430\u0435\u043c \u044d\u043c\u0431\u0435\u0434\u0438\u043d\u0433\u0438 \u0441\u0432\u0435\u0441\u0442\u0435 \u0441 \u0432\u0435\u0441\u0430\u043c\u0438 TFIDF \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430</p> In\u00a0[23]: Copied! <pre>def get_weighted_fasttext_vectors_batch(texts, model, tfidf_vectorizer, tfidf_matrix, batch_size=1000):\n    embeddings = []\n    num_batches = (len(texts) + batch_size - 1) // batch_size\n\n    for i in tqdm(range(0, len(texts), batch_size), total=num_batches, desc=\"Generating weighted FastText vectors\"):\n        batch = texts[i:i+batch_size]\n        batch_embeddings = []\n\n        for j, text in enumerate(batch):\n            words = text.split()\n            word_vectors = []\n            weights = []\n\n            for word in words:\n                if word in model and word in tfidf_vectorizer.vocabulary_:\n                    word_vectors.append(model[word])\n                    tfidf_index = tfidf_vectorizer.vocabulary_[word]\n                    weight = tfidf_matrix[i+j, tfidf_index]\n                    weights.append(weight)\n\n            if word_vectors:\n                weighted_vectors = np.array(word_vectors) * np.array(weights)[:, np.newaxis]\n                avg_vector = np.sum(weighted_vectors, axis=0) / np.sum(weights)\n            else:\n                avg_vector = np.zeros(model.vector_size)\n\n            batch_embeddings.append(avg_vector)\n\n        embeddings.extend(batch_embeddings)\n\n    return np.array(embeddings)\n\n\nmovies['overview'] = movies['overview'].fillna('')\n\noverviews = movies['overview'].tolist()\n\ntfidf_vectorizer = TfidfVectorizer(stop_words='english')\ntfidf_matrix = tfidf_vectorizer.fit_transform(overviews)\n\nfasttext_vectors = get_weighted_fasttext_vectors_batch(overviews, model, tfidf_vectorizer, tfidf_matrix)\n\n\nfasttext_vectors.shape\n</pre> def get_weighted_fasttext_vectors_batch(texts, model, tfidf_vectorizer, tfidf_matrix, batch_size=1000):     embeddings = []     num_batches = (len(texts) + batch_size - 1) // batch_size      for i in tqdm(range(0, len(texts), batch_size), total=num_batches, desc=\"Generating weighted FastText vectors\"):         batch = texts[i:i+batch_size]         batch_embeddings = []          for j, text in enumerate(batch):             words = text.split()             word_vectors = []             weights = []              for word in words:                 if word in model and word in tfidf_vectorizer.vocabulary_:                     word_vectors.append(model[word])                     tfidf_index = tfidf_vectorizer.vocabulary_[word]                     weight = tfidf_matrix[i+j, tfidf_index]                     weights.append(weight)              if word_vectors:                 weighted_vectors = np.array(word_vectors) * np.array(weights)[:, np.newaxis]                 avg_vector = np.sum(weighted_vectors, axis=0) / np.sum(weights)             else:                 avg_vector = np.zeros(model.vector_size)              batch_embeddings.append(avg_vector)          embeddings.extend(batch_embeddings)      return np.array(embeddings)   movies['overview'] = movies['overview'].fillna('')  overviews = movies['overview'].tolist()  tfidf_vectorizer = TfidfVectorizer(stop_words='english') tfidf_matrix = tfidf_vectorizer.fit_transform(overviews)  fasttext_vectors = get_weighted_fasttext_vectors_batch(overviews, model, tfidf_vectorizer, tfidf_matrix)   fasttext_vectors.shape <pre>Generating weighted FastText vectors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:03&lt;00:00,  1.53it/s]\n</pre> Out[23]: <pre>(4803, 300)</pre> <p>\u0418\u043c\u0435\u044f \u0444\u0438\u0447\u0443 \u043c\u0430\u0442\u0440\u0438\u0446\u0443 <code>fasttext_vectors</code> \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0441\u0445\u043e\u0436\u043e\u0441\u0442\u0438, \u043a\u0430\u043a \u043c\u044b \u0434\u0435\u043b\u0430\u043b\u0438 \u0438 \u0440\u0430\u043d\u044c\u0448\u0435 <code>fasttext_cosine_sim</code></p> In\u00a0[24]: Copied! <pre>from sklearn.metrics.pairwise import linear_kernel\n\nfasttext_cosine_sim = linear_kernel(fasttext_vectors, fasttext_vectors)\n\nindices = pd.Series(movies.index, index=movies['title']).drop_duplicates()\n</pre> from sklearn.metrics.pairwise import linear_kernel  fasttext_cosine_sim = linear_kernel(fasttext_vectors, fasttext_vectors)  indices = pd.Series(movies.index, index=movies['title']).drop_duplicates() <p>\u0414\u0435\u043b\u0430\u0435\u043c \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u0444\u0438\u043b\u044c\u043c\u0430 <code>The Dark Knight Rise</code></p> In\u00a0[25]: Copied! <pre>get_recommendations('The Dark Knight Rises', fasttext_cosine_sim)\n</pre> get_recommendations('The Dark Knight Rises', fasttext_cosine_sim) Out[25]: title similarity 2435 Running Scared 0.346712 2910 A Tale of Three Cities 0.344737 4367 The Broken Hearts Club: A Romantic Comedy 0.341655 176 The Revenant 0.339179 3475 Casa De Mi Padre 0.337685 3374 Veer-Zaara 0.336817 2807 The Perfect Game 0.336533 935 Herbie Fully Loaded 0.334470 2850 Tales from the Crypt: Demon Knight 0.332519 1293 Frankenweenie 0.331367 <ul> <li>\u041d\u0435\u0441\u043c\u043e\u0442\u0440\u044f \u043d\u0430 \u0442\u043e, \u0447\u0442\u043e \u043d\u0430\u0448\u0430 \u0441\u0438\u0441\u0442\u0435\u043c\u0430 \u043d\u0435\u043f\u043b\u043e\u0445\u043e \u0441\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0441 \u043f\u043e\u0438\u0441\u043a\u043e\u043c \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u0441 \u043f\u043e\u0445\u043e\u0436\u0438\u043c \u0441\u044e\u0436\u0435\u0442\u043e\u043c, \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439 \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0436\u0435\u043b\u0430\u0442\u044c \u043b\u0443\u0447\u0448\u0435\u0433\u043e.</li> <li>\"\u0422\u0451\u043c\u043d\u044b\u0439 \u0440\u044b\u0446\u0430\u0440\u044c: \u0412\u043e\u0437\u0440\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043b\u0435\u0433\u0435\u043d\u0434\u044b\" \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0432\u0441\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u043e \u0411\u044d\u0442\u043c\u0435\u043d\u0435, \u0432 \u0442\u043e \u0432\u0440\u0435\u043c\u044f \u043a\u0430\u043a \u043b\u044e\u0434\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043f\u043e\u043d\u0440\u0430\u0432\u0438\u043b\u0441\u044f \u044d\u0442\u043e\u0442 \u0444\u0438\u043b\u044c\u043c, \u0441\u043a\u043e\u0440\u0435\u0435 \u0432\u0441\u0435\u0433\u043e, \u0431\u043e\u043b\u044c\u0448\u0435 \u0441\u043a\u043b\u043e\u043d\u043d\u044b \u043b\u044e\u0431\u0438\u0442\u044c \u0434\u0440\u0443\u0433\u0438\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u041a\u0440\u0438\u0441\u0442\u043e\u0444\u0435\u0440\u0430 \u041d\u043e\u043b\u0430\u043d\u0430. \u042d\u0442\u043e \u0442\u043e, \u0447\u0442\u043e \u043d\u0435 \u043c\u043e\u0436\u0435\u0442 \u0443\u043b\u043e\u0432\u0438\u0442\u044c \u0442\u0435\u043a\u0443\u0449\u0430\u044f \u0441\u0438\u0441\u0442\u0435\u043c\u0430.</li> </ul>"},{"location":"portfolio/course_recsys/recsys_nlp.html","title":"\u0420\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u044f \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u043f\u043e \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u043c\u0443 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u044e\u00b6","text":""},{"location":"portfolio/course_recsys/recsys_nlp.html#1","title":"1 | \u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/course_recsys/recsys_nlp.html","title":"\u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u0438\u0437 \u0442\u0435\u043a\u0441\u0442\u0430\u00b6","text":"<ul> <li>\u0422\u0435\u043a\u0441\u043e\u0432\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 \u0441\u0435\u0431\u044f \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u043d\u0430\u0431\u043e\u0440 \u043d\u0435\u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445</li> <li>\u0421\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u044e\u0442 \u0440\u0430\u0437\u043d\u044b\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044b \u0434\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f \u043d\u0443\u0436\u043d\u043e\u0439 \u043d\u0430\u043c \u0438\u043d\u043e\u0444\u0440\u043c\u0430\u0446\u0438\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u0430\u0434\u0443\u0442 \u043d\u0430\u043c \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0440\u0435\u043a\u043e\u043c\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445</li> </ul>"},{"location":"portfolio/course_recsys/recsys_nlp.html#nlp","title":"NLP \u043f\u043e\u0434\u0445\u043e\u0434\u044b \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0444\u0438\u0447\u0435\u0439\u00b6","text":"<ul> <li>\u041f\u0435\u0440\u0432\u044b\u0445 \u043f\u043e\u0434\u0445\u043e\u0434 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043c\u044b \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0442\u0435\u043a\u0441\u0442\u0430 \u0441\u043e\u0437\u0434\u0430\u0435\u0442 \u043d\u0430\u0431\u043e\u0440 \u0444\u0438\u0447\u0435\u0439 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u043d \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u0442 \u0432 \u0442\u0435\u043a\u0441\u0442\u0435, \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u044f \u043d\u0443\u0436\u043d\u0443\u044e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u0438\u0437 \u0432\u0441\u0435\u0445 \u0442\u0435\u043a\u0441\u0442\u043e\u0432, \u043c\u044b \u0444\u0438\u043a\u0441\u0438\u0440\u0443\u0435\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043e\u043b\u043e\u043d\u043e\u043a. \u041a\u043e\u043b\u0438\u0447\u043d\u0441\u0442\u0432\u043e \u043a\u043e\u043b\u043e\u043d\u043e\u043a \u043f\u0440\u043e\u043f\u043e\u0440\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u0430 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0443 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0441\u043b\u043e\u0432 \u0432\u043e \u0432\u0441\u0435\u0445 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0445; <code>Bag of Words</code> (BoW) \u043f\u043e\u0434\u0445\u043e\u0434\u044b</li> <li>\u0412\u0442\u043e\u0440\u043e\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u0442\u0430\u043a \u0436\u0435 \u0441\u043e\u0437\u0434\u0430\u0435\u0442 \u043d\u0430\u0431\u043e\u0440  \u0444\u0438\u0447\u0435\u0439, \u043d\u043e \u043c\u044b \u0437\u0430\u0440\u0430\u043d\u0435\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u044d\u0433\u043e \u0440\u0430\u0437\u043c\u0435\u0440. \u041e\u0431\u0443\u0447\u0438\u0432 \u044d\u0442\u0438 \u0444\u0438\u0447\u0438 \u0443 \u043d\u0430\u0441 \u0434\u043b\u044f \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430 \u0435\u0441\u0442\u044c \u0441\u0432\u043e\u0439 \u0432\u0435\u043a\u0442\u043e\u0440 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0439 \u0432 \u043c\u043d\u043e\u0433\u043e\u0432\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u043c \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0435. \u042d\u0442\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u0445\u0440\u0430\u043d\u044f\u0442\u0441\u044f \u0432 \u0441\u0430\u043c\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0438\u0445 \u043c\u043e\u0436\u043d\u043e \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c \u043f\u0440\u0438 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438, \u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043d\u0430 \u043d\u043e\u0432\u044b\u0445 \u0434\u043e\u043a\u0443\u043c\u0430\u0435\u0442\u0430\u0445. \u041f\u043e\u0434\u0430\u0432\u0430\u044f \u043d\u043e\u0432\u044b\u0435 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u044b \u0432 \u043c\u043e\u0434\u0435\u043b\u044c \u043c\u044b \u043e\u0431\u044b\u0447\u043d\u043e \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0430\u0440\u0438\u0444\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432 \u0432\u0441\u0435\u0445 \u0441\u043b\u043e\u0432 \u0432 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0435, \u0442\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c \u043f\u043e\u043b\u0443\u0447\u0430\u044f \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430 \u0444\u0438\u043a\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u0432\u0435\u0440\u043a\u043e\u0440; <code>\u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432\u044b\u0439</code> \u043f\u043e\u0434\u0445\u043e\u0434</li> </ul>"},{"location":"portfolio/course_recsys/recsys_nlp.html","title":"\u041e\u0446\u0435\u043d\u043a\u0430 \u0441\u0445\u043e\u0436\u043e\u0441\u0442\u0438\u00b6","text":"<ul> <li>\u041a\u0430\u043a \u0438 \u0440\u0430\u043d\u044c\u0448\u0435, \u043f\u043e\u043b\u0443\u0447\u0438\u0432 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0439 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430, \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043e\u0446\u0435\u043d\u043a\u0443 \u0441\u0445\u043e\u0436\u043e\u0441\u0442\u0438, \u044d\u0442\u043e \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c <code>cosine_similarity</code> \u0438 \u0442\u0434.</li> <li>\u041c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0443\u043f\u0440\u043e\u0449\u0435\u043d\u043d\u044b\u0439 \u0432\u0430\u0440\u0438\u0430\u043d\u0442 <code>linear_kernel</code></li> </ul>"},{"location":"portfolio/course_recsys/recsys_nlp.html#2","title":"2 | \u0427\u0442\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<p>\u0414\u0430\u0432\u0430\u0439\u0442\u0435 \u0442\u0435\u043f\u0435\u0440\u044c \u0437\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435. \u0414\u043b\u044f \u044d\u0442\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 \u043c\u044b \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0434\u0430\u043d\u043d\u044b\u0435 \u043a\u0440\u0430\u0442\u043a\u0438\u0445 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0438 \u0440\u0430\u0437\u043d\u044b\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432. \u0418 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u044d\u0442\u0438\u0445 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0431\u0443\u0434\u0435\u043c \u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0440\u0435\u043a\u043e\u043c</p>"},{"location":"portfolio/course_recsys/recsys_nlp.html#3","title":"3 | \u041f\u043e\u0434\u0445\u043e\u0434 \u0412\u0437\u0432\u0435\u0448\u0435\u043d\u043d\u043e\u0433\u043e \u0420\u0435\u0439\u0442\u0438\u043d\u0433\u0430\u00b6","text":"<p>\u041d\u0435 \u0437\u0430\u0431\u0438\u0433\u0430\u044f \u0432\u043f\u0435\u0440\u0435\u0434, \u0432\u0441\u043f\u043e\u043c\u043d\u0438\u043c \u043a\u0430\u043a \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u043d\u0435\u043a\u0438\u0439 \u0431\u0435\u0438\u0441\u043b\u0430\u0439\u043d \u0434\u043b\u044f \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438;</p> <ul> <li>C\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043e\u0434\u043d\u0443 \u0438 \u0442\u0443 \u0436\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u044e</li> </ul> <p>\u0424\u0438\u043b\u044c\u043c\u043e\u0432 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u043c\u043d\u043e\u0433\u043e, \u043d\u043e \u043d\u0435 \u0432\u0441\u0435 \u043c\u044b \u0445\u043e\u0442\u0435\u043b\u0438 \u0431\u044b \u0440\u0435\u043a\u043e\u043c\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u0442\u044c, \u043e\u0434\u0438\u043d \u0438\u0437 \u043f\u0440\u043e\u0441\u0442\u0435\u0439\u0448\u0438\u0445 \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u0432 \u044d\u0442\u043e \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u044f \u043d\u0430 \u043e\u0441\u0442\u043d\u043e\u0432\u0435 \u043a\u0430\u043a\u0438\u0445 \u0442\u043e \u043c\u0435\u0442\u0440\u0438\u043a</p> <ul> <li>\u041f\u0440\u0435\u0436\u0434\u0435 \u0447\u0435\u043c \u043f\u0440\u0438\u0441\u0442\u0443\u043f\u0438\u0442\u044c \u043a \u0440\u0430\u0431\u043e\u0442\u0435, \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043c\u0435\u0442\u0440\u0438\u043a\u0443 \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u0444\u0438\u043b\u044c\u043c\u0430</li> <li>\u0420\u0430\u0441\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043e\u0446\u0435\u043d\u043a\u0443 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0444\u0438\u043b\u044c\u043c\u0430, \u043e\u0442\u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043e\u0446\u0435\u043d\u043a\u0438 \u0438 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u0442\u044c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c \u0444\u0438\u043b\u044c\u043c\u044b \u0441 \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u043c \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u043c</li> <li>\u041c\u044b \u043c\u043e\u0436\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>\u0441\u0440\u0435\u0434\u043d\u0438\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433</code> \u0444\u0438\u043b\u044c\u043c\u0430 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0446\u0435\u043d\u043a\u0438, \u043d\u043e \u044d\u0442\u043e \u0431\u0443\u0434\u0435\u0442 \u043d\u0435 \u0441\u043e\u0432\u0441\u0435\u043c \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u043e, \u0442\u0430\u043a \u043a\u0430\u043a \u0444\u0438\u043b\u044c\u043c \u0441\u043e \u0441\u0440\u0435\u0434\u043d\u0438\u043c \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u043c 8,9 \u0438 \u0432\u0441\u0435\u0433\u043e 3 \u0433\u043e\u043b\u043e\u0441\u0430\u043c\u0438 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442 \u0441\u0447\u0438\u0442\u0430\u0442\u044c\u0441\u044f \u043b\u0443\u0447\u0448\u0438\u043c, \u0447\u0435\u043c \u0444\u0438\u043b\u044c\u043c \u0441\u043e \u0441\u0440\u0435\u0434\u043d\u0438\u043c \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u043c 7,8, \u043d\u043e 40 \u0433\u043e\u043b\u043e\u0441\u0430\u043c\u0438</li> <li>\u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0446\u0435\u043d\u043a\u0438 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>\u0432\u0437\u0432\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433</code> IMDB (wr)</li> </ul>"},{"location":"portfolio/course_recsys/recsys_nlp.html#4-tfidf","title":"4 | \u041f\u043e\u0434\u0445\u043e\u0434 TFIDF \u0412\u0435\u043a\u0442\u043e\u0440\u043e\u0432\u00b6","text":"<ul> <li>\u041c\u044b \u0440\u0430\u0441\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043f\u043e\u043f\u0430\u0440\u043d\u044b\u0435 \u043e\u0446\u0435\u043d\u043a\u0438 \u0441\u0445\u043e\u0434\u0441\u0442\u0432\u0430 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u044f \u0438\u0445 \u0441\u044e\u0436\u0435\u0442\u0430 \u0438 \u0431\u0443\u0434\u0435\u043c \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u0442\u044c \u0444\u0438\u043b\u044c\u043c\u044b \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u044d\u0442\u0438\u0445 \u043e\u0446\u0435\u043d\u043e\u043a \u0441\u0445\u043e\u0434\u0441\u0442\u0432\u0430.</li> <li>\u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0441\u044e\u0436\u0435\u0442\u0430 \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u043e \u0432 <code>overview</code> \u0444\u0438\u0447\u0435 \u043d\u0430\u0448\u0435\u0433\u043e \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445. \u0414\u0430\u0432\u0430\u0439\u0442\u0435 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0435.</li> </ul>"},{"location":"portfolio/course_recsys/recsys_nlp.html#5","title":"5 | \u041f\u043e\u0434\u0445\u043e\u0434 \u042d\u043c\u0431\u0435\u0434\u0438\u043d\u0433\u043e\u0432 \u042f\u0437\u044b\u043a\u043e\u0432\u043e\u0439 \u041c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<ul> <li>\u0414\u0430\u043b\u043b\u0435\u0435 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0432\u0430\u0440\u0438\u0430\u043d\u0442 \u0443\u0436\u0435 \u0441 <code>\u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0430\u043c\u0438</code></li> <li>\u041c\u044b \u043c\u043e\u0436\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u044f\u0437\u043e\u043a\u043e\u0432\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u0441\u043e\u043f\u0430\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0441\u043b\u043e\u0432\u0430\u043c (\u0438\u0437 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432) \u043d\u0435\u043a\u0438\u0439 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433 \u0432\u0435\u043a\u0442\u043e\u0440 \u0438 \u0443\u0441\u0440\u0435\u0434\u043d\u0438\u0442\u044c, \u043f\u043e\u043b\u0443\u0447\u0438\u0432 \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u0444\u0438\u0447\u0435 <code>overview</code></li> <li>\u041e\u0434\u043d\u0430 \u0438\u0437 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u044d\u0442\u043e \u043c\u043e\u0434\u0435\u043b\u0442 <code>LaBSE</code>, \u0432 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u044d\u043c\u0431\u0435\u043b\u0434\u0434\u0438\u043d\u0433\u0438 \u0441\u043b\u043e\u0432 \u0440\u0430\u0437\u043c\u0435\u0440\u0430 768</li> <li>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u043e\u0439 <code>transformers</code> \u0434\u043b\u044f \u0431\u044b\u0441\u0442\u0440\u043e\u0439 \u043f\u043e\u0434\u0433\u0440\u0443\u0437\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u044d\u0433\u043e \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0442\u043e\u0440\u0430. \u0422\u043e\u043a\u0435\u043d\u0438\u0449\u0430\u0442\u043e\u0440 \u043d\u0430\u043c \u043d\u0443\u0436\u0435\u043d \u0431\u0443\u0434\u0435\u0442 \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u0440\u0430\u0437\u0431\u0438\u0442\u044c \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u044b \u043d\u0430 \u0447\u0430\u0441\u0442\u0438, \u0441\u043e\u0437\u0434\u0430\u0432 \u0432\u0432\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 <code>inputs</code>, \u043c\u044b \u0442\u0430\u043a \u0436\u0435 \u0432 \u0443\u0441\u043b\u043e\u0432\u0438\u0438 \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u0438 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>truncate</code> \u0438 \u043c\u0430\u043a\u0441\u043c\u043c\u0430\u043b\u044c\u043d\u0443\u044e \u0434\u043b\u0438\u043d\u0443 512</li> </ul>"},{"location":"portfolio/course_recsys/recsys_nlp.html#6-fasttext","title":"6 | \u041f\u043e\u0434\u0445\u043e\u0434 FastText \u042d\u043c\u0431\u0435\u0434\u0438\u043d\u0433\u043e\u0432\u00b6","text":"<ul> <li>\u041f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u043c\u044b \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u044d\u0442\u043e \u043f\u043e\u0434\u0445\u043e\u0434 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0445 <code>fasttext</code> \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432.</li> <li>\u0420\u0435\u0448\u0430\u044f \u0437\u0430\u0434\u0430\u0447\u0443 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442\u0430 \u043d\u0430 \u043e\u0441\u0442\u043d\u043e\u0432\u0435 \u0441\u0442\u0440\u0435\u0434\u043d\u0435\u0433\u043e \u0441\u043b\u043e\u0432\u0430 \u0438 \u043d\u0430 \u043e\u0441\u0442\u043d\u043e\u0432\u0435 \u0441\u0440\u0435\u0434\u043d\u0435\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442\u043d\u044b\u0445 \u0441\u043b\u043e\u0432 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u044c Fasttext \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043d\u0430\u043c \u043e\u0431\u0443\u0447\u0438\u044c \u044d\u043c\u0431\u0435\u0434\u0438\u043d\u0433\u043e\u0432\u044b\u0439 \u0441\u043b\u043e\u0439.</li> <li>\u041a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043c\u0431\u0435\u0434\u0438\u043d\u0433\u043e\u0432\u044b\u0445 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432 \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u043a\u043e\u0440\u043f\u0443\u0441\u0430 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u043c\u044b \u0438\u0445 \u043e\u0431\u0443\u0447\u0438\u043b\u0438</li> <li>\u0412 \u0434\u0430\u043d\u043d\u043e\u043c \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u044d\u043c\u0431\u0435\u0434\u0438\u043d\u0433\u0438 \u043e\u0431\u0443\u0435\u043d\u044b \u043d\u0430 \u043a\u043e\u0440\u043f\u0443\u0441\u0435 \u043d\u043e\u0432\u043e\u0441\u0442\u0435\u0439 <code>fasttext-wiki-news-subwords-300</code> \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u0441\u043b\u043e\u0432 \u0438\u043c\u0435\u044e\u0442 \u0440\u0430\u0437\u043c\u0435\u0440 300 \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432</li> </ul>"},{"location":"portfolio/course_recsys/recsys_rule_knn.html","title":"Recsys rule knn","text":"In\u00a0[1]: Copied! <pre>import warnings\nwarnings.simplefilter('ignore')\nfrom collections import Counter\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.base import BaseEstimator\nfrom sklearn.metrics.pairwise import cosine_similarity\n</pre> import warnings warnings.simplefilter('ignore') from collections import Counter  import pandas as pd import numpy as np import matplotlib.pyplot as plt from tqdm import tqdm_notebook from sklearn.metrics import mean_squared_error from sklearn.base import BaseEstimator from sklearn.metrics.pairwise import cosine_similarity In\u00a0[2]: Copied! <pre>ratings = pd.read_csv('./ml-latest-small/ratings.csv', parse_dates=['timestamp'])\nratings.head()\n</pre> ratings = pd.read_csv('./ml-latest-small/ratings.csv', parse_dates=['timestamp']) ratings.head() Out[2]: userId movieId rating timestamp 0 1 1 4.0 964982703 1 1 3 4.0 964981247 2 1 6 4.0 964982224 3 1 47 5.0 964983815 4 1 50 5.0 964982931 In\u00a0[3]: Copied! <pre># \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043e\u0446\u0435\u043d\u043e\u043a \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439\nratings.rating.value_counts()\n</pre> # \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043e\u0446\u0435\u043d\u043e\u043a \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 ratings.rating.value_counts() Out[3]: <pre>4.0    26818\n3.0    20047\n5.0    13211\n3.5    13136\n4.5     8551\n2.0     7551\n2.5     5550\n1.0     2811\n1.5     1791\n0.5     1370\nName: rating, dtype: int64</pre> In\u00a0[4]: Copied! <pre>len(ratings) / (ratings.userId.nunique() * ratings.movieId.nunique())\n</pre> len(ratings) / (ratings.userId.nunique() * ratings.movieId.nunique()) Out[4]: <pre>0.016999683055613623</pre> In\u00a0[5]: Copied! <pre>ratings['timestamp'] = ratings['timestamp'].astype(int)\n</pre> ratings['timestamp'] = ratings['timestamp'].astype(int) In\u00a0[6]: Copied! <pre>def train_test_split(X):\n    \n    X.sort_values(by=['timestamp'], inplace=True) \n    # \u0444\u0438\u043b\u044c\u043c\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u0434\u0430\u043b\u044c \u043e\u0446\u0435\u043d\u043a\u0443\n    movies = X.groupby(['userId'], sort=False)['movieId'].apply(list).reset_index() \n    # \u043e\u0446\u0435\u043d\u043a\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u0434\u0430\u043b \n    ratings = X.groupby(['userId'], sort=False)['rating'].apply(list).reset_index() \n    # \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\n    X = movies.merge(ratings)\n    \n    train = pd.DataFrame(X['userId'])  # \u043e\u0442\u0434\u0435\u043b\u044f\u0435\u043c \u0432\u0441\u0435 \u043a\u0440\u043e\u043c\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0433\u043e \u043e\u0446\u0435\u043d\u0435\u043d\u043d\u043e\u0433\u043e \u0444\u0438\u043b\u044c\u043c\u0430 \u0432 train\n    train['movieId'] = X.movieId.apply(lambda x: x[:-1]) \n    train['rating'] = X.rating.apply(lambda x: x[:-1]) \n    \n    # \u043e\u0442\u0434\u0435\u043b\u044f\u0435\u043c \u0432\u0441\u0435 \u043a\u0440\u043e\u043c\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0433\u043e \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0438 \u0432 train\n    train = train.explode(['movieId', 'rating'])\n\n    \"\"\"\n    \n    \u0422\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0412\u044b\u0431\u043e\u0440\u043a\u0430 (\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u043f\u043e\u043b\u044c\u0437.)\n    \n    \"\"\"    \n    test = pd.DataFrame(X['userId'])\n    test['movieId'] = X.movieId.apply(lambda x: x[-1:]) # \u043e\u0442\u0434\u0435\u043b\u044f\u0435\u043c \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u0444\u0438\u043b\u044c\u043c \u0432 test\n    test['rating'] = X.rating.apply(lambda x: x[-1:]) # \u043e\u0442\u0434\u0435\u043b\u044f\u0435\u043c \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0432 test\n    test = test.explode(['movieId', 'rating'])\n    \n    return train, test\n</pre> def train_test_split(X):          X.sort_values(by=['timestamp'], inplace=True)      # \u0444\u0438\u043b\u044c\u043c\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u0434\u0430\u043b\u044c \u043e\u0446\u0435\u043d\u043a\u0443     movies = X.groupby(['userId'], sort=False)['movieId'].apply(list).reset_index()      # \u043e\u0446\u0435\u043d\u043a\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u0434\u0430\u043b      ratings = X.groupby(['userId'], sort=False)['rating'].apply(list).reset_index()      # \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f     X = movies.merge(ratings)          train = pd.DataFrame(X['userId'])  # \u043e\u0442\u0434\u0435\u043b\u044f\u0435\u043c \u0432\u0441\u0435 \u043a\u0440\u043e\u043c\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0433\u043e \u043e\u0446\u0435\u043d\u0435\u043d\u043d\u043e\u0433\u043e \u0444\u0438\u043b\u044c\u043c\u0430 \u0432 train     train['movieId'] = X.movieId.apply(lambda x: x[:-1])      train['rating'] = X.rating.apply(lambda x: x[:-1])           # \u043e\u0442\u0434\u0435\u043b\u044f\u0435\u043c \u0432\u0441\u0435 \u043a\u0440\u043e\u043c\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0433\u043e \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0438 \u0432 train     train = train.explode(['movieId', 'rating'])      \"\"\"          \u0422\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0412\u044b\u0431\u043e\u0440\u043a\u0430 (\u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u043f\u043e\u043b\u044c\u0437.)          \"\"\"         test = pd.DataFrame(X['userId'])     test['movieId'] = X.movieId.apply(lambda x: x[-1:]) # \u043e\u0442\u0434\u0435\u043b\u044f\u0435\u043c \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u0444\u0438\u043b\u044c\u043c \u0432 test     test['rating'] = X.rating.apply(lambda x: x[-1:]) # \u043e\u0442\u0434\u0435\u043b\u044f\u0435\u043c \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0432 test     test = test.explode(['movieId', 'rating'])          return train, test In\u00a0[7]: Copied! <pre>train, test = train_test_split(ratings)\nprint(test.shape)\n</pre> train, test = train_test_split(ratings) print(test.shape) <pre>(610, 3)\n</pre> In\u00a0[8]: Copied! <pre>test = test[test['rating'] == 5]\ntest.shape\n</pre> test = test[test['rating'] == 5] test.shape Out[8]: <pre>(117, 3)</pre> In\u00a0[9]: Copied! <pre>test\n</pre> test Out[9]: userId movieId rating 2 191 673 5.0 8 40 685 5.0 11 136 188 5.0 21 385 750 5.0 24 584 60 5.0 ... ... ... ... 581 382 55247 5.0 600 296 4144 5.0 602 556 112852 5.0 604 258 4995 5.0 606 25 187593 5.0 <p>117 rows \u00d7 3 columns</p> In\u00a0[10]: Copied! <pre># mean reciprocal rate\ndef mrr(df: pd.DataFrame, pred_col='preds', true_col='true') -&gt; float:\n    mrr_values = []\n    for _, row in df.iterrows():\n      try:\n        user_mrr = 1 / (row[pred_col].index(row[true_col]) + 1)\n      except ValueError:\n        user_mrr = 0\n      mrr_values.append(user_mrr)\n    return np.mean(mrr_values)\n\n# hit rate \ndef hr(df: pd.DataFrame, pred_col='preds', true_col='true') -&gt; float:\n    hr_values = []\n    for _, row in df.iterrows():\n      hr_values.append(int(row[true_col] in row[pred_col]))\n    return np.mean(hr_values)\n\n# normalised discounted cumulative gain\ndef ndcg(df: pd.DataFrame, pred_col='preds', true_col='true') -&gt; float:\n    # ideal dcg == 1 \u043f\u0440\u0438 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0438 \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u044f leave-one-out\n    ndcg_values = []\n    for _, row in df.iterrows():\n      try:\n        user_ndcg = 1 / np.log2(row[pred_col].index(row[true_col]) + 2)\n      except ValueError:\n        user_ndcg = 0\n      ndcg_values.append(user_ndcg)\n    return np.mean(ndcg_values)\n\n\ndef coverage(train_df: pd.DataFrame, pred_df: pd.DataFrame, item_id='item_id', pred_col='preds') -&gt; float:\n    total_items_num = train_df[item_id].nunique()\n    pred_items_num = len(set.union(*pred_df[pred_col].map(lambda x: set(x))))\n    return pred_items_num / total_items_num\n</pre> # mean reciprocal rate def mrr(df: pd.DataFrame, pred_col='preds', true_col='true') -&gt; float:     mrr_values = []     for _, row in df.iterrows():       try:         user_mrr = 1 / (row[pred_col].index(row[true_col]) + 1)       except ValueError:         user_mrr = 0       mrr_values.append(user_mrr)     return np.mean(mrr_values)  # hit rate  def hr(df: pd.DataFrame, pred_col='preds', true_col='true') -&gt; float:     hr_values = []     for _, row in df.iterrows():       hr_values.append(int(row[true_col] in row[pred_col]))     return np.mean(hr_values)  # normalised discounted cumulative gain def ndcg(df: pd.DataFrame, pred_col='preds', true_col='true') -&gt; float:     # ideal dcg == 1 \u043f\u0440\u0438 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0438 \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u044f leave-one-out     ndcg_values = []     for _, row in df.iterrows():       try:         user_ndcg = 1 / np.log2(row[pred_col].index(row[true_col]) + 2)       except ValueError:         user_ndcg = 0       ndcg_values.append(user_ndcg)     return np.mean(ndcg_values)   def coverage(train_df: pd.DataFrame, pred_df: pd.DataFrame, item_id='item_id', pred_col='preds') -&gt; float:     total_items_num = train_df[item_id].nunique()     pred_items_num = len(set.union(*pred_df[pred_col].map(lambda x: set(x))))     return pred_items_num / total_items_num In\u00a0[11]: Copied! <pre>def calculate_metrics(train_df: pd.DataFrame, pred_df: pd.DataFrame, item_id='item_id', pred_col='preds', true_col='true'):\n    print(f'mrr = {mrr(pred_df, true_col=true_col).round(4)}')\n    print(f'hr = {hr(pred_df, true_col=true_col).round(4)}')\n    print(f'ndcg = {ndcg(pred_df, true_col=true_col).round(4)}')\n    print(f'coverage = {round(coverage(train_df, pred_df, item_id=item_id),4)}')\n</pre> def calculate_metrics(train_df: pd.DataFrame, pred_df: pd.DataFrame, item_id='item_id', pred_col='preds', true_col='true'):     print(f'mrr = {mrr(pred_df, true_col=true_col).round(4)}')     print(f'hr = {hr(pred_df, true_col=true_col).round(4)}')     print(f'ndcg = {ndcg(pred_df, true_col=true_col).round(4)}')     print(f'coverage = {round(coverage(train_df, pred_df, item_id=item_id),4)}') In\u00a0[12]: Copied! <pre>k = 10\n\n# \u0444\u0438\u043b\u044c\u043c, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \ncount_items = Counter(train.movieId)\ncount_items = [*count_items.items()]\ncount_items.sort(key=lambda x: x[1], reverse=True)\n</pre> k = 10  # \u0444\u0438\u043b\u044c\u043c, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e  count_items = Counter(train.movieId) count_items = [*count_items.items()] count_items.sort(key=lambda x: x[1], reverse=True) In\u00a0[13]: Copied! <pre># top 10 \u0441\u0430\u043c\u044b\u0445 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u0432\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \npred_items = [k for k, v in count_items[:k]]\npred_items\n</pre> # top 10 \u0441\u0430\u043c\u044b\u0445 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u0432\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432  pred_items = [k for k, v in count_items[:k]] pred_items Out[13]: <pre>[356, 318, 296, 593, 2571, 260, 110, 480, 589, 2959]</pre> In\u00a0[14]: Copied! <pre>pred = test.copy()\npred['preds'] = [pred_items] * len(pred)\npred\n</pre> pred = test.copy() pred['preds'] = [pred_items] * len(pred) pred Out[14]: userId movieId rating preds 2 191 673 5.0 [356, 318, 296, 593, 2571, 260, 110, 480, 589,... 8 40 685 5.0 [356, 318, 296, 593, 2571, 260, 110, 480, 589,... 11 136 188 5.0 [356, 318, 296, 593, 2571, 260, 110, 480, 589,... 21 385 750 5.0 [356, 318, 296, 593, 2571, 260, 110, 480, 589,... 24 584 60 5.0 [356, 318, 296, 593, 2571, 260, 110, 480, 589,... ... ... ... ... ... 581 382 55247 5.0 [356, 318, 296, 593, 2571, 260, 110, 480, 589,... 600 296 4144 5.0 [356, 318, 296, 593, 2571, 260, 110, 480, 589,... 602 556 112852 5.0 [356, 318, 296, 593, 2571, 260, 110, 480, 589,... 604 258 4995 5.0 [356, 318, 296, 593, 2571, 260, 110, 480, 589,... 606 25 187593 5.0 [356, 318, 296, 593, 2571, 260, 110, 480, 589,... <p>117 rows \u00d7 4 columns</p> <p>\u0414\u043e\u0441\u0442\u0430\u0435\u043c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043e \u0444\u0438\u043b\u043c\u0435 \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043c\u044b \u0440\u0435\u043a\u043e\u043c\u043c\u0435\u043d\u0434\u0443\u0435\u043c, \u0432\u0441\u0435\u043c \u0444\u0438\u043b\u044c\u043c\u0430\u043c \u043f\u043e\u0441\u0442\u0430\u0432\u0438\u043b\u0438 \u043e\u0446\u0435\u043d\u043a\u0443 5</p> In\u00a0[15]: Copied! <pre>movies = pd.read_csv('./ml-latest-small/movies.csv')\nmovies[movies['movieId'].isin(pred_items)]\n</pre> movies = pd.read_csv('./ml-latest-small/movies.csv') movies[movies['movieId'].isin(pred_items)] Out[15]: movieId title genres 97 110 Braveheart (1995) Action|Drama|War 224 260 Star Wars: Episode IV - A New Hope (1977) Action|Adventure|Sci-Fi 257 296 Pulp Fiction (1994) Comedy|Crime|Drama|Thriller 277 318 Shawshank Redemption, The (1994) Crime|Drama 314 356 Forrest Gump (1994) Comedy|Drama|Romance|War 418 480 Jurassic Park (1993) Action|Adventure|Sci-Fi|Thriller 507 589 Terminator 2: Judgment Day (1991) Action|Sci-Fi 510 593 Silence of the Lambs, The (1991) Crime|Horror|Thriller 1939 2571 Matrix, The (1999) Action|Sci-Fi|Thriller 2226 2959 Fight Club (1999) Action|Crime|Drama|Thriller In\u00a0[16]: Copied! <pre>calculate_metrics(train, pred, item_id='movieId', true_col='movieId')\n</pre> calculate_metrics(train, pred, item_id='movieId', true_col='movieId') <pre>mrr = 0.0218\nhr = 0.0598\nndcg = 0.0306\ncoverage = 0.001\n</pre> In\u00a0[17]: Copied! <pre>count_items = Counter(train[train.rating == 5]['movieId'])\ncount_items = [*count_items.items()]\ncount_items.sort(key=lambda x: x[1], reverse=True)\n</pre> count_items = Counter(train[train.rating == 5]['movieId']) count_items = [*count_items.items()] count_items.sort(key=lambda x: x[1], reverse=True) In\u00a0[18]: Copied! <pre>pred_items = [k for k, v in count_items[:k]]\n\npred = test.copy()\npred['preds'] = [pred_items] * len(pred)\n</pre> pred_items = [k for k, v in count_items[:k]]  pred = test.copy() pred['preds'] = [pred_items] * len(pred) In\u00a0[19]: Copied! <pre>movies = pd.read_csv('./ml-latest-small/movies.csv')\nmovies[movies['movieId'].isin(pred_items)]\n</pre> movies = pd.read_csv('./ml-latest-small/movies.csv') movies[movies['movieId'].isin(pred_items)] Out[19]: movieId title genres 224 260 Star Wars: Episode IV - A New Hope (1977) Action|Adventure|Sci-Fi 257 296 Pulp Fiction (1994) Comedy|Crime|Drama|Thriller 277 318 Shawshank Redemption, The (1994) Crime|Drama 314 356 Forrest Gump (1994) Comedy|Drama|Romance|War 461 527 Schindler's List (1993) Drama|War 510 593 Silence of the Lambs, The (1991) Crime|Horror|Thriller 659 858 Godfather, The (1972) Crime|Drama 898 1196 Star Wars: Episode V - The Empire Strikes Back... Action|Adventure|Sci-Fi 1939 2571 Matrix, The (1999) Action|Sci-Fi|Thriller 2226 2959 Fight Club (1999) Action|Crime|Drama|Thriller In\u00a0[20]: Copied! <pre>calculate_metrics(train, pred, item_id='movieId', true_col='movieId')\n</pre> calculate_metrics(train, pred, item_id='movieId', true_col='movieId') <pre>mrr = 0.0233\nhr = 0.0598\nndcg = 0.0318\ncoverage = 0.001\n</pre> In\u00a0[21]: Copied! <pre># \u043c\u0430\u043f\u043f\u0438\u043d\u0433 id \u0432 \u0438\u043d\u0434\u0435\u043a\u0441\u044b\ndef get_mapping(data):\n    \n    # \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u044b\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438 \u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\n    user_ids = data['userId'].unique().tolist()\n      # \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u0432 \u043d\u0430\u0448\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\n    item_ids = data['movieId'].unique().tolist()\n\n    n_users = len(user_ids)\n    n_items = len(item_ids)\n\n    user_idx = range(n_users)\n    item_idx = range(n_items)\n    # \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u0434\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u0444\u0438\u043b\u044c\u043c\u043e\u0432\n    user_mapping = dict(zip(user_ids, user_idx)) # {user_id: user_ind}\n    item_mapping = dict(zip(item_ids, item_idx)) # {item_id: item_ind}\n    return user_mapping, item_mapping\n\nuser_mapping, item_mapping = get_mapping(ratings)\ntrain['userId'] = train['userId'].map(user_mapping)\ntest['userId'] = test['userId'].map(user_mapping)\ntrain['movieId'] = train['movieId'].map(item_mapping)\ntest['movieId'] = test['movieId'].map(item_mapping)\n</pre> # \u043c\u0430\u043f\u043f\u0438\u043d\u0433 id \u0432 \u0438\u043d\u0434\u0435\u043a\u0441\u044b def get_mapping(data):          # \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u044b\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438 \u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0435     user_ids = data['userId'].unique().tolist()       # \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u0432 \u043d\u0430\u0448\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435     item_ids = data['movieId'].unique().tolist()      n_users = len(user_ids)     n_items = len(item_ids)      user_idx = range(n_users)     item_idx = range(n_items)     # \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u0434\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u0444\u0438\u043b\u044c\u043c\u043e\u0432     user_mapping = dict(zip(user_ids, user_idx)) # {user_id: user_ind}     item_mapping = dict(zip(item_ids, item_idx)) # {item_id: item_ind}     return user_mapping, item_mapping  user_mapping, item_mapping = get_mapping(ratings) train['userId'] = train['userId'].map(user_mapping) test['userId'] = test['userId'].map(user_mapping) train['movieId'] = train['movieId'].map(item_mapping) test['movieId'] = test['movieId'].map(item_mapping) <p>\u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c \u0444\u043e\u0440\u043c\u0443\u043b\u0430 \u0441\u0445\u043e\u0436\u0435\u0441\u0442\u0438 \u0434\u0432\u0443\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439:</p> <p>$$     \\textit{sim(u, v)} = \\frac     {\\sum_i{\\big(r_{ui} \\times r_{vi}\\big)}}     {\\sqrt{\\sum_i{r_{ui}^2}} \\times \\sqrt{\\sum_i{r_{vi}^2}}} $$</p> <p>\u0418\u043d\u0442\u0443\u0438\u0442\u0438\u0432\u043d\u043e \u043f\u043e\u043d\u044f\u0442\u043d\u043e, \u0447\u0442\u043e \u043f\u0440\u0435\u0434\u043f\u043e\u043b\u0430\u0433\u0430\u0435\u043c\u044b\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0434\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u043c\u043e\u0436\u043d\u043e \u043e\u0446\u0435\u043d\u0438\u0442\u044c \u043a\u0430\u043a \u0441\u0440\u0435\u0434\u043d\u0438\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u043c\u0435\u0436\u0434\u0443 \u0441\u0445\u043e\u0436\u0438\u043c\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c\u0438, \u043d\u043e, \u0431\u043b\u0430\u0433\u043e\u0434\u0430\u0440\u044f \u0432\u0432\u0435\u0434\u0435\u043d\u0438\u044e \u043f\u043e\u043d\u044f\u0442\u0438\u044f \u0441\u0445\u043e\u0436\u0435\u0441\u0442\u0438, \u043c\u043e\u0436\u043d\u043e \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u044d\u0442\u0443 \u043e\u0446\u0435\u043d\u043a\u0443, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0432\u0437\u0432\u0435\u0448\u0435\u043d\u043d\u044b\u0435 \u0432\u0435\u0441\u0430 \u0438 \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u044f \u0432\u0441\u0435\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u043b\u0438 \u044d\u0442\u043e\u0442 \u0444\u0438\u043b\u044c\u043c.</p> <p>\u0418\u0442\u0430\u043a, \u043f\u0440\u0438\u0431\u043b\u0438\u0436\u0430\u0435\u043c \u043d\u043e\u0432\u044b\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u043a\u0430\u043a \u0441\u0443\u043c\u043c\u0443 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432 \u0434\u0440\u0443\u0433\u0438\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439, \u0432\u0437\u0432\u0435\u0448\u0435\u043d\u043d\u044b\u0445 \u0432\u0435\u0441\u0430\u043c\u0438 (\u0442. \u0435. \u043f\u043e\u0445\u043e\u0436\u0435\u0441\u0442\u044c\u044e):</p> <p>$$     r_{ui} = \\frac     {\\sum_{v \\in User_i}\\big(\\textit{sim(u, v)} \\times r_{vi}\\big)}     {\\sum_{v \\in User_i}\\textit{|sim(u, v)|}} $$</p> In\u00a0[22]: Copied! <pre>\"\"\"\n\nUserKNN\n\n\"\"\"\n\ndef userKNN(train_data, test_data, k_neighb, k):\n    \n    \"\"\"\n    \n    Rating Matrix for each user (row)\n    \n    \"\"\"\n    \n    # shape = n_users * n_items\n    R = pd.pivot_table(train_data, \n                       values='rating', \n                       index='userId', \n                       columns='movieId', fill_value=0) \n    \n    \"\"\"\n    \n    Matrix of similarity for each user (row)\n    \n    \"\"\"\n    \n    # \u0421\u0445\u043e\u0434\u0441\u0442\u0432\u043e \u043c\u0435\u0436\u0434\u0443 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c\u0438\n    user_sim = cosine_similarity(R) # shape = n_users * n_users\n    \n    # \u0440\u0430\u0441\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 similarity\n    # plt.hist(user_sim.flatten(), bins=30);\n    \n    # \u0434\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\n    preds = []\n    for _, row in tqdm_notebook(test_data.iterrows()):\n        \n        user = int(row['userId']) # current user row\n        \n        # sort and reverse similarity vector values for current row user (exclude itself)\n        # choose the number of similar users \n        neighb_inds = np.argsort(user_sim[user])[::-1][1: k_neighb + 1] # argument \n        neighb_sim = np.sort(user_sim[user])[::-1][1: k_neighb + 1] # value of neighbours\n\n        # .astype(bool) : \u0424\u0430\u043a\u0442 \u0442\u043e\u0433\u043e \u0447\u0442\u043e \u0431\u044b\u043b\u0430 \u043e\u0446\u0435\u043d\u043a\u0430\n        # rating of indicies \n        \n        # multiply each neighbour movie rating vector by coefficient of its similarity to the user\n        sum_ratings = (R.iloc[neighb_inds] * neighb_sim.reshape(k_neighb, -1)).sum(axis=0) # numerator\n        sum_sim = (R.iloc[neighb_inds].astype(bool) * neighb_sim.reshape(k_neighb, -1)).sum(axis=0) # denominator\n        user_ratings = sum_ratings / (sum_sim + 1e-10)\n        \n        user_ratings = pd.DataFrame(user_ratings, columns=['pred']).reset_index()\n        user_ratings.sort_values(by=['pred'], ascending=False, inplace=True)\n        preds.append(user_ratings['movieId'][:k].tolist())\n\n    preds_data = test_data.copy()\n    preds_data['preds'] = preds\n    return preds_data\n</pre> \"\"\"  UserKNN  \"\"\"  def userKNN(train_data, test_data, k_neighb, k):          \"\"\"          Rating Matrix for each user (row)          \"\"\"          # shape = n_users * n_items     R = pd.pivot_table(train_data,                         values='rating',                         index='userId',                         columns='movieId', fill_value=0)           \"\"\"          Matrix of similarity for each user (row)          \"\"\"          # \u0421\u0445\u043e\u0434\u0441\u0442\u0432\u043e \u043c\u0435\u0436\u0434\u0443 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c\u0438     user_sim = cosine_similarity(R) # shape = n_users * n_users          # \u0440\u0430\u0441\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 similarity     # plt.hist(user_sim.flatten(), bins=30);          # \u0434\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435     preds = []     for _, row in tqdm_notebook(test_data.iterrows()):                  user = int(row['userId']) # current user row                  # sort and reverse similarity vector values for current row user (exclude itself)         # choose the number of similar users          neighb_inds = np.argsort(user_sim[user])[::-1][1: k_neighb + 1] # argument          neighb_sim = np.sort(user_sim[user])[::-1][1: k_neighb + 1] # value of neighbours          # .astype(bool) : \u0424\u0430\u043a\u0442 \u0442\u043e\u0433\u043e \u0447\u0442\u043e \u0431\u044b\u043b\u0430 \u043e\u0446\u0435\u043d\u043a\u0430         # rating of indicies                   # multiply each neighbour movie rating vector by coefficient of its similarity to the user         sum_ratings = (R.iloc[neighb_inds] * neighb_sim.reshape(k_neighb, -1)).sum(axis=0) # numerator         sum_sim = (R.iloc[neighb_inds].astype(bool) * neighb_sim.reshape(k_neighb, -1)).sum(axis=0) # denominator         user_ratings = sum_ratings / (sum_sim + 1e-10)                  user_ratings = pd.DataFrame(user_ratings, columns=['pred']).reset_index()         user_ratings.sort_values(by=['pred'], ascending=False, inplace=True)         preds.append(user_ratings['movieId'][:k].tolist())      preds_data = test_data.copy()     preds_data['preds'] = preds     return preds_data In\u00a0[23]: Copied! <pre>k_neighb = 5\npred = userKNN(train, test, k_neighb, k)\n</pre> k_neighb = 5 pred = userKNN(train, test, k_neighb, k) <pre>0it [00:00, ?it/s]</pre> In\u00a0[24]: Copied! <pre>pred.head()\n</pre> pred.head() Out[24]: userId movieId rating preds 2 2 134 5.0 [112, 129, 66, 82, 349, 77, 116, 187, 113, 117] 8 8 210 5.0 [264, 107, 200, 219, 78, 238, 35, 33, 112, 277] 11 11 215 5.0 [64, 246, 256, 248, 128, 20, 347, 187, 190, 194] 21 21 817 5.0 [264, 128, 48, 617, 717, 704, 960, 814, 535, 631] 24 24 41 5.0 [55, 64, 346, 107, 9, 440, 68, 33, 16, 106] In\u00a0[25]: Copied! <pre>calculate_metrics(train, pred, item_id='movieId', true_col='movieId')\n</pre> calculate_metrics(train, pred, item_id='movieId', true_col='movieId') <pre>mrr = 0.0182\nhr = 0.0598\nndcg = 0.0277\ncoverage = 0.0598\n</pre> In\u00a0[26]: Copied! <pre># \u0441\u0441\u044b\u043b\u0430\u0435\u043c\u0441\u044f \u043a \u0434\u0430\u043d\u043d\u044b\u043c \u0438\u0437\ntrain.head()\n</pre> # \u0441\u0441\u044b\u043b\u0430\u0435\u043c\u0441\u044f \u043a \u0434\u0430\u043d\u043d\u044b\u043c \u0438\u0437 train.head() Out[26]: userId movieId rating 0 0 0 5.0 0 0 1 5.0 0 0 2 5.0 0 0 3 5.0 0 0 4 3.0 In\u00a0[27]: Copied! <pre>\"\"\"\n\nitemKNN\n\n\"\"\"\n\ndef itemKNN(train_data, test_data, k_neighb, k):\n    \n    # shape = n_users * n_items\n    R = pd.pivot_table(train_data, \n                       values='rating',\n                       index='userId', \n                       columns='movieId', fill_value=0) \n    \n    # movies vs user cosine similarity\n    item_sim = cosine_similarity(R.T) # shape = n_items * n_items\n    \n    preds = []\n    for _, row in tqdm_notebook(test_data.iterrows()):\n    \n        \"\"\"\n        \n        User interactions subset selection (user_interactions)\n        \n        \"\"\"\n    \n        user = row['userId']\n        user_interactions = train_data[train_data['userId'] == user] # user rating subset\n        user_interactions = user_interactions[user_interactions['rating'] == 5] # select ratings of 5 only\n\n        \"\"\"\n        \n        For each user interaction row\n        \n        \tuserId\tmovieId\trating\n        0\t     0\t      0\t   5.0  \n        \n        \"\"\"\n        \n        # for each user interaction row (user ratings of 5 only)\n        item_preds = pd.DataFrame()\n        for _, row in user_interactions.iterrows():\n            \n            \"\"\"\n            \n            Find top k similar movies \n            \n            \"\"\"\n            \n            # find the most similar movies (index and value)\n            item = row['movieId']  # index of movie \n            \n            # find similar to it movies index and value\n            neighb_inds = np.argsort(item_sim[item])[::-1][1: k_neighb + 1] \n            neighb_sim = np.sort(item_sim[item])[::-1][1: k_neighb + 1] \n\n            sum_ratings = (R.T.iloc[neighb_inds] * neighb_sim.reshape(k_neighb, -1)).sum(axis=1) # numerator\n            sum_sim = (R.T.iloc[neighb_inds].astype(bool) * neighb_sim.reshape(k_neighb, -1)).sum(axis=1) # denominator\n            item_ratings = sum_ratings / (sum_sim + 1e-10)\n            \n            item_ratings = pd.DataFrame(item_ratings, columns=['pred']).reset_index()\n            item_preds = pd.concat([item_preds, item_ratings])\n\n        item_preds.sort_values(by=['pred'], ascending=False, inplace=True)\n        preds.append(item_preds['movieId'][:k].tolist())\n\n    preds_data = test_data.copy()\n    preds_data['preds'] = preds\n    return preds_data\n</pre> \"\"\"  itemKNN  \"\"\"  def itemKNN(train_data, test_data, k_neighb, k):          # shape = n_users * n_items     R = pd.pivot_table(train_data,                         values='rating',                        index='userId',                         columns='movieId', fill_value=0)           # movies vs user cosine similarity     item_sim = cosine_similarity(R.T) # shape = n_items * n_items          preds = []     for _, row in tqdm_notebook(test_data.iterrows()):              \"\"\"                  User interactions subset selection (user_interactions)                  \"\"\"              user = row['userId']         user_interactions = train_data[train_data['userId'] == user] # user rating subset         user_interactions = user_interactions[user_interactions['rating'] == 5] # select ratings of 5 only          \"\"\"                  For each user interaction row                  \tuserId\tmovieId\trating         0\t     0\t      0\t   5.0                    \"\"\"                  # for each user interaction row (user ratings of 5 only)         item_preds = pd.DataFrame()         for _, row in user_interactions.iterrows():                          \"\"\"                          Find top k similar movies                           \"\"\"                          # find the most similar movies (index and value)             item = row['movieId']  # index of movie                           # find similar to it movies index and value             neighb_inds = np.argsort(item_sim[item])[::-1][1: k_neighb + 1]              neighb_sim = np.sort(item_sim[item])[::-1][1: k_neighb + 1]               sum_ratings = (R.T.iloc[neighb_inds] * neighb_sim.reshape(k_neighb, -1)).sum(axis=1) # numerator             sum_sim = (R.T.iloc[neighb_inds].astype(bool) * neighb_sim.reshape(k_neighb, -1)).sum(axis=1) # denominator             item_ratings = sum_ratings / (sum_sim + 1e-10)                          item_ratings = pd.DataFrame(item_ratings, columns=['pred']).reset_index()             item_preds = pd.concat([item_preds, item_ratings])          item_preds.sort_values(by=['pred'], ascending=False, inplace=True)         preds.append(item_preds['movieId'][:k].tolist())      preds_data = test_data.copy()     preds_data['preds'] = preds     return preds_data In\u00a0[28]: Copied! <pre>pred = itemKNN(train, test, k_neighb, k)\n</pre> pred = itemKNN(train, test, k_neighb, k) <pre>0it [00:00, ?it/s]</pre> In\u00a0[29]: Copied! <pre>pred.head()\n</pre> pred.head() Out[29]: userId movieId rating preds 2 2 134 5.0 [67, 67, 67, 67, 67, 67, 132, 132, 132, 108] 8 8 210 5.0 [112, 349, 817, 678, 114, 114, 372, 601, 264, ... 11 11 215 5.0 [673, 814, 121, 121, 121, 166, 246, 246, 161, ... 21 21 817 5.0 [1497, 1698, 717, 617, 673, 673, 545, 114, 372... 24 24 41 5.0 [112, 372, 814, 121, 121, 121, 166, 166, 246, ... In\u00a0[30]: Copied! <pre>calculate_metrics(train, pred, item_id='movieId', true_col='movieId')\n</pre> calculate_metrics(train, pred, item_id='movieId', true_col='movieId') <pre>mrr = 0.0105\nhr = 0.0427\nndcg = 0.018\ncoverage = 0.0355\n</pre> <p>\u041a\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u0437\u0430\u043c\u0435\u0442\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u044b \u0438\u043c\u0435\u044e\u0442 \u043e\u0431\u0449\u0438\u0435 \u043d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043a\u0438, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440:</p> <ol> <li>\u041e\u043d\u0438 \u043d\u0435 \u043c\u043e\u0433\u0443\u0442 \u0440\u0435\u0448\u0438\u0442\u044c \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0443 \u0445\u043e\u043b\u043e\u0434\u043d\u043e\u0433\u043e \u0441\u0442\u0430\u0440\u0442\u0430, \u043a\u043e\u0433\u0434\u0430 \u0443 \u043d\u0430\u0441 \u043d\u0435\u0442 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435 \u0438\u043b\u0438 \u0442\u043e\u0432\u0430\u0440\u0435.</li> <li>\u0414\u043b\u044f \u043d\u0435\u0442\u0438\u043f\u0438\u0447\u043d\u044b\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438\u043b\u0438 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043e\u0432 \u0441\u043b\u043e\u0436\u043d\u043e \u0434\u0430\u0442\u044c \u043e\u0431\u044a\u0435\u043a\u0442\u0438\u0432\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f, \u0442\u0430\u043a \u043a\u0430\u043a \u043e\u043d\u0438 \u043c\u0430\u043b\u043e \u043f\u043e\u0445\u043e\u0436\u0438 \u043d\u0430 \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0445.</li> </ol> <p>\u0421\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u044e\u0442 \u0438 \u0431\u043e\u043b\u0435\u0435 \u0441\u043b\u043e\u0436\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043a\u043e\u043b\u043b\u0430\u0431\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u043e\u0439 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438. \u041a\u0440\u043e\u043c\u0435 \u0442\u043e\u0433\u043e \u0435\u0441\u0442\u044c \u0438 \u0434\u0440\u0443\u0433\u0438\u0435 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u044b:</p> <ul> <li>\u041a\u043e\u043d\u0442\u0435\u043d\u0442\u043d\u0430\u044f \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u044f</li> <li>\u0421\u043c\u0435\u0448\u0430\u043d\u043d\u044b\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044b $-$ \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u044e\u0442 \u0432 \u0441\u0435\u0431\u0435 content based \u0438 collaborative \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438.</li> </ul>"},{"location":"portfolio/course_recsys/recsys_rule_knn.html","title":"\u042d\u0432\u0440\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438. \u041a\u043e\u043b\u043b\u0430\u0431\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u0430\u044f \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u044f\u00b6","text":""},{"location":"portfolio/course_recsys/recsys_rule_knn.html","title":"\u0414\u0430\u0442\u0430\u0441\u0435\u0442 \u0440\u0435\u0438\u0442\u0438\u043d\u0433\u043e\u0432 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u00b6","text":"<p>\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043e\u0442 <code>GroupLens</code> $-$ <code>MovieLens</code>: \u042d\u0442\u043e \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0437 $27 000$ \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u0438 $138 000$ \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439, \u0441 \u043e\u0431\u0449\u0438\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e\u043c \u043e\u0446\u0435\u043d\u043e\u043a \u0432 $20$ \u043c\u0438\u043b\u043b\u0438\u043e\u043d\u043e\u0432.</p> <p>\u041d\u043e \u043c\u044b \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0443\u043c\u0435\u043d\u044c\u0448\u0435\u043d\u043d\u043e\u0439 \u0432\u0435\u0440\u0441\u0438\u0435\u0439 \u0434\u043b\u044f \u0431\u044b\u0441\u0442\u0440\u043e\u0442\u044b \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439: $9 000$ \u0444\u0438\u043b\u044c\u043c\u043e\u0432, $700$ \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439, $100 000$ \u043e\u0446\u0435\u043d\u043e\u043a. \u0421\u043a\u0430\u0447\u0430\u0442\u044c \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043c\u043e\u0436\u043d\u043e \u043f\u043e \u044d\u0442\u043e\u0439 \u0441\u0441\u044b\u043b\u043a\u0435</p>"},{"location":"portfolio/course_recsys/recsys_rule_knn.html","title":"\u0414\u043b\u044f \u043d\u0430\u0447\u0430\u043b\u0430 \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0435\u00b6","text":"<ul> <li><code>links.csv</code> $-$ \u0441\u0432\u044f\u0437\u044c \u043c\u0435\u0436\u0434\u0443 <code>id</code> \u0444\u0438\u043b\u044c\u043c\u0430 \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u0438 <code>id</code> \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0433\u043e \u0444\u0438\u043b\u044c\u043c\u0430 \u043d\u0430 <code>imdb.com</code> \u0438 <code>themoviedb.org</code>;</li> <li><code>movies.csv</code> $-$ \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0444\u0438\u043b\u044c\u043c\u0430 \u0441 \u0435\u0433\u043e \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435\u043c \u0438 \u0436\u0430\u043d\u0440\u0430\u043c\u0438;</li> <li><code>ratings.csv</code> $-$ \u043e\u0446\u0435\u043d\u043a\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u0441 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u043e\u0442\u043c\u0435\u0442\u043a\u043e\u0439;</li> <li><code>tags.csv</code> $-$ \u0441\u043f\u0438\u0441\u043e\u043a \u0442\u0435\u0433\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u0441\u0442\u0430\u0432\u0438\u043b \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u0444\u0438\u043b\u044c\u043c\u0443, \u0441 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u043e\u0442\u043c\u0435\u0442\u043a\u043e\u0439.</li> </ul> <p>\u0414\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 \u043d\u0430\u043c \u043f\u043e\u043d\u0430\u0434\u043e\u0431\u044f\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u0447\u0430\u0441\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0445 $-$ \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u043e \u0442\u043e\u043c, \u043a\u0430\u043a\u043e\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0441\u0442\u0430\u0432\u0438\u043b\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438 \u0444\u0438\u043b\u044c\u043c\u0430\u043c.</p>"},{"location":"portfolio/course_recsys/recsys_rule_knn.html","title":"\u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430\u00b6","text":""},{"location":"portfolio/course_recsys/recsys_rule_knn.html#leave-one-out-train-test","title":"Leave-one-out \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 train \u0438 test\u00b6","text":"<p>\u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0431\u0435\u0440\u0435\u043c \u0432\u0441\u0435 \u043a\u0440\u043e\u043c\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0433\u043e \u043e\u0442\u0437\u044b\u0432\u0430 \u0438 \u043a\u043b\u0430\u0434\u0435\u043c \u0438\u0445 \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443. \u0412 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0437\u0430\u043a\u0438\u0434\u044b\u0432\u0430\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u044e\u044e \u043e\u0446\u0435\u043d\u0443</p>"},{"location":"portfolio/course_recsys/recsys_rule_knn.html","title":"\u0422\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0412\u044b\u0431\u043e\u0440\u043a\u0430\u00b6","text":"<p>\u041c\u044b \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u0444\u0438\u043b\u044c\u043c\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u043e\u043b\u0436\u043d\u044b \u043f\u043e\u043d\u0440\u0430\u0432\u0438\u0442\u044c\u0441\u044f, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0442\u044c \u0431\u0443\u0434\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043d\u0430 \u0444\u0438\u043b\u044c\u043c\u0430\u0445 \u0441 \u0432\u044b\u0441\u043e\u043a\u0438\u043c \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u043c \u043e\u0442 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f.</p>"},{"location":"portfolio/course_recsys/recsys_rule_knn.html","title":"\u041c\u0435\u0442\u0440\u0438\u043a\u0438\u00b6","text":""},{"location":"portfolio/course_recsys/recsys_rule_knn.html#1-mrr","title":"(1) MRR\u00b6","text":"<ul> <li>\u042d\u0442\u043e \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u043e\u0431\u0440\u0430\u0442\u043d\u043e\u0435 \u0440\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435. \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u0441\u0438\u0441\u0442\u0435\u043c, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u044e\u0442 \u0443\u043f\u043e\u0440\u044f\u0434\u043e\u0447\u0435\u043d\u043d\u044b\u0435 \u0441\u043f\u0438\u0441\u043a\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432.</li> <li>\u0420\u0430\u0441\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u043a\u0430\u043a \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0431\u0440\u0430\u0442\u043d\u044b\u0445 \u0440\u0430\u043d\u0433\u043e\u0432 \u043f\u0435\u0440\u0432\u044b\u0445 \u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u044b\u0445 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432.</li> </ul>"},{"location":"portfolio/course_recsys/recsys_rule_knn.html#2-hr","title":"(2) HR\u00b6","text":"<ul> <li>\u042d\u0442\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0430, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u0434\u043e\u043b\u044e \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432, \u0434\u043b\u044f \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0445\u043e\u0442\u044f \u0431\u044b \u043e\u0434\u0438\u043d \u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u044b\u0439 \u044d\u043b\u0435\u043c\u0435\u043d\u0442 \u0431\u044b\u043b \u043d\u0430\u0439\u0434\u0435\u043d \u0432 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u0445.</li> <li>\u0420\u0430\u0441\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u043a\u0430\u043a \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0443\u0441\u043f\u0435\u0448\u043d\u044b\u0445 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 (\u0433\u0434\u0435 \u043d\u0430\u0439\u0434\u0435\u043d \u0445\u043e\u0442\u044f \u0431\u044b \u043e\u0434\u0438\u043d \u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u044b\u0439 \u044d\u043b\u0435\u043c\u0435\u043d\u0442) \u043a \u043e\u0431\u0449\u0435\u043c\u0443 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0443 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432: HR = \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0443\u0441\u043f\u0435\u0448\u043d\u044b\u0445 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432/\u041e\u0431\u0449\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432</li> </ul>"},{"location":"portfolio/course_recsys/recsys_rule_knn.html#3-ndcg","title":"(3) NDCG\u00b6","text":"<ul> <li>\u042d\u0442\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0430, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u0442 \u043a\u0430\u043a \u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u043e\u0441\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432, \u0442\u0430\u043a \u0438 \u0438\u0445 \u043f\u043e\u0440\u044f\u0434\u043e\u043a.</li> <li>\u041e\u043d\u0430 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u043d\u0430\u043a\u043e\u043f\u043b\u0435\u043d\u043d\u0443\u044e \u043f\u043e\u043b\u0435\u0437\u043d\u043e\u0441\u0442\u044c \u0441 \u0443\u0447\u0435\u0442\u043e\u043c \u043f\u043e\u0437\u0438\u0446\u0438\u0438, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442.</li> </ul>"},{"location":"portfolio/course_recsys/recsys_rule_knn.html#4-coverage","title":"(4) Coverage\u00b6","text":"<ul> <li>\u041c\u0435\u0442\u0440\u0438\u043a\u0430 coverage (\u043f\u043e\u043a\u0440\u044b\u0442\u0438\u0435) \u0432 \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0441\u0438\u0441\u0442\u0435\u043c \u0438\u0437\u043c\u0435\u0440\u044f\u0435\u0442, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0445\u043e\u0440\u043e\u0448\u043e \u0441\u0438\u0441\u0442\u0435\u043c\u0430 \u043e\u0445\u0432\u0430\u0442\u044b\u0432\u0430\u0435\u0442 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u0435 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u044b (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0442\u043e\u0432\u0430\u0440\u044b, \u0444\u0438\u043b\u044c\u043c\u044b, \u043f\u0435\u0441\u043d\u0438 \u0438 \u0442.\u0434.) \u0432 \u0441\u0432\u043e\u0438\u0445 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u044f\u0445.</li> <li>\u041e\u043d\u0430 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u0434\u043e\u043b\u044e \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u044b\u043b\u0438 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u043d\u044b \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c, \u043f\u043e \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044e \u0441 \u043e\u0431\u0449\u0438\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e\u043c \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u0441\u0438\u0441\u0442\u0435\u043c\u0435.</li> </ul>"},{"location":"portfolio/course_recsys/recsys_rule_knn.html","title":"\u042d\u0432\u0440\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u041f\u043e\u0434\u0445\u043e\u0434\u044b\u00b6","text":""},{"location":"portfolio/course_recsys/recsys_rule_knn.html#1","title":"(1) \u041f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c\u00b6","text":"<p>\u0412\u044b\u0431\u0440\u0430\u0442\u044c \u0442\u043e\u043f 10</p> <ul> <li>\u0441\u0430\u043c\u044b\u0445 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 (\u043f\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0443 \u043e\u0442\u0437\u044b\u0432\u043e\u0432)</li> <li>\u041e\u043d\u0438 \u043c\u043e\u0433\u0443\u0442 \u0438\u043c\u0435\u0442\u044c \u0440\u0430\u0437\u043d\u044b\u0435 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0438</li> </ul>"},{"location":"portfolio/course_recsys/recsys_rule_knn.html#2","title":"(2) \u041f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435 \u0441 \u0432\u044b\u0441\u043e\u043a\u0438\u043c \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u043c\u00b6","text":"<p>\u0412\u044b\u0431\u0440\u0430\u0442\u044c \u0442\u043e\u043f 10</p> <ul> <li>\u0420\u0435\u043a\u043e\u043c\u043c\u0435\u043d\u0434\u0443\u0435\u043c \u0441\u0430\u043c\u044b\u0435 \u0432\u044b\u0441\u043e\u043a\u043e \u043e\u0446\u0435\u043d\u0435\u043d\u043d\u044b\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u0432\u0441\u0435\u043c\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c\u0438</li> <li>\u0412\u0441\u0435 \u0438\u043c\u0435\u044e\u0442 \u0442\u043e\u043b\u043a\u043e \u043e\u0446\u0435\u043d\u043a\u0443 5</li> </ul>"},{"location":"portfolio/course_recsys/recsys_rule_knn.html","title":"\u041c\u0435\u0442\u043e\u0434\u044b \u043a\u043e\u043b\u043b\u0430\u0431\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u043e\u0439 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438\u00b6","text":""},{"location":"portfolio/course_recsys/recsys_rule_knn.html#1-user-based-userknn","title":"(1) User-based (userKNN)\u00b6","text":"<p>User-based model \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043c\u043e\u0434\u0435\u043b\u044c\u044e \u043a\u043e\u043b\u043b\u0430\u0431\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u043e\u0439 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438, \u043e\u0441\u043d\u043e\u0432\u043d\u0430\u044f \u0438\u0434\u0435\u044f \u043a\u043e\u0442\u043e\u0440\u043e\u0439 <code>\u043f\u043e\u0445\u043e\u0436\u0438\u043c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c \u043e\u0431\u044b\u0447\u043d\u043e \u043d\u0440\u0430\u0432\u044f\u0442\u0441\u044f \u043f\u043e\u0445\u043e\u0436\u0438\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u044b</code></p> <p>\u0418\u0434\u0435\u044f \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430:</p> <ul> <li><p>\u041d\u0430\u0439\u0442\u0438, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0434\u0440\u0443\u0433\u0438\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438 \u0432 \u0431\u0430\u0437\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u043e\u0445\u043e\u0436\u0438 \u043d\u0430 \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f.</p> </li> <li><p>\u041f\u043e \u043e\u0446\u0435\u043d\u043a\u0430\u043c \u0434\u0440\u0443\u0433\u0438\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c, \u043a\u0430\u043a\u0443\u044e \u043e\u0446\u0435\u043d\u043a\u0443 \u0434\u0430\u0441\u0442 \u0434\u0430\u043d\u043d\u044b\u0439 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u0434\u0430\u043d\u043d\u043e\u043c\u0443 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0443, \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u044f \u0441 \u0431\u043e\u043b\u044c\u0448\u0438\u043c \u0432\u0435\u0441\u043e\u043c \u0442\u0435\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u043e\u043b\u044c\u0448\u0435 \u043f\u043e\u0445\u043e\u0436\u0438 \u043d\u0430 \u0434\u0430\u043d\u043d\u043e\u0433\u043e.</p> </li> <li><p>\u041e\u0442\u0440\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0442\u043e\u0432\u0430\u0440\u044b \u0432 \u043f\u043e\u0440\u044f\u0434\u043a\u0435 \u0443\u0431\u044b\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0445 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u0432 \u0438 \u0432\u0437\u044f\u0442\u044c top k.</p> </li> </ul> <p>\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0442\u044c \u0441\u0445\u043e\u0436\u0435\u0441\u0442\u044c \u0434\u0432\u0443\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043c\u043e\u0436\u043d\u043e \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043a\u043e\u0441\u0438\u043d\u0443\u0441\u043d\u043e\u0439 \u043c\u0435\u0440\u044b \u0431\u043b\u0438\u0437\u043e\u0441\u0442\u0438 \u043c\u0435\u0436\u0434\u0443 \u0432\u0435\u043a\u0442\u043e\u0440\u0430\u043c\u0438 \u043f\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0445 \u043e\u0446\u0435\u043d\u043e\u043a</p>"},{"location":"portfolio/course_recsys/recsys_rule_knn.html#2-item-based-model-itemknn","title":"(2) Item-based model (itemKNN)\u00b6","text":"<p>Item-based model \u043e\u0447\u0435\u043d\u044c \u043f\u043e\u0445\u043e\u0436\u0430 \u043d\u0430 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u043f\u043e \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0435, \u043d\u043e \u0442\u0435\u043f\u0435\u0440\u044c \u043c\u044b <code>\u0438\u0449\u0435\u043c \u043f\u043e\u0445\u043e\u0436\u0438\u0435 \u0442\u043e\u0432\u0430\u0440\u044b</code>, \u0430 \u043d\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439. \u0410 \u0438\u043c\u0435\u043d\u043d\u043e:</p> <ul> <li><p>\u0414\u043b\u044f \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u0446\u0435\u043d\u0438\u043b \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u043d\u0430\u0439\u0442\u0438, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043e\u043d\u0438 \u043f\u043e\u0445\u043e\u0436\u0438 \u043d\u0430 \u0434\u0440\u0443\u0433\u0438\u0435 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u044b \u0432 \u0431\u0430\u0437\u0435 \u0434\u0430\u043d\u043d\u044b\u0445.</p> </li> <li><p>\u041f\u043e \u043e\u0446\u0435\u043d\u043a\u0430\u043c \u0434\u0440\u0443\u0433\u0438\u0445 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u043e\u0432 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c, \u043a\u0430\u043a\u0443\u044e \u043e\u0446\u0435\u043d\u043a\u0443 \u0434\u0430\u0441\u0442 \u0434\u0430\u043d\u043d\u044b\u0439 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u0434\u0430\u043d\u043d\u043e\u043c\u0443 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0443, \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u044f \u0441 \u0431\u043e\u043b\u044c\u0448\u0438\u043c \u0432\u0435\u0441\u043e\u043c \u0442\u0435 \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u043e\u043b\u044c\u0448\u0435 \u043f\u043e\u0445\u043e\u0436\u0438 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0439.</p> </li> </ul> <p>\u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u043f\u0440\u0438 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 $r_{ui}$ \u043c\u044b \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0432\u0441\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f $u$, \u043e\u0446\u0435\u043d\u0438\u043c \u0438\u0445 \u0441\u0445\u043e\u0436\u0435\u0441\u0442\u044c \u0441 \u0444\u0438\u043b\u044c\u043c\u043e\u043c $i$ \u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0432\u0437\u0432\u0435\u0448\u0435\u043d\u043d\u0443\u044e \u0441\u0443\u043c\u043c\u0443:</p> <p>$$     r_{ui} = \\frac     {\\sum_{j \\in Item_u}\\big(\\textit{sim(i, j)} \\times r_{uj} \\big)}     {\\sum_{j \\in Item_u}\\textit{|sim(i, j)|}} $$</p> <p>\u041e\u0446\u0435\u043d\u0438\u0432\u0430\u0442\u044c \u0436\u0435 \u0441\u0445\u043e\u0436\u0435\u0441\u0442\u044c \u0434\u0432\u0443\u0445 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u0431\u0443\u0434\u0435\u043c \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0442\u043e\u0439 \u0436\u0435 \u043a\u043e\u0441\u0438\u043d\u0443\u0441\u043d\u043e\u0439 \u043c\u0435\u0440\u044b \u0431\u043b\u0438\u0437\u043e\u0441\u0442\u0438:</p> <p>$$     \\textit{sim(i, j)} = \\frac     {\\sum_u{\\big(r_{ui} \\times r_{uj}\\big)}}     {\\sqrt{\\sum_u{r_{ui}^2}} \\times \\sqrt{\\sum_u{r_{uj}^2}}} $$</p>"},{"location":"portfolio/course_recsys/recsys_twomodel.html","title":"Recsys twomodel","text":"In\u00a0[2]: Copied! <pre>!pip install implicit -qqq\n!pip install catboost -qqq\n</pre> !pip install implicit -qqq !pip install catboost -qqq In\u00a0[3]: Copied! <pre>import datetime\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport scipy.sparse as sparse\n\nfrom catboost import CatBoostClassifier\nimport implicit\nimport warnings; warnings.filterwarnings('ignore')\n</pre> import datetime import numpy as np import pandas as pd from tqdm.auto import tqdm import matplotlib.pyplot as plt import seaborn as sns from sklearn.utils import shuffle from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score import scipy.sparse as sparse  from catboost import CatBoostClassifier import implicit import warnings; warnings.filterwarnings('ignore') In\u00a0[5]: Copied! <pre>interactions = pd.read_csv(\"/kaggle/input/kion-data/interactions_df.csv\")\nitems = pd.read_csv(\"/kaggle/input/kion-data/items.csv\")\nusers = pd.read_csv(\"/kaggle/input/kion-data/users.csv\")\n</pre> interactions = pd.read_csv(\"/kaggle/input/kion-data/interactions_df.csv\") items = pd.read_csv(\"/kaggle/input/kion-data/items.csv\") users = pd.read_csv(\"/kaggle/input/kion-data/users.csv\") In\u00a0[6]: Copied! <pre># \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u0442\u044b\ninteractions['last_watch_dt'] = pd.to_datetime(interactions['last_watch_dt']).map(lambda x: x.date())\n\nprint(f\"\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u044e\u0437\u0435\u0440\u043e\u0432 \u0432 interactions: {interactions['user_id'].nunique()}\")\nprint(f\"\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0430\u0439\u0442\u0435\u043c\u043e\u0432 \u0432 interactions: {interactions['item_id'].nunique()}\")\n</pre> # \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u0442\u044b interactions['last_watch_dt'] = pd.to_datetime(interactions['last_watch_dt']).map(lambda x: x.date())  print(f\"\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u044e\u0437\u0435\u0440\u043e\u0432 \u0432 interactions: {interactions['user_id'].nunique()}\") print(f\"\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0430\u0439\u0442\u0435\u043c\u043e\u0432 \u0432 interactions: {interactions['item_id'].nunique()}\") <pre>\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u044e\u0437\u0435\u0440\u043e\u0432 \u0432 interactions: 962179\n\u0423\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0430\u0439\u0442\u0435\u043c\u043e\u0432 \u0432 interactions: 15706\n</pre> In\u00a0[7]: Copied! <pre>interactions.head()\n</pre> interactions.head() Out[7]: user_id item_id last_watch_dt total_dur watched_pct 0 176549 9506 2021-05-11 4250 72.0 1 699317 1659 2021-05-29 8317 100.0 2 656683 7107 2021-05-09 10 0.0 3 864613 7638 2021-07-05 14483 100.0 4 964868 9506 2021-04-30 6725 100.0 \u0414\u0430\u043d\u043d\u044b\u0435 \u043e \u0442\u043e\u0432\u0430\u0440\u0430\u0445 <p>\u0414\u0430\u043d\u043d\u044b\u0435 \u043e \u0441\u0430\u043c\u0438\u0445 \u0442\u043e\u0432\u0430\u0440\u0430\u0445</p> <ul> <li><code>content_type</code> - \u0442\u0438\u043f \u043a\u043e\u043d\u0442\u0435\u043d\u0442\u0430</li> <li><code>title</code> - \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u043e\u043c</li> <li><code>title_orig</code> - \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0435</li> <li><code>release_year</code> - \u0433\u043e\u0434 \u0432\u044b\u043f\u0443\u0441\u043a\u0430</li> <li><code>countries</code> - \u0441\u0442\u0440\u0430\u043d\u044b</li> <li><code>for_kids</code> - \u0444\u043b\u0430\u0433 \u043a\u043e\u043d\u0442\u0435\u043d\u0442 \u0434\u043b\u044f \u0434\u0435\u0442\u0435\u0439</li> <li><code>age_rating</code>- \u0412\u043e\u0437\u0440\u0430\u0441\u0442\u043d\u043e\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433</li> <li><code>studios</code> - \u0441\u0442\u0443\u0434\u0438\u0438</li> <li><code>directors</code> - \u0440\u0435\u0436\u0438\u0441\u0441\u0435\u0440\u044b</li> <li><code>actors</code>- \u0430\u043a\u0442\u0435\u0440\u044b</li> <li><code>keywords</code> - \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u0435 \u0441\u043b\u043e\u0432\u0430</li> <li><code>description</code> - \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435</li> </ul> In\u00a0[8]: Copied! <pre>items.head()\n</pre> items.head() Out[8]: item_id content_type title title_orig release_year genres countries for_kids age_rating studios directors actors description keywords 0 10711 film \u041f\u043e\u0433\u043e\u0432\u043e\u0440\u0438 \u0441 \u043d\u0435\u0439 Hable con ella 2002.0 \u0434\u0440\u0430\u043c\u044b, \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435, \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0432\u044b, \u043c\u0435\u043b\u043e\u0434\u0440\u0430\u043c\u044b \u0418\u0441\u043f\u0430\u043d\u0438\u044f NaN 16.0 NaN \u041f\u0435\u0434\u0440\u043e \u0410\u043b\u044c\u043c\u043e\u0434\u043e\u0432\u0430\u0440 \u0410\u0434\u043e\u043b\u044c\u0444\u043e \u0424\u0435\u0440\u043d\u0430\u043d\u0434\u0435\u0441, \u0410\u043d\u0430 \u0424\u0435\u0440\u043d\u0430\u043d\u0434\u0435\u0441, \u0414\u0430\u0440\u0438\u043e \u0413\u0440\u0430\u043d\u0434\u0438... \u041c\u0435\u043b\u043e\u0434\u0440\u0430\u043c\u0430 \u043b\u0435\u0433\u0435\u043d\u0434\u0430\u0440\u043d\u043e\u0433\u043e \u041f\u0435\u0434\u0440\u043e \u0410\u043b\u044c\u043c\u043e\u0434\u043e\u0432\u0430\u0440\u0430 \u00ab\u041f\u043e\u0433\u043e... \u041f\u043e\u0433\u043e\u0432\u043e\u0440\u0438, \u043d\u0435\u0439, 2002, \u0418\u0441\u043f\u0430\u043d\u0438\u044f, \u0434\u0440\u0443\u0437\u044c\u044f, \u043b\u044e\u0431\u043e\u0432\u044c, ... 1 2508 film \u0413\u043e\u043b\u044b\u0435 \u043f\u0435\u0440\u0446\u044b Search Party 2014.0 \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435, \u043f\u0440\u0438\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f, \u043a\u043e\u043c\u0435\u0434\u0438\u0438 \u0421\u0428\u0410 NaN 16.0 NaN \u0421\u043a\u043e\u0442 \u0410\u0440\u043c\u0441\u0442\u0440\u043e\u043d\u0433 \u0410\u0434\u0430\u043c \u041f\u0430\u043b\u043b\u0438, \u0411\u0440\u0430\u0439\u0430\u043d \u0425\u0430\u0441\u043a\u0438, \u0414\u0436.\u0411. \u0421\u043c\u0443\u0432, \u0414\u0436\u0435\u0439\u0441\u043e\u043d ... \u0423\u043c\u043e\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u043a\u043e\u043c\u0435\u0434\u0438\u044f \u043d\u0430 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u0443\u044e... \u0413\u043e\u043b\u044b\u0435, \u043f\u0435\u0440\u0446\u044b, 2014, \u0421\u0428\u0410, \u0434\u0440\u0443\u0437\u044c\u044f, \u0441\u0432\u0430\u0434\u044c\u0431\u044b, \u043f\u0440\u0435\u043e... 2 10716 film \u0422\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0441\u0438\u043b\u0430 Tactical Force 2011.0 \u043a\u0440\u0438\u043c\u0438\u043d\u0430\u043b, \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435, \u0442\u0440\u0438\u043b\u043b\u0435\u0440\u044b, \u0431\u043e\u0435\u0432\u0438\u043a\u0438, \u043a\u043e\u043c\u0435\u0434\u0438\u0438 \u041a\u0430\u043d\u0430\u0434\u0430 NaN 16.0 NaN \u0410\u0434\u0430\u043c \u041f. \u041a\u0430\u043b\u0442\u0440\u0430\u0440\u043e \u0410\u0434\u0440\u0438\u0430\u043d \u0425\u043e\u043b\u043c\u0441, \u0414\u0430\u0440\u0440\u0435\u043d \u0428\u0430\u043b\u0430\u0432\u0438, \u0414\u0436\u0435\u0440\u0440\u0438 \u0412\u0430\u0441\u0441\u0435\u0440\u043c\u0430\u043d,... \u041f\u0440\u043e\u0444\u0435\u0441\u0441\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0440\u0435\u0441\u0442\u043b\u0435\u0440 \u0421\u0442\u0438\u0432 \u041e\u0441\u0442\u0438\u043d (\u00ab\u0412\u0441\u0435 \u0438\u043b\u0438 ... \u0422\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f, \u0441\u0438\u043b\u0430, 2011, \u041a\u0430\u043d\u0430\u0434\u0430, \u0431\u0430\u043d\u0434\u0438\u0442\u044b, \u0433\u0430\u043d\u0433... 3 7868 film 45 \u043b\u0435\u0442 45 Years 2015.0 \u0434\u0440\u0430\u043c\u044b, \u0437\u0430\u0440\u0443\u0431\u0435\u0436\u043d\u044b\u0435, \u043c\u0435\u043b\u043e\u0434\u0440\u0430\u043c\u044b \u0412\u0435\u043b\u0438\u043a\u043e\u0431\u0440\u0438\u0442\u0430\u043d\u0438\u044f NaN 16.0 NaN \u042d\u043d\u0434\u0440\u044e \u0425\u044d\u0439 \u0410\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440\u0430 \u0420\u0438\u0434\u0434\u043b\u0441\u0442\u043e\u043d-\u0411\u0430\u0440\u0440\u0435\u0442\u0442, \u0414\u0436\u0435\u0440\u0430\u043b\u044c\u0434\u0438\u043d \u0414\u0436\u0435\u0439\u043c... \u0428\u0430\u0440\u043b\u043e\u0442\u0442\u0430 \u0420\u044d\u043c\u043f\u043b\u0438\u043d\u0433, \u0422\u043e\u043c \u041a\u043e\u0440\u0442\u043d\u0438, \u0414\u0436\u0435\u0440\u0430\u043b\u044c\u0434\u0438\u043d \u0414\u0436\u0435\u0439... 45, \u043b\u0435\u0442, 2015, \u0412\u0435\u043b\u0438\u043a\u043e\u0431\u0440\u0438\u0442\u0430\u043d\u0438\u044f, \u0431\u0440\u0430\u043a, \u0436\u0438\u0437\u043d\u044c, \u043b\u044e... 4 16268 film \u0412\u0441\u0435 \u0440\u0435\u0448\u0430\u0435\u0442 \u043c\u0433\u043d\u043e\u0432\u0435\u043d\u0438\u0435 NaN 1978.0 \u0434\u0440\u0430\u043c\u044b, \u0441\u043f\u043e\u0440\u0442, \u0441\u043e\u0432\u0435\u0442\u0441\u043a\u0438\u0435, \u043c\u0435\u043b\u043e\u0434\u0440\u0430\u043c\u044b \u0421\u0421\u0421\u0420 NaN 12.0 \u041b\u0435\u043d\u0444\u0438\u043b\u044c\u043c \u0412\u0438\u043a\u0442\u043e\u0440 \u0421\u0430\u0434\u043e\u0432\u0441\u043a\u0438\u0439 \u0410\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440 \u0410\u0431\u0434\u0443\u043b\u043e\u0432, \u0410\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440 \u0414\u0435\u043c\u044c\u044f\u043d\u0435\u043d\u043a\u043e, \u0410\u043b\u0435\u043a\u0441... \u0420\u0430\u0441\u0447\u0435\u0442\u043b\u0438\u0432\u0430\u044f \u0447\u0430\u0440\u043e\u0432\u043d\u0438\u0446\u0430 \u0438\u0437 \u0441\u043e\u0432\u0435\u0442\u0441\u043a\u043e\u0433\u043e \u043a\u0438\u043d\u043e\u0445\u0438\u0442\u0430 \u00ab... \u0412\u0441\u0435, \u0440\u0435\u0448\u0430\u0435\u0442, \u043c\u0433\u043d\u043e\u0432\u0435\u043d\u0438\u0435, 1978, \u0421\u0421\u0421\u0420, \u0441\u0438\u043b\u044c\u043d\u044b\u0435, \u0436... In\u00a0[9]: Copied! <pre>users.head()\n</pre> users.head() Out[9]: user_id age income sex kids_flg 0 973171 age_25_34 income_60_90 \u041c 1 1 962099 age_18_24 income_20_40 \u041c 0 2 1047345 age_45_54 income_40_60 \u0416 0 3 721985 age_45_54 income_20_40 \u0416 0 4 704055 age_35_44 income_60_90 \u0416 0 In\u00a0[10]: Copied! <pre>\"\"\"\n\nPreprocessing\n\n- Filter from [interactions] views with less than 300 second views\n- Filter [users] (user_id) who have less than 10 film views\n- Filter out [movies] (item_id) which have less than 10 film views\n\n\"\"\"\n</pre> \"\"\"  Preprocessing  - Filter from [interactions] views with less than 300 second views - Filter [users] (user_id) who have less than 10 film views - Filter out [movies] (item_id) which have less than 10 film views  \"\"\" Out[10]: <pre>'\\n\\nPreprocessing\\n\\n- Filter from [interactions] views with less than 300 second views\\n- Filter [users] (user_id) who have less than 10 film views\\n- Filter out [movies] (item_id) which have less than 10 film views\\n\\n'</pre> In\u00a0[11]: Copied! <pre># \u041d\u0435 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0432\u0441\u0435 \u0434\u0430\u043d\u043d\u044b\u0435, \u043d\u0430 \u043f\u0435\u0440\u0432\u043e\u0439 \u0441\u0442\u0430\u0434\u0438\u0439 \u043e\u0442\u0444\u0438\u043b\u044c\u0440\u0443\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0435 \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u044b \u0438\u0437 interactions\ninteractions = interactions[interactions['total_dur'] &gt;= 300]\n</pre> # \u041d\u0435 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0432\u0441\u0435 \u0434\u0430\u043d\u043d\u044b\u0435, \u043d\u0430 \u043f\u0435\u0440\u0432\u043e\u0439 \u0441\u0442\u0430\u0434\u0438\u0439 \u043e\u0442\u0444\u0438\u043b\u044c\u0440\u0443\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0435 \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u044b \u0438\u0437 interactions interactions = interactions[interactions['total_dur'] &gt;= 300] In\u00a0[12]: Copied! <pre>user_interactions_count = interactions.groupby('user_id')[['item_id']].count().reset_index()\nfiltered_users = user_interactions_count[user_interactions_count['item_id'] &gt;= 10][['user_id']]\ninteractions = filtered_users.merge(interactions, how='left')\n</pre> user_interactions_count = interactions.groupby('user_id')[['item_id']].count().reset_index() filtered_users = user_interactions_count[user_interactions_count['item_id'] &gt;= 10][['user_id']] interactions = filtered_users.merge(interactions, how='left') In\u00a0[13]: Copied! <pre>item_interactions_count = interactions.groupby('item_id')[['user_id']].count().reset_index()\nfiltered_items = item_interactions_count[item_interactions_count['user_id'] &gt;= 10][['item_id']]\ninteractions = filtered_items.merge(interactions, how='left')\n</pre> item_interactions_count = interactions.groupby('item_id')[['user_id']].count().reset_index() filtered_items = item_interactions_count[item_interactions_count['user_id'] &gt;= 10][['item_id']] interactions = filtered_items.merge(interactions, how='left') In\u00a0[14]: Copied! <pre>max_date = interactions['last_watch_dt'].max()\nmin_date = interactions['last_watch_dt'].min()\n\nprint(f\"min \u0434\u0430\u0442\u0430 \u0432 interactions: {min_date}\")\nprint(f\"max \u0434\u0430\u0442\u0430 \u0432 interactions: {max_date}\")\n</pre> max_date = interactions['last_watch_dt'].max() min_date = interactions['last_watch_dt'].min()  print(f\"min \u0434\u0430\u0442\u0430 \u0432 interactions: {min_date}\") print(f\"max \u0434\u0430\u0442\u0430 \u0432 interactions: {max_date}\") <pre>min \u0434\u0430\u0442\u0430 \u0432 interactions: 2021-03-13\nmax \u0434\u0430\u0442\u0430 \u0432 interactions: 2021-08-22\n</pre> <ul> <li><p>[test] : \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0435 7 \u0434\u043d\u0435\u0439 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439</p> </li> <li><p>[train_val] : train &amp; validation dataset &lt;= 7 days</p> </li> <li><p>[train_val] then split into two :</p> <ul> <li>[train] up to last 60 days of interactions (to test start date)</li> <li>[val] last 60 days of interactions (to test start date)</li> </ul> </li> </ul> In\u00a0[15]: Copied! <pre>test_threshold = max_date - pd.Timedelta(days=7)\nval_threshold = test_threshold - pd.Timedelta(days=60) # \u0434\u0432\u0430 \u043c\u0435\u0441\u044f\u0446\u0430 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0432\u0442\u043e\u0440\u043e\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f\n\ntest = interactions[(interactions['last_watch_dt'] &gt;= test_threshold)]\ntrain_val = interactions[(interactions['last_watch_dt'] &lt; test_threshold)]\n\nval = train_val[(train_val['last_watch_dt'] &gt;= val_threshold)]\ntrain = train_val[(train_val['last_watch_dt'] &lt; val_threshold)]\n\nprint(f\"train: {train.shape}\")\nprint(f\"val: {val.shape}\")\nprint(f\"test: {test.shape}\")\n</pre> test_threshold = max_date - pd.Timedelta(days=7) val_threshold = test_threshold - pd.Timedelta(days=60) # \u0434\u0432\u0430 \u043c\u0435\u0441\u044f\u0446\u0430 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0432\u0442\u043e\u0440\u043e\u0433\u043e \u0443\u0440\u043e\u0432\u043d\u044f  test = interactions[(interactions['last_watch_dt'] &gt;= test_threshold)] train_val = interactions[(interactions['last_watch_dt'] &lt; test_threshold)]  val = train_val[(train_val['last_watch_dt'] &gt;= val_threshold)] train = train_val[(train_val['last_watch_dt'] &lt; val_threshold)]  print(f\"train: {train.shape}\") print(f\"val: {val.shape}\") print(f\"test: {test.shape}\") <pre>train: (881660, 5)\nval: (1246263, 5)\ntest: (172593, 5)\n</pre> In\u00a0[17]: Copied! <pre># \u0412 val \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u044b\u043b\u0438 \u0432 train\nprint(val.shape)\nval = val[val['user_id'].isin(train['user_id'].unique())]\nprint(val.shape)\n</pre> # \u0412 val \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u044b\u043b\u0438 \u0432 train print(val.shape) val = val[val['user_id'].isin(train['user_id'].unique())] print(val.shape) <pre>(1246263, 5)\n(694032, 5)\n</pre> In\u00a0[18]: Copied! <pre># train model on [train]\nusers_id = list(np.sort(train.user_id.unique()))\nitems_train = list(train.item_id.unique())\nratings_train = list(train.watched_pct)\n\nrows_train = train.user_id.astype('category').cat.codes\ncols_train = train.item_id.astype('category').cat.codes\n\ntrain_sparse = sparse.csr_matrix((ratings_train, (rows_train, cols_train)), \n                                 shape=(len(users_id), len(items_train)))\n\nalgo = implicit.bpr.BayesianPersonalizedRanking(factors=50, \n                                                regularization=0.01, \n                                                iterations=50, \n                                                use_gpu=False)\nalgo.fit((train_sparse).astype('double'))\n</pre> # train model on [train] users_id = list(np.sort(train.user_id.unique())) items_train = list(train.item_id.unique()) ratings_train = list(train.watched_pct)  rows_train = train.user_id.astype('category').cat.codes cols_train = train.item_id.astype('category').cat.codes  train_sparse = sparse.csr_matrix((ratings_train, (rows_train, cols_train)),                                   shape=(len(users_id), len(items_train)))  algo = implicit.bpr.BayesianPersonalizedRanking(factors=50,                                                  regularization=0.01,                                                  iterations=50,                                                  use_gpu=False) algo.fit((train_sparse).astype('double')) <pre>  0%|          | 0/50 [00:00&lt;?, ?it/s]</pre> In\u00a0[19]: Copied! <pre># Output [1] from first model; \n# user and item factorisation matrices\nuser_vecs = algo.user_factors\nitem_vecs = algo.item_factors\n</pre> # Output [1] from first model;  # user and item factorisation matrices user_vecs = algo.user_factors item_vecs = algo.item_factors In\u00a0[20]: Copied! <pre>def predict(user_vecs, item_vecs, k=10):\n    \n    \"\"\"\n    \n    Helper function for matrix factorisation prediction\n    \n    \"\"\"\n    \n    id2user = dict(zip(rows_train, train.user_id))\n    id2item = dict(zip(cols_train, train.item_id))\n    scores = user_vecs.dot(item_vecs.T)\n\n    ind_part = np.argpartition(scores, -k + 1)[:, -k:].copy()\n    scores_not_sorted = np.take_along_axis(scores, ind_part, axis=1)\n    ind_sorted = np.argsort(scores_not_sorted, axis=1)\n    indices = np.take_along_axis(ind_part, ind_sorted, axis=1)\n    indices = np.flip(indices, 1)\n    preds = pd.DataFrame({\n        'user_id': range(user_vecs.shape[0]),\n        'preds': indices.tolist(),\n        })\n    preds['user_id'] = preds['user_id'].map(id2user)\n    preds['preds'] = preds['preds'].map(lambda inds: [id2item[i] for i in inds])\n    return preds\n</pre> def predict(user_vecs, item_vecs, k=10):          \"\"\"          Helper function for matrix factorisation prediction          \"\"\"          id2user = dict(zip(rows_train, train.user_id))     id2item = dict(zip(cols_train, train.item_id))     scores = user_vecs.dot(item_vecs.T)      ind_part = np.argpartition(scores, -k + 1)[:, -k:].copy()     scores_not_sorted = np.take_along_axis(scores, ind_part, axis=1)     ind_sorted = np.argsort(scores_not_sorted, axis=1)     indices = np.take_along_axis(ind_part, ind_sorted, axis=1)     indices = np.flip(indices, 1)     preds = pd.DataFrame({         'user_id': range(user_vecs.shape[0]),         'preds': indices.tolist(),         })     preds['user_id'] = preds['user_id'].map(id2user)     preds['preds'] = preds['preds'].map(lambda inds: [id2item[i] for i in inds])     return preds In\u00a0[21]: Copied! <pre>k=30\nval_user_history = val.groupby('user_id')[['item_id']].agg(lambda x: list(x))\npred_bpr = predict(user_vecs, item_vecs, k)\npred_bpr = val_user_history.merge(pred_bpr, how='left', on='user_id')\npred_bpr.head()\n</pre> k=30 val_user_history = val.groupby('user_id')[['item_id']].agg(lambda x: list(x)) pred_bpr = predict(user_vecs, item_vecs, k) pred_bpr = val_user_history.merge(pred_bpr, how='left', on='user_id') pred_bpr.head() Out[21]: user_id item_id preds 0 2 [242, 3628, 5819, 7106, 7921, 8482, 9164, 1077... [1267, 4475, 6470, 4072, 6738, 85, 3166, 9164,... 1 21 [308, 3784, 4495, 5077, 6384, 7102, 7571, 8251... [849, 24, 1053, 11237, 14703, 8636, 12659, 496... 2 30 [1107, 2346, 2743, 3031, 7250, 9728, 9842, 112... [142, 16201, 13865, 10440, 4880, 1465, 2303, 1... 3 46 [10440] [142, 4880, 6809, 10440, 8636, 5287, 1465, 999... 4 60 [1179, 1343, 1590, 3550, 6044, 6606, 8612, 972... [4880, 13865, 7571, 4151, 1449, 598, 2657, 710... In\u00a0[22]: Copied! <pre>def recall(df: pd.DataFrame, pred_col='preds', true_col='item_id', k=30) -&gt; float:\n    recall_values = []\n    for _, row in df.iterrows():\n      num_relevant = len(set(row[true_col]) &amp; set(row[pred_col][:k]))\n      num_true = len(row[true_col])\n      recall_values.append(num_relevant / num_true)\n    return np.mean(recall_values)\n\ndef precision(df: pd.DataFrame, pred_col='preds', true_col='item_id', k=30) -&gt; float:\n    precision_values = []\n    for _, row in df.iterrows():\n      num_relevant = len(set(row[true_col]) &amp; set(row[pred_col][:k]))\n      num_true = min(k, len(row[true_col]))\n      precision_values.append(num_relevant / num_true)\n    return np.mean(precision_values)\n\ndef mrr(df: pd.DataFrame, pred_col='preds', true_col='item_id', k=30) -&gt; float:\n    mrr_values = []\n    for _, row in df.iterrows():\n      intersection = set(row[true_col]) &amp; set(row[pred_col][:k])\n      user_mrr = 0\n      if len(intersection) &gt; 0:\n          for item in intersection:\n              user_mrr = max(user_mrr, 1 / (row[pred_col].index(item) + 1))\n      mrr_values.append(user_mrr)\n    return np.mean(mrr_values)\n</pre> def recall(df: pd.DataFrame, pred_col='preds', true_col='item_id', k=30) -&gt; float:     recall_values = []     for _, row in df.iterrows():       num_relevant = len(set(row[true_col]) &amp; set(row[pred_col][:k]))       num_true = len(row[true_col])       recall_values.append(num_relevant / num_true)     return np.mean(recall_values)  def precision(df: pd.DataFrame, pred_col='preds', true_col='item_id', k=30) -&gt; float:     precision_values = []     for _, row in df.iterrows():       num_relevant = len(set(row[true_col]) &amp; set(row[pred_col][:k]))       num_true = min(k, len(row[true_col]))       precision_values.append(num_relevant / num_true)     return np.mean(precision_values)  def mrr(df: pd.DataFrame, pred_col='preds', true_col='item_id', k=30) -&gt; float:     mrr_values = []     for _, row in df.iterrows():       intersection = set(row[true_col]) &amp; set(row[pred_col][:k])       user_mrr = 0       if len(intersection) &gt; 0:           for item in intersection:               user_mrr = max(user_mrr, 1 / (row[pred_col].index(item) + 1))       mrr_values.append(user_mrr)     return np.mean(mrr_values) In\u00a0[23]: Copied! <pre>print('recall',round(recall(pred_bpr),3))\nprint('precision',round(precision(pred_bpr),3))\nprint('mrr',round(mrr(pred_bpr),3))\n</pre> print('recall',round(recall(pred_bpr),3)) print('precision',round(precision(pred_bpr),3)) print('mrr',round(mrr(pred_bpr),3)) <pre>recall 0.118\nprecision 0.119\nmrr 0.136\n</pre> In\u00a0[24]: Copied! <pre># \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u043f\u0440\u0435\u0434\u043a\u0430\u0437\u0430\u043d\u0438\u044f\ncandidates = pred_bpr[['user_id', 'preds']]\ncandidates = candidates.explode('preds').rename(columns={'preds': 'item_id'})\ncandidates['rank'] = candidates.groupby('user_id').cumcount() + 1\ncandidates.head()\n</pre> # \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u043f\u0440\u0435\u0434\u043a\u0430\u0437\u0430\u043d\u0438\u044f candidates = pred_bpr[['user_id', 'preds']] candidates = candidates.explode('preds').rename(columns={'preds': 'item_id'}) candidates['rank'] = candidates.groupby('user_id').cumcount() + 1 candidates.head() Out[24]: user_id item_id rank 0 2 1267 1 0 2 4475 2 0 2 6470 3 0 2 4072 4 0 2 6738 5 In\u00a0[26]: Copied! <pre># \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u044b (item_id) \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043d\u0430\u0445\u043e\u0434\u044f\u0442\u0441\u044f \n# \u0432 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 (future) \u0443 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f (user_id)\npos = candidates.merge(val,\n                       on=['user_id', 'item_id'],\n                       how='inner')\n\npos['target'] = 1\npos.head()\n</pre> # \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u044b (item_id) \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043d\u0430\u0445\u043e\u0434\u044f\u0442\u0441\u044f  # \u0432 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 (future) \u0443 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f (user_id) pos = candidates.merge(val,                        on=['user_id', 'item_id'],                        how='inner')  pos['target'] = 1 pos.head() Out[26]: user_id item_id rank last_watch_dt total_dur watched_pct target 0 2 9164 8 2021-06-23 6650 100.0 1 1 2 8482 11 2021-06-18 5886 100.0 1 2 46 10440 4 2021-07-05 7449 20.0 1 3 81 10440 4 2021-06-16 5940 20.0 1 4 81 6317 9 2021-06-25 3837 72.0 1 <ul> <li><code>target = 0</code> \u041d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f = \u0432\u0441\u0435 \u0447\u0442\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u041d\u0415 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u043b \u0438\u0437 <code>\u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432 bpr</code> (\u043f\u0435\u0440\u0432\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c). \u0414\u043e\u0431\u0430\u0432\u0438\u043c \u0441\u044d\u043c\u043f\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430, \u0447\u0442\u043e\u0431\u044b \u0441\u043e\u0431\u043b\u044e\u0441\u0442\u0438 \u0431\u0430\u043b\u0430\u043d\u0441 \u043a\u043b\u0430\u0441\u0441\u043e\u0432</li> </ul> In\u00a0[27]: Copied! <pre># \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u043e\u043d\u0438 \u043d\u0435 \u0441\u043c\u043e\u0442\u0440\u0435\u043b\u0438 \u0432 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\nneg = candidates.set_index(['user_id', 'item_id'])\\\n        .join(val.set_index(['user_id', 'item_id']))\n\nneg = neg[neg['watched_pct'].isnull()].reset_index()\nneg = neg.sample(frac=0.07)\nneg['target'] = 0\nneg.head()\n</pre> # \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u043e\u043d\u0438 \u043d\u0435 \u0441\u043c\u043e\u0442\u0440\u0435\u043b\u0438 \u0432 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 neg = candidates.set_index(['user_id', 'item_id'])\\         .join(val.set_index(['user_id', 'item_id']))  neg = neg[neg['watched_pct'].isnull()].reset_index() neg = neg.sample(frac=0.07) neg['target'] = 0 neg.head() Out[27]: user_id item_id rank last_watch_dt total_dur watched_pct target 453077 275088 1112 17 NaN NaN NaN 0 222321 137576 13191 21 NaN NaN NaN 0 313583 192361 13193 30 NaN NaN NaN 0 1765573 1069563 5658 11 NaN NaN NaN 0 52667 32283 8447 7 NaN NaN NaN 0 In\u00a0[28]: Copied! <pre>'''\n\nSplit the [val] user_id into two parts for the second model\n\n'''\n\nctb_train_users, ctb_test_users = train_test_split(val['user_id'].unique(),\n                                                  random_state=1,\n                                                  test_size=0.2)\n\n# \u0432\u044b\u0434\u0435\u043b\u044f\u0435\u043c 10% \u043f\u043e\u0434 \u043c\u0435\u0445\u0430\u043d\u0438\u0437\u043c early stopping\nctb_train_users, ctb_eval_users = train_test_split(ctb_train_users,\n                                                  random_state=1,\n                                                  test_size=0.1)\n\nprint(ctb_train_users)\nprint(ctb_eval_users)\nprint(ctb_test_users)\n</pre> '''  Split the [val] user_id into two parts for the second model  '''  ctb_train_users, ctb_test_users = train_test_split(val['user_id'].unique(),                                                   random_state=1,                                                   test_size=0.2)  # \u0432\u044b\u0434\u0435\u043b\u044f\u0435\u043c 10% \u043f\u043e\u0434 \u043c\u0435\u0445\u0430\u043d\u0438\u0437\u043c early stopping ctb_train_users, ctb_eval_users = train_test_split(ctb_train_users,                                                   random_state=1,                                                   test_size=0.1)  print(ctb_train_users) print(ctb_eval_users) print(ctb_test_users) <pre>[ 769823 1042326  885613 ...  244793  190557  420273]\n[369391 116371 382606 ... 696553 200712 264251]\n[336196 145367 754473 ... 415363 308596 533105]\n</pre> In\u00a0[29]: Copied! <pre>select_col = ['user_id', 'item_id', 'rank', 'target']\n\n# Catboost train\nctb_train = shuffle(\n    pd.concat([\n        pos[pos['user_id'].isin(ctb_train_users)],\n        neg[neg['user_id'].isin(ctb_train_users)]\n])[select_col]\n)\ndisplay(ctb_train.head())\n\n\n# Catboost test\nctb_test = shuffle(\n    pd.concat([\n        pos[pos['user_id'].isin(ctb_test_users)],\n        neg[neg['user_id'].isin(ctb_test_users)]\n])[select_col]\n)\n\nctb_eval = shuffle(\n    pd.concat([\n        pos[pos['user_id'].isin(ctb_eval_users)],\n        neg[neg['user_id'].isin(ctb_eval_users)]\n])[select_col]\n)\n</pre> select_col = ['user_id', 'item_id', 'rank', 'target']  # Catboost train ctb_train = shuffle(     pd.concat([         pos[pos['user_id'].isin(ctb_train_users)],         neg[neg['user_id'].isin(ctb_train_users)] ])[select_col] ) display(ctb_train.head())   # Catboost test ctb_test = shuffle(     pd.concat([         pos[pos['user_id'].isin(ctb_test_users)],         neg[neg['user_id'].isin(ctb_test_users)] ])[select_col] )  ctb_eval = shuffle(     pd.concat([         pos[pos['user_id'].isin(ctb_eval_users)],         neg[neg['user_id'].isin(ctb_eval_users)] ])[select_col] ) user_id item_id rank target 860476 520330 1348 23 0 1345100 813295 2657 11 0 715585 431557 13452 21 0 1676873 1015851 9986 27 0 26785 451018 15297 15 1 <p>\u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u0434\u043b\u044f \u0442\u0435\u0445 \u0436\u0435 \u0432\u044b\u0431\u043e\u0440\u043e\u043a</p> In\u00a0[30]: Copied! <pre>'''\n\nAdd User and Item Features to [ctb_train] &amp; [ctb_eval]\n\n- [ctb_test] will be defined later\n\n'''\n\nuser_col = ['user_id', 'age', 'income', 'sex', 'kids_flg']\nitem_col = ['item_id', 'content_type', 'countries', 'for_kids', 'age_rating', 'studios']\n\n# train   \ntrain_feat = (ctb_train\n              .merge(users[user_col], on=['user_id'], how='left')\n              .merge(items[item_col], on=['item_id'], how='left'))\n\n# evaluation dataset with train for early stopping\neval_feat = (ctb_eval\n             .merge(users[user_col], on=['user_id'], how='left')\n             .merge(items[item_col], on=['item_id'], how='left'))\n\ntrain_feat.head()\n</pre> '''  Add User and Item Features to [ctb_train] &amp; [ctb_eval]  - [ctb_test] will be defined later  '''  user_col = ['user_id', 'age', 'income', 'sex', 'kids_flg'] item_col = ['item_id', 'content_type', 'countries', 'for_kids', 'age_rating', 'studios']  # train    train_feat = (ctb_train               .merge(users[user_col], on=['user_id'], how='left')               .merge(items[item_col], on=['item_id'], how='left'))  # evaluation dataset with train for early stopping eval_feat = (ctb_eval              .merge(users[user_col], on=['user_id'], how='left')              .merge(items[item_col], on=['item_id'], how='left'))  train_feat.head() Out[30]: user_id item_id rank target age income sex kids_flg content_type countries for_kids age_rating studios 0 520330 1348 23 0 age_25_34 income_40_60 \u0416 0.0 film \u0421\u0428\u0410 NaN 16.0 NaN 1 813295 2657 11 0 NaN NaN NaN NaN series \u0420\u043e\u0441\u0441\u0438\u044f NaN 16.0 NaN 2 431557 13452 21 0 age_45_54 income_20_40 \u041c 1.0 film \u0420\u043e\u0441\u0441\u0438\u044f 0.0 18.0 NaN 3 1015851 9986 27 0 age_25_34 income_40_60 \u0416 1.0 series \u0420\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430 \u041a\u043e\u0440\u0435\u044f NaN 16.0 NaN 4 451018 15297 15 1 age_35_44 income_60_90 \u0416 1.0 series \u0420\u043e\u0441\u0441\u0438\u044f NaN 18.0 NaN In\u00a0[31]: Copied! <pre>'''\n\nDefine column information for model \n\n- drop columns [user_id], [item_id]\n- target column: [target]\n- categorical columns: [age] [income] [sex] [content_type] [countries] [studios]\n\n'''\n\ndrop_col = ['user_id', 'item_id']\ntarget_col = ['target']\ncat_col = ['age', 'income', 'sex', 'content_type', 'countries', 'studios']\n\nX_train, y_train = train_feat.drop(drop_col + target_col, axis=1), train_feat[target_col]\nX_val, y_val = eval_feat.drop(drop_col + target_col, axis=1), eval_feat[target_col]\nX_train.shape, y_train.shape, X_val.shape, y_val.shape\nX_train.head()\n</pre> '''  Define column information for model   - drop columns [user_id], [item_id] - target column: [target] - categorical columns: [age] [income] [sex] [content_type] [countries] [studios]  '''  drop_col = ['user_id', 'item_id'] target_col = ['target'] cat_col = ['age', 'income', 'sex', 'content_type', 'countries', 'studios']  X_train, y_train = train_feat.drop(drop_col + target_col, axis=1), train_feat[target_col] X_val, y_val = eval_feat.drop(drop_col + target_col, axis=1), eval_feat[target_col] X_train.shape, y_train.shape, X_val.shape, y_val.shape X_train.head() Out[31]: rank age income sex kids_flg content_type countries for_kids age_rating studios 0 23 age_25_34 income_40_60 \u0416 0.0 film \u0421\u0428\u0410 NaN 16.0 NaN 1 11 NaN NaN NaN NaN series \u0420\u043e\u0441\u0441\u0438\u044f NaN 16.0 NaN 2 21 age_45_54 income_20_40 \u041c 1.0 film \u0420\u043e\u0441\u0441\u0438\u044f 0.0 18.0 NaN 3 27 age_25_34 income_40_60 \u0416 1.0 series \u0420\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430 \u041a\u043e\u0440\u0435\u044f NaN 16.0 NaN 4 15 age_35_44 income_60_90 \u0416 1.0 series \u0420\u043e\u0441\u0441\u0438\u044f NaN 18.0 NaN In\u00a0[32]: Copied! <pre># fillna for catboost with the most frequent value\nX_train = X_train.fillna(X_train.mode().iloc[0])\n\n# fillna for catboost with the most frequent value\nX_val = X_val.fillna(X_train.mode().iloc[0])\n</pre> # fillna for catboost with the most frequent value X_train = X_train.fillna(X_train.mode().iloc[0])  # fillna for catboost with the most frequent value X_val = X_val.fillna(X_train.mode().iloc[0]) In\u00a0[33]: Copied! <pre>X_train.head()\n</pre> X_train.head() Out[33]: rank age income sex kids_flg content_type countries for_kids age_rating studios 0 23 age_25_34 income_40_60 \u0416 0.0 film \u0421\u0428\u0410 0.0 16.0 HBO 1 11 age_35_44 income_20_40 \u041c 0.0 series \u0420\u043e\u0441\u0441\u0438\u044f 0.0 16.0 HBO 2 21 age_45_54 income_20_40 \u041c 1.0 film \u0420\u043e\u0441\u0441\u0438\u044f 0.0 18.0 HBO 3 27 age_25_34 income_40_60 \u0416 1.0 series \u0420\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a\u0430 \u041a\u043e\u0440\u0435\u044f 0.0 16.0 HBO 4 15 age_35_44 income_60_90 \u0416 1.0 series \u0420\u043e\u0441\u0441\u0438\u044f 0.0 18.0 HBO In\u00a0[34]: Copied! <pre>y_train.head()\n</pre> y_train.head() Out[34]: target 0 0 1 0 2 0 3 0 4 1 In\u00a0[35]: Copied! <pre>'''\n\nDefine Hyperparameters for Classifier\n\n'''\n\n# \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\nest_params = {\n  'subsample': 0.9,\n  'max_depth': 5,\n  'n_estimators': 2000,\n  'learning_rate': 0.01,\n  'thread_count': 20,\n  'random_state': 42,\n  'verbose': 200,\n}\n\nctb_model = CatBoostClassifier(**est_params)\n</pre> '''  Define Hyperparameters for Classifier  '''  # \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f est_params = {   'subsample': 0.9,   'max_depth': 5,   'n_estimators': 2000,   'learning_rate': 0.01,   'thread_count': 20,   'random_state': 42,   'verbose': 200, }  ctb_model = CatBoostClassifier(**est_params) In\u00a0[36]: Copied! <pre>import warnings; warnings.filterwarnings('ignore')\nctb_model.fit(X_train,\n              y_train,\n              eval_set=(X_val, y_val),\n              early_stopping_rounds=100,\n              cat_features=cat_col)\n</pre> import warnings; warnings.filterwarnings('ignore') ctb_model.fit(X_train,               y_train,               eval_set=(X_val, y_val),               early_stopping_rounds=100,               cat_features=cat_col) <pre>TBB Warning: The number of workers is currently limited to 3. The request for 19 workers is ignored. Further requests for more workers will be silently ignored until the limit changes.\n\n</pre> <pre>0:\tlearn: 0.6902969\ttest: 0.6903719\tbest: 0.6903719 (0)\ttotal: 181ms\tremaining: 6m 1s\n200:\tlearn: 0.5354166\ttest: 0.5395160\tbest: 0.5395160 (200)\ttotal: 19.2s\tremaining: 2m 51s\n400:\tlearn: 0.5279576\ttest: 0.5323657\tbest: 0.5323657 (400)\ttotal: 39s\tremaining: 2m 35s\n600:\tlearn: 0.5254369\ttest: 0.5302019\tbest: 0.5302019 (600)\ttotal: 58.8s\tremaining: 2m 16s\n800:\tlearn: 0.5241116\ttest: 0.5291537\tbest: 0.5291523 (799)\ttotal: 1m 18s\tremaining: 1m 57s\n1000:\tlearn: 0.5231913\ttest: 0.5284756\tbest: 0.5284747 (995)\ttotal: 1m 38s\tremaining: 1m 37s\n1200:\tlearn: 0.5223263\ttest: 0.5278324\tbest: 0.5278321 (1199)\ttotal: 1m 56s\tremaining: 1m 17s\n1400:\tlearn: 0.5215318\ttest: 0.5273351\tbest: 0.5273351 (1400)\ttotal: 2m 15s\tremaining: 58s\n1600:\tlearn: 0.5210299\ttest: 0.5271259\tbest: 0.5271247 (1597)\ttotal: 2m 33s\tremaining: 38.3s\n1800:\tlearn: 0.5205906\ttest: 0.5269580\tbest: 0.5269575 (1798)\ttotal: 2m 51s\tremaining: 19s\n1999:\tlearn: 0.5202115\ttest: 0.5268008\tbest: 0.5268008 (1999)\ttotal: 3m 8s\tremaining: 0us\n\nbestTest = 0.5268008195\nbestIteration = 1999\n\n</pre> Out[36]: <pre>&lt;catboost.core.CatBoostClassifier at 0x787ab457cb80&gt;</pre> In\u00a0[37]: Copied! <pre>y_pred = ctb_model.predict_proba(X_train)\nf\"ROC AUC score = {roc_auc_score(y_train, y_pred[:, 1]):.2f}\"\n</pre> y_pred = ctb_model.predict_proba(X_train) f\"ROC AUC score = {roc_auc_score(y_train, y_pred[:, 1]):.2f}\" Out[37]: <pre>'ROC AUC score = 0.78'</pre> In\u00a0[38]: Copied! <pre>'''\n\nVisualise the feature importances of features\n\n'''\n\nimp_catboost = pd.DataFrame(X_train.columns, columns = ['feature'])\nimp_catboost['importance'] = ctb_model.feature_importances_\n\nsns.barplot(data=imp_catboost.sort_values(by='importance', ascending=False), x='importance', y='feature', palette=\"BuGn_r\")\nplt.title('Top feature importances');\n</pre> '''  Visualise the feature importances of features  '''  imp_catboost = pd.DataFrame(X_train.columns, columns = ['feature']) imp_catboost['importance'] = ctb_model.feature_importances_  sns.barplot(data=imp_catboost.sort_values(by='importance', ascending=False), x='importance', y='feature', palette=\"BuGn_r\") plt.title('Top feature importances'); In\u00a0[39]: Copied! <pre>'''\n\nPrepare Test Set\n\n'''\n\ntest_feat = (ctb_test\n             .merge(users[user_col], on=['user_id'], how='left')\n             .merge(items[item_col], on=['item_id'], how='left'))\n\n# fillna for catboost with the most frequent value\ntest_feat = test_feat.fillna(X_train.mode().iloc[0])\n\nX_test, y_test = test_feat.drop(drop_col + target_col, axis=1), test_feat['target']\n\n'''\n\nMake prediction on test set\n\n'''\n\ny_pred = ctb_model.predict_proba(X_test)\nf\"ROC AUC score = {roc_auc_score(y_test, y_pred[:, 1]):.2f}\"\n</pre> '''  Prepare Test Set  '''  test_feat = (ctb_test              .merge(users[user_col], on=['user_id'], how='left')              .merge(items[item_col], on=['item_id'], how='left'))  # fillna for catboost with the most frequent value test_feat = test_feat.fillna(X_train.mode().iloc[0])  X_test, y_test = test_feat.drop(drop_col + target_col, axis=1), test_feat['target']  '''  Make prediction on test set  '''  y_pred = ctb_model.predict_proba(X_test) f\"ROC AUC score = {roc_auc_score(y_test, y_pred[:, 1]):.2f}\" Out[39]: <pre>'ROC AUC score = 0.77'</pre> In\u00a0[40]: Copied! <pre>'''\n\nGlobal Test [test] \n\n'''\n\n# group [item_id] for each [user_id] in test (main test)\ntest = test[test['user_id'].isin(val['user_id'].unique())] # test user_id must contain val user_id\ntest_user_history = test.groupby('user_id')[['item_id']].agg(lambda x: list(x))\ndisplay(test_user_history.head())\n</pre> '''  Global Test [test]   '''  # group [item_id] for each [user_id] in test (main test) test = test[test['user_id'].isin(val['user_id'].unique())] # test user_id must contain val user_id test_user_history = test.groupby('user_id')[['item_id']].agg(lambda x: list(x)) display(test_user_history.head()) item_id user_id 21 [13787, 14488] 30 [4181, 8584, 8636] 98 [89, 512] 106 [337, 1439, 2808, 2836, 5411, 6267, 10544, 128... 241 [6162, 8986, 10440, 12138] In\u00a0[41]: Copied! <pre>'''\n\n(1) User first model for prediction of recommendations for [test] set\n\n'''\n\n# first model prediction for k=100\npred_bpr = predict(user_vecs, item_vecs, k=100)\npred_bpr = test_user_history.merge(pred_bpr, how='left', on='user_id')\ndisplay(pred_bpr.head()) # overlap b/w [train] (user_vect/item_vecs) prediction and test set (item_id)\n\n# determine overlapping metrics b/w test and train\nprint('recall',round(recall(pred_bpr, k=20),3))\nprint('precision',round(precision(pred_bpr, k=20),3))\nprint('mrr',round(mrr(pred_bpr, k=20),3))\n</pre> '''  (1) User first model for prediction of recommendations for [test] set  '''  # first model prediction for k=100 pred_bpr = predict(user_vecs, item_vecs, k=100) pred_bpr = test_user_history.merge(pred_bpr, how='left', on='user_id') display(pred_bpr.head()) # overlap b/w [train] (user_vect/item_vecs) prediction and test set (item_id)  # determine overlapping metrics b/w test and train print('recall',round(recall(pred_bpr, k=20),3)) print('precision',round(precision(pred_bpr, k=20),3)) print('mrr',round(mrr(pred_bpr, k=20),3)) user_id item_id preds 0 21 [13787, 14488] [849, 24, 1053, 11237, 14703, 8636, 12659, 496... 1 30 [4181, 8584, 8636] [142, 16201, 13865, 10440, 4880, 1465, 2303, 1... 2 98 [89, 512] [2786, 16201, 14337, 4350, 11402, 14053, 12994... 3 106 [337, 1439, 2808, 2836, 5411, 6267, 10544, 128... [16166, 14942, 16270, 3632, 11539, 15084, 1522... 4 241 [6162, 8986, 10440, 12138] [13653, 16174, 2823, 7571, 13018, 7889, 16166,... <pre>recall 0.044\nprecision 0.044\nmrr 0.023\n</pre> In\u00a0[42]: Copied! <pre>'''\n\n(1a) Improve the first model factorisation parameter (k=100)\n\n'''\n\n# \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u043f\u0440\u0435\u0434\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043e\u0442 \u043f\u0435\u0440\u0432\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 (k=100)\npred_bpr = pred_bpr[['user_id', 'preds']] \npred_bpr = pred_bpr.explode('preds').rename(columns={'preds': 'item_id'})\npred_bpr['rank'] = pred_bpr.groupby('user_id').cumcount() + 1 # give rank to each item_id order\npred_bpr.head()\n</pre> '''  (1a) Improve the first model factorisation parameter (k=100)  '''  # \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u043f\u0440\u0435\u0434\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043e\u0442 \u043f\u0435\u0440\u0432\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 (k=100) pred_bpr = pred_bpr[['user_id', 'preds']]  pred_bpr = pred_bpr.explode('preds').rename(columns={'preds': 'item_id'}) pred_bpr['rank'] = pred_bpr.groupby('user_id').cumcount() + 1 # give rank to each item_id order pred_bpr.head() Out[42]: user_id item_id rank 0 21 849 1 0 21 24 2 0 21 1053 3 0 21 11237 4 0 21 14703 5 In\u00a0[43]: Copied! <pre>'''\n\n(2a) Prepare the dataset for the second model prediction\n\n'''\n\npred_bpr_ctb = pred_bpr.copy()\n\n# \u0444\u0438\u0447\u0438 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u0430\nscore_feat = (pred_bpr_ctb\n              .merge(users[user_col], on=['user_id'], how='left')\n              .merge(items[item_col], on=['item_id'], how='left'))\n\n# fillna for catboost with the most frequent value\nscore_feat = score_feat.fillna(X_train.mode().iloc[0])\nscore_feat.head()\n</pre> '''  (2a) Prepare the dataset for the second model prediction  '''  pred_bpr_ctb = pred_bpr.copy()  # \u0444\u0438\u0447\u0438 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u0430 score_feat = (pred_bpr_ctb               .merge(users[user_col], on=['user_id'], how='left')               .merge(items[item_col], on=['item_id'], how='left'))  # fillna for catboost with the most frequent value score_feat = score_feat.fillna(X_train.mode().iloc[0]) score_feat.head() Out[43]: user_id item_id rank age income sex kids_flg content_type countries for_kids age_rating studios 0 21 849 1 age_45_54 income_20_40 \u0416 0.0 film \u0421\u0428\u0410 0.0 18.0 HBO 1 21 24 2 age_45_54 income_20_40 \u0416 0.0 series \u0413\u0435\u0440\u043c\u0430\u043d\u0438\u044f 0.0 16.0 HBO 2 21 1053 3 age_45_54 income_20_40 \u0416 0.0 film \u0421\u0428\u0410 0.0 18.0 HBO 3 21 11237 4 age_45_54 income_20_40 \u0416 0.0 film \u0420\u043e\u0441\u0441\u0438\u044f 0.0 16.0 HBO 4 21 14703 5 age_45_54 income_20_40 \u0416 0.0 film \u0421\u0428\u0410 0.0 18.0 HBO In\u00a0[44]: Copied! <pre>'''\n\n(2b) User second model; trained on validation dataset \n\n'''\n\nctb_prediction = ctb_model.predict_proba(score_feat.drop(drop_col, axis=1, errors='ignore'))\n\npred_bpr_ctb['ctb_pred'] = ctb_prediction[:, 1] # prob for positive class\npred_bpr_ctb.head(3)\n</pre> '''  (2b) User second model; trained on validation dataset   '''  ctb_prediction = ctb_model.predict_proba(score_feat.drop(drop_col, axis=1, errors='ignore'))  pred_bpr_ctb['ctb_pred'] = ctb_prediction[:, 1] # prob for positive class pred_bpr_ctb.head(3) Out[44]: user_id item_id rank ctb_pred 0 21 849 1 0.269009 0 21 24 2 0.228995 0 21 1053 3 0.261176 <ul> <li>\u041f\u0440\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0438\u0442\u043e\u0433\u043e\u0432\u044b\u0439 \u0440\u0430\u043d\u043a \u0434\u0432\u0443\u0445\u044d\u0442\u0430\u043f\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 <code>rank_ctb</code></li> </ul> In\u00a0[45]: Copied! <pre># \u0441\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043f\u043e \u0441\u043a\u043e\u0440\u0443 \u0432\u043d\u0443\u0442\u0440\u0438 \u043e\u0434\u043d\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0438 \u043f\u0440\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u043d\u043e\u0432\u044b\u0439 \u0440\u0430\u043d\u0433\npred_bpr_ctb = pred_bpr_ctb.sort_values(\n                                        by=['user_id', 'ctb_pred'], \n                                        ascending=[True, False])\npred_bpr_ctb['rank_ctb'] = pred_bpr_ctb.groupby('user_id').cumcount() + 1\npred_bpr_ctb.head()\n</pre> # \u0441\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043f\u043e \u0441\u043a\u043e\u0440\u0443 \u0432\u043d\u0443\u0442\u0440\u0438 \u043e\u0434\u043d\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0438 \u043f\u0440\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u043d\u043e\u0432\u044b\u0439 \u0440\u0430\u043d\u0433 pred_bpr_ctb = pred_bpr_ctb.sort_values(                                         by=['user_id', 'ctb_pred'],                                          ascending=[True, False]) pred_bpr_ctb['rank_ctb'] = pred_bpr_ctb.groupby('user_id').cumcount() + 1 pred_bpr_ctb.head() Out[45]: user_id item_id rank ctb_pred rank_ctb 0 21 11237 4 0.481346 1 0 21 8636 6 0.370781 2 0 21 11661 9 0.350955 3 0 21 496 8 0.336498 4 0 21 8252 36 0.289617 5 <p>\u041f\u043e\u0434\u0441\u0447\u0435\u0442 \u043c\u0435\u0442\u0440\u0438\u043a \u043f\u0440\u0435\u043a\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043f\u0435\u0440\u0432\u0440\u043e\u0439 + \u0432\u0442\u043e\u0440\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u0440\u043e\u0441\u043c\u043e\u0440\u043e\u0432 \u0432 \u0442\u0435 \u0442\u043e\u0432\u043e\u0439 \u044b\u044b\u0431\u043e\u0440\u043a\u0435</p> In\u00a0[46]: Copied! <pre>'''\n\n1+2 \u044d\u0442\u0430\u043f\u044b bpr + \u0421atboost: \u041c\u0435\u0442\u0440\u0438\u043a\u0438\n\n'''\n\ntrue_items = test.groupby('user_id').agg(lambda x: list(x))[['item_id']].reset_index()\npred_items = pred_bpr_ctb.groupby('user_id').agg(lambda x: list(x))[['item_id']].reset_index().rename(columns={'item_id': 'preds'})\ntrue_pred_items = true_items.merge(pred_items, how='left')\ntrue_pred_items.head()\n</pre> '''  1+2 \u044d\u0442\u0430\u043f\u044b bpr + \u0421atboost: \u041c\u0435\u0442\u0440\u0438\u043a\u0438  '''  true_items = test.groupby('user_id').agg(lambda x: list(x))[['item_id']].reset_index() pred_items = pred_bpr_ctb.groupby('user_id').agg(lambda x: list(x))[['item_id']].reset_index().rename(columns={'item_id': 'preds'}) true_pred_items = true_items.merge(pred_items, how='left') true_pred_items.head() Out[46]: user_id item_id preds 0 21 [13787, 14488] [11237, 8636, 11661, 496, 8252, 12701, 10313, ... 1 30 [4181, 8584, 8636] [1465, 4880, 10440, 4151, 14192, 15297, 1261, ... 2 98 [89, 512] [14192, 16201, 7829, 1299, 12346, 5648, 10152,... 3 106 [337, 1439, 2808, 2836, 5411, 6267, 10544, 128... [16166, 7571, 14942, 11539, 13243, 3632, 3834,... 4 241 [6162, 8986, 10440, 12138] [16174, 7571, 13018, 16166, 5979, 242, 13913, ... In\u00a0[47]: Copied! <pre>print('recall',round(recall(true_pred_items, k=20),3))\nprint('precision',round(precision(true_pred_items, k=20),3))\nprint('mrr',round(mrr(true_pred_items, k=20),3))\n</pre> print('recall',round(recall(true_pred_items, k=20),3)) print('precision',round(precision(true_pred_items, k=20),3)) print('mrr',round(mrr(true_pred_items, k=20),3)) <pre>recall 0.055\nprecision 0.055\nmrr 0.033\n</pre>"},{"location":"portfolio/course_recsys/recsys_twomodel.html#2-kion-dataset","title":"2 | KION Dataset\u00b6","text":"<p>\u041f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u044b \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u0438 \u0441\u0435\u0440\u0438\u0430\u043b\u043e\u0432 \u0432 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0438 KION:</p> <ul> <li>\u0434\u0430\u043d\u043d\u044b\u0435 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0441 \u0442\u043e\u0432\u0430\u0440\u0430\u043c\u0438 (\u0444\u0438\u043b\u044c\u043c\u044b) <code>interactions</code></li> <li>\u0434\u0430\u043d\u043d\u044b\u0435 \u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u0445 <code>users</code></li> <li>\u0434\u0430\u043d\u043d\u044b\u0435 \u043e\u0431 \u0430\u0439\u0442\u0435\u043c\u0430\u0445 (\u0444\u0438\u043b\u044c\u043c\u044b, \u0441\u0435\u0440\u0438\u0430\u043b\u044b \u0438 \u0442.\u0434.) <code>items</code></li> </ul>"},{"location":"portfolio/course_recsys/recsys_twomodel.html","title":"\u0414\u0430\u043d\u043d\u044b\u0435 \u043e \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u0445 \u0441 \u0444\u0438\u043b\u044c\u043c\u0430\u043c\u0438 \u00b6","text":"<p>\u041a\u0430\u043a \u0438 \u0440\u0430\u043d\u0448\u0435 \u0443 \u043d\u0430\u0441 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u044b\u0435 \u0444\u0438\u0447\u0438</p> <ul> <li><code>user_id</code> : \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c</li> <li><code>item_id</code> : \u0444\u0438\u043b\u044c\u043c</li> </ul> <p>\u0423 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u0444\u0438\u0447\u0430 \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0433\u043e \u0437\u0430\u043f\u0443\u0441\u043a\u0430 \u0444\u0438\u043b\u044c\u043c\u0430</p> <ul> <li><code>last_watch_dt</code> \u0434\u0430\u0442\u0430 \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u0430</li> </ul> <p>\u0418 \u0444\u0438\u0447\u0438 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439:</p> <ul> <li><code>total_dur</code> \u0434\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u0430</li> <li><code>watched_pct</code> \u0434\u043e\u043b\u044f \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u0430 \u0432 %</li> </ul> <p>\u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u044d\u0442\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u043a\u0430\u043a \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u0439 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438</p>"},{"location":"portfolio/course_recsys/recsys_twomodel.html","title":"\u0414\u0430\u043d\u043d\u044b\u0435 \u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u0445\u00b6","text":"<p>\u0412\u0441\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 - \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0441\u043e\u0446\u0434\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u0435\u0439</p> <ul> <li><code>age</code> : \u0412\u043e\u0437\u0440\u043e\u0441\u0442\u043d\u0430\u044f \u0433\u0440\u0443\u043f\u043f\u0430</li> <li><code>income</code> : \u0413\u0440\u0443\u043f\u043f\u0430 \u0434\u043e\u0445\u043e\u0434\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</li> <li><code>sex</code> : \u041f\u043e\u043b \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</li> <li><code>kids_flg</code> : \u0444\u043b\u0430\u0433 \u043d\u0430\u043b\u0438\u0447\u0438\u044f \u0434\u0435\u0442\u0435\u0439</li> </ul>"},{"location":"portfolio/course_recsys/recsys_twomodel.html#3-preprocessing","title":"3 | Preprocessing\u00b6","text":""},{"location":"portfolio/course_recsys/recsys_twomodel.html#1","title":"1) \u0424\u0438\u043b\u044c\u0442\u0440\u0443\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0435 \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u044b\u00b6","text":"<p>\u041d\u0435 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0432\u0441\u0435 \u0434\u0430\u043d\u043d\u044b\u0435, \u043d\u0430 \u043f\u0435\u0440\u0432\u043e\u0439 \u0441\u0442\u0430\u0434\u0438\u0439 \u043e\u0442\u0444\u0438\u043b\u044c\u0440\u0443\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0435 \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u044b \u0438\u0437 <code>interactions</code></p>"},{"location":"portfolio/course_recsys/recsys_twomodel.html#2","title":"2) \u0424\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439\u00b6","text":"<p>\u0414\u0430\u043b\u0435\u0435 \u043e\u0442\u0444\u0438\u043b\u044c\u0442\u0440\u0443\u0435\u043c \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u044b\u0445 \u0444\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438\u0437 \u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u043d\u0430\u0441 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u0443\u044e\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438 \u0443 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 10 \u0438\u043b\u0438 \u0431\u043e\u043b\u044c\u0448\u0435 \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432</p>"},{"location":"portfolio/course_recsys/recsys_twomodel.html#3","title":"3) \u0424\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u044f \u0444\u0438\u043b\u044c\u043c\u043e\u0432\u00b6","text":"<p>\u0414\u0430\u043b\u0435\u0435 \u0431\u0443\u0434\u0435\u043c \u0444\u0438\u043b\u044c\u0442\u0440\u043e\u0432\u0430\u0442\u044c \u0443\u0436\u0435 \u043f\u043e \u0444\u0438\u043b\u044c\u043c\u0430\u043c, \u043d\u0430\u0441 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u0443\u044e\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u0444\u0438\u043b\u044c\u043c\u044b \u0443 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0431\u043e\u043b\u044c\u0448\u0435 10 \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432 \u0432\u0440\u0435\u0434\u0438 \u0432\u0441\u0435\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439</p>"},{"location":"portfolio/course_recsys/recsys_twomodel.html#4-traintest-splitting","title":"4 | Train/Test Splitting\u00b6","text":""},{"location":"portfolio/course_recsys/recsys_twomodel.html","title":"\u0414\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u044b \u0432\u0440\u0435\u043c\u0435\u043d\u0438\u00b6","text":"<p>\u0422\u0430\u043a \u043a\u0430\u043a \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0442\u044c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 \u0447\u0430\u0441\u0442\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0432\u0440\u0435\u043c\u044f, \u043d\u0430\u0439\u0434\u0435\u043c \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u0441\u044f \u0432 \u0434\u0430\u043d\u043d\u044b\u0445</p>"},{"location":"portfolio/course_recsys/recsys_twomodel.html#5-stage-1-model-bpr-matrix-decomposition","title":"5 | Stage 1 Model : BPR Matrix Decomposition \u00b6","text":"<ul> <li><p>[val] : must contain users found in [train]</p> </li> <li><p>[train] : used to train first level model</p> </li> <li><p>use [train] to create an initial selection of user recommendations for each user we select 30 items for each user, stored in [pred_bpr]</p> </li> <li><p>Having created recommendations using [train], we compare them with the films they watched in the validation dataset (ie future) and evaluate metrics</p> </li> </ul>"},{"location":"portfolio/course_recsys/recsys_twomodel.html#traintest","title":"train/test \u043f\u0435\u0440\u0435\u0441\u0435\u0447\u0435\u043d\u0438\u0435\u00b6","text":"<p>\u0418\u043c\u0435\u044f \u0440\u0435\u043a\u0435\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u043e\u0442 \u043f\u0435\u0440\u0432\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 (\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0430\u044f \u043d\u0430 [train]), \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0441\u043e\u0432\u043f\u0430\u0434\u0435\u043d\u0438\u044f \u043c\u0435\u0436\u0434\u0443 [train] \u0438 [val]</p>"},{"location":"portfolio/course_recsys/recsys_twomodel.html#2","title":"\u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432 2\u0439 \u043c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<p>\u041a \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\u043c \u0434\u043e\u0431\u0430\u0432\u0438\u043c \u0440\u0430\u043d\u043a \u043f\u043e\u0440\u044f\u0442\u043a\u0430 \u0441\u0430\u043c\u043e\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f</p>"},{"location":"portfolio/course_recsys/recsys_twomodel.html#6-stage-2-model-gradient-biositng-classifier","title":"6 | Stage 2 Model : Gradient Biositng Classifier\u00b6","text":"<ul> <li>\u0421\u0442\u0430\u0432\u0438\u043c \u0437\u0430\u0434\u0430\u0447\u0443 \u043a\u0430\u043a <code>\u0431\u0438\u043d\u0430\u0440\u043d\u0443\u044e \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044e</code>, \u0431\u0443\u0434\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u043d\u0430 \u0441\u043a\u043e\u043b\u044c\u043a\u043e <code>user_id</code> \u043f\u043e\u043d\u0440\u0430\u0432\u0438\u0442\u0441\u044f \u0444\u0438\u043b\u044c\u043c \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043b\u0438 \u0432 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u044b</li> <li>\u041c\u044b \u0438\u043c\u0435\u0435\u043c \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0443; \u0437\u043d\u0430\u0435\u043c \u043a\u0430\u043a\u0438\u0435 \u0444\u0438\u043b\u044c\u043c\u044b \u043e\u043d\u0438 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u044f\u0442 \u0432 \u0431\u0443\u0434\u0443\u0439\u0449\u0435\u043c, \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e <code>train</code></li> <li>\u041c\u044b \u0442\u0430\u043a \u0436\u0435 \u0438\u043c\u0435\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438</li> <li>\u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c <code>CatBoostClassifier</code>, \u0442\u0430\u043a \u043a\u0430\u043a \u043c\u043d\u043e\u0433\u043e \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432</li> <li>\u041c\u0430\u0442\u043f\u0438\u0446\u0430 \u0444\u0438\u0447\u0435\u0439: <code>\u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u044e\u0437\u0435\u0440\u0430</code>, <code>\u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0430\u0439\u0442\u0435\u043c\u0430</code>, <code>\u0432\u0437\u0430\u0438\u043c\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438</code>,<code>\u0440\u0430\u043d\u043a\u0438/\u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438</code></li> </ul>"},{"location":"portfolio/course_recsys/recsys_twomodel.html","title":"\u0421\u043e\u0431\u0438\u0440\u0430\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443\u00b6","text":"<p>\u041c\u044b \u0437\u043d\u0430\u0435\u043c \u0447\u0442\u043e \u0432 \u0431\u0443\u0434\u0443\u0439\u0449\u0435\u043c \u043e\u043d \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u0442 \u0444\u0438\u043b\u044c\u043c\u044b \u0438\u0437 <code>val</code></p> <ul> <li><code>target = 1</code> \u041f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u044b\u0435 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f = \u0432\u0441\u0435 \u0447\u0442\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u043b \u0438\u0437 <code>\u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432 bpr</code>(\u043f\u0435\u0440\u0432\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c)</li> </ul>"},{"location":"portfolio/course_recsys/recsys_twomodel.html#2","title":"\u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0434\u043b\u044f 2\u0439 \u043c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<ul> <li><p>\u0412\u0430\u0436\u043d\u043e! \u0414\u0435\u043b\u0438\u043c \u043f\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c (<code>user_id</code>), \u0430 \u043d\u0435 \u043f\u043e \u0434\u0430\u0442\u0435</p> </li> <li><p>\u0414\u043b\u044f \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439 \u043d\u0435\u0442 \u0434\u0430\u0442\u044b</p> </li> <li><p>\u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u0443\u0441\u0442\u043e\u0439\u0447\u0438\u0432\u043e\u0441\u0442\u044c \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u0430 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043d\u043e\u0432\u044b\u0445 (\u0445\u043e\u043b\u043e\u0434\u043d\u044b\u0445) \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439</p> </li> <li><p><code>ctb_train_users</code> (\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430, \u043f\u0440\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0439 \u0431\u0443\u0434\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f <code>ctb_eval_users</code> \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u0430</p> </li> <li><p><code>ctb_test_users</code> \u0432\u044b\u0431\u043e\u0440\u043a\u0430 \u0434\u043b\u044f \u043e\u0442\u043b\u043e\u0436\u0435\u043d\u043d\u043e\u0439 \u043e\u0446\u0435\u043d\u043a\u0438 \u043c\u0435\u0442\u0440\u0438\u043a</p> </li> </ul>"},{"location":"portfolio/course_recsys/recsys_twomodel.html","title":"\u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u0424\u0438\u0447\u0438\u00b6","text":"<ul> <li>\u0412 \u044d\u0442\u043e\u043c \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435 \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0435\u043d \u0431\u0435\u0439\u0437\u043b\u0430\u0439\u043d \u0441 \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u043e\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 - \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0435\u0435 \u0434\u043b\u044f Catboost</li> <li>\u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u0440\u043e\u0441\u0442\u044b\u0435 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u043d\u0443\u0436\u043d\u043e \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0442\u044c</li> <li>\u041a\u0430\u043a \u043f\u0440\u0438\u0437\u043d\u0430\u043a \u0442\u0430\u043a\u0436\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c <code>rank</code> \u043e\u0442 \u043c\u043e\u0434\u0435\u043b\u0438 1 \u044d\u0442\u0430\u043f\u0430</li> <li>\u0432 \u043f\u0435\u0440\u0432\u043e\u0439 \u0447\u0430\u0441\u0442\u0438 \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043a\u043e\u043b\u043e\u043d\u043a\u0438 ('user_id', 'item_id', 'rank', 'target')</li> </ul>"},{"location":"portfolio/course_recsys/recsys_twomodel.html#2","title":"\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u041c\u043e\u0434\u0435\u043b\u044c 2\u0433\u043e \u042d\u0442\u0430\u043f\u0430\u00b6","text":"<ul> <li>\u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u043e 1 \u044d\u0442\u0430\u043f\u0443, \u043f\u0440\u0435\u0434\u043f\u043e\u043b\u0430\u0433\u0430\u0435\u043c \u0447\u0442\u043e \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0435 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0443\u0436\u0435 \u0432\u044b\u0431\u0440\u0430\u043d\u044b</li> <li>\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043c\u0435\u0445\u0430\u043d\u0438\u0437\u043c early_stopping \u0434\u043b\u044f \u0432\u044b\u0431\u043e\u0440\u0430 \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 \u043d\u0430 \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435</li> </ul>"},{"location":"portfolio/course_recsys/recsys_twomodel.html#2","title":"\u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u041c\u043e\u0434\u0435\u043b\u0438 2 \u044d\u0442\u0430\u043f\u0430 \u043d\u0430 \u043e\u0442\u043b\u043e\u0436\u0435\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\u00b6","text":""},{"location":"portfolio/course_recsys/recsys_twomodel.html#7-test","title":"7 | \u041c\u0435\u0442\u0440\u0438\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043d\u0430 \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u043e\u043c test\u00b6","text":"<ul> <li>\u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e \u043c\u043e\u0434\u0435\u043b\u044c\u044e 1 \u044d\u0442\u0430\u043f\u0430 - bpr</li> <li>\u041e\u0442\u0434\u0435\u043b\u044c\u043d\u043e \u0434\u0432\u0443\u0445\u044d\u0442\u0430\u043f\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u044c\u044e bpr + Catboost</li> <li>\u0421\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0435\u043c</li> </ul>"},{"location":"portfolio/course_recsys/recsys_twomodel.html#bpr","title":"\u041c\u0435\u0442\u0440\u0438\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 bpr\u00b6","text":""},{"location":"portfolio/course_recsys/recsys_twomodel.html#bpr-catboost","title":"\u041c\u0435\u0442\u0440\u0438\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u0434\u0432\u0443\u0445\u044d\u0442\u0430\u043f\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438: <code>bpr</code> + <code>Catboost</code>\u00b6","text":"<ul> <li><code>pred_bpr</code> contains predictions from first model with k=100</li> <li>Add user and item based features to recommendations from first model</li> <li>Use 2nd stage model, trained on validation set <code>val</code> to get probability of positive class (<code>ctb_pred</code>)</li> </ul>"},{"location":"portfolio/internships/bcgx_powerco.html","title":"BCGX PowerCo","text":""},{"location":"portfolio/internships/bcgx_powerco.html#retaining-customer-clients-vulnerable-to-churn","title":"Retaining customer clients vulnerable to churn","text":"<p> GitHub Repository</p> <p>Part of the BCG Data Science Internship program </p> <p>In this project, we work with another client who is a major gas and electricity utility that supplies to small and medium sized enterprises</p> <ul> <li>The energy market has had a lot of change in recent years and there are more options than ever for customers to choose from</li> <li>They are concerned about their customers leaving for better offers from other energy providers</li> <li>We investigate whether price sensitivity is the most influential factor for a customer churning</li> <li>Conduct feature engineering that is used to test hypotheses related to customer churn</li> <li>And finally we utilise predictive modelling so that it can be used to highlight customers at risk of churn</li> </ul> <p></p> <ul> <li> <p> Business Understanding &amp; Hypothesis Framing</p> <p> What you'll learn</p> <ul> <li>Meet your client PowerCo - a major gas and electricity utility who is concerned about losing customers</li> <li>How to interpret the business context</li> <li>How to break down the problem before you start your data analysis</li> </ul> <p> What you'll do</p> <ul> <li>Determine the client data needed for analysis</li> <li>Outline the techniques you'll use to investigate your client's problem</li> <li>Write an email to your Associate Director summarizing your approach</li> </ul> </li> <li> <p> Exploratory Data Analysis</p> <p> What you'll learn</p> <ul> <li>How to investigate whether price sensitivity is the most influential factor for a customer churning</li> <li>How to use frameworks to conduct exploratory data analysis</li> </ul> <p> What you'll do</p> <ul> <li>Use python to analyze client data</li> <li>Create data visualizations to help you interpret key trends</li> </ul> </li> <li> <p> Feature Engineering &amp; Modelling</p> <p> What you'll learn</p> <ul> <li>How feature engineering can be used to test hypotheses </li> <li>How to build features to analyse the data for PowerCo</li> </ul> <p> What you'll do</p> <ul> <li>Use Python to build a new feature for your analysis</li> </ul> </li> <li> <p> Findings &amp; Recommendations</p> <p> What you'll learn</p> <ul> <li>How predictive modelling can be used to indicate churn risk</li> <li>How to communicate your insights with clients</li> </ul> <p> What you'll do</p> <ul> <li>Build a predictive model for churn using a random forest technique</li> <li>Write an executive summary with your findings</li> </ul> </li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html","title":"Financial consumer complaint analysis","text":"In\u00a0[1]: Copied! <pre>pip install -U kaleido\n</pre> pip install -U kaleido <pre>Collecting kaleido\r\n  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\r\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 79.9/79.9 MB 13.6 MB/s eta 0:00:00\r\nInstalling collected packages: kaleido\r\nSuccessfully installed kaleido-0.2.1\r\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\r\nNote: you may need to restart the kernel to use updated packages.\n</pre> <p>USEFULNESS OF CONSUMER FEEDBACK</p> <p>Can your customers tell you something important? | Source</p> <p>If you run your own business, I know you do your best to please your customers  Satisfy their needs, and keep them loyal to your brand.   But how can you be sure that your efforts bring desired results?   If you do not try to find out what your clients think about your service  You will never be able to give them the best customer experience.  Their opinions about their experience with your brand are helpful information That you can use to adjust your business to fit their needs more accurately</p> <ul> <li>The source clearly outlines that consumer feedback is quite critical for any business</li> <li>Consumer feedback in our problem is related to a consumer having an <code>issue</code> with a particualr financial <code>product</code> or alike</li> </ul> <p>CONSUMER FEEDBACK EXAMPLES</p> <p>Let's look at a couple of examples of a consumer's addresses to a company:</p> <p>Product: Credit reporting | Issue: Incorrect information on credit report</p> <p>After looking at my credit report I saw a collection account that does not belong to me. I am not allowed to dispute this information online on Experian or over the phone making it impossible for me. This false information is ruining my credit and knowing full well this people did not do their job and allow people to just post false accounts on my report. They need to delete this information immediately and do a proper investigation as this information is not mine. '</p> <p>Product: Credit card | Issue: Credit line increase/decrease</p> <p>\"XXXX i receive an email from citibank regarding my XXXX credit card. It was an offer to request a credit increase and it clearly stated that there would be NO Credit bureau inquiry made. I clicked on the link in the email and entered the requested information. a couple of days later I received an alert from my credit bureau monitoring service that a hard inquiry was done. Upon looking at the report it showed Citibank credit cards making a hard credit inquiry which was completely opposite of what their email stated. I called citi and they confimred that the email stated there would be no creidt inquiry done however they said that the request was made on a different citibank credit card which is why the hard inquiry was made. I explained to the rep I clicked on the link they provided and if was for a different account of mine it was not my issue but theirs and they need to remove the inquiry. They told me to send a letter to their credit dispute department explaining it. I sent the letter after waiting more than a month I received a blunt statement stating the it was a valid credit request and they will not remove the inquiry from my credit bureau. Citi performed bait and switch by offering a no inquiry credit request and then doing a hard inquiry which has negatively affected my credit score. I asked to remove it and received a generic letter stating they would not with no number to contact the department that sent the letter when i called the main customer service number they said that department dosent talk to customers and there was nothing else they can do. This has negatively affected my credit score and will remain on my credit report for 2 years because citi 's False advertising. and then their lack of fixing their error \"</p> <p>After reading this long complaint:</p> <ul> <li>It should become apparent that manual evaluation of each consumer issue can can a while to process and is very inefficient</li> <li>For a timely &amp; helpful consumer response, the relevant problem not only must be processed in a timely manner, but passed on to a specific expert that has experience dealing with the particular issue </li> </ul> <p> <p> </p> In\u00a0[2]: Copied! <pre>%%time\nimport pandas as pd\nimport plotly.express as px\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('/kaggle/input/complaintsfull/main.csv',low_memory=False)\ndf = df.drop(['Unnamed: 0'],axis=1)\n</pre> %%time import pandas as pd import plotly.express as px import numpy as np import seaborn as sns import matplotlib.pyplot as plt  df = pd.read_csv('/kaggle/input/complaintsfull/main.csv',low_memory=False) df = df.drop(['Unnamed: 0'],axis=1) <pre>CPU times: user 22.3 s, sys: 3.15 s, total: 25.5 s\nWall time: 36.9 s\n</pre> In\u00a0[3]: Copied! <pre>import missingno as ms\nms.matrix(df)\n</pre> import missingno as ms ms.matrix(df) Out[3]: <pre>&lt;AxesSubplot:&gt;</pre> <ul> <li> We have quite a bit of missing data, we have already removed rows, which have missing data in our text target (consumer complaint narrative)</li> <li> And quite a heavy dataset, let's utilise only the relevant data for our problem (by the end of this section) which should reduce the number of rows in our data significatntly </li></ul> In\u00a0[4]: Copied! <pre>print('Dataset Features')\ndf.columns\n</pre> print('Dataset Features') df.columns <pre>Dataset Features\n</pre> Out[4]: <pre>Index(['Date received', 'Product', 'Sub-product', 'Issue', 'Sub-issue',\n       'Consumer complaint narrative', 'Company public response', 'Company',\n       'State', 'ZIP code', 'Tags', 'Consumer consent provided?',\n       'Submitted via', 'Date sent to company', 'Company response to consumer',\n       'Timely response?', 'Consumer disputed?', 'Complaint ID', 'Year',\n       'Month', 'Day', 'DoW'],\n      dtype='object')</pre> <ul> <li>Date received - When the complaint was addressed</li> <li>Product - Complaint Type</li> <li>Issue - Brief summary of the issue</li> <li>Consumer complaint narrative - What the customer wrote (documents)</li> <li>Company public response - How did the company respond</li> <li>State - State in which the complaint was made</li> <li>Submitted - Form of complaint</li> <li>Customer disputed? - Did the customer dispute the response</li> </ul> In\u00a0[5]: Copied! <pre>def object_to_datetime_features(df,column):\n\n    df[column] = df[column].astype('datetime64[ns]')\n    df['Year'] = df[column].dt.year\n    df['Month'] = df[column].dt.month\n    df['Day'] = df[column].dt.day\n    df['DoW'] = df[column].dt.dayofweek\n    df['DoW'] = df['DoW'].replace({0:'Monday',1:'Tuesday',2:'Wednesday',\n                                   3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'})\n    return df\n\ndf = object_to_datetime_features(df,'Date received')\ndf.columns\n</pre> def object_to_datetime_features(df,column):      df[column] = df[column].astype('datetime64[ns]')     df['Year'] = df[column].dt.year     df['Month'] = df[column].dt.month     df['Day'] = df[column].dt.day     df['DoW'] = df[column].dt.dayofweek     df['DoW'] = df['DoW'].replace({0:'Monday',1:'Tuesday',2:'Wednesday',                                    3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'})     return df  df = object_to_datetime_features(df,'Date received') df.columns Out[5]: <pre>Index(['Date received', 'Product', 'Sub-product', 'Issue', 'Sub-issue',\n       'Consumer complaint narrative', 'Company public response', 'Company',\n       'State', 'ZIP code', 'Tags', 'Consumer consent provided?',\n       'Submitted via', 'Date sent to company', 'Company response to consumer',\n       'Timely response?', 'Consumer disputed?', 'Complaint ID', 'Year',\n       'Month', 'Day', 'DoW'],\n      dtype='object')</pre> In\u00a0[6]: Copied! <pre># lower the register of columns\n\ndef normalise_column_names(df):\n    \n    normalised_features = [i.lower() for i in list(df.columns)]\n    df.columns = normalised_features\n    return df\n\ndf = normalise_column_names(df)\n</pre> # lower the register of columns  def normalise_column_names(df):          normalised_features = [i.lower() for i in list(df.columns)]     df.columns = normalised_features     return df  df = normalise_column_names(df) In\u00a0[7]: Copied! <pre># show the names of each subset\n\ndef show_subset_names(df,column):    \n    return df[column].value_counts().index\n\nshow_subset_names(df,'product')\n</pre> # show the names of each subset  def show_subset_names(df,column):         return df[column].value_counts().index  show_subset_names(df,'product') Out[7]: <pre>Index(['Credit reporting, credit repair services, or other personal consumer reports',\n       'Debt collection', 'Mortgage', 'Credit card or prepaid card',\n       'Checking or savings account', 'Student loan', 'Credit reporting',\n       'Money transfer, virtual currency, or money service',\n       'Vehicle loan or lease', 'Credit card', 'Bank account or service',\n       'Payday loan, title loan, or personal loan', 'Consumer Loan',\n       'Payday loan', 'Money transfers', 'Prepaid card',\n       'Other financial service', 'Virtual currency'],\n      dtype='object')</pre> In\u00a0[8]: Copied! <pre>def normalise_subset_names(df,column):\n    subset_names = list(df[column].value_counts().index)\n    norm_subset_names = [i.lower() for i in subset_names]\n    dict_replace = dict(zip(subset_names,norm_subset_names))\n    df[column] = df[column].replace(dict_replace)    \n    return df\n\ndf = normalise_subset_names(df,'product')\nshow_subset_names(df,'product')\n</pre> def normalise_subset_names(df,column):     subset_names = list(df[column].value_counts().index)     norm_subset_names = [i.lower() for i in subset_names]     dict_replace = dict(zip(subset_names,norm_subset_names))     df[column] = df[column].replace(dict_replace)         return df  df = normalise_subset_names(df,'product') show_subset_names(df,'product') Out[8]: <pre>Index(['credit reporting, credit repair services, or other personal consumer reports',\n       'debt collection', 'mortgage', 'credit card or prepaid card',\n       'checking or savings account', 'student loan', 'credit reporting',\n       'money transfer, virtual currency, or money service',\n       'vehicle loan or lease', 'credit card', 'bank account or service',\n       'payday loan, title loan, or personal loan', 'consumer loan',\n       'payday loan', 'money transfers', 'prepaid card',\n       'other financial service', 'virtual currency'],\n      dtype='object')</pre> In\u00a0[9]: Copied! <pre># keep only specific subset in a feature\n\ndef keep_subset(df,column,lst):\n    \n    all_features = list(df[column].value_counts().index)\n    keep_features = lst\n    \n    # subset data\n    subset_data = dict(tuple(df.groupby(column)))\n    subset_data_filter = lambda x, y: dict([ (i,x[i]) for i in x if i in set(y) ])\n    \n    # dictionary with only selected keys\n    filtered_data = subset_data_filter(subset_data,lst)\n    filtered_df = pd.concat(filtered_data.values())\n    filtered_df.reset_index(drop=True,inplace=True)\n    return filtered_df\n    \n# remove specific subset from feature\n\ndef remove_subset(df,column,lst):\n    \n    all_features = list(df[column].value_counts().index)\n    keep_features = lst\n    \n    # subset data\n    subset_data = dict(tuple(df.groupby(column)))\n    set_all_features = set(all_features)\n    set_keep_features = set(lst)\n    \n    # features of dictionary which should remain\n    remaining_features = set_all_features - set_keep_features\n \n    subset_data_filter = lambda x, y: dict([ (i,x[i]) for i in x if i in set(y) ])\n    filtered_data = subset_data_filter(subset_data,remaining_features)\n    filtered_df = pd.concat(filtered_data.values())\n    filtered_df.reset_index(drop=True,inplace=True)\n    return filtered_df\n</pre> # keep only specific subset in a feature  def keep_subset(df,column,lst):          all_features = list(df[column].value_counts().index)     keep_features = lst          # subset data     subset_data = dict(tuple(df.groupby(column)))     subset_data_filter = lambda x, y: dict([ (i,x[i]) for i in x if i in set(y) ])          # dictionary with only selected keys     filtered_data = subset_data_filter(subset_data,lst)     filtered_df = pd.concat(filtered_data.values())     filtered_df.reset_index(drop=True,inplace=True)     return filtered_df      # remove specific subset from feature  def remove_subset(df,column,lst):          all_features = list(df[column].value_counts().index)     keep_features = lst          # subset data     subset_data = dict(tuple(df.groupby(column)))     set_all_features = set(all_features)     set_keep_features = set(lst)          # features of dictionary which should remain     remaining_features = set_all_features - set_keep_features       subset_data_filter = lambda x, y: dict([ (i,x[i]) for i in x if i in set(y) ])     filtered_data = subset_data_filter(subset_data,remaining_features)     filtered_df = pd.concat(filtered_data.values())     filtered_df.reset_index(drop=True,inplace=True)     return filtered_df In\u00a0[10]: Copied! <pre>lst_keep = ['credit reporting', 'debt collection', 'mortgage', 'credit card',\n            'bank account or service', 'consumer Loan', 'student loan',\n            'payday loan', 'prepaid card', 'money transfers',\n            'other financial service', 'virtual currency']\n\ndf = keep_subset(df,'product',lst_keep)\n# sdf = remove_subset(df,'product',lst_remove)\ndf['product'].value_counts()\n</pre> lst_keep = ['credit reporting', 'debt collection', 'mortgage', 'credit card',             'bank account or service', 'consumer Loan', 'student loan',             'payday loan', 'prepaid card', 'money transfers',             'other financial service', 'virtual currency']  df = keep_subset(df,'product',lst_keep) # sdf = remove_subset(df,'product',lst_remove) df['product'].value_counts() Out[10]: <pre>debt collection            195373\nmortgage                    99141\nstudent loan                33606\ncredit reporting            31587\ncredit card                 18838\nbank account or service     14885\npayday loan                  1746\nmoney transfers              1497\nprepaid card                 1450\nother financial service       292\nvirtual currency               16\nName: product, dtype: int64</pre> In\u00a0[11]: Copied! <pre>df['year'].value_counts()\n</pre> df['year'].value_counts() Out[11]: <pre>2016    73146\n2017    59087\n2015    51779\n2021    50757\n2022    42867\n2018    41708\n2020    40801\n2019    38286\nName: year, dtype: int64</pre> In\u00a0[12]: Copied! <pre>ldf = df.groupby('product').count()['day'].to_frame().sort_values(ascending=False,by='day')\n\nldf.style\\\n    .bar(align='mid',\n         color=['#d65f5f','#F1A424'])\n</pre> ldf = df.groupby('product').count()['day'].to_frame().sort_values(ascending=False,by='day')  ldf.style\\     .bar(align='mid',          color=['#d65f5f','#F1A424']) Out[12]: day product debt collection 195373 mortgage 99141 student loan 33606 credit reporting 31587 credit card 18838 bank account or service 14885 payday loan 1746 money transfers 1497 prepaid card 1450 other financial service 292 virtual currency 16 <p>We still have 184 thousand samples to go by, which should still be sufficient for our task</p> In\u00a0[13]: Copied! <pre>df = df[df['year'].isin([2015,2016,2017])]\nprint(f'final shape: {df.shape}')\n</pre> df = df[df['year'].isin([2015,2016,2017])] print(f'final shape: {df.shape}') <pre>final shape: (184012, 22)\n</pre> <p> </p> In\u00a0[14]: Copied! <pre>fig = px.bar((df['product']\n              .value_counts(ascending=False)\n              .to_frame()),\n             x='product',\n             template='plotly_white',\n             title='Product Subset Distribution')\n\nfig.update_traces(marker_line_color='#F1A424',\n                  marker_line_width=0.1,\n                  marker={'color':'#F1A424'},width=0.5)\n\nfig.show(\"png\")\n</pre> fig = px.bar((df['product']               .value_counts(ascending=False)               .to_frame()),              x='product',              template='plotly_white',              title='Product Subset Distribution')  fig.update_traces(marker_line_color='#F1A424',                   marker_line_width=0.1,                   marker={'color':'#F1A424'},width=0.5)  fig.show(\"png\") <ul> <li> We can note that we have some target class inbalance, in both product and issue features, </li> <li> Lets stick stratification, we need to make sure each class is represented in both datasets </li></ul> <p> </p> In\u00a0[16]: Copied! <pre>df['submitted via'].value_counts(ascending=True)\n</pre> df['submitted via'].value_counts(ascending=True) Out[16]: <pre>Web    184012\nName: submitted via, dtype: int64</pre> In\u00a0[17]: Copied! <pre># plot subset value counts \n\ndef plot_subset_counts(df,column,orient='h',top=None):\n    \n    ldf = df[column].value_counts(ascending=False).to_frame()\n    ldf.columns = ['values']\n    \n    if(top):\n        ldf = ldf[:top]\n    \n    if(orient is 'h'):\n        fig = px.bar(data_frame=ldf,\n                     x = ldf.index,\n                     y = 'values',\n                     template='plotly_white',\n                     title='Subset Value-Counts')\n\n    elif('v'):\n\n        fig = px.bar(data_frame=ldf,\n                             y = ldf.index,\n                             x = 'values',\n                             template='plotly_white',\n                             title='Subset Value-Counts')\n        \n    fig.update_layout(height=400)\n    fig.update_traces(marker_line_color='white',\n                      marker_line_width=0.5,\n                      marker={'color':'#F1A424'},\n                      width=0.75)\n    \n    fig.show(\"png\")\n</pre> # plot subset value counts   def plot_subset_counts(df,column,orient='h',top=None):          ldf = df[column].value_counts(ascending=False).to_frame()     ldf.columns = ['values']          if(top):         ldf = ldf[:top]          if(orient is 'h'):         fig = px.bar(data_frame=ldf,                      x = ldf.index,                      y = 'values',                      template='plotly_white',                      title='Subset Value-Counts')      elif('v'):          fig = px.bar(data_frame=ldf,                              y = ldf.index,                              x = 'values',                              template='plotly_white',                              title='Subset Value-Counts')              fig.update_layout(height=400)     fig.update_traces(marker_line_color='white',                       marker_line_width=0.5,                       marker={'color':'#F1A424'},                       width=0.75)          fig.show(\"png\") In\u00a0[18]: Copied! <pre>plot_subset_counts(df,'company',orient='v',top=10)\n</pre> plot_subset_counts(df,'company',orient='v',top=10) In\u00a0[19]: Copied! <pre>ldf = df['company public response'].value_counts(ascending=False).to_frame()\n\nldf.style\\\n    .bar(align='mid',\n         color=['#3b3745','#F1A424'])\n</pre> ldf = df['company public response'].value_counts(ascending=False).to_frame()  ldf.style\\     .bar(align='mid',          color=['#3b3745','#F1A424']) Out[19]: company public response Company has responded to the consumer and the CFPB and chooses not to provide a public response 41598 Company chooses not to provide a public response 18935 Company believes it acted appropriately as authorized by contract or law 17907 Company believes the complaint is the result of a misunderstanding 1839 Company disputes the facts presented in the complaint 1792 Company believes complaint caused principally by actions of third party outside the control or direction of the company 1436 Company believes complaint is the result of an isolated error 1345 Company believes complaint represents an opportunity for improvement to better serve consumers 813 Company can't verify or dispute the facts in the complaint 768 Company believes complaint relates to a discontinued policy or procedure 18 <ul> <li>Most of the consumer complaints were addressed in private as opposed to public</li> </ul> In\u00a0[20]: Copied! <pre># Daily Complaints\ncomplaints = df.copy()\ncomplaints_daily = complaints.groupby(['date received']).agg(\"count\")[[\"product\"]] # daily addresses\n\n# Sample weekly\ncomplaints_weekly = complaints_daily.reset_index()\ncomplaints_weekly = complaints_weekly.resample('W', on='date received').sum() # weekly addresses\n\nfig = px.line(complaints_weekly,complaints_weekly.index,y=\"product\",\n              template=\"plotly_white\",title=\"Weekly Complaints\",height=400)\nfig.update_traces(line_color='#F1A424')\nfig.show(\"png\")\n</pre> # Daily Complaints complaints = df.copy() complaints_daily = complaints.groupby(['date received']).agg(\"count\")[[\"product\"]] # daily addresses  # Sample weekly complaints_weekly = complaints_daily.reset_index() complaints_weekly = complaints_weekly.resample('W', on='date received').sum() # weekly addresses  fig = px.line(complaints_weekly,complaints_weekly.index,y=\"product\",               template=\"plotly_white\",title=\"Weekly Complaints\",height=400) fig.update_traces(line_color='#F1A424') fig.show(\"png\") <ul> <li>We can observe an increasing trend in complaints registed (this could be because users simply regitered the complaints more)</li> <li>After April 2nd, 2017, there is a rapid decline in registered complaints     <li>Some interesting peaks with an unusually high number of registed complaints occured in 2017 (January,September)</li> <li>We can note that the pandemic had also an effect on the number of complaints registed</li> </li></ul> In\u00a0[21]: Copied! <pre>fig = px.bar(df['day'].value_counts(ascending=True).to_frame(),y='day',\n             template='plotly_white',height=300,\n             title='Day of the Month Complaint Trends')\nfig.update_xaxes(tickvals = [i for i in range(0,32,1)])\nfig.update_traces(marker_line_color='#F1A424',marker_line_width=0.1,\n                  marker={'color':'#F1A424'},width=0.5)\nfig.update_traces(textfont_size=12, textangle=0, \n                  textposition=\"outside\", cliponaxis=False)\nfig.show(\"png\")\n</pre> fig = px.bar(df['day'].value_counts(ascending=True).to_frame(),y='day',              template='plotly_white',height=300,              title='Day of the Month Complaint Trends') fig.update_xaxes(tickvals = [i for i in range(0,32,1)]) fig.update_traces(marker_line_color='#F1A424',marker_line_width=0.1,                   marker={'color':'#F1A424'},width=0.5) fig.update_traces(textfont_size=12, textangle=0,                    textposition=\"outside\", cliponaxis=False) fig.show(\"png\") In\u00a0[22]: Copied! <pre>fig = px.bar(df['month'].value_counts(ascending=False).to_frame(),y='month',\n             template='plotly_white',height=300,\n             title='Month of the Year Complaint Trends')\nfig.update_xaxes(tickvals = [i for i in range(0,13,1)])\nfig.update_traces(marker_line_color='#F1A424',marker_line_width=0.1,\n                  marker={'color':'#F1A424'},width=0.5)\nfig.show(\"png\")\n</pre> fig = px.bar(df['month'].value_counts(ascending=False).to_frame(),y='month',              template='plotly_white',height=300,              title='Month of the Year Complaint Trends') fig.update_xaxes(tickvals = [i for i in range(0,13,1)]) fig.update_traces(marker_line_color='#F1A424',marker_line_width=0.1,                   marker={'color':'#F1A424'},width=0.5) fig.show(\"png\") In\u00a0[23]: Copied! <pre># By DoW\nfig = px.bar(df['dow'].value_counts(ascending=False).to_frame(),y='dow',\n             template='plotly_white',height=300,\n             title='Day of the Week Complaint Trends')\nfig.update_traces(marker_line_color='#F1A424',marker_line_width=0.1,\n                  marker={'color':'#F1A424'},width=0.5)\nfig.update_traces(textfont_size=12, textangle=0, textposition=\"outside\", cliponaxis=False)\nfig.show(\"png\")\n</pre> # By DoW fig = px.bar(df['dow'].value_counts(ascending=False).to_frame(),y='dow',              template='plotly_white',height=300,              title='Day of the Week Complaint Trends') fig.update_traces(marker_line_color='#F1A424',marker_line_width=0.1,                   marker={'color':'#F1A424'},width=0.5) fig.update_traces(textfont_size=12, textangle=0, textposition=\"outside\", cliponaxis=False) fig.show(\"png\") <ul> <li>Day of the month seems to be quite a cyclic trend, with lower numbers closer to the weekends</li> <li>July, August &amp; September are associated with increased complaints, December, January &amp; February associated with lower number of complaints</li> <li>Novermber, December, January &amp; February don't have adequate data</li> <li>Tuesdays &amp; Wednesdays tend to be the most common day a consumer will write a complaint</li> <li>Consumers don't tend to write complaints on weekends (Saturday,Sunday)</li> </ul> In\u00a0[24]: Copied! <pre>ldf = df['state'].value_counts(ascending=True).to_frame()[50:]\n\nfig = px.bar(ldf,x='state',template='plotly_white',\n       title='State of Complaint',height=400)\n\nfig.update_traces(marker_line_color='#F1A424',marker_line_width=1,\n                  marker={'color':'#F1A424'},width=0.4)\nfig.show(\"png\")\n</pre> ldf = df['state'].value_counts(ascending=True).to_frame()[50:]  fig = px.bar(ldf,x='state',template='plotly_white',        title='State of Complaint',height=400)  fig.update_traces(marker_line_color='#F1A424',marker_line_width=1,                   marker={'color':'#F1A424'},width=0.4) fig.show(\"png\") <ul> <li>Most of the consumer complaints are from California, Florida, Texas, Georgia and New York</li> </ul> In\u00a0[25]: Copied! <pre>disputed = df[['product','consumer disputed?','month']]\n\nfig = px.histogram(disputed, y='product', \n                   color='consumer disputed?',\n                   template='plotly_white',\n                   height = 700,\n                   barmode='group',\n                   color_discrete_sequence=['#F1A424','#3b3745'],\n                   facet_col_wrap=3,\n                   facet_col='month')\n\nfig.update_layout(showlegend=False)\nfig.update_layout(barmode=\"overlay\")\nfig.update_traces(opacity=0.5)\nfig.show(\"png\")\n</pre> disputed = df[['product','consumer disputed?','month']]  fig = px.histogram(disputed, y='product',                     color='consumer disputed?',                    template='plotly_white',                    height = 700,                    barmode='group',                    color_discrete_sequence=['#F1A424','#3b3745'],                    facet_col_wrap=3,                    facet_col='month')  fig.update_layout(showlegend=False) fig.update_layout(barmode=\"overlay\") fig.update_traces(opacity=0.5) fig.show(\"png\") <ul> <li>Morgage &amp; Debt Collection tend to be quite often disputed all year round</li> <li>September &amp; October have the highest disputed cases for Morgage &amp; Debt Collection </li> <li>July &amp; August have an unusually high ammount of credit reporting complaints</li> </ul> <p> </p> In\u00a0[26]: Copied! <pre>df['product'].value_counts(ascending=False).to_frame().sum()\n</pre> df['product'].value_counts(ascending=False).to_frame().sum() Out[26]: <pre>product    184012\ndtype: int64</pre> In\u00a0[27]: Copied! <pre>df['product'].value_counts(ascending=False).to_frame().tail(3)\n</pre> df['product'].value_counts(ascending=False).to_frame().tail(3) Out[27]: product prepaid card 1450 other financial service 292 virtual currency 16 In\u00a0[28]: Copied! <pre>print('Sample from virtual currency:')\nvc = dict(tuple(df.groupby(by='product')))['virtual currency']\nvc.iloc[[0]]['consumer complaint narrative'].values[0]\n</pre> print('Sample from virtual currency:') vc = dict(tuple(df.groupby(by='product')))['virtual currency'] vc.iloc[[0]]['consumer complaint narrative'].values[0] <pre>Sample from virtual currency:\n</pre> Out[28]: <pre>'Signedup XXXX family members for referrals on Coinbase.com. Coinbase at that time offered {$75.00} for each person referred to their service. I referred all XXXX and they met the terms Coinbase intially offered. Signup took a while do to money transfer timeframes setup by Coinbase. In that time, Coinbase changed their promotion and terms to {$10.00} for referrals. When asked why, they said they could change terms at anytime ( even if signup up for {$75.00} referral bonus ) and that family members did not meet the terms either. Felt like they just change terms to disclude giving out referral bonuses.'</pre> In\u00a0[29]: Copied! <pre># When we have a feature that contains subsets, we can remove unwanted subsets\n        \ndef remove_subset(df,feature,lst_groups):\n    \n    ndf = df.copy()\n\n    # Let's down sample all classes with frequencies above 4k\n    group = dict(tuple(ndf.groupby(by=feature)))\n    subset_group = list(group.keys()) # subsets in feature\n    \n    # Check if features exist in columns\n    if(set(lst_groups).issubset(subset_group)):\n\n        # remove unwanted subset\n        for k in lst_groups:\n            group.pop(k, None)\n\n        df = pd.concat(list(group.values()))\n        df.reset_index(inplace=True,drop=True)\n\n        return df    \n</pre> # When we have a feature that contains subsets, we can remove unwanted subsets          def remove_subset(df,feature,lst_groups):          ndf = df.copy()      # Let's down sample all classes with frequencies above 4k     group = dict(tuple(ndf.groupby(by=feature)))     subset_group = list(group.keys()) # subsets in feature          # Check if features exist in columns     if(set(lst_groups).issubset(subset_group)):          # remove unwanted subset         for k in lst_groups:             group.pop(k, None)          df = pd.concat(list(group.values()))         df.reset_index(inplace=True,drop=True)          return df     <ul> <li>We are down to 611,233 complaints (about have of what we had)</li> <li>Let's also confirm that our function remove_subset works correctly</li> </ul> In\u00a0[30]: Copied! <pre># Downsample selected \n\ndef downsample_subset(df,feature,lst_groups,samples=4000):\n\n    ndf = df.copy()\n\n    # Let's down sample all classes with frequencies above 4k\n    group = dict(tuple(ndf.groupby(by=feature)))\n    subset_group = list(group.keys())\n\n    # Check if features exist in columns\n    if(set(lst_groups).issubset(subset_group)):\n\n        dict_downsamples = {}\n        for feature in lst_groups:\n            dict_downsamples[feature] = group[feature].sample(samples)\n\n        # remove old data\n        for k in lst_groups:\n            group.pop(k, None)\n\n        # read them back\n        group.update(dict_downsamples)\n\n        df = pd.concat(list(group.values()))\n        df.reset_index(inplace=True,drop=True)\n\n        return df\n    \n    else:\n        print('feature not found in dataframe')\n        \n</pre> # Downsample selected   def downsample_subset(df,feature,lst_groups,samples=4000):      ndf = df.copy()      # Let's down sample all classes with frequencies above 4k     group = dict(tuple(ndf.groupby(by=feature)))     subset_group = list(group.keys())      # Check if features exist in columns     if(set(lst_groups).issubset(subset_group)):          dict_downsamples = {}         for feature in lst_groups:             dict_downsamples[feature] = group[feature].sample(samples)          # remove old data         for k in lst_groups:             group.pop(k, None)          # read them back         group.update(dict_downsamples)          df = pd.concat(list(group.values()))         df.reset_index(inplace=True,drop=True)          return df          else:         print('feature not found in dataframe')          In\u00a0[31]: Copied! <pre># Select subset features which have more than 4000 samples\nsubset_list = list(df['product'].value_counts()[df['product'].value_counts().values &gt; 4000].index)\ndf = downsample_subset(df,'product',subset_list,samples=4000)\n</pre> # Select subset features which have more than 4000 samples subset_list = list(df['product'].value_counts()[df['product'].value_counts().values &gt; 4000].index) df = downsample_subset(df,'product',subset_list,samples=4000) <ul> <li>Let's check if our function <code>downsample_subset</code> works correctly</li> </ul> In\u00a0[32]: Copied! <pre>df['product'].value_counts()\n</pre> df['product'].value_counts() Out[32]: <pre>debt collection            4000\nmortgage                   4000\ncredit reporting           4000\ncredit card                4000\nstudent loan               4000\nbank account or service    4000\npayday loan                1746\nmoney transfers            1497\nprepaid card               1450\nother financial service     292\nvirtual currency             16\nName: product, dtype: int64</pre> In\u00a0[33]: Copied! <pre># Select only relevant data\ndf_data = df[['consumer complaint narrative','product']]\ndf_data.columns = ['text','label']\ndf_data.head()\n</pre> # Select only relevant data df_data = df[['consumer complaint narrative','product']] df_data.columns = ['text','label'] df_data.head() Out[33]: text label 0 I made a wire transfer through Citibank to XXX... money transfers 1 I purchased a money order on XX/XX/2016 ( to c... money transfers 2 I have complained of false online transfer num... money transfers 3 I paid by bank wire transfer on XXXX/XXXX/XXXX... money transfers 4 I found a XXXX Bulldog for sale on XXXX after ... money transfers In\u00a0[34]: Copied! <pre>from sklearn.model_selection import train_test_split as tts\n\ntrain_files,test_files, train_labels, test_labels = tts(df_data['text'],\n                                                        df_data['label'],\n                                                        test_size=0.1,\n                                                        random_state=32,\n                                                        stratify=df_data['label'])\n\ntrain_files = pd.DataFrame(train_files)\ntest_files = pd.DataFrame(test_files)\ntrain_files['label'] = train_labels\ntest_files['label'] = test_labels\n</pre> from sklearn.model_selection import train_test_split as tts  train_files,test_files, train_labels, test_labels = tts(df_data['text'],                                                         df_data['label'],                                                         test_size=0.1,                                                         random_state=32,                                                         stratify=df_data['label'])  train_files = pd.DataFrame(train_files) test_files = pd.DataFrame(test_files) train_files['label'] = train_labels test_files['label'] = test_labels In\u00a0[35]: Copied! <pre>print(type(train_files))\nprint('Training Data',train_files.shape)\nprint('Validation Data',test_files.shape)\n</pre> print(type(train_files)) print('Training Data',train_files.shape) print('Validation Data',test_files.shape) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nTraining Data (26100, 2)\nValidation Data (2901, 2)\n</pre> In\u00a0[36]: Copied! <pre>import plotly.express as px\n\ntrain_values = train_files['label'].value_counts()\ntest_values = test_files['label'].value_counts()\nvisual = pd.concat([train_values,test_values],axis=1)\nvisual = visual.T\nvisual.index = ['train','test']\n\nfig = px.bar(visual,template='plotly_white',\n       barmode='group',text_auto=True,height=300,\n       title='Train/Test Split Distribution')\n\nfig.show(\"png\")\n</pre> import plotly.express as px  train_values = train_files['label'].value_counts() test_values = test_files['label'].value_counts() visual = pd.concat([train_values,test_values],axis=1) visual = visual.T visual.index = ['train','test']  fig = px.bar(visual,template='plotly_white',        barmode='group',text_auto=True,height=300,        title='Train/Test Split Distribution')  fig.show(\"png\") <p></p> In\u00a0[37]: Copied! <pre>train_files\n</pre> train_files Out[37]: text label 8290 I paid off all of my bills and should not have... debt collection 1520 Several checks were issued from XXXX for possi... other financial service 23071 In XXXX, we my husband and myself took out a l... student loan 4123 I use an Amex Serve card ( a prepaid debit car... prepaid card 16470 Despite YEARS of stellar credit reports and sc... credit reporting ... ... ... 21733 I know that I am victim of student loan scam. ... student loan 27578 On XX/XX/XXXX I made a payment of {$380.00} to... bank account or service 26297 XXXX XXXX XXXX XXXX XXXX, AZ XXXX : ( XXXX ) X... bank account or service 17867 After nearly a decade of business with Bank of... credit card 144 On XXXX XXXX, 2015, I made a purchase on EBay.... money transfers <p>26100 rows \u00d7 2 columns</p> In\u00a0[38]: Copied! <pre>import transformers\ntransformers.logging.set_verbosity_error()\nimport warnings; warnings.filterwarnings('ignore')\nimport os; os.environ['WANDB_DISABLED'] = 'true'\nfrom datasets import Dataset,Features,Value,ClassLabel, DatasetDict \n\ntraindts = Dataset.from_pandas(train_files)\ntraindts = traindts.class_encode_column(\"label\")\ntestdts = Dataset.from_pandas(test_files)\ntestdts = testdts.class_encode_column(\"label\")\n</pre> import transformers transformers.logging.set_verbosity_error() import warnings; warnings.filterwarnings('ignore') import os; os.environ['WANDB_DISABLED'] = 'true' from datasets import Dataset,Features,Value,ClassLabel, DatasetDict   traindts = Dataset.from_pandas(train_files) traindts = traindts.class_encode_column(\"label\") testdts = Dataset.from_pandas(test_files) testdts = testdts.class_encode_column(\"label\") In\u00a0[39]: Copied! <pre># Pandas indicies not reset ie. __index_level_0__ additional column is added, resetting index\ncorpus = DatasetDict({\"train\" : traindts , \n                      \"validation\" : testdts })\ncorpus['train']\n</pre> # Pandas indicies not reset ie. __index_level_0__ additional column is added, resetting index corpus = DatasetDict({\"train\" : traindts ,                        \"validation\" : testdts }) corpus['train'] Out[39]: <pre>Dataset({\n    features: ['text', 'label', '__index_level_0__'],\n    num_rows: 26100\n})</pre> In\u00a0[40]: Copied! <pre>from transformers import AutoTokenizer\n\n# Load parameters of the tokeniser\nmodel_ckpt = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\n# Tokenisation function\ndef tokenise(batch):\n    return tokenizer(batch[\"text\"], \n                     padding=True, \n                     truncation=True)\n\n# apply to the entire dataset (train,test and validation dataset)\ncorpus_tokenised = corpus.map(tokenise, \n                              batched=True, \n                              batch_size=None)\n\nprint(corpus_tokenised[\"train\"].column_names)\n</pre> from transformers import AutoTokenizer  # Load parameters of the tokeniser model_ckpt = \"distilbert-base-uncased\" tokenizer = AutoTokenizer.from_pretrained(model_ckpt)  # Tokenisation function def tokenise(batch):     return tokenizer(batch[\"text\"],                       padding=True,                       truncation=True)  # apply to the entire dataset (train,test and validation dataset) corpus_tokenised = corpus.map(tokenise,                                batched=True,                                batch_size=None)  print(corpus_tokenised[\"train\"].column_names) <pre>['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask']\n</pre> In\u00a0[41]: Copied! <pre>from transformers import AutoModel\nimport torch\n\n# load a pretrained transformer model\nmodel_ckpt = \"distilbert-base-uncased\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# move model to device\nmodel = AutoModel.from_pretrained(model_ckpt).to(device)\n</pre> from transformers import AutoModel import torch  # load a pretrained transformer model model_ckpt = \"distilbert-base-uncased\" device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") print(device)  # move model to device model = AutoModel.from_pretrained(model_ckpt).to(device) <pre>cuda\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n</pre> In\u00a0[42]: Copied! <pre># Function used to store last_hidden_state data of distilbert-base-uncased\ndef extract_hidden_states(batch):\n    \n    # Place model inputs on the GPU\n    inputs = {k:v.to(device) for k,v in batch.items()\n              if k in tokenizer.model_input_names}\n    \n    # Extract last hidden states\n    with torch.no_grad():\n        last_hidden_state = model(**inputs).last_hidden_state\n        \n    # Return vector for [CLS] token\n    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}\n</pre> # Function used to store last_hidden_state data of distilbert-base-uncased def extract_hidden_states(batch):          # Place model inputs on the GPU     inputs = {k:v.to(device) for k,v in batch.items()               if k in tokenizer.model_input_names}          # Extract last hidden states     with torch.no_grad():         last_hidden_state = model(**inputs).last_hidden_state              # Return vector for [CLS] token     return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()} In\u00a0[43]: Copied! <pre># Change Data to Torch tensor\ncorpus_tokenised.set_format(\"torch\",\n                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\ncorpus_tokenised\n</pre> # Change Data to Torch tensor corpus_tokenised.set_format(\"torch\",                             columns=[\"input_ids\", \"attention_mask\", \"label\"]) corpus_tokenised Out[43]: <pre>DatasetDict({\n    train: Dataset({\n        features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n        num_rows: 26100\n    })\n    validation: Dataset({\n        features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n        num_rows: 2901\n    })\n})</pre> In\u00a0[44]: Copied! <pre># Extract last hidden states (faster w/ GPU)\ncorpus_hidden = corpus_tokenised.map(extract_hidden_states, \n                                     batched=True,\n                                     batch_size=32)\ncorpus_hidden[\"train\"].column_names\n</pre> # Extract last hidden states (faster w/ GPU) corpus_hidden = corpus_tokenised.map(extract_hidden_states,                                       batched=True,                                      batch_size=32) corpus_hidden[\"train\"].column_names Out[44]: <pre>['text',\n 'label',\n '__index_level_0__',\n 'input_ids',\n 'attention_mask',\n 'hidden_state']</pre> In\u00a0[45]: Copied! <pre># Empty cache\ntorch.cuda.empty_cache()\n</pre> # Empty cache torch.cuda.empty_cache() <p>The extracted hidden state corpus <code>corpus_hidden</code> has been uploaded to dataset</p> In\u00a0[46]: Copied! <pre># Save our data\ncorpus_hidden.set_format(type=\"pandas\")\n\n# Add label data to dataframe\ndef label_int2str(row):\n    return corpus[\"train\"].features[\"label\"].int2str(row)\n\nldf = corpus_hidden[\"train\"][:]\nldf[\"label_name\"] = ldf[\"label\"].apply(label_int2str)\nldf.to_pickle('training.df')\n\nldf = corpus_hidden[\"validation\"][:]\nldf[\"label_name\"] = ldf[\"label\"].apply(label_int2str)\nldf.to_pickle('validation.df')\n</pre> # Save our data corpus_hidden.set_format(type=\"pandas\")  # Add label data to dataframe def label_int2str(row):     return corpus[\"train\"].features[\"label\"].int2str(row)  ldf = corpus_hidden[\"train\"][:] ldf[\"label_name\"] = ldf[\"label\"].apply(label_int2str) ldf.to_pickle('training.df')  ldf = corpus_hidden[\"validation\"][:] ldf[\"label_name\"] = ldf[\"label\"].apply(label_int2str) ldf.to_pickle('validation.df') In\u00a0[47]: Copied! <pre>!ls /kaggle/working/\n# !ls /kaggle/input/hiddenstatedata/\n</pre> !ls /kaggle/working/ # !ls /kaggle/input/hiddenstatedata/ <pre>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n__notebook__.ipynb  training.df  validation.df\r\n</pre> In\u00a0[48]: Copied! <pre># reload saved data\nimport pandas as pd\nimport pickle\n\n# Load hidden state data\n# training = pd.read_pickle('/kaggle/input/hiddenstatedata/training.df')\n# validation = pd.read_pickle('/kaggle/input/hiddenstatedata/validation.df')\ntraining = pd.read_pickle('training.df')\nvalidation = pd.read_pickle('validation.df')\ntraining.head()\n\nlabels = training[['label','label_name']]\n</pre> # reload saved data import pandas as pd import pickle  # Load hidden state data # training = pd.read_pickle('/kaggle/input/hiddenstatedata/training.df') # validation = pd.read_pickle('/kaggle/input/hiddenstatedata/validation.df') training = pd.read_pickle('training.df') validation = pd.read_pickle('validation.df') training.head()  labels = training[['label','label_name']] In\u00a0[49]: Copied! <pre>label = []\nfor i in labels.label.unique():\n    label.append(labels[labels['label'] == i].iloc[[0]]['label_name'].values[0])\n    \nlabel\n</pre> label = [] for i in labels.label.unique():     label.append(labels[labels['label'] == i].iloc[[0]]['label_name'].values[0])      label Out[49]: <pre>['debt collection',\n 'other financial service',\n 'student loan',\n 'prepaid card',\n 'credit reporting',\n 'mortgage',\n 'payday loan',\n 'credit card',\n 'bank account or service',\n 'money transfers',\n 'virtual currency']</pre> In\u00a0[50]: Copied! <pre># # Define our training &amp; validation datasets\n\nimport numpy as np\nX_train = np.stack(training['hidden_state'])\nX_valid = np.stack(validation[\"hidden_state\"])\ny_train = np.array(training[\"label\"])\ny_valid = np.array(validation[\"label\"])\nprint(f'Training Dataset: {X_train.shape}')\nprint(f'Validation Dataset {X_valid.shape}')\n</pre> # # Define our training &amp; validation datasets  import numpy as np X_train = np.stack(training['hidden_state']) X_valid = np.stack(validation[\"hidden_state\"]) y_train = np.array(training[\"label\"]) y_valid = np.array(validation[\"label\"]) print(f'Training Dataset: {X_train.shape}') print(f'Validation Dataset {X_valid.shape}') <pre>Training Dataset: (26100, 768)\nValidation Dataset (2901, 768)\n</pre> In\u00a0[51]: Copied! <pre>%%time\n\nfrom sklearn.linear_model import LogisticRegression as LR\n\n# We increase `max_iter` to guarantee convergence\nlr_clf = LR(max_iter = 2000)\nlr_clf.fit(X_train, y_train)\n</pre> %%time  from sklearn.linear_model import LogisticRegression as LR  # We increase `max_iter` to guarantee convergence lr_clf = LR(max_iter = 2000) lr_clf.fit(X_train, y_train) <pre>CPU times: user 8min 59s, sys: 39.2 s, total: 9min 38s\nWall time: 5min 5s\n</pre> Out[51]: <pre>LogisticRegression(max_iter=2000)</pre> In\u00a0[52]: Copied! <pre># Predictions\ny_preds_train = lr_clf.predict(X_train)\ny_preds_valid = lr_clf.predict(X_valid)\nprint('LogisticRegression:')\nprint(f'training accuracy: {round(lr_clf.score(X_train, y_train),3)}')\nprint(f'validation accuracy: {round(lr_clf.score(X_valid, y_valid),3)}')\n</pre> # Predictions y_preds_train = lr_clf.predict(X_train) y_preds_valid = lr_clf.predict(X_valid) print('LogisticRegression:') print(f'training accuracy: {round(lr_clf.score(X_train, y_train),3)}') print(f'validation accuracy: {round(lr_clf.score(X_valid, y_valid),3)}') <pre>LogisticRegression:\ntraining accuracy: 0.807\nvalidation accuracy: 0.772\n</pre> In\u00a0[53]: Copied! <pre># save sklearn model\nimport joblib\n\nfilename = 'classifier.joblib.pkl'\n_ = joblib.dump(lr_clf, filename, compress=9)\n\n# load sklearn model\n# lr_clf = joblib.load('/kaggle/input/hiddenstatedata/' + filename)\n# lr_clf\n</pre> # save sklearn model import joblib  filename = 'classifier.joblib.pkl' _ = joblib.dump(lr_clf, filename, compress=9)  # load sklearn model # lr_clf = joblib.load('/kaggle/input/hiddenstatedata/' + filename) # lr_clf In\u00a0[54]: Copied! <pre>import matplotlib.pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\ndef plot_confusion_matrix(y_model, y_true, labels):\n    cm = confusion_matrix(y_true,y_model,normalize='true')\n    fig, ax = plt.subplots(figsize=(8,8))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm.round(2).copy(), display_labels=labels)\n    disp.plot(ax=ax, colorbar=False)\n    plt.title(\"Confusion matrix\")\n    plt.xticks(rotation = 90) # Rotates X-Axis Ticks by 45-degrees\n    plt.tight_layout()\n    plt.show()\n</pre> import matplotlib.pyplot as plt from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix  def plot_confusion_matrix(y_model, y_true, labels):     cm = confusion_matrix(y_true,y_model,normalize='true')     fig, ax = plt.subplots(figsize=(8,8))     disp = ConfusionMatrixDisplay(confusion_matrix=cm.round(2).copy(), display_labels=labels)     disp.plot(ax=ax, colorbar=False)     plt.title(\"Confusion matrix\")     plt.xticks(rotation = 90) # Rotates X-Axis Ticks by 45-degrees     plt.tight_layout()     plt.show() In\u00a0[55]: Copied! <pre>labels = list(training.label_name.value_counts().index)\n\n# Validation Dataset Confusion Matrix\nplot_confusion_matrix(y_preds_valid, y_valid, labels)\n</pre> labels = list(training.label_name.value_counts().index)  # Validation Dataset Confusion Matrix plot_confusion_matrix(y_preds_valid, y_valid, labels) <ul> <li>Compressed distilbert embedding features work quite well in this this problem</li> <li>Looks like we have quite a good model to begin with, scoring a validation accuracy of 0.77</li> <li>We can note that the model payday loan &amp; virtual currency are very pooly predicted subsets</li> <li>other financial services is predicted quite well (which is surprising because for the transformer model, we have the opposite) </li></ul> <p> In\u00a0[56]: Copied! <pre># Empty cache\ntorch.cuda.empty_cache()\n</pre> # Empty cache torch.cuda.empty_cache() In\u00a0[57]: Copied! <pre># Change Data to Torch tensor\ncorpus_tokenised.set_format(\"torch\",\n                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\ncorpus_tokenised\n</pre> # Change Data to Torch tensor corpus_tokenised.set_format(\"torch\",                             columns=[\"input_ids\", \"attention_mask\", \"label\"]) corpus_tokenised Out[57]: <pre>DatasetDict({\n    train: Dataset({\n        features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n        num_rows: 26100\n    })\n    validation: Dataset({\n        features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n        num_rows: 2901\n    })\n})</pre> In\u00a0[58]: Copied! <pre>from transformers import AutoModelForSequenceClassification\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_ckpt = \"distilbert-base-uncased\"\n\nmodel = (AutoModelForSequenceClassification\n         .from_pretrained(model_ckpt, \n                          num_labels=len(labels))\n         .to(device))\n</pre> from transformers import AutoModelForSequenceClassification import torch  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") model_ckpt = \"distilbert-base-uncased\"  model = (AutoModelForSequenceClassification          .from_pretrained(model_ckpt,                            num_labels=len(labels))          .to(device)) In\u00a0[59]: Copied! <pre>from sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1}\n</pre> from sklearn.metrics import accuracy_score, f1_score  def compute_metrics(pred):     labels = pred.label_ids     preds = pred.predictions.argmax(-1)     f1 = f1_score(labels, preds, average=\"weighted\")     acc = accuracy_score(labels, preds)     return {\"accuracy\": acc, \"f1\": f1} In\u00a0[60]: Copied! <pre>from transformers import Trainer, TrainingArguments\nfrom transformers import Trainer\n\nbs = 16 # batch size\nmodel_name = f\"{model_ckpt}-finetuned-financial\"\nlabels = corpus_tokenised[\"train\"].features[\"label\"].names\n\n# Training Arguments\ntraining_args = TrainingArguments(output_dir=model_name,\n                                  num_train_epochs=3,             # number of training epochs\n                                  learning_rate=2e-5,             # model learning rate\n                                  per_device_train_batch_size=bs, # batch size\n                                  per_device_eval_batch_size=bs,  # batch size\n                                  weight_decay=0.01,\n                                  evaluation_strategy=\"epoch\",\n                                  disable_tqdm=False, \n                                  report_to=\"none\",\n                                  push_to_hub=False,\n                                  log_level=\"error\")\n\n\ntrainer = Trainer(model=model,                                 # Model\n                  args=training_args,                          # Training arguments (above)\n                  compute_metrics=compute_metrics,             # Computational Metrics\n                  train_dataset=corpus_tokenised[\"train\"],     # Training Dataset   \n                  eval_dataset=corpus_tokenised[\"validation\"], # Evaluation Dataset\n                  tokenizer=tokenizer)\n</pre> from transformers import Trainer, TrainingArguments from transformers import Trainer  bs = 16 # batch size model_name = f\"{model_ckpt}-finetuned-financial\" labels = corpus_tokenised[\"train\"].features[\"label\"].names  # Training Arguments training_args = TrainingArguments(output_dir=model_name,                                   num_train_epochs=3,             # number of training epochs                                   learning_rate=2e-5,             # model learning rate                                   per_device_train_batch_size=bs, # batch size                                   per_device_eval_batch_size=bs,  # batch size                                   weight_decay=0.01,                                   evaluation_strategy=\"epoch\",                                   disable_tqdm=False,                                    report_to=\"none\",                                   push_to_hub=False,                                   log_level=\"error\")   trainer = Trainer(model=model,                                 # Model                   args=training_args,                          # Training arguments (above)                   compute_metrics=compute_metrics,             # Computational Metrics                   train_dataset=corpus_tokenised[\"train\"],     # Training Dataset                      eval_dataset=corpus_tokenised[\"validation\"], # Evaluation Dataset                   tokenizer=tokenizer) In\u00a0[62]: Copied! <pre>%%time\n\n# Train &amp; save model\ntrainer.train()\ntrainer.save_model()\n</pre> %%time  # Train &amp; save model trainer.train() trainer.save_model()        [4896/4896 37:20, Epoch 3/3]      Epoch Training Loss Validation Loss Accuracy F1 1 0.535000 0.528666 0.842468 0.839433 2 0.410300 0.483131 0.858669 0.855979 3 0.312500 0.477747 0.866942 0.864213 <p> </p> <pre>CPU times: user 36min 55s, sys: 16.5 s, total: 37min 12s\nWall time: 37min 21s\n</pre> In\u00a0[63]: Copied! <pre># from transformers import pipeline\n# load from previously saved model\n# classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-financial\")\n</pre> # from transformers import pipeline # load from previously saved model # classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-financial\") In\u00a0[64]: Copied! <pre># Predict on Validation Dataset\npred_output = trainer.predict(corpus_tokenised[\"validation\"])\npred_output\n</pre> # Predict on Validation Dataset pred_output = trainer.predict(corpus_tokenised[\"validation\"]) pred_output        [182/182 00:26]      Out[64]: <pre>PredictionOutput(predictions=array([[ 0.4898075 , -1.1539025 , -2.4000468 , ...,  2.664791  ,\n        -1.7696137 , -1.85924   ],\n       [ 5.524978  ,  1.4433606 , -1.2144918 , ..., -2.0602236 ,\n        -2.2785227 , -3.1501598 ],\n       [-0.99464035,  1.410322  ,  5.1908035 , ..., -2.1883903 ,\n        -1.5084958 , -3.9436107 ],\n       ...,\n       [ 0.8029568 ,  2.751484  , -1.0246688 , ..., -2.6813054 ,\n         0.32632264, -3.8141603 ],\n       [ 0.27522528, -1.2025931 , -0.14139701, ..., -2.684458  ,\n        -1.0188793 , -3.16859   ],\n       [-0.17902231, -1.83018   , -0.8901656 , ..., -3.2526188 ,\n         0.6185259 , -3.5121055 ]], dtype=float32), label_ids=array([4, 1, 2, ..., 0, 5, 5]), metrics={'test_loss': 0.47774738073349, 'test_accuracy': 0.8669424336435712, 'test_f1': 0.8642125632561599, 'test_runtime': 26.9558, 'test_samples_per_second': 107.621, 'test_steps_per_second': 6.752})</pre> In\u00a0[65]: Copied! <pre>print(f'Output Predition: {pred_output.predictions.shape}')\nprint(pred_output.predictions)\n</pre> print(f'Output Predition: {pred_output.predictions.shape}') print(pred_output.predictions) <pre>Output Predition: (2901, 11)\n[[ 0.4898075  -1.1539025  -2.4000468  ...  2.664791   -1.7696137\n  -1.85924   ]\n [ 5.524978    1.4433606  -1.2144918  ... -2.0602236  -2.2785227\n  -3.1501598 ]\n [-0.99464035  1.410322    5.1908035  ... -2.1883903  -1.5084958\n  -3.9436107 ]\n ...\n [ 0.8029568   2.751484   -1.0246688  ... -2.6813054   0.32632264\n  -3.8141603 ]\n [ 0.27522528 -1.2025931  -0.14139701 ... -2.684458   -1.0188793\n  -3.16859   ]\n [-0.17902231 -1.83018    -0.8901656  ... -3.2526188   0.6185259\n  -3.5121055 ]]\n</pre> In\u00a0[66]: Copied! <pre>import numpy as np\n\n# Decode the predictions greedily using argmax (highest value of all classes)\ny_preds = np.argmax(pred_output.predictions,axis=1)\nprint(f'Output Prediction:{y_preds.shape}')\nprint(f'Predictions: {y_preds}')\n</pre> import numpy as np  # Decode the predictions greedily using argmax (highest value of all classes) y_preds = np.argmax(pred_output.predictions,axis=1) print(f'Output Prediction:{y_preds.shape}') print(f'Predictions: {y_preds}') <pre>Output Prediction:(2901,)\nPredictions: [4 0 2 ... 1 5 5]\n</pre> In\u00a0[67]: Copied! <pre># Validation \nplot_confusion_matrix(y_preds,y_valid,labels)\n</pre> # Validation  plot_confusion_matrix(y_preds,y_valid,labels) <ul> <li>Fine-tuning a pretrained-transformed works sufficiently better than our baseline approach</li> <li>virtual currency is also not predicted very well (same as the linear model)</li> <li>payday loan is predicted well but other financial services poorly </li></ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#1-background","title":"1 | BACKGROUND\u00b6","text":"<p>In this section we will outline what customer feedback is, why it is an important part of any business, not only for financial companies, but in general. Show some examples, which should show that it can take some time to manually read and analyse what each consumer complaint is about. How can we utilise consumer feedback to streamline the process of consumer-company interaction (the need)</p> </p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#consumer-feedback","title":"<p>CONSUMER FEEDBACK</p>\u00b6","text":"<p>Let's point our some key points about consumer feedback:</p> <p>Consumer feedback is an important part of day to day financial business operations Companies offering products must be able to know what their consumers think of their products Eg. positive &amp; negative feedback &amp; can be obtained from a number of sources (eg. twitter) In this case, we obtain data from a database, which registers consumers feedback of financial products Customers have specific issue on a number of topics they want want the company to address The form of consumer communication with the financial institution is via the web (as will be shown later)</p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#2-notebook-workflow","title":"2 | NOTEBOOK WORKFLOW\u00b6","text":""},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#study-aim","title":"STUDY AIM\u00b6","text":"<ul> <li>In this notebook, we'll be utilising machine learning, to create a model(s) that will be able to classify the type of complaint (as we did above) (by both product &amp; issue)</li> <li>Such a model can be useful for a company to quickly understand the type of complaint &amp; appoint a financial expert that will be able to solve the problem</li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#automated-ticket-classification-model","title":"AUTOMATED TICKET CLASSIFICATION MODEL\u00b6","text":"<p>Our approach will include separate models, that will be in charge of classifying data on different subsets of data</p> <ul> <li>M1 will be classifying a  product based on the customer's input complaint (<code>text</code>) (eg. Credit Reporting)</li> <li>M2 will be in charge of classifying the particular issue to which the complaint belongs to (text)</li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#3-data-preprocessing","title":"3 | DATA PREPROCESSING\u00b6","text":"<p>In this section, we will dive in to the dataset, making some slight adjustments; loading the data, looking at missing data, looking at the features &amp; make some slight adustments</p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#consumer-complaint-database","title":"CONSUMER COMPLAINT DATABASE\u00b6","text":"<ul> <li>Download full Dataset from the provided link if you want to have the up to date data</li> <li>Complaints that the Consumer Financial Protection Bureau (CFPB) sends to companies for response are published in the Consumer Complaint Database after the company responds</li> <li>Confirming a commercial relationship with the consumer, or after 15 days, whichever comes first</li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#load-dataset","title":"LOAD DATASET\u00b6","text":"<p>We start off by loading the dataset (we are loading the dataset without any missing data in <code>consumer complaint narrative</code> (which is the complaint)</p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#target-labels","title":"TARGET LABELS\u00b6","text":"<p>A quick glimps into the dataset gives us the view of the features that will be of interest to us in this study</p> <ul> <li>Product (Type of financial product)</li> <li>Sub-product  (A more detailed subset of product)</li> <li>Issue (What was the problem)</li> <li>Sub-issue (A more detailed subset of product)</li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#missing-data","title":"MISSING DATA\u00b6","text":"<p>Visualise missing data in the dataset, looks like we have quite a bit overall &amp; some in target variables (<code>Sub-Product</code> &amp; <code>Sub-Issue</code>)</p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#feature-description","title":"FEATURE DESCRIPTION\u00b6","text":"<p>Brief summary of what each feature represents in our dataset</p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#adding-datetime-features","title":"ADDING DATETIME FEATURES\u00b6","text":"<p>Lets add time-series based features, normalise column and column subset names &amp; remove some column subset data for our target variable</p> <ul> <li>We have two timeline features, <code>Date received</code> &amp; <code>Date sent to company</code></li> <li>Lets extract additional features which can be useful for EDA</li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#column-name-normalisation","title":"COLUMN NAME NORMALISATION\u00b6","text":"<p>Lets convert all column names to a lower register</p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#normalisation-of-subset-names","title":"NORMALISATION OF SUBSET NAMES\u00b6","text":"<p>Lets convert all subset feature names into a lower register as well</p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#filter-subset-data","title":"FILTER SUBSET DATA\u00b6","text":"<p>Let's keep only specific subsets of data in the product column</p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#select-subset-of-data","title":"SELECT SUBSET OF DATA\u00b6","text":"<ul> <li>Let's choose only a specific subset of time series data because to reduce the computational load</li> <li>We can note that data beyond 2017 has only three of the available subsets</li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#4-target-distribution","title":"4 | TARGET DISTRIBUTION\u00b6","text":"<p>The target variable in our problem:</p> <ul> <li>Product Value Distribution (target variable for M1)</li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#product","title":"PRODUCT\u00b6","text":"<p>Let's plot the grouped subset distribution for feature <code>Product</code></p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#5-exploratory-data-analysis","title":"5 | EXPLORATORY DATA ANALYSIS\u00b6","text":"<p>The dataset contains a lot of categorical which can be grouped &amp; analysed:</p> <ul> <li>Consumer Complaint Method (How was the complaint made?)</li> <li>Company to which the consumer complained (To which company was the complaint made)</li> <li>Consumer Complaint Timeline (When were the complaints made?)</li> <li>Consumer Complaint Timeline Group Trends (grouping data; are there any trends in the timeline)</li> <li>Consumer Address State (In which state was the complaint made?)</li> <li>Company Response to Consumer Complaints (What was the response to the complaint?)</li> <li>Consumer Response to company response (Did the consumer dispuse the response?)</li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#consumer-complaint-method","title":"CONSUMER COMPLAINT METHOD\u00b6","text":"<ul> <li>All customers addresses for which we have text data are submitted via the web</li> <li>This implies that all other forms of text data of complaints were not registered in the data for other forms of complaint submission</li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#financial-company","title":"FINANCIAL COMPANY\u00b6","text":"<p>Let's visualise the top 10 companies for which we have consumer complaint feedback</p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#company-response","title":"COMPANY RESPONSE\u00b6","text":"<p>Let's look at the data about what the company decided to do about the registered complaint</p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#complaint-timeline","title":"COMPLAINT TIMELINE\u00b6","text":"<ul> <li>Let's look at the weekly (using <code>resample</code>) complaint addresses (let's see if there are some trends in the time series data)</li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#consumer-complaint-timeline-trends","title":"CONSUMER COMPLAINT TIMELINE TRENDS\u00b6","text":"<p>Let's group all complaint data into groups for Day of the month (DoM), time of the year (ToY) &amp; day of the week (DoW)</p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#complaint-origin-state","title":"COMPLAINT ORIGIN (STATE) \u00b6","text":"<p>Let's investigate the distribution from which geographical state the complaint was made from</p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#comparing-response-to-disputes","title":"COMPARING RESPONSE TO DISPUTES\u00b6","text":"<p>Lastly, we can combine the last two sections and see for each <code>Product</code>, how many disputes there have been each month</p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#6-preparing-data-for-modeling","title":"6 | PREPARING DATA FOR MODELING\u00b6","text":"<ul> <li>We've done some exploratory data analysis &amp; understand our problem target variables a little better, let's focus on preparing the data for machine learning</li> <li>In total, we have 1159430 complaints, although not evenly distributed as we saw in Section 3</li> <li>Our smallest class (virtual currency) has only 16 issues (which is very little data)</li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#review-class-disbalance","title":"REVIEW CLASS DISBALANCE\u00b6","text":"<p>There doesn't seem to be any error associated with labelling, so let's not remove this subset</p> <p>Circle is a Boston-based financial services company that uses blockchain technology for its peer-to-peer payments and cryptocurrency-related products.</p> <p>Despite wanting to utilise Stratification, it seems like we may not have enough data for the model to be able to classify complaints with little data available</p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#product-subset-ambiguity","title":"PRODUCT SUBSET AMBIGUITY\u00b6","text":"<ul> <li>We have a feature <code>Credit reporting, credit repair services, or other personal consumer reports</code>, it seems like this feature is not quite sorted</li> <li>We already have a separate subgroup for Credit reporting, but not for cedit repair services or other consumer reports</li> <li>There is the possibility that this subgroup will contain complaints of other subgroups, which would affect the model accuracy</li> <li>Lets remove the subset: Credit reporting, credit repair services, or other personal consumer reports for the time being</li> </ul> <p>Some of the approaches we could take are:</p> <ul> <li>Try to sort the data by keywords</li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#limit-target-class-subset-samples","title":"LIMIT TARGET CLASS SUBSET SAMPLES\u00b6","text":"<p>We saw that we have quite a bit of data available</p> <ul> <li>Most of which are in particular classes (eg. credit collection ,debt reporting &amp; mortgage)</li> <li>Let's limit our data to 4000 text samples from each class using function <code>downsample_features</code></li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#train-test-subset-splitting","title":"TRAIN-TEST SUBSET SPLITTING\u00b6","text":"<ul> <li>Next, as per standard requirements, we need to be able to validate the model after training</li> <li>We will split the data into two groups, training &amp; validation subsets with a validation size of 0.2</li> <li>Stratification will also be applied to both groups, in order to guarantee all classes in both subgroups</li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#7-linear-baseline-model","title":"7 | LINEAR BASELINE MODEL\u00b6","text":"<p>In order to create a baseline model, let's extract the hidden state data from <code>DistilBERT</code> and use it as features for our linear model</p> </p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#generating-dataset","title":"GENERATING DATASET\u00b6","text":"<ul> <li>We have a daframe containing text &amp; label data for both training &amp; validation datasets</li> <li>We'll use HF's more intuitive to use <code>Dataset</code> class (which allows us to convert between types very easily)</li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#tokenisation","title":"TOKENISATION\u00b6","text":"<ul> <li>Time to tokenise the text data</li> <li>We'll be using the <code>AutoTokenizer</code> class from a pretrained model <code>distilbert-base-uncased</code> in order to generate subword tokens</li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#loading-preset-model","title":"LOADING PRESET MODEL\u00b6","text":"<p>HuggingFace allows us to load a variety of pretrained models:</p> <ul> <li>Let's utlilise the distilbert-base-uncased model</li> </ul> <p>The model was trained to predict [mask] values</p> <ul> <li>Given an input sequence (the above link shows an example)</li> </ul> <p>Let's use it to extract the last hidden state of each input sequence</p> <ul> <li>Use it to train a more tradition machine learning model (baseline M1 model)</li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#extract-model-hidden-state","title":"EXTRACT MODEL HIDDEN STATE\u00b6","text":"<ul> <li>For each tokenised input <code>text</code>, we can utilise the loaded model &amp; extract the last hidden state that can be used as features for machine learning models</li> <li>The same strategy was applied in notebook Twitter Emotion Classification</li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#to-tensors","title":"TO TENSORS\u00b6","text":"<p>Before training using the <code>pytorch</code> model, we need to set the corresponding format using <code>set_format</code></p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#inference-extract-hidden-state","title":"INFERENCE &amp; EXTRACT HIDDEN STATE\u00b6","text":"<p>Uisng <code>map</code> we can apply the function <code>extract_hidden_states</code> to both training &amp; validation datasets</p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#defining-hidden-state-dataset","title":"DEFINING HIDDEN STATE DATASET\u00b6","text":"<ul> <li>Having extracted the last hidden state data from model distilbert-base-uncased, let's define the training data</li> <li>The process is quite long so we'll load the data saved in the last section from training.csv &amp; validation.csv</li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#train-model","title":"TRAIN MODEL\u00b6","text":"<p>Let's start with something quite simplistic, <code>LogisticRegression</code> often work quite well</p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#8-fine-tune-distilbert","title":"8 | FINE-TUNE DISTILBERT\u00b6","text":"<ul> <li>With the fine-tune approach, we do not use the hidden states as fixed features</li> <li>Instead, we train them from a given model state</li> <li>This requires the classification head to be differentiable (neural network for classification)</li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#load-pretrained-model","title":"LOAD PRETRAINED MODEL\u00b6","text":"<ul> <li>We'll load the same DistilBERT model using <code>model_ckpt</code> \"distilbert-base-uncased\"</li> <li>This time however we will be loading <code>AutoModelForSequenceClassification</code> (we used <code>AutoModel</code> when we extracted embedding features)</li> <li><code>AutoModelForSequenceClassification</code> model has a classification head on top of the pretrained model outputs</li> <li>We only need to specify the number of labels the model has to predict <code>num_labels</code></li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#evaluation-metrics","title":"EVALUATION METRICS\u00b6","text":"<p>We'll monitor the <code>F1 score</code>  &amp; <code>accuracy</code>, the function is required to be passed in the <code>Trainer</code> class</p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#define-trainer","title":"DEFINE TRAINER\u00b6","text":"<ul> <li>Next we need to define the model training parameters, which can be done using <code>TrainingArguments</code></li> <li>Let's train the DistilBERT model for 3 iterations with a learning rate of 2e-5 and a batch size of 64</li> <li>The <code>Trainer</code> requires inputs of a model, model arguments, metrics, the datasets (train,validation) &amp; the tokeniser</li> </ul>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#train-model","title":"TRAIN MODEL\u00b6","text":"<p>Let's finally fine-tune our transform model to fit our classification problem</p>"},{"location":"portfolio/kaggle/financial-consumer-complaint-analysis.html#inference","title":"INFERENCE\u00b6","text":"<p>Let's utilise our fine-tuned transformer model for inference on the validation dataset</p>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html","title":"Ovarian phase classification in felids","text":"In\u00a0[1]: Copied! <pre>pip install -U kaleido\n</pre> pip install -U kaleido <pre>Collecting kaleido\r\n  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 79.9 MB 99.5 MB/s            \r\nInstalling collected packages: kaleido\r\nSuccessfully installed kaleido-0.2.1\r\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\r\nNote: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[2]: Copied! <pre>from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom plotly.subplots import make_subplots\nfrom sklearn.metrics import confusion_matrix,plot_confusion_matrix\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom catboost import CatBoostClassifier\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</pre> from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn import metrics from sklearn.linear_model import LogisticRegression from sklearn.metrics import roc_curve, auc, roc_auc_score from plotly.subplots import make_subplots from sklearn.metrics import confusion_matrix,plot_confusion_matrix import plotly.graph_objects as go import plotly.figure_factory as ff from catboost import CatBoostClassifier  import numpy as np import pandas as pd  import matplotlib.pyplot as plt import plotly.express as px import plotly.offline as py py.init_notebook_mode(connected=True) import os import warnings warnings.filterwarnings(\"ignore\") <pre>/opt/conda/lib/python3.7/site-packages/geopandas/_compat.py:115: UserWarning: The Shapely GEOS version (3.9.1-CAPI-1.14.2) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n  shapely_geos_version, geos_capi_version_string\n</pre> In\u00a0[3]: Copied! <pre># read the data\ndf = pd.read_csv('/kaggle/input/feline-pregnancy/feline_pregnancy.csv',delimiter=';')\n\n# generate names (as csv uses abbreviations)\nfeatures = ['e-basal','e-estrus','p-basal','p-nplp','p-plp',\n            'fem-basal','fem-estrus','fpm-basal','fpm-nplp','fpm-plp']\nstat = ['mean','min','max','n','ns']\nnew_names = ['linage','species']\nfor i in features:\n    for j in stat:\n        new_names.append(i+j) \n        \n# change some minor things\ndf.columns = new_names # set column names to those we generated\ndf = df.apply(lambda x: x.str.replace(',','.')) \ndf = df.apply(lambda x: x.str.replace('~','')) # approximation sign\n\nfeline_progest = df\n# some data uses &lt;1, so let's approximate it\nfeline_progest = feline_progest.apply(lambda x: x.str.replace('&lt;1','0.5'))\n# convert data to float\nfeline_progest.iloc[:,2:] = feline_progest.iloc[:,2:].astype('float')\n\n# check if rows are na\n# feline_progest['is_na'] = df[df.columns].isnull().apply(lambda x: all(x), axis=1) \n# feline_progest['is_na'].value_counts()\n</pre> # read the data df = pd.read_csv('/kaggle/input/feline-pregnancy/feline_pregnancy.csv',delimiter=';')  # generate names (as csv uses abbreviations) features = ['e-basal','e-estrus','p-basal','p-nplp','p-plp',             'fem-basal','fem-estrus','fpm-basal','fpm-nplp','fpm-plp'] stat = ['mean','min','max','n','ns'] new_names = ['linage','species'] for i in features:     for j in stat:         new_names.append(i+j)           # change some minor things df.columns = new_names # set column names to those we generated df = df.apply(lambda x: x.str.replace(',','.'))  df = df.apply(lambda x: x.str.replace('~','')) # approximation sign  feline_progest = df # some data uses &lt;1, so let's approximate it feline_progest = feline_progest.apply(lambda x: x.str.replace('&lt;1','0.5')) # convert data to float feline_progest.iloc[:,2:] = feline_progest.iloc[:,2:].astype('float')  # check if rows are na # feline_progest['is_na'] = df[df.columns].isnull().apply(lambda x: all(x), axis=1)  # feline_progest['is_na'].value_counts() <ul> <li>The dataset contains a combination of features outlined in Section 2.1, totalling 51 features, which is quite high in comparison to the number of entries: <ul> <li>Features 2-12 are measurements of estradiol in circulating blood during basal/estrus phases</li> <li>Features 13-27 are measurements of progesterone in circulating blood during basal/diestrus phases</li> <li>Features 28-37 are measurements of fecal estradiol metabolites in feces during basal/estrus phases</li> <li>Features 38-52 are measurements of fecal progesterone metabolites in feces during basal/diestrus phases</li> </ul> </li> <li>We have quite a few gaps in our dataset, especially for circulating blood data, which indicates that endocrine monitoring was not done as consistently as fecal protein assessments and less data was provided by individual authors</li> <li>From the first few data points as shown in <code>.head()</code> alone, we can note that we have various combinations of missing data</li> </ul> In\u00a0[4]: Copied! <pre>feline_progest.info()\n</pre> feline_progest.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 109 entries, 0 to 108\nData columns (total 52 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   linage          109 non-null    object\n 1   species         109 non-null    object\n 2   e-basalmean     18 non-null     object\n 3   e-basalmin      19 non-null     object\n 4   e-basalmax      19 non-null     object\n 5   e-basaln        19 non-null     object\n 6   e-basalns       18 non-null     object\n 7   e-estrusmean    14 non-null     object\n 8   e-estrusmin     13 non-null     object\n 9   e-estrusmax     13 non-null     object\n 10  e-estrusn       16 non-null     object\n 11  e-estrusns      15 non-null     object\n 12  p-basalmean     27 non-null     object\n 13  p-basalmin      20 non-null     object\n 14  p-basalmax      20 non-null     object\n 15  p-basaln        25 non-null     object\n 16  p-basalns       25 non-null     object\n 17  p-nplpmean      16 non-null     object\n 18  p-nplpmin       14 non-null     object\n 19  p-nplpmax       14 non-null     object\n 20  p-nplpn         14 non-null     object\n 21  p-nplpns        16 non-null     object\n 22  p-plpmean       10 non-null     object\n 23  p-plpmin        7 non-null      object\n 24  p-plpmax        7 non-null      object\n 25  p-plpn          10 non-null     object\n 26  p-plpns         10 non-null     object\n 27  fem-basalmean   40 non-null     object\n 28  fem-basalmin    30 non-null     object\n 29  fem-basalmax    30 non-null     object\n 30  fem-basaln      40 non-null     object\n 31  fem-basalns     41 non-null     object\n 32  fem-estrusmean  44 non-null     object\n 33  fem-estrusmin   41 non-null     object\n 34  fem-estrusmax   41 non-null     object\n 35  fem-estrusn     44 non-null     object\n 36  fem-estrusns    46 non-null     object\n 37  fpm-basalmean   37 non-null     object\n 38  fpm-basalmin    28 non-null     object\n 39  fpm-basalmax    27 non-null     object\n 40  fpm-basaln      41 non-null     object\n 41  fpm-basalns     41 non-null     object\n 42  fpm-nplpmean    38 non-null     object\n 43  fpm-nplpmin     39 non-null     object\n 44  fpm-nplpmax     39 non-null     object\n 45  fpm-nplpn       38 non-null     object\n 46  fpm-nplpns      39 non-null     object\n 47  fpm-plpmean     27 non-null     object\n 48  fpm-plpmin      22 non-null     object\n 49  fpm-plpmax      22 non-null     object\n 50  fpm-plpn        26 non-null     object\n 51  fpm-plpns       27 non-null     object\ndtypes: object(52)\nmemory usage: 44.4+ KB\n</pre> In\u00a0[5]: Copied! <pre>pd.set_option('max_columns', None)\nfeline_progest.head()\n</pre> pd.set_option('max_columns', None) feline_progest.head() Out[5]: linage species e-basalmean e-basalmin e-basalmax e-basaln e-basalns e-estrusmean e-estrusmin e-estrusmax e-estrusn e-estrusns p-basalmean p-basalmin p-basalmax p-basaln p-basalns p-nplpmean p-nplpmin p-nplpmax p-nplpn p-nplpns p-plpmean p-plpmin p-plpmax p-plpn p-plpns fem-basalmean fem-basalmin fem-basalmax fem-basaln fem-basalns fem-estrusmean fem-estrusmin fem-estrusmax fem-estrusn fem-estrusns fpm-basalmean fpm-basalmin fpm-basalmax fpm-basaln fpm-basalns fpm-nplpmean fpm-nplpmin fpm-nplpmax fpm-nplpn fpm-nplpns fpm-plpmean fpm-plpmin fpm-plpmax fpm-plpn fpm-plpns 0 Domestic cat Domestic cat NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1.0 NaN NaN 3.0 3.0 25.8 NaN NaN 5.0 12.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 Domestic cat Domestic cat 8.1 4.3 11.9 4.0 12.0 59.5 46.1 72.9 4.0 13.0 0.5 NaN NaN NaN NaN 24.6 19.0 31.0 4.0 4.0 34.9 29.0 41.0 2.0 2.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 Domestic cat Domestic cat 11.7 6.9 16.5 39.0 106.0 NaN 50.0 70.0 39.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 Domestic cat Domestic cat NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 0.2 0.1 0.3 7.0 32.0 17.2 9.0 25.0 7.0 12.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 Domestic cat Domestic cat NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 0.5 NaN NaN NaN NaN NaN 30.9 87.8 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN In\u00a0[6]: Copied! <pre># Function that plots the number of NaN data in the entire dataset\ndef plot_na(df):\n    series1 = df.isna().sum()\n    series2 = df.notnull().sum()\n    series3 = series1 + series2\n    series1.name = 'NaN'\n    series3.name = 'All Data'\n    fig = go.Figure(data=[px.bar(series1)['data'][0],\n                          px.bar(series3)['data'][0]])\n\n    fig.update_layout(template='plotly_white',height=300,\n                      font=dict(family='sans-serif',size=12)) \n    fig.update_layout(showlegend=False,title='NaN Distribution in Entire Dataset')\n    fig.update_traces(marker_color='rgb(158,202,225)', marker_line_color='rgb(8,48,107)',\n                      marker_line_width=1.5, opacity=0.4)\n    \n    fig.show('svg',dpi=300)\n</pre> # Function that plots the number of NaN data in the entire dataset def plot_na(df):     series1 = df.isna().sum()     series2 = df.notnull().sum()     series3 = series1 + series2     series1.name = 'NaN'     series3.name = 'All Data'     fig = go.Figure(data=[px.bar(series1)['data'][0],                           px.bar(series3)['data'][0]])      fig.update_layout(template='plotly_white',height=300,                       font=dict(family='sans-serif',size=12))      fig.update_layout(showlegend=False,title='NaN Distribution in Entire Dataset')     fig.update_traces(marker_color='rgb(158,202,225)', marker_line_color='rgb(8,48,107)',                       marker_line_width=1.5, opacity=0.4)          fig.show('svg',dpi=300) In\u00a0[7]: Copied! <pre>plot_na(feline_progest)\n</pre> plot_na(feline_progest) In\u00a0[8]: Copied! <pre>import seaborn as sns; sns.set(style='whitegrid')\n\n''' Plot Verticle Heatmap using Plotly '''\ndef plotlyoff_heatmap(hm,size=None):    \n    fig,ax = plt.subplots(ncols=2,figsize=(15,7),)\n    sns.heatmap(hm[0],ax=ax[0],annot=False)\n    sns.heatmap(hm[1],ax=ax[1],annot=False)\n    ax[0].set_title(\"Number of Felines\")\n    ax[1].set_title(\"Number of Samples\")\n    plt.tight_layout()\n    plt.show()\n</pre> import seaborn as sns; sns.set(style='whitegrid')  ''' Plot Verticle Heatmap using Plotly ''' def plotlyoff_heatmap(hm,size=None):         fig,ax = plt.subplots(ncols=2,figsize=(15,7),)     sns.heatmap(hm[0],ax=ax[0],annot=False)     sns.heatmap(hm[1],ax=ax[1],annot=False)     ax[0].set_title(\"Number of Felines\")     ax[1].set_title(\"Number of Samples\")     plt.tight_layout()     plt.show() In\u00a0[9]: Copied! <pre>sample_sizes = feline_progest[['linage','species','e-basaln','e-basalns','e-estrusn',\n                               'e-estrusns','p-basaln','p-basalns','p-plpn','p-plpns',\n                               'p-nplpn','p-nplpns','fem-basaln','fem-basalns',\n                               'fem-estrusn','fem-estrusns','fpm-basaln','fpm-basalns',\n                               'fpm-nplpn','fpm-nplpns','fpm-plpn','fpm-plpns']]\n\n# sample_sizes.groupby('species').max()\npt = pd.pivot_table(sample_sizes,index=['species','linage'])\npt.index=pt.index.get_level_values(0)+\"(\"+pt.index.get_level_values(1)+\")\" # merge multindex\n\nn_pt = pt.iloc[:,::2] # number of animals\nns_pt = pt[['e-basalns','e-estrusns','p-basalns','p-plpns','p-nplpns',\n 'fem-basalns','fem-estrusns','fpm-basalns','fpm-nplpns',\n 'fpm-plpn','fpm-plpns']]\n</pre> sample_sizes = feline_progest[['linage','species','e-basaln','e-basalns','e-estrusn',                                'e-estrusns','p-basaln','p-basalns','p-plpn','p-plpns',                                'p-nplpn','p-nplpns','fem-basaln','fem-basalns',                                'fem-estrusn','fem-estrusns','fpm-basaln','fpm-basalns',                                'fpm-nplpn','fpm-nplpns','fpm-plpn','fpm-plpns']]  # sample_sizes.groupby('species').max() pt = pd.pivot_table(sample_sizes,index=['species','linage']) pt.index=pt.index.get_level_values(0)+\"(\"+pt.index.get_level_values(1)+\")\" # merge multindex  n_pt = pt.iloc[:,::2] # number of animals ns_pt = pt[['e-basalns','e-estrusns','p-basalns','p-plpns','p-nplpns',  'fem-basalns','fem-estrusns','fpm-basalns','fpm-nplpns',  'fpm-plpn','fpm-plpns']] In\u00a0[10]: Copied! <pre>plotlyoff_heatmap([n_pt,ns_pt])\n</pre> plotlyoff_heatmap([n_pt,ns_pt]) <ul> <li>We'll also drop features relating to sample size &amp; number of animals in each study (<code>ns</code> &amp; <code>n</code>)</li> <li>Although there may be some benefit of including them, however it seems they aren't really necessary in this problem</li> </ul> In\u00a0[11]: Copied! <pre># Drop Sample number data\nfeline_progest.drop(['e-basaln','e-basalns','e-estrusn','e-estrusns',\n                    'p-basaln','p-basalns','p-plpn','p-plpns','p-nplpn','p-nplpns',\n                    'fem-basaln','fem-basalns','fem-estrusn','fem-estrusns',\n                     'fpm-basaln','fpm-basalns','fpm-nplpn','fpm-nplpns',\n                     'fpm-plpn','fpm-plpns'],axis=1,inplace=True)\n</pre> # Drop Sample number data feline_progest.drop(['e-basaln','e-basalns','e-estrusn','e-estrusns',                     'p-basaln','p-basalns','p-plpn','p-plpns','p-nplpn','p-nplpns',                     'fem-basaln','fem-basalns','fem-estrusn','fem-estrusns',                      'fpm-basaln','fpm-basalns','fpm-nplpn','fpm-nplpns',                      'fpm-plpn','fpm-plpns'],axis=1,inplace=True) In\u00a0[12]: Copied! <pre># Our premodel dataset\nfeline_progest.info()\n</pre> # Our premodel dataset feline_progest.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 109 entries, 0 to 108\nData columns (total 32 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   linage          109 non-null    object\n 1   species         109 non-null    object\n 2   e-basalmean     18 non-null     object\n 3   e-basalmin      19 non-null     object\n 4   e-basalmax      19 non-null     object\n 5   e-estrusmean    14 non-null     object\n 6   e-estrusmin     13 non-null     object\n 7   e-estrusmax     13 non-null     object\n 8   p-basalmean     27 non-null     object\n 9   p-basalmin      20 non-null     object\n 10  p-basalmax      20 non-null     object\n 11  p-nplpmean      16 non-null     object\n 12  p-nplpmin       14 non-null     object\n 13  p-nplpmax       14 non-null     object\n 14  p-plpmean       10 non-null     object\n 15  p-plpmin        7 non-null      object\n 16  p-plpmax        7 non-null      object\n 17  fem-basalmean   40 non-null     object\n 18  fem-basalmin    30 non-null     object\n 19  fem-basalmax    30 non-null     object\n 20  fem-estrusmean  44 non-null     object\n 21  fem-estrusmin   41 non-null     object\n 22  fem-estrusmax   41 non-null     object\n 23  fpm-basalmean   37 non-null     object\n 24  fpm-basalmin    28 non-null     object\n 25  fpm-basalmax    27 non-null     object\n 26  fpm-nplpmean    38 non-null     object\n 27  fpm-nplpmin     39 non-null     object\n 28  fpm-nplpmax     39 non-null     object\n 29  fpm-plpmean     27 non-null     object\n 30  fpm-plpmin      22 non-null     object\n 31  fpm-plpmax      22 non-null     object\ndtypes: object(32)\nmemory usage: 27.4+ KB\n</pre> In\u00a0[13]: Copied! <pre># function to show value_counts using plotly\ndef plot_count(df,feature,orie='v',h=400):\n\n    series = df[feature].value_counts()\n    fig = px.bar(series,orientation=orie,color='value')\n    fig.update_layout(template='plotly_white',height=h,\n                      font=dict(family='sans-serif',size=12)) \n    fig.update_layout(showlegend=False)\n    fig.update_traces(marker_color='rgb(158,202,225)', \n                      marker_line_color='rgb(8,48,107)',\n                      marker_line_width=1.5, opacity=0.6)\n    fig.update_traces(width=0.75)\n    \n    fig.show('svg',dpi=300)\n</pre> # function to show value_counts using plotly def plot_count(df,feature,orie='v',h=400):      series = df[feature].value_counts()     fig = px.bar(series,orientation=orie,color='value')     fig.update_layout(template='plotly_white',height=h,                       font=dict(family='sans-serif',size=12))      fig.update_layout(showlegend=False)     fig.update_traces(marker_color='rgb(158,202,225)',                        marker_line_color='rgb(8,48,107)',                       marker_line_width=1.5, opacity=0.6)     fig.update_traces(width=0.75)          fig.show('svg',dpi=300) In\u00a0[14]: Copied! <pre>plot_count(feline_progest,'linage',orie='h',h=300)\n</pre> plot_count(feline_progest,'linage',orie='h',h=300) In\u00a0[15]: Copied! <pre>plot_count(feline_progest,'species',orie='v',h=400)\n</pre> plot_count(feline_progest,'species',orie='v',h=400) In\u00a0[16]: Copied! <pre># function to show data distribution using plotly boxplot\ndef plot_strip(ldf,features,plot_id='box',\n               color=None,title=None):\n    \n    tdf = ldf[features]\n    del tdf['species']\n    \n    if(plot_id is 'box'):\n        fig = px.box(tdf,orientation='v',color=color,facet_col=color,\n                     color_discrete_sequence= px.colors.sequential.Plasma_r,\n                    facet_col_wrap =4)\n    elif(plot_id is 'strip'):\n        fig = px.strip(tdf,orientation='v',color=color)\n        \n    fig.update_layout(template='plotly_white',height=700,\n                      title=f'{title}',\n                      font=dict(family='sans-serif',size=12)) \n    fig.update_traces(width=0.25)\n    fig.update_layout(showlegend=False)\n    fig.update_traces(marker_color='#056293', marker_line_color='rgb(8,48,107)',\n                      marker_line_width=1.5, opacity=0.7)\n\n    fig.show('svg',dpi=300)\n</pre> # function to show data distribution using plotly boxplot def plot_strip(ldf,features,plot_id='box',                color=None,title=None):          tdf = ldf[features]     del tdf['species']          if(plot_id is 'box'):         fig = px.box(tdf,orientation='v',color=color,facet_col=color,                      color_discrete_sequence= px.colors.sequential.Plasma_r,                     facet_col_wrap =4)     elif(plot_id is 'strip'):         fig = px.strip(tdf,orientation='v',color=color)              fig.update_layout(template='plotly_white',height=700,                       title=f'{title}',                       font=dict(family='sans-serif',size=12))      fig.update_traces(width=0.25)     fig.update_layout(showlegend=False)     fig.update_traces(marker_color='#056293', marker_line_color='rgb(8,48,107)',                       marker_line_width=1.5, opacity=0.7)      fig.show('svg',dpi=300) In\u00a0[17]: Copied! <pre>main_lst = feline_progest.columns.tolist()\nlst = map(lambda x:main_lst[x],[0,1,\n                                2,5,\n                                3,6,\n                                4,7])\n    \nplot_strip(feline_progest,lst,'box',\n           'linage','Serum Concentrations - Estradiol (pg/ml)')\n</pre> main_lst = feline_progest.columns.tolist() lst = map(lambda x:main_lst[x],[0,1,                                 2,5,                                 3,6,                                 4,7])      plot_strip(feline_progest,lst,'box',            'linage','Serum Concentrations - Estradiol (pg/ml)') In\u00a0[18]: Copied! <pre>main_lst = feline_progest.columns.tolist()\nlst = map(lambda x:main_lst[x],[0,1,\n                                17,20,\n                                18,21,\n                                19,22])\n    \nplot_strip(feline_progest,lst,'box',\n           'linage','Fecal Metabolities (FEM) - Estradiol (ng/g)')\n</pre> main_lst = feline_progest.columns.tolist() lst = map(lambda x:main_lst[x],[0,1,                                 17,20,                                 18,21,                                 19,22])      plot_strip(feline_progest,lst,'box',            'linage','Fecal Metabolities (FEM) - Estradiol (ng/g)') In\u00a0[19]: Copied! <pre>main_lst = feline_progest.columns.tolist()\nlst = map(lambda x:main_lst[x],[0,1,\n                                8,11,14,\n                                9,12,15,\n                                10,13,16])\n    \nplot_strip(feline_progest,lst,'box',\n           'linage','Serum Concentrations - Progasterone (ng/ml)')\n\nmain_lst = feline_progest.columns.tolist()\nlst = map(lambda x:main_lst[x],[0,1,\n                                23,26,29,\n                                24,27,30,\n                                25,28,31])\n    \nplot_strip(feline_progest,lst,'box',\n           'linage','Fecal Metabolities (FPM) - Progasterone (\u00b5g/g)')\n</pre> main_lst = feline_progest.columns.tolist() lst = map(lambda x:main_lst[x],[0,1,                                 8,11,14,                                 9,12,15,                                 10,13,16])      plot_strip(feline_progest,lst,'box',            'linage','Serum Concentrations - Progasterone (ng/ml)')  main_lst = feline_progest.columns.tolist() lst = map(lambda x:main_lst[x],[0,1,                                 23,26,29,                                 24,27,30,                                 25,28,31])      plot_strip(feline_progest,lst,'box',            'linage','Fecal Metabolities (FPM) - Progasterone (\u00b5g/g)') In\u00a0[20]: Copied! <pre># Standard Train/Test Split Validation \ndef model_eval(data,  # data input\n               ts=0.3, # train/test split ratio\n               target = 'id', # target feature \n               clf=None, # model list\n               clf_name='model',\n               classif_id='binary', # type of classification\n               show_id=['roc']): # output options\n\n    # train/test split\n    y = data[target]\n    X = data.drop(target,axis=1)\n\n    if(clf is not None):\n        clf = clf[1]\n    # default model if not model is selected\n    else:\n        clf = RandomForestClassifier(max_depth=10,\n                                     random_state=0)\n    \n    # Train/Test split our dataset\n    X_train, X_test, y_train, y_test = train_test_split(X,y,\n                                                        test_size=ts)\n    \n    # Show the split distribution\n    print(f'Training Samples: {X_train.shape[0]}')\n    print(f'Test Samples: {X_test.shape[0]}')\n\n    # train model\n    clf.fit(X_train,y_train)\n    clf.save_model(f\"{clf_name}\")\n    \n    # predict on training data &amp; test data\n    y_pred_train = clf.predict(X_train)\n    y_pred_test = clf.predict(X_test)\n    \n    # Evaluate Metrics\n#     print(\"Accuracy:\",metrics.accuracy_score(y_train, y_pred_train))\n#     print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_test))\n    \n    # Plot Confusion Matrix for Training / Test Data\n    if('conf' in show_id):\n        \n        data1 = confusion_matrix(y_train,y_pred_train)\n        data2 = confusion_matrix(y_test,y_pred_test)\n  \n        ''' Plot Verticle Heatmap using Plotly '''\n        def plotlyoff_heatmap(hm,size=None):    \n            fig,ax = plt.subplots(ncols=2,figsize=(8,4),)\n            sns.heatmap(hm[0],ax=ax[0],annot=True)\n            sns.heatmap(hm[1],ax=ax[1],annot=True)\n            ax[0].set_title(\"Training Confusion Matrix\")\n            ax[1].set_title(\"Test Confusion Matrix\")\n            plt.tight_layout()\n            plt.show()\n            \n        data1 = pd.DataFrame(data1)\n        data1.index = clf.classes_\n        data1.columns = clf.classes_\n        data2 = pd.DataFrame(data2)\n        data2.index = clf.classes_\n        data2.columns = clf.classes_\n        plotlyoff_heatmap([data1,data2])\n        \n    \n    # Plot ROC Curves for Training / Test Data\n    if('roc' in show_id):\n        \n        fig = make_subplots(rows=1,cols=2,subplot_titles=['Train','Test'])\n        \n        if(classif_id is 'binary'):\n\n            y_score_train = clf.predict_proba(X_train)[:, 1]\n            y_score_test = clf.predict_proba(X_test)[:, 1]\n\n            ii=-1; iii=0\n            lst_in_X = [X_train,X_test]\n            lst_in_y = [y_train,y_test]\n            lst_subgroup = [y_score_train,y_score_test]\n            lst_name = ['train','test']\n            for group in lst_subgroup:\n\n                ii+=1;iii+=1\n                y_true = lst_in_y[ii]\n                y_score = lst_subgroup[ii]\n                y_true = y_true.map({'basal': 0, 'estrus':1})\n                fpr, tpr, _ = roc_curve(y_true, y_score)\n                auc_score = roc_auc_score(y_true, y_score)\n\n                name = f\"({lst_name[ii]} AUC={auc_score:.2f})\"\n                fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines',name=name),\n                              col=iii,row=1)\n                fig.add_shape(type='line', line=dict(dash='dash'),\n                              x0=0, x1=1, y0=0, y1=1,col=iii,row=1)\n        \n        else:\n\n            y_score_train = clf.predict_proba(X_train)\n            y_score_test = clf.predict_proba(X_test)\n\n            ii=-1; iii=0\n            lst_in_X = [X_train,X_test]\n            \n            lst_in_y = [y_train,y_test]\n            lst_subgroup = [y_score_train,y_score_test]\n            for group in lst_subgroup:\n\n                ii+=1; iii+=1\n                y_onehot = pd.get_dummies(lst_in_y[ii], columns=clf.classes_)\n                clf.predict_proba(lst_in_X[ii])\n\n                # Multiclass ROC Curves\n                iiii=-1\n                lst_colour = ['#1C76A5','#69BCE7','#BBE1F5']\n                for i in range(group.shape[1]):\n\n                    iiii+=1\n                    y_true = y_onehot.iloc[:, i]\n                    y_score = group[:, i]\n\n                    fpr, tpr, _ = roc_curve(y_true, y_score)\n                    auc_score = roc_auc_score(y_true, y_score)\n\n                    name = f\"{y_onehot.columns[i]} (AUC={auc_score:.2f})\"\n                    fig.add_trace(go.Scatter(x=fpr, y=tpr,\n                                             line=dict(color=f\"{lst_colour[iiii]}\"),\n                                             name=name, mode='lines'),col=iii,row=1)\n                fig.add_shape(type='line', line=dict(dash='dash'),x0=0, \n                              x1=1, y0=0, y1=1,col=iii,row=1)\n\n        # Plot Aesthetics\n        fig.update_xaxes(title_text=f'False Positive Rate', \n                         row=1, col=1, scaleanchor=\"x\", scaleratio=1)\n        fig.update_xaxes(title_text=f'False Positive Rate',\n                         row=1, col=2, scaleanchor=\"x\", scaleratio=1)\n        fig.update_yaxes(title_text=f'True Positive Rate',\n                         row=1, col=1, constrain='domain')\n        fig.update_yaxes(title_text=f'True Positive Rate',\n                         row=1, col=2, constrain='domain')\n        fig.update_layout(template='plotly_white',height=400)\n        fig.update_layout(title=f\"CatBoost Classifier | ROC Curve\") \n        fig.show('svg',dpi=300)\n</pre> # Standard Train/Test Split Validation  def model_eval(data,  # data input                ts=0.3, # train/test split ratio                target = 'id', # target feature                 clf=None, # model list                clf_name='model',                classif_id='binary', # type of classification                show_id=['roc']): # output options      # train/test split     y = data[target]     X = data.drop(target,axis=1)      if(clf is not None):         clf = clf[1]     # default model if not model is selected     else:         clf = RandomForestClassifier(max_depth=10,                                      random_state=0)          # Train/Test split our dataset     X_train, X_test, y_train, y_test = train_test_split(X,y,                                                         test_size=ts)          # Show the split distribution     print(f'Training Samples: {X_train.shape[0]}')     print(f'Test Samples: {X_test.shape[0]}')      # train model     clf.fit(X_train,y_train)     clf.save_model(f\"{clf_name}\")          # predict on training data &amp; test data     y_pred_train = clf.predict(X_train)     y_pred_test = clf.predict(X_test)          # Evaluate Metrics #     print(\"Accuracy:\",metrics.accuracy_score(y_train, y_pred_train)) #     print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_test))          # Plot Confusion Matrix for Training / Test Data     if('conf' in show_id):                  data1 = confusion_matrix(y_train,y_pred_train)         data2 = confusion_matrix(y_test,y_pred_test)            ''' Plot Verticle Heatmap using Plotly '''         def plotlyoff_heatmap(hm,size=None):                 fig,ax = plt.subplots(ncols=2,figsize=(8,4),)             sns.heatmap(hm[0],ax=ax[0],annot=True)             sns.heatmap(hm[1],ax=ax[1],annot=True)             ax[0].set_title(\"Training Confusion Matrix\")             ax[1].set_title(\"Test Confusion Matrix\")             plt.tight_layout()             plt.show()                      data1 = pd.DataFrame(data1)         data1.index = clf.classes_         data1.columns = clf.classes_         data2 = pd.DataFrame(data2)         data2.index = clf.classes_         data2.columns = clf.classes_         plotlyoff_heatmap([data1,data2])                   # Plot ROC Curves for Training / Test Data     if('roc' in show_id):                  fig = make_subplots(rows=1,cols=2,subplot_titles=['Train','Test'])                  if(classif_id is 'binary'):              y_score_train = clf.predict_proba(X_train)[:, 1]             y_score_test = clf.predict_proba(X_test)[:, 1]              ii=-1; iii=0             lst_in_X = [X_train,X_test]             lst_in_y = [y_train,y_test]             lst_subgroup = [y_score_train,y_score_test]             lst_name = ['train','test']             for group in lst_subgroup:                  ii+=1;iii+=1                 y_true = lst_in_y[ii]                 y_score = lst_subgroup[ii]                 y_true = y_true.map({'basal': 0, 'estrus':1})                 fpr, tpr, _ = roc_curve(y_true, y_score)                 auc_score = roc_auc_score(y_true, y_score)                  name = f\"({lst_name[ii]} AUC={auc_score:.2f})\"                 fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines',name=name),                               col=iii,row=1)                 fig.add_shape(type='line', line=dict(dash='dash'),                               x0=0, x1=1, y0=0, y1=1,col=iii,row=1)                  else:              y_score_train = clf.predict_proba(X_train)             y_score_test = clf.predict_proba(X_test)              ii=-1; iii=0             lst_in_X = [X_train,X_test]                          lst_in_y = [y_train,y_test]             lst_subgroup = [y_score_train,y_score_test]             for group in lst_subgroup:                  ii+=1; iii+=1                 y_onehot = pd.get_dummies(lst_in_y[ii], columns=clf.classes_)                 clf.predict_proba(lst_in_X[ii])                  # Multiclass ROC Curves                 iiii=-1                 lst_colour = ['#1C76A5','#69BCE7','#BBE1F5']                 for i in range(group.shape[1]):                      iiii+=1                     y_true = y_onehot.iloc[:, i]                     y_score = group[:, i]                      fpr, tpr, _ = roc_curve(y_true, y_score)                     auc_score = roc_auc_score(y_true, y_score)                      name = f\"{y_onehot.columns[i]} (AUC={auc_score:.2f})\"                     fig.add_trace(go.Scatter(x=fpr, y=tpr,                                              line=dict(color=f\"{lst_colour[iiii]}\"),                                              name=name, mode='lines'),col=iii,row=1)                 fig.add_shape(type='line', line=dict(dash='dash'),x0=0,                                x1=1, y0=0, y1=1,col=iii,row=1)          # Plot Aesthetics         fig.update_xaxes(title_text=f'False Positive Rate',                           row=1, col=1, scaleanchor=\"x\", scaleratio=1)         fig.update_xaxes(title_text=f'False Positive Rate',                          row=1, col=2, scaleanchor=\"x\", scaleratio=1)         fig.update_yaxes(title_text=f'True Positive Rate',                          row=1, col=1, constrain='domain')         fig.update_yaxes(title_text=f'True Positive Rate',                          row=1, col=2, constrain='domain')         fig.update_layout(template='plotly_white',height=400)         fig.update_layout(title=f\"CatBoost Classifier | ROC Curve\")          fig.show('svg',dpi=300) In\u00a0[21]: Copied! <pre># select the relevant data from the main dataframe\nupd = feline_progest.iloc[:,0:8]\ndisplay(upd.head())\n</pre> # select the relevant data from the main dataframe upd = feline_progest.iloc[:,0:8] display(upd.head()) linage species e-basalmean e-basalmin e-basalmax e-estrusmean e-estrusmin e-estrusmax 0 Domestic cat Domestic cat NaN NaN NaN NaN NaN NaN 1 Domestic cat Domestic cat 8.1 4.3 11.9 59.5 46.1 72.9 2 Domestic cat Domestic cat 11.7 6.9 16.5 NaN 50.0 70.0 3 Domestic cat Domestic cat NaN NaN NaN NaN NaN NaN 4 Domestic cat Domestic cat NaN NaN NaN NaN NaN NaN In\u00a0[22]: Copied! <pre># melt but keep some column values \nmolten = pd.melt(upd,\n                id_vars=['linage','species'])\n\nmolten.loc[(molten['variable'] == 'e-basalmean') |\n           (molten['variable'] == 'e-basalmin') |\n           (molten['variable'] == 'e-basalmax')\n           , 'id'] = 'basal'\nmolten.loc[(molten['variable'] == 'e-estrusmean') |\n           (molten['variable'] == 'e-estrusmin') |\n           (molten['variable'] == 'e-estrusmax')\n           , 'id'] = 'estrus'\n\nmolten.dropna(inplace=True)\nmolten_ohe = pd.get_dummies(molten,columns=['linage','species','variable'])\n</pre> # melt but keep some column values  molten = pd.melt(upd,                 id_vars=['linage','species'])  molten.loc[(molten['variable'] == 'e-basalmean') |            (molten['variable'] == 'e-basalmin') |            (molten['variable'] == 'e-basalmax')            , 'id'] = 'basal' molten.loc[(molten['variable'] == 'e-estrusmean') |            (molten['variable'] == 'e-estrusmin') |            (molten['variable'] == 'e-estrusmax')            , 'id'] = 'estrus'  molten.dropna(inplace=True) molten_ohe = pd.get_dummies(molten,columns=['linage','species','variable']) In\u00a0[23]: Copied! <pre>models = []\nmodels.append(('CAT',CatBoostClassifier(silent=True,\n                                        n_estimators=25)))\n\nmodel_eval(molten_ohe,\n           clf=models[0],\n           clf_name='model1',\n           show_id=['roc','conf'])\n</pre> models = [] models.append(('CAT',CatBoostClassifier(silent=True,                                         n_estimators=25)))  model_eval(molten_ohe,            clf=models[0],            clf_name='model1',            show_id=['roc','conf']) <pre>Training Samples: 67\nTest Samples: 29\n</pre> In\u00a0[24]: Copied! <pre># Select the relevant data\nupd2 = feline_progest.iloc[:,pd.np.r_[0:2, 8:17]]\nupd2.head()\n</pre> # Select the relevant data upd2 = feline_progest.iloc[:,pd.np.r_[0:2, 8:17]] upd2.head() Out[24]: linage species p-basalmean p-basalmin p-basalmax p-nplpmean p-nplpmin p-nplpmax p-plpmean p-plpmin p-plpmax 0 Domestic cat Domestic cat 1.0 NaN NaN 25.8 NaN NaN NaN NaN NaN 1 Domestic cat Domestic cat 0.5 NaN NaN 24.6 19.0 31.0 34.9 29.0 41.0 2 Domestic cat Domestic cat NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 Domestic cat Domestic cat 0.2 0.1 0.3 17.2 9.0 25.0 NaN NaN NaN 4 Domestic cat Domestic cat 0.5 NaN NaN NaN 30.9 87.8 NaN NaN NaN In\u00a0[25]: Copied! <pre># melt but keep some column values \nmolten = pd.melt(upd2,\n                id_vars=['linage','species'])\n\nmolten.loc[(molten['variable'] == 'p-basalmean') |\n           (molten['variable'] == 'p-basalmin') |\n           (molten['variable'] == 'p-basalmax')\n           , 'id'] = 'basal'\nmolten.loc[(molten['variable'] == 'p-nplpmean') |\n           (molten['variable'] == 'p-nplpmin') |\n           (molten['variable'] == 'p-nplpmax')\n           , 'id'] = 'nplp'\nmolten.loc[(molten['variable'] == 'p-plpmean') |\n           (molten['variable'] == 'p-plpmin') |\n           (molten['variable'] == 'p-plpmax')\n           , 'id'] = 'plp'\n</pre> # melt but keep some column values  molten = pd.melt(upd2,                 id_vars=['linage','species'])  molten.loc[(molten['variable'] == 'p-basalmean') |            (molten['variable'] == 'p-basalmin') |            (molten['variable'] == 'p-basalmax')            , 'id'] = 'basal' molten.loc[(molten['variable'] == 'p-nplpmean') |            (molten['variable'] == 'p-nplpmin') |            (molten['variable'] == 'p-nplpmax')            , 'id'] = 'nplp' molten.loc[(molten['variable'] == 'p-plpmean') |            (molten['variable'] == 'p-plpmin') |            (molten['variable'] == 'p-plpmax')            , 'id'] = 'plp' In\u00a0[26]: Copied! <pre>molten.dropna(inplace=True)\nmolten_ohe = pd.get_dummies(molten,columns=['linage','species','variable'])\nmolten_ohe\n</pre> molten.dropna(inplace=True) molten_ohe = pd.get_dummies(molten,columns=['linage','species','variable']) molten_ohe Out[26]: value id linage_Domestic cat linage_Lynx linage_Ocelot linage_Panthera linage_Puma species_Bobcat species_Cheetah species_Clouded leopard species_Domestic cat species_Eurasian lynx species_Iberian lynx species_Jagaurondi species_Jaguar species_Leopard species_Lion species_Ocelot species_Puma species_Snow leopard species_Tigers variable_p-basalmax variable_p-basalmean variable_p-basalmin variable_p-nplpmax variable_p-nplpmean variable_p-nplpmin variable_p-plpmax variable_p-plpmean variable_p-plpmin 0 1.0 basal 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0.5 basal 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 3 0.2 basal 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 4 0.5 basal 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 6 0.5 basal 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 878 30.0 plp 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 892 39.9 plp 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 930 27.1 plp 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 932 74.4 plp 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 934 168.0 plp 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 <p>135 rows \u00d7 30 columns</p> In\u00a0[27]: Copied! <pre>models = []\nmodels.append(('CAT',CatBoostClassifier(silent=True,\n                                        n_estimators=25)))\n\nmodel_eval(molten_ohe,\n           clf=models[0],\n           clf_name='model2',\n           classif_id='multi',\n           show_id=['roc','conf'])\n</pre> models = [] models.append(('CAT',CatBoostClassifier(silent=True,                                         n_estimators=25)))  model_eval(molten_ohe,            clf=models[0],            clf_name='model2',            classif_id='multi',            show_id=['roc','conf']) <pre>Training Samples: 94\nTest Samples: 41\n</pre> In\u00a0[28]: Copied! <pre>upd3 = feline_progest.iloc[:,pd.np.r_[0:2, 17:23]]\nupd3.head()\n</pre> upd3 = feline_progest.iloc[:,pd.np.r_[0:2, 17:23]] upd3.head() Out[28]: linage species fem-basalmean fem-basalmin fem-basalmax fem-estrusmean fem-estrusmin fem-estrusmax 0 Domestic cat Domestic cat NaN NaN NaN NaN NaN NaN 1 Domestic cat Domestic cat NaN NaN NaN NaN NaN NaN 2 Domestic cat Domestic cat NaN NaN NaN NaN NaN NaN 3 Domestic cat Domestic cat NaN NaN NaN NaN NaN NaN 4 Domestic cat Domestic cat NaN NaN NaN NaN NaN NaN In\u00a0[29]: Copied! <pre># melt but keep some column values \nmolten = pd.melt(upd3,\n                id_vars=['linage','species'])\n\nmolten.loc[(molten['variable'] == 'fem-basalmean') |\n           (molten['variable'] == 'fem-basalmin') |\n           (molten['variable'] == 'fem-basalmax')\n           , 'id'] = 'basal'\nmolten.loc[(molten['variable'] == 'fem-estrusmean') |\n           (molten['variable'] == 'fem-estrusmin') |\n           (molten['variable'] == 'fem-estrusmax')\n           , 'id'] = 'estrus'\n\nmolten.dropna(inplace=True)\nmolten_ohe = pd.get_dummies(molten,columns=['linage','species','variable'])\n</pre> # melt but keep some column values  molten = pd.melt(upd3,                 id_vars=['linage','species'])  molten.loc[(molten['variable'] == 'fem-basalmean') |            (molten['variable'] == 'fem-basalmin') |            (molten['variable'] == 'fem-basalmax')            , 'id'] = 'basal' molten.loc[(molten['variable'] == 'fem-estrusmean') |            (molten['variable'] == 'fem-estrusmin') |            (molten['variable'] == 'fem-estrusmax')            , 'id'] = 'estrus'  molten.dropna(inplace=True) molten_ohe = pd.get_dummies(molten,columns=['linage','species','variable']) In\u00a0[30]: Copied! <pre>molten.dropna(inplace=True)\nmolten_ohe = pd.get_dummies(molten,columns=['linage','species','variable'])\nmolten_ohe\n</pre> molten.dropna(inplace=True) molten_ohe = pd.get_dummies(molten,columns=['linage','species','variable']) molten_ohe Out[30]: value id linage_Bay cat linage_Caracal linage_Domestic cat linage_Leopard cat linage_Lynx linage_Ocelot linage_Panthera linage_Puma species_Asiatic golden cat species_Black footed cat species_Canadian lynx species_Caracal species_Cheetah species_Clouded leopard species_Domestic cat species_Fishing cat species_Jaguar species_Leopard species_Leopard cat species_Lion species_Margay species_Pallas cat species_Snow leopard species_Tigers species_tigrina variable_fem-basalmax variable_fem-basalmean variable_fem-basalmin variable_fem-estrusmax variable_fem-estrusmean variable_fem-estrusmin 16 134.0 basal 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 17 127.1 basal 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 18 34.3 basal 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 22 38.2 basal 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 23 40.0 basal 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 646 2031.0 estrus 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 649 15980.0 estrus 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 650 2293.0 estrus 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 651 250.0 estrus 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 652 354.0 estrus 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 <p>226 rows \u00d7 33 columns</p> In\u00a0[31]: Copied! <pre>models = []\nmodels.append(('CAT',CatBoostClassifier(silent=True,\n                                        n_estimators=25)))\n\nmodel_eval(molten_ohe,\n           clf=models[0],\n           clf_name='model3',\n           classif_id='multi',\n           show_id=['roc','conf'])\n</pre> models = [] models.append(('CAT',CatBoostClassifier(silent=True,                                         n_estimators=25)))  model_eval(molten_ohe,            clf=models[0],            clf_name='model3',            classif_id='multi',            show_id=['roc','conf']) <pre>Training Samples: 158\nTest Samples: 68\n</pre> In\u00a0[32]: Copied! <pre>upd4 = feline_progest.iloc[:,pd.np.r_[0:2, 23:32]]\nupd4.head()\n</pre> upd4 = feline_progest.iloc[:,pd.np.r_[0:2, 23:32]] upd4.head() Out[32]: linage species fpm-basalmean fpm-basalmin fpm-basalmax fpm-nplpmean fpm-nplpmin fpm-nplpmax fpm-plpmean fpm-plpmin fpm-plpmax 0 Domestic cat Domestic cat NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 Domestic cat Domestic cat NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 Domestic cat Domestic cat NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 Domestic cat Domestic cat NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 Domestic cat Domestic cat NaN NaN NaN NaN NaN NaN NaN NaN NaN In\u00a0[33]: Copied! <pre># melt but keep some column values \nmolten = pd.melt(upd4,\n                id_vars=['linage','species'])\n\nmolten.loc[(molten['variable'] == 'fpm-basalmean') |\n           (molten['variable'] == 'fpm-basalmin') |\n           (molten['variable'] == 'fpm-basalmax')\n           , 'id'] = 'basal'\nmolten.loc[(molten['variable'] == 'fpm-nplpmean') |\n           (molten['variable'] == 'fpm-nplpmin') |\n           (molten['variable'] == 'fpm-nplpmax')\n           , 'id'] = 'nplp'\nmolten.loc[(molten['variable'] == 'fpm-plpmean') |\n           (molten['variable'] == 'fpm-plpmin') |\n           (molten['variable'] == 'fpm-plpmax')\n           , 'id'] = 'plp'\n</pre> # melt but keep some column values  molten = pd.melt(upd4,                 id_vars=['linage','species'])  molten.loc[(molten['variable'] == 'fpm-basalmean') |            (molten['variable'] == 'fpm-basalmin') |            (molten['variable'] == 'fpm-basalmax')            , 'id'] = 'basal' molten.loc[(molten['variable'] == 'fpm-nplpmean') |            (molten['variable'] == 'fpm-nplpmin') |            (molten['variable'] == 'fpm-nplpmax')            , 'id'] = 'nplp' molten.loc[(molten['variable'] == 'fpm-plpmean') |            (molten['variable'] == 'fpm-plpmin') |            (molten['variable'] == 'fpm-plpmax')            , 'id'] = 'plp' In\u00a0[34]: Copied! <pre>molten.dropna(inplace=True)\nmolten_ohe = pd.get_dummies(molten,columns=['linage','species','variable'])\nmolten_ohe\n</pre> molten.dropna(inplace=True) molten_ohe = pd.get_dummies(molten,columns=['linage','species','variable']) molten_ohe Out[34]: value id linage_Caracal linage_Domestic cat linage_Leopard cat linage_Lynx linage_Ocelot linage_Panthera linage_Puma species_Black footed cat species_Canadian lynx species_Caracal species_Cheetah species_Clouded leopard species_Domestic cat species_Fishing cat species_Jaguar species_Leopard species_Leopard cat species_Lion species_Margay species_Pallas cat species_Puma species_Snow leopard species_Tigers species_tigrina variable_fpm-basalmax variable_fpm-basalmean variable_fpm-basalmin variable_fpm-nplpmax variable_fpm-nplpmean variable_fpm-nplpmin variable_fpm-plpmax variable_fpm-plpmean variable_fpm-plpmin 16 10.1 basal 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 17 9.1 basal 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 18 20.3 basal 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 22 2.8 basal 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 23 2.6 basal 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 970 28.7 plp 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 976 40.6 plp 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 977 13.8 plp 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 978 345.0 plp 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 979 166.5 plp 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 <p>279 rows \u00d7 35 columns</p> In\u00a0[35]: Copied! <pre>models = []\nmodels.append(('CAT',CatBoostClassifier(silent=True,\n                                        n_estimators=25)))\n\nmodel_eval(molten_ohe,\n           clf=models[0],\n           clf_name='model4',\n           classif_id='multi',\n           show_id=['roc','conf'])\n</pre> models = [] models.append(('CAT',CatBoostClassifier(silent=True,                                         n_estimators=25)))  model_eval(molten_ohe,            clf=models[0],            clf_name='model4',            classif_id='multi',            show_id=['roc','conf']) <pre>Training Samples: 195\nTest Samples: 84\n</pre> In\u00a0[36]: Copied! <pre># Select the relevant data\nupd5 = feline_progest.iloc[:,pd.np.r_[0:2, 8:17,23:32]]\n\n# melt but keep some column values \nmolten5 = pd.melt(upd5,\n                id_vars=['linage','species'])\n\nmolten5.loc[(molten5['variable'] == 'p-basalmean') |\n           (molten5['variable'] == 'p-basalmin') |\n           (molten5['variable'] == 'p-basalmax')\n           , 'id'] = 'basal'\nmolten5.loc[(molten5['variable'] == 'p-nplpmean') |\n           (molten5['variable'] == 'p-nplpmin') |\n           (molten5['variable'] == 'p-nplpmax')\n           , 'id'] = 'nplp'\nmolten5.loc[(molten5['variable'] == 'p-plpmean') |\n           (molten5['variable'] == 'p-plpmin') |\n           (molten5['variable'] == 'p-plpmax')\n           , 'id'] = 'plp'\n\nmolten5.loc[(molten5['variable'] == 'fpm-basalmean') |\n           (molten5['variable'] == 'fpm-basalmin') |\n           (molten5['variable'] == 'fpm-basalmax')\n           , 'id'] = 'basal'\nmolten5.loc[(molten5['variable'] == 'fpm-nplpmean') |\n           (molten5['variable'] == 'fpm-nplpmin') |\n           (molten5['variable'] == 'fpm-nplpmax')\n           , 'id'] = 'nplp'\nmolten5.loc[(molten5['variable'] == 'fpm-plpmean') |\n           (molten5['variable'] == 'fpm-plpmin') |\n           (molten5['variable'] == 'fpm-plpmax')\n           , 'id'] = 'plp'\n\nmolten5.dropna(inplace=True)\nmolten_ohe5 = pd.get_dummies(molten5,columns=['linage','species','variable'])\nmolten_ohe5.value = molten_ohe5.value.astype('float')\nmolten_ohe5\n</pre> # Select the relevant data upd5 = feline_progest.iloc[:,pd.np.r_[0:2, 8:17,23:32]]  # melt but keep some column values  molten5 = pd.melt(upd5,                 id_vars=['linage','species'])  molten5.loc[(molten5['variable'] == 'p-basalmean') |            (molten5['variable'] == 'p-basalmin') |            (molten5['variable'] == 'p-basalmax')            , 'id'] = 'basal' molten5.loc[(molten5['variable'] == 'p-nplpmean') |            (molten5['variable'] == 'p-nplpmin') |            (molten5['variable'] == 'p-nplpmax')            , 'id'] = 'nplp' molten5.loc[(molten5['variable'] == 'p-plpmean') |            (molten5['variable'] == 'p-plpmin') |            (molten5['variable'] == 'p-plpmax')            , 'id'] = 'plp'  molten5.loc[(molten5['variable'] == 'fpm-basalmean') |            (molten5['variable'] == 'fpm-basalmin') |            (molten5['variable'] == 'fpm-basalmax')            , 'id'] = 'basal' molten5.loc[(molten5['variable'] == 'fpm-nplpmean') |            (molten5['variable'] == 'fpm-nplpmin') |            (molten5['variable'] == 'fpm-nplpmax')            , 'id'] = 'nplp' molten5.loc[(molten5['variable'] == 'fpm-plpmean') |            (molten5['variable'] == 'fpm-plpmin') |            (molten5['variable'] == 'fpm-plpmax')            , 'id'] = 'plp'  molten5.dropna(inplace=True) molten_ohe5 = pd.get_dummies(molten5,columns=['linage','species','variable']) molten_ohe5.value = molten_ohe5.value.astype('float') molten_ohe5 Out[36]: value id linage_Caracal linage_Domestic cat linage_Leopard cat linage_Lynx linage_Ocelot linage_Panthera linage_Puma species_Black footed cat species_Bobcat species_Canadian lynx species_Caracal species_Cheetah species_Clouded leopard species_Domestic cat species_Eurasian lynx species_Fishing cat species_Iberian lynx species_Jagaurondi species_Jaguar species_Leopard species_Leopard cat species_Lion species_Margay species_Ocelot species_Pallas cat species_Puma species_Snow leopard species_Tigers species_tigrina variable_fpm-basalmax variable_fpm-basalmean variable_fpm-basalmin variable_fpm-nplpmax variable_fpm-nplpmean variable_fpm-nplpmin variable_fpm-plpmax variable_fpm-plpmean variable_fpm-plpmin variable_p-basalmax variable_p-basalmean variable_p-basalmin variable_p-nplpmax variable_p-nplpmean variable_p-nplpmin variable_p-plpmax variable_p-plpmean variable_p-plpmin 0 1.0 basal 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0.5 basal 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 3 0.2 basal 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 4 0.5 basal 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 6 0.5 basal 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 1951 28.7 plp 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1957 40.6 plp 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1958 13.8 plp 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1959 345.0 plp 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1960 166.5 plp 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 <p>414 rows \u00d7 49 columns</p> In\u00a0[37]: Copied! <pre>models = []\nmodels.append(('CAT',CatBoostClassifier(silent=True,\n                                        n_estimators=25)))\n\nmodel_eval(molten_ohe5,\n           clf=models[0],\n           clf_name='model5',\n           classif_id='multi',\n           show_id=['roc','conf'])\n</pre> models = [] models.append(('CAT',CatBoostClassifier(silent=True,                                         n_estimators=25)))  model_eval(molten_ohe5,            clf=models[0],            clf_name='model5',            classif_id='multi',            show_id=['roc','conf']) <pre>Training Samples: 289\nTest Samples: 125\n</pre> In\u00a0[38]: Copied! <pre>from sklearn.feature_selection import SelectKBest,f_regression\nfrom xgboost import plot_importance,XGBRegressor\nfrom catboost import CatBoostClassifier,CatBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import preprocessing\nimport shap\nimport seaborn as sns\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Plot Correlation to Target Variable only\ndef corrMat(df,target='id',figsize=(9,0.5),ret_id=False):\n    \n    corr_mat = df.corr().round(2)\n    shape = corr_mat.shape[0]\n    corr_mat = corr_mat.transpose()\n    corr = corr_mat.loc[:, df.columns == target].transpose().copy()\n    \n    if(ret_id):\n        return corr\n\n''' Feature Importance '''\n# Various Approaches for quick FI evaluation\n\ndef fi(ldf,target='id',n_est=25,drop_id=None,target_cat=True):\n    \n    ldf = ldf.copy()\n    # If target is categorical string variable\n    if(target_cat):\n        cats = ldf[target].unique()\n        cats_id = [i for i in range(0,len(cats))]\n        maps = dict(zip(cats,cats_id))    \n        ldf[target] = ldf[target].map(maps)\n    \n    # If any features are desired to be droped \n    if(drop_id is not None):\n        ldf = ldf.drop(drop_id,axis=1)\n\n    # Input dataframe containing feature &amp; target variable\n    y = ldf[target]\n    X = ldf.drop(target,axis=1)\n    \n#   CORRELATION\n    imp = corrMat(ldf,target,figsize=(15,0.5),ret_id=True)\n    del imp[target]\n    s1 = imp.squeeze(axis=0);s1 = abs(s1)\n    s1.name = 'CORR'\n    \n#   SHAP\n    model = CatBoostRegressor(silent=True,n_estimators=n_est).fit(X,y)\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X)\n    shap_sum = np.abs(shap_values).mean(axis=0)\n    s2 = pd.Series(shap_sum,index=X.columns,name='CAT_SHAP').T\n    \n#   CATBOOST\n    model = CatBoostRegressor(silent=True,n_estimators=n_est).fit(X,y)\n    fit = model.fit(X,y)\n    rf_fi = pd.DataFrame(model.feature_importances_,index=X.columns,\n                                         columns=['CAT'])\n    rf_fi.sort_values('CAT',ascending=False)\n    s3 = rf_fi.T.squeeze(axis=0)\n    \n#   RANDOMFOREST\n    model = RandomForestRegressor(n_est,random_state=0, n_jobs=-1)\n    fit = model.fit(X,y)\n    rf_fi = pd.DataFrame(model.feature_importances_,index=X.columns,\n                                         columns=['RF'])\n    rf_fi.sort_values('RF',ascending=False)\n    s4 = rf_fi.T.squeeze(axis=0)\n\n#   XGB \n    model=XGBRegressor(n_estimators=n_est,learning_rate=0.5,verbosity = 0)\n    model.fit(X,y)\n    data = model.feature_importances_\n    s5 = pd.Series(data,index=X.columns,name='XGB').T\n\n#   KBEST\n    model = SelectKBest(k=5, score_func=f_regression)\n    fit = model.fit(X,y)\n    data = fit.scores_\n    s6 = pd.Series(data,index=X.columns,name='KBEST')\n\n    # Combine Scores\n    df0 = pd.concat([s1,s2,s3,s4,s5,s6],axis=1)\n    df0.rename(columns={'target':'lin corr'})\n\n    # MinMax Scaler\n    x = df0.values \n    min_max_scaler = preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(x)\n    df = pd.DataFrame(x_scaled,index=df0.index,columns=df0.columns)\n    df = df.rename_axis(f'&lt;b&gt;FI APPROACH&lt;/b&gt;', axis=1)\n    df = df.rename_axis('Feature', axis=0)\n    \n    pd.options.plotting.backend = \"plotly\"\n    fig = df.plot(kind='bar',title='&lt;b&gt;SCALED FEATURE IMPORTANCE&lt;/b&gt;',\n                  color_discrete_sequence=px.colors.qualitative.T10)\n    fig.update_layout(template='plotly_white',height=400,\n                     font=dict(family='sans-serif',size=12),\n                     margin=dict(l=60, r=40, t=50, b=10))\n    fig.update_traces(width=0.85)\n    fig.show('svg',dpi=300)\n</pre> from sklearn.feature_selection import SelectKBest,f_regression from xgboost import plot_importance,XGBRegressor from catboost import CatBoostClassifier,CatBoostRegressor from sklearn.ensemble import RandomForestRegressor from sklearn import preprocessing import shap import seaborn as sns  cmap = sns.diverging_palette(220, 10, as_cmap=True)  # Plot Correlation to Target Variable only def corrMat(df,target='id',figsize=(9,0.5),ret_id=False):          corr_mat = df.corr().round(2)     shape = corr_mat.shape[0]     corr_mat = corr_mat.transpose()     corr = corr_mat.loc[:, df.columns == target].transpose().copy()          if(ret_id):         return corr  ''' Feature Importance ''' # Various Approaches for quick FI evaluation  def fi(ldf,target='id',n_est=25,drop_id=None,target_cat=True):          ldf = ldf.copy()     # If target is categorical string variable     if(target_cat):         cats = ldf[target].unique()         cats_id = [i for i in range(0,len(cats))]         maps = dict(zip(cats,cats_id))             ldf[target] = ldf[target].map(maps)          # If any features are desired to be droped      if(drop_id is not None):         ldf = ldf.drop(drop_id,axis=1)      # Input dataframe containing feature &amp; target variable     y = ldf[target]     X = ldf.drop(target,axis=1)      #   CORRELATION     imp = corrMat(ldf,target,figsize=(15,0.5),ret_id=True)     del imp[target]     s1 = imp.squeeze(axis=0);s1 = abs(s1)     s1.name = 'CORR'      #   SHAP     model = CatBoostRegressor(silent=True,n_estimators=n_est).fit(X,y)     explainer = shap.TreeExplainer(model)     shap_values = explainer.shap_values(X)     shap_sum = np.abs(shap_values).mean(axis=0)     s2 = pd.Series(shap_sum,index=X.columns,name='CAT_SHAP').T      #   CATBOOST     model = CatBoostRegressor(silent=True,n_estimators=n_est).fit(X,y)     fit = model.fit(X,y)     rf_fi = pd.DataFrame(model.feature_importances_,index=X.columns,                                          columns=['CAT'])     rf_fi.sort_values('CAT',ascending=False)     s3 = rf_fi.T.squeeze(axis=0)      #   RANDOMFOREST     model = RandomForestRegressor(n_est,random_state=0, n_jobs=-1)     fit = model.fit(X,y)     rf_fi = pd.DataFrame(model.feature_importances_,index=X.columns,                                          columns=['RF'])     rf_fi.sort_values('RF',ascending=False)     s4 = rf_fi.T.squeeze(axis=0)  #   XGB      model=XGBRegressor(n_estimators=n_est,learning_rate=0.5,verbosity = 0)     model.fit(X,y)     data = model.feature_importances_     s5 = pd.Series(data,index=X.columns,name='XGB').T  #   KBEST     model = SelectKBest(k=5, score_func=f_regression)     fit = model.fit(X,y)     data = fit.scores_     s6 = pd.Series(data,index=X.columns,name='KBEST')      # Combine Scores     df0 = pd.concat([s1,s2,s3,s4,s5,s6],axis=1)     df0.rename(columns={'target':'lin corr'})      # MinMax Scaler     x = df0.values      min_max_scaler = preprocessing.MinMaxScaler()     x_scaled = min_max_scaler.fit_transform(x)     df = pd.DataFrame(x_scaled,index=df0.index,columns=df0.columns)     df = df.rename_axis(f'FI APPROACH', axis=1)     df = df.rename_axis('Feature', axis=0)          pd.options.plotting.backend = \"plotly\"     fig = df.plot(kind='bar',title='SCALED FEATURE IMPORTANCE',                   color_discrete_sequence=px.colors.qualitative.T10)     fig.update_layout(template='plotly_white',height=400,                      font=dict(family='sans-serif',size=12),                      margin=dict(l=60, r=40, t=50, b=10))     fig.update_traces(width=0.85)     fig.show('svg',dpi=300) In\u00a0[39]: Copied! <pre>fi(molten_ohe5,target='id')\n</pre> fi(molten_ohe5,target='id') In\u00a0[40]: Copied! <pre>tmolten_ohe5 = molten_ohe5.iloc[:,0:31]\nprint(tmolten_ohe5.columns)\n\nmodels = []\nmodels.append(('CAT',CatBoostClassifier(silent=True,\n                                        n_estimators=25)))\n\nmodel_eval(tmolten_ohe5,\n           clf=models[0],\n           clf_name='tmodel5',\n           classif_id='multi',\n           show_id=['roc','conf'])\n</pre> tmolten_ohe5 = molten_ohe5.iloc[:,0:31] print(tmolten_ohe5.columns)  models = [] models.append(('CAT',CatBoostClassifier(silent=True,                                         n_estimators=25)))  model_eval(tmolten_ohe5,            clf=models[0],            clf_name='tmodel5',            classif_id='multi',            show_id=['roc','conf']) <pre>Index(['value', 'id', 'linage_Caracal', 'linage_Domestic cat',\n       'linage_Leopard cat', 'linage_Lynx', 'linage_Ocelot', 'linage_Panthera',\n       'linage_Puma', 'species_Black footed cat', 'species_Bobcat ',\n       'species_Canadian lynx', 'species_Caracal', 'species_Cheetah',\n       'species_Clouded leopard', 'species_Domestic cat',\n       'species_Eurasian lynx', 'species_Fishing cat ', 'species_Iberian lynx',\n       'species_Jagaurondi', 'species_Jaguar', 'species_Leopard',\n       'species_Leopard cat', 'species_Lion', 'species_Margay',\n       'species_Ocelot ', 'species_Pallas cat ', 'species_Puma',\n       'species_Snow leopard', 'species_Tigers', 'species_tigrina'],\n      dtype='object')\nTraining Samples: 289\nTest Samples: 125\n</pre> <p>\u276e CONCLUSION START \u276f</p> <ul> <li>Monitoring ovarian function and detecting pregnancy in felids is important for wild felines</li> <li>In this notebook, we focused on creating machine learning models that would classify the ovarian phase based on estradiol &amp; progesterone levels</li> <li>We tried two different approaches (basal/estrus), (basal/nplp/plp) based on serum data, as well as fecal only data as well. These models performed very well, the machine learning model has no problem differentiating between the different classes.</li> <li>Both fecal and serum data were combined to test the above split groups again (basal,nplp,plp), where the models performed perfectly once again</li> <li>However when utilising linage data, we saw a dip in model accuracy, where plp and nplp phases were often missclassified</li> </ul> <p>\u276e CONCLUSION END \u276f</p>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#1-background","title":"1 \u276f BACKGROUND\u00b6","text":""},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#machine-learning-veterinary-science","title":"\u276f\u276f MACHINE LEARNING &amp; VETERINARY SCIENCE\u00b6","text":"<ul> <li>There are definite parallels between veterinary science and human medical science</li> <li>Probably due to funding, veterinary science has tended to lack the same degree of ML,DS &amp; AI incorporation as the human medical field</li> <li>Even on Kaggle, we can note the large number of medical related competition, but veterinary related ones?</li> <li>Despite its absence on Kaggle, there definitely is no shortage of studies that attempt to use tools like a machine learning in order to solve certain problems related to vertinary science, take the study by Schofield et al, just as one good example &amp; a recent review; ML applications in the veterinary field</li> <li>There is also no shortage of experts in the veterinary field (@avma article) that praise and encourage the use of ML/AI in the field, but clinics aren't exactly in a rush to integrate DS &amp; ML engineers, mostly only in places where there is substantial funding.</li> <li>Another obvious problem arises as well; where to get data? There obviously isn't a strong desire to release clinical data publically, yet there is a lot of commonality between the two fields: take ultrasound imaging for example, veterinary related analyses also utilise ultrasound machines. The dataset used in this notebook, is a summation of different journals that from which the data has been nicely assembled at published on @data.mendeley, so that's one place we can access data.</li> <li>Veterinary science &amp; Machine learning is definitely an exciting field, especially if you love animals &amp; just to fit the theme, here we'll be looking at a classification machine learning problem using the CatBoost classifier</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#application-in-reproductology","title":"\u276f\u276f APPLICATION IN REPRODUCTOLOGY\u00b6","text":"<ul> <li>This notebook is about feline reproductology, as a good friend of mine works as one, and hence I wanted to understand the topic a little better for myself</li> <li>The job of reproductologists is mainly to guide &amp; ensure safe delivery of offspring. Felines, just like humans go through similar processes, so that's what we'll look at here.</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#pregnancy-diagnosis","title":"\u276f\u276f PREGNANCY DIAGNOSIS\u00b6","text":"<p>Main methods for pregnancy diagnosis:</p> <ul> <li>Ultrasound imaging (most common method for domestic cats)</li> <li>Longitudinal Endocrine Assessments (progestins, prostaglandins, relaxin) (most common method overall)</li> <li>Fecal Protein Assessments (hormone assessments for non-domestic cats)</li> <li>Vaginal Crytology</li> <li>Laparoscopy</li> </ul> <p>Outlined approaches are relevant to the dataset used in this notebook &amp; we'll actually be looking at ways to combine information from both, and do some classification</p>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#phases-of-an-estrous-cycle","title":"\u276f\u276f PHASES OF AN ESTROUS CYCLE\u00b6","text":"<p>The article Estrous Cycle outlines four main phases of the estrous cycle:</p> <ul> <li>Proestrus</li> <li>Estrus</li> <li>Metestrus/Diestrus</li> <li>Anestrus/Basal</li> </ul> <ul> <li>The dataset includes reference to three of the four stages: Estrus,Diestrus &amp; Basal phases</li> <li>This is quite a general division &amp; indeed in this dataset, diestrus, is further be divided into two phases:</li> </ul> <ul> <li>NPLP (pseudo-pregnancy)</li> <li>PLP (pregnant)</li> <li>Description added in Section 2.2 about the differences between the two</li> </ul> <p>Another reference that outlines the behavioural phases as well | Recreated from Amanda Petersen PhD shows the order of the estrous cycle</p>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#estrous-cycle-related-hormones","title":"\u276f\u276f ESTROUS CYCLE RELATED HORMONES\u00b6","text":"<ul> <li>The dataset makes reference to two hormones; estradiol &amp; progesterone, which are both reproductive hormones</li> <li>Despite the human reproduction related description, their properties shouldn't really change for animals, although the human &amp; animal pregnancy does have its differences</li> <li>Of the two hormones, I am led to believe that measurement of progesterone levels instead of estradiol are quite popular in veterinarian clinics</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#estradiol","title":"ESTRADIOL\u00b6","text":"<p>Estradiol (PubChem NCBI) | Snipplet from Endocrine.org about estradiol in humans:</p> <p>Also called Oestradiol (E2) is the strongest of the three estrogens and an important player in the female reproductive system and the most common type for women of childbearing age. While men and women have estradiol, and it has a role in both of their bodies, women have much higher levels of the hormone than men.</p> <p>Estradiol has several functions in the female body. Its main function is to mature and then maintain the reproductive system. During the menstrual cycle, increased estradiol levels cause the maturation and release of the egg, as well as the thickening of the uterus lining to allow a fertilized egg to implant. The hormone is made primarily in the ovaries, so levels decline as women age and decrease significantly during menopause. In men, proper estradiol levels help with bone maintenance, nitric oxide production, and brain function. While men need lower levels than women, they still require this important hormone to function well.</p>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#progesterone","title":"PROGESTERONE\u00b6","text":"<p>Progesterone (PubChem NCBI) | Snipplet from Endocrine.org about progesterone in humans:</p> <p>Progesterone is a steroid hormone belonging to a class of hormones called progestogens. It is secreted by the corpus luteum, a temporary endocrine gland that the female body produces after ovulation during the second half of the menstrual cycle.</p> <p>Progesterone prepares the endometrium for the potential of pregnancy after ovulation. It triggers the lining to thicken to accept a fertilized egg. It also prohibits the muscle contractions in the uterus that would cause the body to reject an egg. While the body is producing high levels of progesterone, the body will not ovulate. If the woman does not become pregnant, the corpus luteum breaks down, lowering the progesterone levels in the body. This change sparks menstruation. If the body does conceive, progesterone continues to stimulate the body to provide the blood vessels in the endometrium that will feed the growing fetus. The hormone also prepares the limit of the uterus further so it can accept the fertilized egg. Once the placenta develops, it also begins to secrete progesterone, supporting the corpus luteum. This causes the levels to remain elevated throughout the pregnancy, so the body does not produce more eggs. It also helps prepare the breasts for milk production.</p>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#monitoring-hormone-levels","title":"\u276f\u276f MONITORING HORMONE LEVELS\u00b6","text":"<p>Two methods of testing estradiol &amp; progesterone levels in felines are used in the dataset:</p>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#endocrine-monitoring","title":"ENDOCRINE MONITORING\u00b6","text":"<p>Endocrine Tests @Topdoctors.co.uk</p> <p>... In many cases, urine and bloods tests are used to check your hormone levels, in some cases, Imaging tests are done to pinpoint or locate a tumor or other abnormalities that may be affecting the endocrine glands.</p> <ul> <li>More invasive approach than fecal protein assessment, so is commonly used for domesticated felines</li> </ul> <p>Elevated levels during pregnancy</p> <ul> <li>Common &amp; quite straighforward method for diagnosing pregnancy in most mammals;</li> </ul> <ul> <li>detection of elevated circulating progesterone &amp; FPM concentrations</li> </ul> <ul> <li>In felids, circulating progesterone &amp; FPM are highly elevated during pregnancy, although peak concentrations vary significantly between different species.</li> </ul> <p>NPLP &amp; PLP Distinguishibility Difficulty</p> <ul> <li>The use of progesterone or FPM concentrations for detecting pregnancy is possible but complicated by the potential for prolonged non-pregnant luteal phases (NPLP)</li> <li>Progesterone levels are similar during both NPLP and pregnant luteal phases (PLP)</li> </ul> <ul> <li>NPLP are approx half the duration of PLP for most felids, thus pregnancy can be confirmed by detecting elevated progesterone or FPM concentrations during the later half of gestation</li> <li>Neither circulating progesterone or FPM concentrations can be used for pregnancy detection in Lynx:</li> </ul> <ul> <li>Progesterone based assessments further complicated by:</li> </ul> <ul> <li>The potential for temporary  mid-gestational decreases in progesterone concentrations, which are thought to be associated with a switch from luteal to placemental progestone productions, leading to false-negative pregnancy diagnoses</li> </ul> <ul> <li>Due to the fact that they exhibit an abnormally long NPLP and PLP, with CL persisting for at least two years</li> </ul> <ul> <li>Meaning that NPLP &amp; PLP cannot be distinguished using progesterone or FPM assessments alone</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#fecal-protein-assessments","title":"FECAL PROTEIN ASSESSMENTS\u00b6","text":"<ul> <li>In felids, estradiol &amp; progesterone metabolities are almost exclusively excreted through feces</li> <li>FEM, FPM are indirect and noninvasive means of monitoring blood concentrations of these hormones</li> <li>Fecal estrogens and progestins tend to fluctuate less than circulating estradiol &amp; progesterone</li> <li>However extraction of progesterone and estradiol is much slower compared to endocrine tests, nevertheless it's still a method commonly used for wild felines</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#machine-learning-application","title":"\u276f\u276f MACHINE LEARNING APPLICATION\u00b6","text":"<p>The Complex Nature of Hormonal Changes</p> <ul> <li>Female felines, just like women go through maternal cycles, during which the body adapts to changes &amp; hormone levels constantly fluctuate</li> <li>The idea here is to use hormone measurement data that have already been classified by humans, and build a unified model that can classify at which phase of the cycle the feline of interest is at the moment of testing, whatever the testing method.</li> <li>This in itself is a very complex task, as felines can have very different homonal backgrounds/profiles, and not to even mention the subtle variation of changes that can occur as a result of attempting to obtain the samples in the first place (when doing evasive type testing)</li> <li>As mentioned above, there is also difficulty in distinguishing between NPLP and PLP phases, which both occur during the diestrus phase:</li> </ul> <ul> <li>NPLP, as the name implies is not at a pregnant phase yet,</li> <li>Wheras as PLP is as the name suggest the pregnant luteal phase</li> </ul> <p>A Classification Problem</p> <ul> <li>The article, Monitoring ovarian function and detecting pregnancy in felids: a review, from which this dataset was obtained, focused on the need to develop methodologies for monitoring of estrus and pregnancy in felines, mostly due to the need for noninvasive testing methods for wild felines</li> <li>This isn't the aim of this notebook, but we can utilise the data for the purposes of applying machine learning methods to an interesting application; classification of estrous cycle based on feline hormonal data (Estradiol &amp; Progesterone) or just ovarian phase classification</li> </ul> <p>Notebook Aim</p> <ul> <li>So the aim is to build a classification model that will unify different testing approaches (fem,fpm,serum) and be able to classify correctly based on either progesterone or estradiol levels, in whichever combination (min,max,mean), at which phase of the estrous cycle the feline is at the moment of testing</li> <li>The reason a unified method may be of interest because: <ul> <li>Insufficient data can result in inconsistent models each time it's trained</li> <li>There should be a correlation between methods (definitely serum &amp; fecal testing)</li> <li>Any new methodology &amp; subsequent feature extraction can complement one another, taking in more information about the entire process</li> </ul> </li> </ul> <p>Purpose of such a model</p> <ul> <li>Such a model can help veterinarians quite rapidly confirm their own diagnosis (through whatever method they were taught to use) or at least be tool to question their own diagnosis.</li> <li>This particular application may not be the most practical as I don't work in the industry, but it serves the purpose outlined below:</li> </ul> <ul> <li>In essence, we'll aim to show that machine learning models can capture tendencies &amp; relations in data that can be hard to notice to the naked eye, which is why their use can help solve many problems in veterinary science, this is one of many possible applications.</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#related-links","title":"\u276f\u276f RELATED LINKS\u00b6","text":"<ul> <li>Female Reproduction @NCBI</li> <li>Proceedings of the British Society of Animal Science</li> <li>A Review of Machine Learning Applications in Veterinary Field</li> <li>Machine-learning based prediction of Cushing\u2019s syndrome in dogs attending UK primary-care veterinary practice</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#2-dataset-features","title":"2 \u276f DATASET FEATURES\u00b6","text":""},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#list-of-features","title":"\u276f\u276f LIST OF FEATURES\u00b6","text":"<p>The features are divided into two main groups / four subgroups of results:</p> <ul> <li><p>0-4 : Plasma or serum concentrations</p> <ul> <li>0-1 : Circulating Estradiol (pg/ml)<ul> <li>0 : Anoestrus/Interestrus Basal</li> <li>1 : Estrus (Peak)</li> </ul> </li> <li>2-4 : Circulating Progesterone (ng/ml)<ul> <li>2 : Basal (i.e. not diestrus)</li> <li>3 : Diestrus/luteal phase (Peak) | Non-pregnant luteal phase NPLP</li> <li>4 : Diestrus/luteal phase (Peak) | Pregnant luteal phase PLP</li> </ul> </li> </ul> </li> <li><p>5-9 : Fecal metabolites</p> <ul> <li>5-6 : Fecal estradiol metabolites (FEM) (ng/g)<ul> <li>5 : Anoestrus or interestrus Basal</li> <li>6 : Estrus (Peak)</li> </ul> </li> <li>7-9 : Fecal progesterone metabolites (FPM) (\u00b5g/g)<ul> <li>7 : Basal (i.e. not diestrus)</li> <li>8 : Diestrus/luteal phase (Peak) | Non-pregnant luteal phase NPLP</li> <li>9 : Diestrus/luteal phase (Peak) | Pregnant luteal phase PLP</li> </ul> </li> </ul> </li> <li><p>For each feature we have 5 features: <code>min</code>, <code>max</code>, <code>mean</code>, <code>number of animals (n)</code> &amp; <code>number of samples (ns)</code></p> </li> <li><p>So from our data we are dealing with sets of results of either estradiol or progesterone levels of <code>ns</code> animals, from whom the authors of the individual sets of data sampled <code>n</code> times</p> </li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#additional-description","title":"\u276f\u276f ADDITIONAL DESCRIPTION\u00b6","text":"<p>Some extracts from references that are relevant to this data:</p> <p>Luteal Phase</p> <ul> <li>The luteal phase plays an important role in early pregnancy, as it's the time when the womb prepares for the implantation of a fertilized egg</li> <li>The luteal phase lasts from the day after ovulation until the day before your period starts</li> </ul> <p>NPLP &amp; PLP Difference - Jilian M. Fazio PhD</p> <ul> <li>A non-pregnant luteal phase (NPLP) was defined as a rise in progestogens 2.0 SD above baseline for at least fourteen days starting from the first to the last dates above baseline. The end of the luteal phase was defined as a return to baseline for at least six days</li> <li>A pregnant luteal phase (PLP) was an elevation of progestogens 2.0 SD above baseline for greater than or equal to fourteen days that resulted in live or stillbirth</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#3-data-exploration","title":"3 \u276f DATA EXPLORATION\u00b6","text":""},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#read-data","title":"\u276f\u276f READ DATA\u00b6","text":"<ul> <li>The feline pregnancy dataset contains column data without explicitly stating what the features are, so having defined them in (Section 2.1), we can generate short names, so it will be more clear what they actually are as we are working with them.</li> <li>Some minor adjustments are also made to replace certain characters as shown below &amp; in some cases \"&lt;1\" is used, for this case it's assumed to be 0.5.</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#missing-data","title":"\u276f\u276f MISSING DATA\u00b6","text":"<ul> <li>This dataset contains measurement data of estradiol &amp; progesterone for both serum &amp; fecal, however the entries are quite highly inconsistent and often only contain one or two readings from the combinations (min,max,mean), as mentioned previously.</li> <li>As a result, we'll be needing to meld the data back into the specific phase group (basal,estrus,nplp &amp; plp), in order to have enough data for creating a model for phase prediction, since that is ultimately our goal in this notebook</li> <li>As we can see from the data below, serum based data sampling tends to have the highest amount of missing data, as opposed to fecal measurements, for which we have much more consistent data.</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#sample-sizes-species","title":"\u276f\u276f SAMPLE SIZES &amp; SPECIES\u00b6","text":"<ul> <li>The data we are dealing with from individual studies provide control group statistic, rather than individual samples (unless of course ns=1, which do exist in the dataset), so what our features indicate are pretty much bounds for each linage &amp; specie combination &amp; and we can extract the individual samples to some extent (min,max cases for certain)</li> <li>Our mean, min &amp; max will have been influenced by the number of samples and animals in each study, however as we don't have individual samples, there seems to be little use for this information</li> <li>Visually, we can see/confirm below that there is quite a substatial variation among different specie &amp; linage combinations</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#feline-specie-distribution","title":"\u276f\u276f FELINE SPECIE DISTRIBUTION\u00b6","text":"<ul> <li>The dataset contains estradiol &amp; progesterone for not only domesticated felines, but also for non-domesticated cats such as Pantheras, Pumas, Ocelots, Leopard, Lynx, Caracals &amp; Bay Cats</li> <li>So despite the large portion of domestic cats, most of the dataset contains data for wild felines, as shown in the graph below</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#estradiol-level-distribution","title":"\u276f\u276f ESTRADIOL LEVEL DISTRIBUTION\u00b6","text":"<ul> <li>Inspecting the estradiol levels for different linage we can note some quite clear tendencies for the mean values</li> </ul> <ul> <li>All felines have increased estradiol levels during estrus, which was expected, the only variation that exists is that for different species the levels were quite different</li> <li>The same tendencies were observed for both serum and fecal data subsets</li> </ul> <ul> <li>The levels for both maximum &amp; minimum levels of estradiol:</li> </ul> <ul> <li>Followed similar trends, however in very few cases, there have been dips in values during estrus as seen in the leopard data</li> <li>Such abnormalities of course can catch the model off guard, as the entire dataset follows an increasing trend</li> </ul> <ul> <li>Another observation can be made about outliers:</li> </ul> <p>There are very cases that fall outside the minimum &amp; upper fence levels, as can be seen some levels of domesticated felines are abnormally high</p>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#pregesterone-level-distribution","title":"\u276f\u276f PREGESTERONE LEVEL DISTRIBUTION\u00b6","text":"<ul> <li>Next we'll look at the progesterone levels, for this hormone subset we have three phases that were recorded; basal &amp; diestrus (nplp/plp), so estrus was not included</li> <li>Like elevated estradiol levels during estrus, progesterone levels are also elevated in post basal phases (nplp,plp)</li> <li>Unlike estradiol, here we have quite a bit more variety when it comes to plp &amp; nplp levels, for different linage variations, looking at the mean values:</li> </ul> <ul> <li>Domesticated felines tend to have slightly higher levels of progasterone during nplp than plp in serum</li> <li>Lynx on the other hand tend to have higher progasterone levels during plp in serum (so the other way round)</li> <li>Panthera tend to follow the same trend as domesticated felines</li> </ul> <p>Fecal data provided some insight into some other linage:</p> <ul> <li>leopard cats, similar to Lynx have tendencies of having larger plp values comared to nplp, Puma also follow this trend</li> <li>It can be noted that the these trend variations are very subtle, both plp &amp; nplp are not too different</li> <li>This seems to suggest that there can be some difficult to distinguish between these two phases if we were to just look at progesterone concentrations alone</li> </ul> <ul> <li>We can also note that we have clear gaps in data for leopard, caracal &amp; bay cat when it comes to serum data, only fecal data was collected</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#4-modeling","title":"4 \u276f MODELING\u00b6","text":""},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#rearranging-data","title":"\u276f\u276f REARRANGING DATA\u00b6","text":"<p>Compiling data from different data sources:</p> <ul> <li>We are left with a lot of variation in the recorded data, thus have lot's of NaN as available data is not consistent since they are taken from different sources</li> <li>We can have any combination of (min,max,mean), but what we are measuring is the same content; estrus or progasterone, from serum or feces.</li> <li>Thus we can simply meld our data &amp; divide the data based on the phase it was allocated into; basal or estrus ..., the actual phase is in the column name.</li> <li>The rearranged data is then simply one-hot encoded, so it's not relevant which measurement (min,max,mean) was recorded</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#list-of-models","title":"\u276f\u276f LIST OF MODELS\u00b6","text":"<ul> <li>As mentioned in Section 2.1, we have two main groups (serum,fecal) &amp; 4 subgroups (serum-e,serum-p,fecal-e,fecal-p) in total</li> <li>We'll first look at making models for each subgroup of data &amp; then make combined models of subgroups</li> </ul> <p>Subgroup Models</p> <ul> <li>First model should be straightforward; binary classification between basal &amp; estrus phases using serum estrous data</li> <li>Second model should be more challenging; multiclass classification between basal &amp; diestrus phases (nplp,plp) using serum progasterone data</li> <li>Third model should be straightforward; binary classification between basal &amp; estrus phases using fecal estrous data this time</li> <li>Fourth model; multiclass classification between basal &amp; diestrus phases (nplp,plp) again, but this time using fecal progasterone data as well</li> </ul> <p>Grouped Models</p> <ul> <li>The fifth model; multiclass classification between all available basal &amp; diestrus(nplp,plp) phase progasterone data</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#evaluation-function","title":"\u276f\u276f EVALUATION FUNCTION\u00b6","text":""},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#model_eval","title":"model_eval\u00b6","text":"<ul> <li>As with most of my other notebooks, I like to create a unified evaluation function that can be reused in different problems, but opted with a function for the time being</li> <li><code>model_eval</code> can be used to evaluate the model on a train/test splitting approach, a little more simpler than for eg. kfold cv (cv is of course important for any ml problem), which I used in another notebook</li> <li>We'll add kfold cross validation for the grouped models as it is a little more viable to split the data into more than two groups &amp; it is definitely desirable to cross validate the models.</li> </ul> <p><code>model_eval</code> function parameters:</p> <ul> <li><code>data</code> : Feature Matrix &amp; Target Variable DataFrame</li> <li><code>ts</code> : Train/Test split ratio</li> <li><code>target</code> : Target Feature in the DataFrame</li> <li><code>clf</code> : Classifier</li> <li><code>clf</code> : Evaluation case name (for unique model save)</li> <li><code>classif_id</code> : Classification Type (could just have automated it)</li> <li><code>show_id</code> : Evaluation options; (conf,roc)</li> </ul> <p>Evaluation Metric</p> <ul> <li>We'll be using very standard metrics for both binary and multiclass classification; The confusion matrix and ROC curves</li> <li>Plotly is of course my prefered way of plotting things &amp; they recently updated their plot library to include ROC &amp; PR Curves, which is quite a helpful reference for ML beginners</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#subset-models","title":"\u276f\u276f SUBSET MODELS\u00b6","text":""},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#serum-model-1-basal-estrus-classification","title":"\u276f\u276f\u276f SERUM MODEL 1 : BASAL | ESTRUS CLASSIFICATION\u00b6","text":"<p>For the first model:</p> <ul> <li>Using the serum subset to build a binary classifier</li> <li>We'll be using the estradiol data in our first model; data from circulating blood</li> <li>We want to create a model that can classify, for the limited features available, whether the feline is in the basal or estrust phase of the estrous cycle</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#summary-model-1","title":"SUMMARY : MODEL 1\u00b6","text":"<ul> <li>Well, it's quite conclusive that the model can very easily distinguish between the two phases very easily for different kind of cat species basal,estrus) from the serum subset</li> <li>Given enough data, a human can quite easily distinguish between the two phases, as the elevated estradiol levels were clearly visible during the estrus phase (Section 3)</li> <li>So creating a model to distinguish between basal &amp; estrus phases is not really necessary and more importantly, there are other phases of an estrous cycle that need to be taken into account, as estradiol levels are also elevated during these phases and if we didn't have an expert define that these elevated levels were during the estrus phase, we could easily have mistaken them for diestrus and so on.</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#serum-model-2-basal-nplp-plp-classification","title":"\u276f\u276f\u276f SERUM  MODEL 2: BASAL | NPLP | PLP CLASSIFICATION\u00b6","text":"<p>For the second model:</p> <ul> <li>Using serum data again, but this time we turn our attention to prograsterone concentrations in circulating blood.</li> <li>For the second model, we'll be using the prograsterone data obtained from serum &amp; create a model that can classify between three states this time (basal, nplp &amp; plp)</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#summary-model-2","title":"SUMMARY : MODEL 2\u00b6","text":"<ul> <li>From the results we can see that the CatBoost model can quite easily distinguish between the three phases (basal,nplp &amp; plp), as seen in the confusion matrix for both training &amp; test sets, there are no false positives, which is very encouraging for the multiclass classifier.</li> <li>Whilst it may seem that using ohe for each combination may be too farfetched which boosts the models performance, it worth mentioning that it's not really understood why there is such a large variation of inconsistencies in data entries among all the data sources. Nevertheless, it's probably worth keeping these options (min,mean,max) as we might have obtain new data in a similar format</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#fecal-model-3-basal-estrus-classification","title":"\u276f\u276f\u276f FECAL MODEL 3 : BASAL | ESTRUS CLASSIFICATION\u00b6","text":"<p>For the third model:</p> <ul> <li>We'll be using only the fecal data this time for estradiol measurements (fem)</li> <li>As with model 1, we'll be making a binary classifier to distinguish between basal &amp; estrus phases</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#summary-model-3","title":"SUMMARY : MODEL 3\u00b6","text":"<ul> <li>As with model 1, it's quite straighforward for the model to distringuish between both classes, which was expected as a human can do this as well</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#fecal-model-4-basal-nplp-plp-classification","title":"\u276f\u276f\u276f FECAL  MODEL 4: BASAL | NPLP | PLP CLASSIFICATION\u00b6","text":"<p>For the fourth model:</p> <ul> <li>Using fecal data this time, we turn our attention to prograsterone (fem) concentrations extracted from fecies</li> <li>As with the second model we want to create a model that can classify between three states this time (basal, nplp &amp; plp)</li> <li>As was seen in the serum model, the subtle difference in progasterone levels can cause some issues for the model</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#summary-model-4","title":"SUMMARY : MODEL 4\u00b6","text":"<ul> <li>Similar to model 2, the fecal model performs quite well, being able to distinguish between nplp and plp phases quite well </li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#unified-models","title":"\u276f\u276f UNIFIED MODELS\u00b6","text":""},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#unified-model-5-basal-nplp-plp-classification","title":"\u276f\u276f\u276f UNIFIED  MODEL 5: BASAL | NPLP | PLP CLASSIFICATION\u00b6","text":""},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#main-model","title":"MAIN MODEL \u00b6","text":"<p>For the fifth model:</p> <ul> <li>We'll combine both serum and fecal data, this leads to 414 rows of data, which is quite a bit more data</li> <li>As noted noted in Section 2.1 and we saw from the plots in Section 3.5 &amp; 3.6, the units are of course different and feature values differ quite substantially</li> <li>So it is more ideal to standardise our data, something you can try yourself</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#instant-relative-feature-importance","title":"INSTANT RELATIVE FEATURE IMPORTANCE \u00b6","text":"<ul> <li>We can look at the Feature Importance (FI) of certain trained models to understand which features &amp; to what extent.</li> <li>We can use such minimalistic functions to quicky evaluat feature importance by relying on variation of approaches &amp; optimised libraries.</li> <li>We can obtain relative feature importance using different libraries , function <code>feature_importance</code> includes:</li> </ul> <ul> <li>Linear Correlation w/ abs() function.</li> <li>SHAP Values of Catboost Regression Model (n_est)</li> <li>RandomForest Regressor (n_est)</li> <li>XGBoost Regressor (n_est)</li> <li>CatBoost Regressor (n_est)</li> <li>SelectKBest (k)</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#post-model-adjustment","title":"POST MODEL ADJUSTMENT \u00b6","text":"<ul> <li>The indivual scores are combined and scaled using <code>MinMaxScaler()</code> &amp; Plot.</li> <li>The y-axis represents the total score (higher score is better, max -&gt; Number of approaches).</li> <li>The x-axis represents the corresponding features of input dataframe.</li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#species-linage-model","title":"SPECIES-LINAGE MODEL \u00b6","text":"<ul> <li><p>If we rely only on using the linage, species &amp; progasterone values itself, the accuracy of the model drops quite significantly</p> </li> <li><p>The model has difficulty correctly classifying between nplp &amp; plp phases as can be seen by the multiclass ROC curves, as well as the confusion matrix</p> </li> <li><p>So adding features relating to fecal or serum division data, as well as whether it is a mean, the maximum case or minimum case improves the model accuracy quite substantially because it provides an accurate picture of the data distribution in the multidimensional data, which is expected as CatBoost is quite an advanced model</p> </li> </ul>"},{"location":"portfolio/kaggle/ovarian-phase-classification-in-felids.html#summary-model-5","title":"SUMMARY : MODEL 5\u00b6","text":"<ul> <li>Having combined two approaches that measure the levels of progesterone in both serum and fecies, we can note that the model can still classify quite well, which is quite nice, there are very few missclassified cases &amp; the difference between nplp and plp phases is distinguished quite well</li> <li>On the other hand, the model doesn't seem to put emphasis on the variation in both species &amp; linage, which is a shame, because in essence, it should be a key factor in the the variation of progesterone levels</li> <li>The model obviously learns to recognise the mean, max &amp; min patterns as well as the variation between serum &amp; fecal data instead to construct an accurate model, so if we don't include this data, the accuracy of the model drop significantly, as the species-linear model demonstrated     </li> </ul>"},{"location":"portfolio/kaggle/testing.html","title":"Testing","text":"1 | BACKGROUND 2 | LOADING DATA <p>Lets finally load and preview our dataset, looking at only the essetial information:</p> <p>We have three sources of data:</p> <ul> <li><code>authors</code> : Information about each author</li> <li><code>blog</code> : Information about the blog post</li> <li><code>ratings</code> : Blog post ratings set by <code>medium</code> users (<code>user_id</code>), no other information is parsed about each user</li> </ul> In\u00a0[1]: Copied! <pre>import pandas as pd \nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n</pre> import pandas as pd  import numpy as np import seaborn as sns import matplotlib.pyplot as plt from collections import Counter <p>Load blog article information (unique blog information)</p> In\u00a0[2]: Copied! <pre>blog = pd.read_csv('blog.csv')\nblog = blog.drop(['blog_img','blog_link'],axis=1)\nblog = blog.rename(columns={'blog_content':'blog_preview'})\nblog.head()\n</pre> blog = pd.read_csv('blog.csv') blog = blog.drop(['blog_img','blog_link'],axis=1) blog = blog.rename(columns={'blog_content':'blog_preview'}) blog.head() Out[2]: blog_id author_id blog_title blog_preview topic scrape_time 0 1 4 Let\u2019s Dominate The Launchpad Space Again Hello, fam! If you\u2019ve been with us since 2021,... ai 2023-02-27 07:37:48 1 3 4 Let\u2019s Dominate The Launchpad Space Again Hello, fam! If you\u2019ve been with us since 2021,... ai 2023-02-27 07:41:47 2 4 7 Using ChatGPT for User Research Applying AI to 4 common user research activiti... ai 2023-02-27 07:41:47 3 5 8 The Automated Stable-Diffusion Checkpoint Merg... Checkpoint merging is powerful. The power of c... ai 2023-02-27 07:41:47 4 6 9 The Art of Lazy Creativity: My Experience Co-W... I was feeling particularly lazy one day and co... ai 2023-02-27 07:41:47 <p>Load blog article author information (all unique authors)</p> In\u00a0[3]: Copied! <pre>authors = pd.read_csv('authors.csv')\nauthors.head()\n</pre> authors = pd.read_csv('authors.csv') authors.head() Out[3]: author_id author_name 0 1 yaksh 1 2 XIT 2 3 Daniel Meyer 3 4 Seedify Fund 4 5 Ifedolapo Shiloh Olotu <p>All ratings set by <code>user_id</code> on <code>blog_id</code></p> In\u00a0[4]: Copied! <pre>ratings = pd.read_csv('ratings.csv')\nratings = ratings.rename(columns={'userId':'user_id'})\nratings.head()\n</pre> ratings = pd.read_csv('ratings.csv') ratings = ratings.rename(columns={'userId':'user_id'}) ratings.head() Out[4]: blog_id user_id ratings 0 9025 11 3.5 1 9320 11 5.0 2 9246 11 3.5 3 9431 11 5.0 4 875 11 2.0 <p>Merge user ratings with the blog information</p> In\u00a0[6]: Copied! <pre>\"\"\"\n\nAll blog article user rating data\n\n\"\"\"\n\nblog_ratings = ratings.merge(blog,on='blog_id',how='left')\nblog_ratings = blog_ratings.merge(authors,on='author_id',how='left')\nblog_ratings.head(3)\n</pre> \"\"\"  All blog article user rating data  \"\"\"  blog_ratings = ratings.merge(blog,on='blog_id',how='left') blog_ratings = blog_ratings.merge(authors,on='author_id',how='left') blog_ratings.head(3) Out[6]: blog_id user_id ratings author_id blog_title blog_preview topic scrape_time author_name 0 9025 11 3.5 5960 How I became a Frontend Developer A little bit of background about me: as a teen... web-development 2023-04-04 08:53:52 Steven Dornan 1 9320 11 5.0 6155 Writing an Algorithm to Calculate Article Read... You have probably noticed a read-time number u... web-development 2023-04-04 08:53:52 Daniel Pericich 2 9246 11 3.5 6114 Diving into HTML and the Tools of the Trade It\u2019s been an incredible first week as a Bytewi... web-development 2023-04-04 08:53:52 Muhammad Naeem Tahir In\u00a0[7]: Copied! <pre>blogs = blog.merge(authors,on='author_id',how='left')\nblogs.head(3)\n</pre> blogs = blog.merge(authors,on='author_id',how='left') blogs.head(3) Out[7]: blog_id author_id blog_title blog_preview topic scrape_time author_name 0 1 4 Let\u2019s Dominate The Launchpad Space Again Hello, fam! If you\u2019ve been with us since 2021,... ai 2023-02-27 07:37:48 Seedify Fund 1 3 4 Let\u2019s Dominate The Launchpad Space Again Hello, fam! If you\u2019ve been with us since 2021,... ai 2023-02-27 07:41:47 Seedify Fund 2 4 7 Using ChatGPT for User Research Applying AI to 4 common user research activiti... ai 2023-02-27 07:41:47 Nick Babich 3 | DATA EXPLORATION In\u00a0[25]: Copied! <pre># user blog ratings\nrating_ordered = ratings.groupby('blog_id').count()['ratings'].sort_values(ascending=False).reset_index()\n\nplt.figure(figsize=(10,4))\nax = rating_ordered['ratings'].hist(bins=60)\n\nax.grid(color='gray',ls='--',lw=1,alpha=0.2)\nsns.despine(top=True,right=True,left=True)\nplt.tick_params(axis='both',colors='#2E2E2E')\nplt.xlabel('Number of blog ratings',color='#2E2E2E')\nplt.ylabel('Frequency',color='#2E2E2E')\nplt.tight_layout()\nplt.show()\n</pre> # user blog ratings rating_ordered = ratings.groupby('blog_id').count()['ratings'].sort_values(ascending=False).reset_index()  plt.figure(figsize=(10,4)) ax = rating_ordered['ratings'].hist(bins=60)  ax.grid(color='gray',ls='--',lw=1,alpha=0.2) sns.despine(top=True,right=True,left=True) plt.tick_params(axis='both',colors='#2E2E2E') plt.xlabel('Number of blog ratings',color='#2E2E2E') plt.ylabel('Frequency',color='#2E2E2E') plt.tight_layout() plt.show() In\u00a0[18]: Copied! <pre>count = ratings['ratings'].value_counts().round(4).sort_index()\nperc = ratings['ratings'].value_counts(normalize=True).round(4).sort_index()*100\nrating_info = pd.concat([count,perc],axis=1)\nrating_info.columns = ['count','percentage']\nrating_info\n</pre> count = ratings['ratings'].value_counts().round(4).sort_index() perc = ratings['ratings'].value_counts(normalize=True).round(4).sort_index()*100 rating_info = pd.concat([count,perc],axis=1) rating_info.columns = ['count','percentage'] rating_info Out[18]: count percentage ratings 0.5 40414 20.19 2.0 49826 24.90 3.5 30286 15.13 5.0 79614 39.78 In\u00a0[20]: Copied! <pre># user based rating statistics\nblog_rating_stats = blog_ratings.groupby('user_id').agg(counts=('ratings','count'),\n                                                        average=('ratings','mean'))\nblog_rating_stats.hist(figsize=(10,3),bins=100)\n</pre> # user based rating statistics blog_rating_stats = blog_ratings.groupby('user_id').agg(counts=('ratings','count'),                                                         average=('ratings','mean')) blog_rating_stats.hist(figsize=(10,3),bins=100) Out[20]: <pre>array([[&lt;Axes: title={'center': 'counts'}&gt;,\n        &lt;Axes: title={'center': 'average'}&gt;]], dtype=object)</pre> In\u00a0[26]: Copied! <pre>blogs = blogs.join(blog_ratings.groupby('blog_id').agg(nratings=('ratings','count')))\nblogs = blogs.join(blog_ratings.groupby('blog_id').agg(rating=('ratings','mean')).round(2))\n\nblogs['nratings'] = blogs['nratings'].fillna(0)\nblogs['nratings'] = blogs['nratings'].astype('int')\n</pre> blogs = blogs.join(blog_ratings.groupby('blog_id').agg(nratings=('ratings','count'))) blogs = blogs.join(blog_ratings.groupby('blog_id').agg(rating=('ratings','mean')).round(2))  blogs['nratings'] = blogs['nratings'].fillna(0) blogs['nratings'] = blogs['nratings'].astype('int') In\u00a0[27]: Copied! <pre>blogs.loc[(blogs['nratings'] == 0),'type'] = 'undiscovered'\nblogs.loc[(blogs['rating'] &gt;= 4),'type'] = 'excellent'\nblogs.loc[(blogs['rating'] &lt;= 2.5),'type'] = 'poor'\nblogs.loc[(blogs['rating'] &gt; 2.5) &amp; (blogs['rating'] &lt; 3.5) ,'type'] = 'average'\nblogs.loc[(blogs['rating'] &gt; 3.5) &amp; (blogs['rating'] &lt; 4) ,'type'] = 'good'\nsns.histplot(blogs,x='rating',hue='type')\n</pre> blogs.loc[(blogs['nratings'] == 0),'type'] = 'undiscovered' blogs.loc[(blogs['rating'] &gt;= 4),'type'] = 'excellent' blogs.loc[(blogs['rating'] &lt;= 2.5),'type'] = 'poor' blogs.loc[(blogs['rating'] &gt; 2.5) &amp; (blogs['rating'] &lt; 3.5) ,'type'] = 'average' blogs.loc[(blogs['rating'] &gt; 3.5) &amp; (blogs['rating'] &lt; 4) ,'type'] = 'good' sns.histplot(blogs,x='rating',hue='type') Out[27]: <pre>&lt;Axes: xlabel='rating', ylabel='Count'&gt;</pre> In\u00a0[28]: Copied! <pre>blogs.groupby('type')['author_name'].count()\n</pre> blogs.groupby('type')['author_name'].count() Out[28]: <pre>type\naverage         6732\nexcellent        316\ngood            1172\npoor             903\nundiscovered     761\nName: author_name, dtype: int64</pre> <ul> <li>We can see that not all <code>topic</code> have articles in <code>excellent</code> group</li> <li>We should combine thr groups <code>good</code> and <code>excellent</code> into one group and use them as top articles to read</li> </ul> In\u00a0[29]: Copied! <pre>print(blogs.groupby(['topic','type'])['author_name'].count().to_string())\n</pre> print(blogs.groupby(['topic','type'])['author_name'].count().to_string()) <pre>topic                 type        \nCryptocurrency        average         250\n                      excellent         7\n                      good             38\n                      poor             23\n                      undiscovered      1\nSoftware-Development  average         272\n                      good             21\n                      poor              5\n                      undiscovered      1\nai                    average         383\n                      excellent        60\n                      good             94\n                      poor            119\n                      undiscovered      6\nandroid               average         361\n                      excellent         2\n                      good             38\n                      poor             14\n                      undiscovered      1\napp-development       average         281\n                      good             29\n                      poor              3\n                      undiscovered      1\nbackend               average         197\n                      good             18\n                      poor              4\n                      undiscovered    118\nbackend-development   average         191\n                      good             19\n                      poor              3\n                      undiscovered     92\nblockchain            average         381\n                      excellent        31\n                      good             85\n                      poor             98\n                      undiscovered      1\ncloud-computing       average         308\n                      excellent        38\n                      good             77\n                      poor             95\n                      undiscovered      1\ncloud-services        average         193\n                      excellent         6\n                      good             44\n                      poor             31\n                      undiscovered     45\ncybersecurity         average         393\n                      excellent        36\n                      good             77\n                      poor             78\n                      undiscovered      8\ndata-analysis         average         361\n                      excellent        28\n                      good             98\n                      poor             77\n                      undiscovered      1\ndata-science          average         316\n                      excellent         9\n                      good             47\n                      poor             46\n                      undiscovered      2\ndeep-learning         average         291\n                      excellent        10\n                      good             68\n                      poor             28\n                      undiscovered      2\ndev-ops               average         256\n                      excellent        13\n                      good             57\n                      poor             42\n                      undiscovered      1\nflutter               average         295\n                      good             30\n                      poor              3\n                      undiscovered     10\nimage-processing      average         238\n                      excellent        12\n                      good             38\n                      poor             25\n                      undiscovered     30\ninformation-security  average         254\n                      excellent        10\n                      good             39\n                      poor             26\n                      undiscovered     30\nmachine-learning      average         299\n                      excellent        19\n                      good             67\n                      poor             47\nnlp                   average         195\n                      excellent         2\n                      good             27\n                      poor             15\n                      undiscovered    194\nsecurity              average         341\n                      excellent        15\n                      good             66\n                      poor             64\n                      undiscovered      4\nweb-development       average         355\n                      excellent         1\n                      good             40\n                      poor             18\n                      undiscovered    210\nweb3                  average         321\n                      excellent        17\n                      good             55\n                      poor             39\n                      undiscovered      2\n</pre> 4 | RECOMMENDATIONS FOR EXISTING USERS <p>First, we will start creating recommendations for existing users. Having a rating system evaluation for registered users only, users are quite likely to be familiar with how things work on the website, and so will probably tend to ingore posts with lower ratigs.</p> In\u00a0[98]: Copied! <pre>recommended_posts = blogs[blogs['type'].isin(['good','excellent'])]\nprint(recommended_posts.shape[0],'articles to recommend')\n\n# indicies of posts which are \"good\", \"excellent\" in blogs\nidx_recommended_posts = recommended_posts.blog_id\n</pre> recommended_posts = blogs[blogs['type'].isin(['good','excellent'])] print(recommended_posts.shape[0],'articles to recommend')  # indicies of posts which are \"good\", \"excellent\" in blogs idx_recommended_posts = recommended_posts.blog_id <pre>1488 articles to recommend\n</pre> In\u00a0[99]: Copied! <pre># recommended blog article ratings\ndata = blog_ratings[blog_ratings.blog_id.isin(idx_recommended_posts)]\n</pre> # recommended blog article ratings data = blog_ratings[blog_ratings.blog_id.isin(idx_recommended_posts)] <p><code>user_id</code> and <code>blog_id</code> contain identifiers that are somewhat out of order, lets use ordered identifiers, utilising mappers</p> In\u00a0[100]: Copied! <pre># import warnings; warnings.filterwarnings('ignore')\n\nall_users = data['user_id'].unique().tolist()\nall_blogs = data['blog_id'].unique().tolist()\n\nn_users = len(all_users); n_items = len(all_blogs)\nuser_id2idx = dict(zip(all_users,range(n_users)))\nitem_id2idx = dict(zip(all_blogs,range(n_items)))\nidx2user_id = {j:i for i,j in user_id2idx.items()}\nidx2item_id = {j:i for i,j in item_id2idx.items()}\n\ndata['user_id'] = data['user_id'].map(user_id2idx)\ndata['blog_id'] = data['blog_id'].map(item_id2idx)\ndata.shape\n</pre> # import warnings; warnings.filterwarnings('ignore')  all_users = data['user_id'].unique().tolist() all_blogs = data['blog_id'].unique().tolist()  n_users = len(all_users); n_items = len(all_blogs) user_id2idx = dict(zip(all_users,range(n_users))) item_id2idx = dict(zip(all_blogs,range(n_items))) idx2user_id = {j:i for i,j in user_id2idx.items()} idx2item_id = {j:i for i,j in item_id2idx.items()}  data['user_id'] = data['user_id'].map(user_id2idx) data['blog_id'] = data['blog_id'].map(item_id2idx) data.shape <pre>&lt;ipython-input-100-58dcb817c81f&gt;:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data['user_id'] = data['user_id'].map(user_id2idx)\n&lt;ipython-input-100-58dcb817c81f&gt;:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data['blog_id'] = data['blog_id'].map(item_id2idx)\n</pre> Out[100]: <pre>(25991, 9)</pre> In\u00a0[101]: Copied! <pre>class BaseFactorizationModel:\n    \n    # constructor, define user, item and rating column names\n    def __init__(self, random_state=0,\n                 user_col='user_id', \n                 item_col='blog_id', \n                 rating_col='ratings'):\n        \n        self.random_state = np.random.RandomState(random_state)\n        self.user_col = user_col\n        self.item_col = item_col\n        self.rating_col = rating_col\n        self.user_matrix = None\n        self.item_matrix = None\n\n    # matrix which we will decompose\n    def get_rating_matrix(self, data):\n        return pd.pivot_table(\n            data,\n            values=self.rating_col,\n            index=self.user_col,\n            columns=self.item_col,\n            fill_value=0\n            ).values\n\n    \n    \"\"\"\n    \n    When we receive the matrix with scores, for each user\n    sort and get the top k \n    \n    \"\"\"\n    \n    def predict(self, \n                scores,  # (user_id,blog_id) score matrix\n                rating_matrix=None, # (user_id,blog_id) rating matrix\n                filter_seen=False, \n                k=5):\n        \n        # filter out films that already have been seen \n        if filter_seen:\n            scores = np.multiply(scores,\n            np.invert(rating_matrix.astype(bool)))\n\n        # scores index : userId column : blog_id \n\n        # get indicies of top k scores (indicies : blog_id) in user array\n        ind_part = np.argpartition(scores, -k + 1)[:, -k:].copy()  \n\n        # get the values of the top k scores \n        scores_not_sorted = np.take_along_axis(scores, ind_part, axis=1)\n\n        # indicies of scores from lowest to highest \n        ind_sorted = np.argsort(scores_not_sorted, axis=1) \n\n        # scores from lowest to highest \n        scores_sorted = np.sort(scores_not_sorted, axis=1) \n\n        # get the indicies of the movieId with the highest scores\n        indices = np.take_along_axis(ind_part, ind_sorted, axis=1)\n\n        # for each user return the movies with the highest scores\n        preds = pd.DataFrame({\n            self.user_col: range(scores.shape[0]), # each user \n            self.item_col: np.flip(indices, axis=1).tolist(), # movieId index \n            self.rating_col: np.flip(scores_sorted.round(3), axis=1).tolist() # movieId score\n            })\n        \n        # convert arrays (user_col,item_col) into rows for each user \n        #preds = preds.explode([self.item_col, self.rating_col])\n\n        return preds\n</pre> class BaseFactorizationModel:          # constructor, define user, item and rating column names     def __init__(self, random_state=0,                  user_col='user_id',                   item_col='blog_id',                   rating_col='ratings'):                  self.random_state = np.random.RandomState(random_state)         self.user_col = user_col         self.item_col = item_col         self.rating_col = rating_col         self.user_matrix = None         self.item_matrix = None      # matrix which we will decompose     def get_rating_matrix(self, data):         return pd.pivot_table(             data,             values=self.rating_col,             index=self.user_col,             columns=self.item_col,             fill_value=0             ).values           \"\"\"          When we receive the matrix with scores, for each user     sort and get the top k           \"\"\"          def predict(self,                  scores,  # (user_id,blog_id) score matrix                 rating_matrix=None, # (user_id,blog_id) rating matrix                 filter_seen=False,                  k=5):                  # filter out films that already have been seen          if filter_seen:             scores = np.multiply(scores,             np.invert(rating_matrix.astype(bool)))          # scores index : userId column : blog_id           # get indicies of top k scores (indicies : blog_id) in user array         ind_part = np.argpartition(scores, -k + 1)[:, -k:].copy()            # get the values of the top k scores          scores_not_sorted = np.take_along_axis(scores, ind_part, axis=1)          # indicies of scores from lowest to highest          ind_sorted = np.argsort(scores_not_sorted, axis=1)           # scores from lowest to highest          scores_sorted = np.sort(scores_not_sorted, axis=1)           # get the indicies of the movieId with the highest scores         indices = np.take_along_axis(ind_part, ind_sorted, axis=1)          # for each user return the movies with the highest scores         preds = pd.DataFrame({             self.user_col: range(scores.shape[0]), # each user              self.item_col: np.flip(indices, axis=1).tolist(), # movieId index              self.rating_col: np.flip(scores_sorted.round(3), axis=1).tolist() # movieId score             })                  # convert arrays (user_col,item_col) into rows for each user          #preds = preds.explode([self.item_col, self.rating_col])          return preds In\u00a0[102]: Copied! <pre>from scipy.sparse import csr_matrix\nfrom scipy.sparse.linalg import svds\n\nclass SVD(BaseFactorizationModel):\n    \n    def __init__(self, \n                 random_state=0, \n                 user_col='user_id', \n                 item_col='blog_id',\n                 n_factors=100): # hyperparameter\n        super().__init__(random_state, user_col, item_col)\n        self.n_factors = n_factors\n\n    \"\"\"\n    \n    Decompose the rating matrix into user_id and item_id matrices\n    followed by the scalar multiplication of the them\n    \n    \"\"\"\n        \n    def fit(self, data):\n        \n        # user,movie rating matrix\n        self.rating_matrix = self.get_rating_matrix(data) # (unique users,unique films) ratings\n        csr_rating_matrix = csr_matrix(self.rating_matrix.astype(float))\n        \n        # svd decomposition \n        user_matrix, singular_values, item_matrix = svds(A=csr_rating_matrix, \n                                                         k=self.n_factors)\n    \n        user_matrix = user_matrix * np.sqrt(singular_values)  # (unique users,k latent features)\n        item_matrix = item_matrix.T * np.sqrt(singular_values) # (unique films, k latent features)\n        self.scores = user_matrix @ item_matrix.T  # score matrix for each user &amp; film (unique users, unique films)\n        \n        self.user_matrix = user_matrix\n        self.item_matrix = item_matrix\n</pre> from scipy.sparse import csr_matrix from scipy.sparse.linalg import svds  class SVD(BaseFactorizationModel):          def __init__(self,                   random_state=0,                   user_col='user_id',                   item_col='blog_id',                  n_factors=100): # hyperparameter         super().__init__(random_state, user_col, item_col)         self.n_factors = n_factors      \"\"\"          Decompose the rating matrix into user_id and item_id matrices     followed by the scalar multiplication of the them          \"\"\"              def fit(self, data):                  # user,movie rating matrix         self.rating_matrix = self.get_rating_matrix(data) # (unique users,unique films) ratings         csr_rating_matrix = csr_matrix(self.rating_matrix.astype(float))                  # svd decomposition          user_matrix, singular_values, item_matrix = svds(A=csr_rating_matrix,                                                           k=self.n_factors)              user_matrix = user_matrix * np.sqrt(singular_values)  # (unique users,k latent features)         item_matrix = item_matrix.T * np.sqrt(singular_values) # (unique films, k latent features)         self.scores = user_matrix @ item_matrix.T  # score matrix for each user &amp; film (unique users, unique films)                  self.user_matrix = user_matrix         self.item_matrix = item_matrix In\u00a0[103]: Copied! <pre># SVD decomposition of rating matrix\nsvd_model = SVD()\nsvd_model.fit(data)\nprint('Rating matrix size',svd_model.rating_matrix.shape)\n\n# extract the too values for each user_id\npreds_svd = svd_model.predict(svd_model.scores, \n                              svd_model.rating_matrix) \n\nprint('Decomposed matrices scalar multiplication (scores)',svd_model.scores.shape)\n</pre> # SVD decomposition of rating matrix svd_model = SVD() svd_model.fit(data) print('Rating matrix size',svd_model.rating_matrix.shape)  # extract the too values for each user_id preds_svd = svd_model.predict(svd_model.scores,                                svd_model.rating_matrix)   print('Decomposed matrices scalar multiplication (scores)',svd_model.scores.shape) <pre>Rating matrix size (4280, 1485)\nDecomposed matrices scalar multiplication (scores) (4280, 1485)\n</pre> In\u00a0[104]: Copied! <pre>def show_user_predictions(user_id):\n    nonmapped = preds_svd[preds_svd['user_id'] == user_id]['blog_id'].explode().tolist()\n    mapped = [idx2item_id[i] for i in nonmapped]\n    return blogs[blogs.blog_id.isin(mapped)]\n\nshow_user_predictions(4277)\n</pre> def show_user_predictions(user_id):     nonmapped = preds_svd[preds_svd['user_id'] == user_id]['blog_id'].explode().tolist()     mapped = [idx2item_id[i] for i in nonmapped]     return blogs[blogs.blog_id.isin(mapped)]  show_user_predictions(4277) Out[104]: blog_id author_id blog_title blog_preview topic scrape_time author_name nratings rating type 2881 2908 2257 Why Do Some Programmers Say Frontend Is Easier... So, you\u2019re wondering if frontend development i... backend-development 2023-04-04 08:53:52 Flatlogic Platform 45 3.70 good 2893 2920 2267 What are keywords in python programming In Python, a keyword is a reserved word that h... backend-development 2023-04-04 08:53:52 Anwar Ali 40 3.76 good 2908 2935 2279 Exploring the Power of JavaScript Maps: A Begi... In JavaScript, a map is a built-in data struct... backend-development 2023-04-04 08:53:52 Mohammad Basit 43 3.57 good 2918 2945 2285 Observer design pattern: Behavioral pattern One of the most commonly used Behavioral patte... backend-development 2023-04-04 08:53:52 Atul Kumar, SDE1 @ Paytm 37 3.91 good 3041 3068 2289 CLASS META- Django models Meta in our English oxford dictionary means se... backend-development 2023-04-04 08:53:52 Kkipngenokoech 37 3.62 good In\u00a0[105]: Copied! <pre># user read and rating history \ndata[data['user_id'] == 4277].sort_values('ratings',ascending=False)\n</pre> # user read and rating history  data[data['user_id'] == 4277].sort_values('ratings',ascending=False) Out[105]: blog_id user_id ratings author_id blog_title blog_preview topic scrape_time author_name 199857 326 4277 5.0 2226 What caused Discord to switch from Go to Rust? NOTE: This was one of the factors that led Dis... backend-development 2023-04-04 08:53:52 Siddharth Pandey 199862 320 4277 5.0 2289 CLASS META- Django models Meta in our English oxford dictionary means se... backend-development 2023-04-04 08:53:52 Kkipngenokoech 199891 321 4277 5.0 2285 Observer design pattern: Behavioral pattern One of the most commonly used Behavioral patte... backend-development 2023-04-04 08:53:52 Atul Kumar, SDE1 @ Paytm 199904 318 4277 5.0 2257 Why Do Some Programmers Say Frontend Is Easier... So, you\u2019re wondering if frontend development i... backend-development 2023-04-04 08:53:52 Flatlogic Platform 199908 315 4277 5.0 2279 Exploring the Power of JavaScript Maps: A Begi... In JavaScript, a map is a built-in data struct... backend-development 2023-04-04 08:53:52 Mohammad Basit 199913 319 4277 5.0 2267 What are keywords in python programming In Python, a keyword is a reserved word that h... backend-development 2023-04-04 08:53:52 Anwar Ali 199863 312 4277 3.5 2359 E se? Ol\u00e1 estou aqui denovo, dessa vez n\u00e3o \u00e9 para fa... backend-development 2023-04-04 08:53:52 Vinicius Mazzeo 199860 566 4277 2.0 2397 APIs lifecycle Management 101 Learn everything about APIs lifecycle manageme... backend-development 2023-04-04 08:53:52 Developer Nation 199866 317 4277 2.0 2372 A arte do desenvolvimento web: um guia para in... Se voc\u00ea est\u00e1 buscando dar os primeiros passos ... backend-development 2023-04-04 08:53:52 Lucas from Woogon 199923 313 4277 0.5 2240 Build A Signup, Login and Logout Feature If you\u2019re getting familiar with Rails, and fee... backend-development 2023-04-04 08:53:52 Rebecca Wollard 5 | SIMILAR ARTICLE RECOMMENDATIONS <p>Now lets look at a different approach, one which utilises textual data information. Whilst the data in column <code>blog_preview</code> contains unstructured data, we can utilise a transformed matrix once again, however this time using Bag of Words (<code>BoW</code>) and a similarity evaluation method, lets use something standard <code>cosine similarity</code></p> In\u00a0[106]: Copied! <pre>from sklearn.feature_selection \n</pre> from sklearn.feature_selection  <pre>\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-106-fd91234d0764&gt; in &lt;module&gt;\n----&gt; 1 from sklearn.feature_selection.text import CountVectorizer\n\nModuleNotFoundError: No module named 'sklearn.feature_selection.text'</pre>"},{"location":"portfolio/kaggle/testing.html#blog-recommendation","title":"Blog Recommendation \u00b6","text":"<p>A large number of blogs are published on a daily basis on a variety of subjects by different users. Its important to keep the user invested in material located on a particular website for a variety of reasons.</p> <p>If a user is interested in a particular article and has made their way from for example a search engine onto the website/blog, its hightly likely that they are interested in something specific at this particular moment in time.</p> <p>As as result, it would be favourable to show the user material that would likely peek their interest and hopefully make them interact with other material on the website, keeping them on the website longer.</p>"},{"location":"portfolio/kaggle/testing.html#notebook-aim","title":"Notebook Aim \u00b6","text":"<p>In this notebook, we will aim to create a user recommendations for blog content found on <code>medium</code>, which is a popular source of information for a variety of topics related to information technology.</p> <p>Only the essential data has been scrapped, in other words, we won't be using the scrapped data of the entire blog article, and will only use a short preview which is selected for each article post.</p>"},{"location":"portfolio/kaggle/testing.html#recommendation-approaches","title":"Recommendation Approaches \u00b6","text":"<p>There are a couple of routes we will go with our data:</p> <ul> <li><p>(a) We have <code>medium</code> user feedback in the form of blog ratings, which means we can not only use this information to filter our recommendations, but also create a rating based recommendation for the users, which they might have missed.</p> </li> <li><p>(b) We have blog article previews, from which we can create recommendations based on similar content for</p> </li> </ul>"},{"location":"portfolio/kaggle/testing.html#blog-ratings-count","title":"Blog Ratings Count \u00b6","text":"<p>Let's check the number of ratings for each blog article in the form of a histogram</p>"},{"location":"portfolio/kaggle/testing.html#blog-rating-distribution","title":"Blog Rating Distribution \u00b6","text":"<p>Most users tend to rate posts that they like or don't like (roughtly 60%)</p>"},{"location":"portfolio/kaggle/testing.html#user-rating-statistics","title":"User Rating Statistics \u00b6","text":"<p>Lets check the <code>user_id</code> statistics; how many times has a user rated a blog article and what is the average rating for all blog article ratings of a particular user</p>"},{"location":"portfolio/kaggle/testing.html#blog-rating-segmentation","title":"Blog Rating Segmentation \u00b6","text":""},{"location":"portfolio/kaggle/testing.html#select-subset","title":"Select subset \u00b6","text":"<p>Lets filter the blog articles into different groups (<code>type</code>), which will help us in providing different types of recommendations</p> <p>Evidently not all article posts are liked the same by users. Some articles have not even been discivered by many, yet we dont really know if we should recommend them. Some posts we should probably avoid recommending at all, so its useful to segment the posts by rating.</p>"},{"location":"portfolio/kaggle/testing.html#define-subset-segment-for-recommendations","title":"Define subset segment for recommendations \u00b6","text":"<p>For this reason, lets create recommendations based on their previously rated content. We will do this by filtering <code>blogs</code> to include only subsets <code>good</code> and <code>excellent</code> only. From the above data exploration, we can see that these two subsets sfill do make a fair portion to choose from.</p>"},{"location":"portfolio/kaggle/testing.html#define-recommendation-approach","title":"Define recommendation approach \u00b6","text":"<p>The first approach we will try is a rating based <code>SVD</code> decomposition approach. We will be using the scipy realisation, and simply have split the the enitr process of data preprocessing and recommendation generation into two separate classes:</p> <ul> <li><p><code>BaseFactorizationModel</code> contains the constructor, which simply defines the names of columns, indicies and ratings, as well as set placeholders for <code>user</code> and <code>item</code> matrices (output of SVD decomposition). The class also contains method <code>get_rating_matrix</code> which generate the rating matrix (matrix contains user rating data for each article) and a generic rating prediction method <code>predict</code>, which simply interprets the decomposed matrices scalar product (scores) and returns the highest values (top k) for each user</p> </li> <li><p><code>SVD</code> contains the <code>fit</code> method, which is the matrix factorisation itself</p> </li> </ul>"},{"location":"portfolio/kaggle/testing.html#check-recommendations","title":"Check recommendations \u00b6","text":"<ul> <li>Now lets check the top recommendations for a user based on their rating history.</li> <li>For this particular user we can see that the user is mostly interested in the topic <code>backend-developement</code></li> <li>Top k recommendations from <code>SVD</code> factorisation model are also from this genre, and their average ratings are also quite high, which indicates that the model works as expected.</li> </ul>"},{"location":"portfolio/projects/ab_testing/hypothesis_testing.html","title":"Hypothesis testing","text":"\u0420\u0430\u0431\u043e\u0442\u0430 \u0441 \u0433\u0438\u043f\u043e\u0442\u0435\u0437\u0430\u043c\u0438 1 | INTRODUCTION \u2871\u2877 \u041e\u0431 \u044d\u043a\u0441\u043f\u0435\u0440\u0435\u043c\u0435\u043d\u0442\u0435 <ul> <li>\u041f\u0440\u0438\u0448\u043b\u043e \u0432\u0440\u0435\u043c\u044f \u043f\u0440\u043e\u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043c\u044b \u043f\u0440\u043e\u0432\u0435\u043b\u0438 \u0432\u043c\u0435\u0441\u0442\u0435 \u0441 \u043a\u043e\u043c\u0430\u043d\u0434\u043e\u0439 \u0434\u0430\u0442\u0430 \u0441\u0430\u0439\u0435\u043d\u0442\u0438\u0441\u0442\u043e\u0432.</li> <li>\u042d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442 \u043f\u0440\u043e\u0445\u043e\u0434\u0438\u043b \u0441 2023-01-01 \u043f\u043e 2023-01-07 (7 \u0434\u043d\u0435\u0439) \u0432\u043a\u043b\u044e\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e. \u0414\u043b\u044f \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0430 \u0431\u044b\u043b\u0438 \u0437\u0430\u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u043d\u044b 2 \u0438 1 \u0433\u0440\u0443\u043f\u043f\u044b</li> <li>\u0412 \u0433\u0440\u0443\u043f\u043f\u0435 2 \u0431\u044b\u043b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d \u043e\u0434\u0438\u043d \u0438\u0437 \u043d\u043e\u0432\u044b\u0445 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u043f\u043e\u0441\u0442\u043e\u0432, \u0433\u0440\u0443\u043f\u043f\u0430 1 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u0430\u0441\u044c \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044f.</li> </ul> \u2871\u2877 \u0413\u0438\u043f\u043e\u0442\u0438\u0437\u0430 <p>\u041e\u0441\u043d\u043e\u0432\u043d\u0430\u044f \u0433\u0438\u043f\u043e\u0442\u0435\u0437\u0430 \u0437\u0430\u043a\u043b\u044e\u0447\u0430\u0435\u0442\u0441\u044f \u0432 \u0442\u043e\u043c, \u0447\u0442\u043e \u043d\u043e\u0432\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0432\u043e 2-\u0439 \u0433\u0440\u0443\u043f\u043f\u0435 \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u0442 \u043a \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u044e CTR.</p> \u2871\u2877 \u041d\u0430\u0448\u0430 \u0437\u0430\u0434\u0430\u0447\u0430 <p>\u0412\u0430\u0448\u0430 \u0437\u0430\u0434\u0430\u0447\u0430 \u2014 \u043f\u0440\u043e\u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u0410B-\u0442\u0435\u0441\u0442\u0430</p> <p>\u0412\u044b\u0431\u0440\u0430\u0442\u044c \u043c\u0435\u0442\u043e\u0434 \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0438 \u0441\u0440\u0430\u0432\u043d\u0438\u0442\u044c CTR \u0432 \u0434\u0432\u0443\u0445 \u0433\u0440\u0443\u043f\u043f\u0430\u0445, \u043c\u044b \u0440\u0430\u0437\u0431\u0438\u0440\u0430\u043b\u0438:</p> <ul> <li><code>t-\u0442\u0435\u0441\u0442</code></li> <li><code>\u041f\u0443\u0430\u0441\u0441\u043e\u043d\u043e\u0432\u0441\u043a\u0438\u0439 \u0431\u0443\u0442\u0441\u0442\u0440\u0435\u043f</code></li> <li><code>\u0442\u0435\u0441\u0442 \u041c\u0430\u043d\u043d\u0430-\u0423\u0438\u0442\u043d\u0438</code></li> <li><code>t-\u0442\u0435\u0441\u0442 \u043d\u0430 \u0441\u0433\u043b\u0430\u0436\u0435\u043d\u043d\u043e\u043c ctr (\u03b1=5)</code></li> <li><code>t-\u0442\u0435\u0441\u0442 \u0438 \u0442\u0435\u0441\u0442 \u041c\u0430\u043d\u043d\u0430-\u0423\u0438\u0442\u043d\u0438 \u043f\u043e\u0432\u0435\u0440\u0445 \u0431\u0430\u043a\u0435\u0442\u043d\u043e\u0433\u043e \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f</code></li> </ul> <ul> <li>\u0421\u0440\u0430\u0432\u043d\u0438\u0442\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u044d\u0442\u0438\u043c\u0438 \u0442\u0435\u0441\u0442\u0430\u043c\u0438. \u0410 \u0435\u0449\u0435 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u0442\u0435 \u043d\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0433\u043b\u0430\u0437\u0430\u043c\u0438. \u041f\u043e\u0447\u0435\u043c\u0443 \u0442\u0435\u0441\u0442\u044b \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b\u0438 \u0442\u0430\u043a \u043a\u0430\u043a \u0441\u0440\u0430\u0431\u043e\u0442\u0430\u043b\u0438?</li> <li>\u041e\u043f\u0438\u0448\u0438\u0442\u0435 \u043f\u043e\u0442\u0435\u043d\u0446\u0438\u0430\u043b\u044c\u043d\u0443\u044e \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u044e, \u043a\u043e\u0433\u0434\u0430 \u0442\u0430\u043a\u043e\u0435 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u043c\u043e\u0433\u043b\u043e \u043f\u0440\u043e\u0438\u0437\u043e\u0439\u0442\u0438. \u0422\u0443\u0442 \u043d\u0435\u0442 \u0438\u0434\u0435\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u043e\u0442\u0432\u0435\u0442\u0430, \u043f\u043e\u0434\u0443\u043c\u0430\u0439\u0442\u0435.</li> <li>\u041d\u0430\u043f\u0438\u0448\u0438\u0442\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u044e, \u0431\u0443\u0434\u0435\u043c \u043b\u0438 \u043c\u044b \u0440\u0430\u0441\u043a\u0430\u0442\u044b\u0432\u0430\u0442\u044c \u043d\u043e\u0432\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u043d\u0430 \u0432\u0441\u0435\u0445 \u043d\u043e\u0432\u044b\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0438\u043b\u0438 \u0432\u0441\u0435-\u0442\u0430\u043a\u0438 \u043d\u0435 \u0441\u0442\u043e\u0438\u0442</li> </ul> 2 | DATA EXPORT <ul> <li>\u0414\u0430\u043d\u043d\u044b\u0435 \u0441 \u0411\u0414 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 4 \u043a\u043e\u043b\u043e\u043d\u043a\u0438; <code>exp_group</code> (\u0433\u0440\u0443\u043f\u043f\u0430 \u0432 \u044d\u043a\u0441\u043f\u0435\u0440\u0435\u043c\u0435\u043d\u0442\u0435) <code>user_id</code> (\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c) <code>likes</code> (\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0445 \u043b\u0430\u0439\u043a\u043e\u0432) <code>views</code> (\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432)</li> </ul> \u2871\u2877 \u0414\u0430\u043d\u043d\u044b\u0435 \u0432 \u0411\u0414 <p>\u0414\u0430\u043d\u043d\u044b\u0435 \u044d\u043a\u0441\u043f\u0435\u0440\u0435\u043c\u0435\u043d\u0442\u0430 \u0437\u0430\u0433\u0440\u0443\u0436\u0435\u043d\u044b \u0432 \u0411\u0414, \u0438 \u043d\u0430\u043c \u043f\u043e\u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u0432\u044b\u0433\u0440\u0443\u0437\u0438\u0442\u044c \u0438\u0445 \u0438 \u043f\u0440\u043e\u0432\u0435\u0441\u0442\u0438 \u0441\u0432\u043e\u0438 \u0430\u043d\u0430\u043b\u0438\u0437</p> <pre><code>      exp_group    user_id    likes    views        ctr\n--  -----------  ---------  -------  -------  ---------\n 0            1     109963        3       15  0.2\n 1            1      26117       32      141  0.22695\n 2            1     138232       18       73  0.246575\n 3            1      26295       33      122  0.270492\n 4            1      18392        7       32  0.21875\n 5            2     131473       14      134  0.104478\n 6            2      23985        7       87  0.0804598\n 7            2      24523       11      101  0.108911\n 8            2      32420       26      128  0.203125\n 9            2      24239       40       90  0.444444\n</code></pre> \u2871\u2877 \u0412\u044b\u0433\u0440\u0443\u0437\u043a\u0430 \u0438\u0437 \u0411\u0414 <p>\u0412\u044b\u0433\u0440\u0443\u0437\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 \u0411\u0414</p> <pre>query = '''\nselect exp_group, \n    user_id, \n    sum(action = 'like') likes,\n    sum(action = 'view') views,\n    likes/views ctr\nfrom {db}.feed_actions \nwhere toDate(time) between '2023-01-01' and '2023-01-07'\n    and exp_group in (1,2)\ngroup by exp_group, user_id \n'''\n\ndf = ph.read_clickhouse(query, connection = connection)\n</pre> <p>\u041d\u0430\u0447\u043d\u0435\u043c \u0441 \u0432\u044b\u0433\u0440\u0443\u0437\u043a\u043e\u0439 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043f\u043e\u0434\u0433\u0440\u0443\u0437\u043a\u0430 \u0432 \u043d\u0430\u0448 \u043d\u043e\u0443\u0442\u0431\u0443\u043a</p> <ul> <li>\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u0441\u0445\u0435\u043c\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043d\u0435\u0435 \u043f\u043e\u0441\u043b\u0435 \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0447\u0442\u043e\u0431\u044b \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c \u0442\u0438\u043f\u044b \u043a\u043e\u043b\u043e\u043d\u043e\u043a</li> <li>\u0422\u0430\u043a \u0432\u044b\u0433\u0440\u0443\u0437\u0438\u043c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u0442\u0440\u043e\u043a \u0438\u0437 \u043d\u0430\u0448\u0435\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430</li> </ul> In\u00a0[220]: Copied! <pre>from scipy import stats\nimport pandas as pd \nimport numpy as np\nimport seaborn as sns;\nsns.set_style('whitegrid')\n</pre> from scipy import stats import pandas as pd  import numpy as np import seaborn as sns; sns.set_style('whitegrid') In\u00a0[34]: Copied! <pre>dtypes = {'exp_group':int, \n          'user_id':int, \n          'likes':int, \n          'views':int}\n\ndf = pd.read_csv('ctr_data.csv',dtype=dtypes)\n</pre> dtypes = {'exp_group':int,            'user_id':int,            'likes':int,            'views':int}  df = pd.read_csv('ctr_data.csv',dtype=dtypes) In\u00a0[35]: Copied! <pre>df.head(5)\n</pre> df.head(5) Out[35]: exp_group user_id likes views ctr 0 1 109963 3 15 0.200000 1 1 26117 32 141 0.226950 2 1 138232 18 73 0.246575 3 1 26295 33 122 0.270492 4 1 18392 7 32 0.218750 3 | DESCRIPTIVE STATISTICS <p>\u0420\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u0434\u0432\u0435 \u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u0442\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0438 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u044b\u0435 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u043f\u043e\u0441\u043b\u0435 \u0447\u0435\u0433\u043e \u0441\u0440\u0430\u0432\u043d\u0438\u043c \u043e\u043f\u0438\u0441\u0430\u0442\u0435\u043b\u044c\u043d\u0443\u044e \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0443</p> <ul> <li><code>\u041a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u0430\u044f</code> : <code>exp_group = 1</code></li> <li><code>\u0422\u0435\u0441\u0442\u043e\u0432\u0430\u044f</code> : <code>exp_group = 2</code></li> </ul> In\u00a0[36]: Copied! <pre>df.groupby('exp_group')['ctr'].describe().T\n</pre> df.groupby('exp_group')['ctr'].describe().T Out[36]: exp_group 1 2 count 10079.000000 9952.000000 mean 0.215605 0.214419 std 0.084160 0.144180 min 0.000000 0.000000 25% 0.160097 0.095238 50% 0.203125 0.142857 75% 0.257654 0.325000 max 0.722222 0.923077 In\u00a0[37]: Copied! <pre>c = df[df['exp_group']==1]  # \u041a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u0430\u044f \u0433\u0440\u0443\u043f\u043f\u0430\nt = df[df['exp_group']==2]  # \u0422\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0433\u0440\u0443\u043f\u043f\u0430 (\u043d\u043e\u0432\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c)\n</pre> c = df[df['exp_group']==1]  # \u041a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u0430\u044f \u0433\u0440\u0443\u043f\u043f\u0430 t = df[df['exp_group']==2]  # \u0422\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0433\u0440\u0443\u043f\u043f\u0430 (\u043d\u043e\u0432\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c) 4 | UNIVARIATE STATISTICS <p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0441\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435, \u043e\u0434\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0435 \u0434\u043b\u044f \u043e\u0431\u043b\u0438\u0445 \u0432\u044b\u0431\u043e\u0440\u043e\u043a</p> <ul> <li>\u041c\u043e\u0436\u0435\u043c \u0432\u0438\u0434\u0438\u0442\u044c \u0447\u0442\u043e \u043e\u0434\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0435 \u0440\u0430\u0441\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0443 \u043e\u0431\u043e\u0438\u0445 \u0432\u044b\u0431\u043e\u0440\u043e\u043a \u043e\u0447\u0435\u043d\u044c \u0440\u0430\u0437\u043d\u043e\u0435</li> <li>\u0414\u043b\u044f <code>\u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043a\u043e\u0439</code> (1) \u0433\u0440\u0443\u043f\u043f\u044b, \u0440\u0430\u0441\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043f\u043e\u0445\u043e\u0436\u0435 \u043d\u0430 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0440\u0430\u0441\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0441 \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u044b\u043c \u043f\u0435\u0440\u0435\u043a\u043e\u0441\u043e\u043c \u0432\u043b\u0435\u0432\u043e</li> <li>\u0410 \u0432\u043e\u0442 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0432\u043e \u0432\u0442\u043e\u0440\u043e\u0439 <code>\u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439</code> (2) \u0433\u0440\u0443\u043f\u043f\u0435 \u0443\u0436\u0435 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u043e\u0441\u044c \u0441 \u044f\u0432\u043d\u043e\u0439 \u0431\u0438\u043c\u043e\u0434\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c</li> </ul> In\u00a0[223]: Copied! <pre>sns.displot(data=df, \n            x='ctr',\n            hue='exp_group',\n            kind='kde',\n            palette=\"tab10\",\n            height=4,aspect=1.5)\n</pre> sns.displot(data=df,              x='ctr',             hue='exp_group',             kind='kde',             palette=\"tab10\",             height=4,aspect=1.5) Out[223]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x798158ef0e10&gt;</pre> <ul> <li>\u0422\u0430\u043a\u043e\u0435 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0432 <code>ctr</code> \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u0447\u0442\u043e \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u043e \u0441 \u0440\u0430\u0441\u0441\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435\u043c \u043c\u043d\u0435\u043d\u0438\u0435\u043c \u043e \u043d\u043e\u0432\u043e\u043c \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0438, \u043a\u043e\u043c\u0443 \u0442\u043e \u043f\u043e\u043d\u0440\u0430\u0432\u0438\u043b\u043e\u0441\u044c, \u0430 \u043a\u043e\u043c\u0443 \u0442\u043e \u043d\u0435\u0442, \u0438 \u0442\u0443\u0442 \u0432\u0438\u0434\u043d\u043e \u0447\u0442\u043e \u043f\u0438\u043a \u0443 \u043b\u0435\u0432\u043e\u0439 \u0432\u044b\u0448\u0435 \u0447\u0435\u043c \u0443 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043a\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b</li> <li>\u0422\u0430\u043a \u0436\u0435 <code>t-test</code> \u043d\u0435\u043b\u044c\u0437\u044f \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0442\u044c \u043d\u0430 \u0442\u0430\u043a\u0438\u0445 \u0440\u0430\u0441\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0438, \u043d\u0443\u0436\u0435\u043d \u0434\u0440\u0443\u0433\u043e\u0439 \u0441\u043f\u043e\u0441\u043e\u0431 \u0441\u043f\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f \u044d\u0444\u0444\u0435\u043a\u0442\u0430 \u043a\u043e\u0432\u043e\u0433\u043e \u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u0430</li> </ul> 5 | STATISTICAL TESTING \u2871\u2877 \u0422\u0435\u0441\u0442 \u0421\u0442\u044c\u044e\u0434\u0435\u043d\u0442\u0430 <ul> <li>$H_0$ - \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f CTR \u0432 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0438 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0430\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043fa\u0445 \u0440\u0430\u0432\u043d\u044b.</li> <li>$H_1$ - \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f CTR \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u0435 \u0432\u044b\u0448\u0435, \u0447\u0435\u043c \u0432 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439.</li> </ul> <ul> <li>\u041d\u0435 \u043c\u043e\u0436\u0435\u0442 \u043e\u0442\u0432\u0435\u0440\u0433\u043d\u0443\u0442\u044c \u043d\u0443\u043b\u0435\u0432\u0443\u044e \u0433\u0438\u043f\u043e\u0442\u0438\u0437\u0443, \u043d\u0435\u0442 \u0434\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044c\u0441\u0442\u0432 \u0442\u043e\u0433\u043e, \u0447\u0442\u043e \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 c.ctr \u043c\u0435\u043d\u044c\u0448\u0435, \u0447\u0435\u043c \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 t.ctr.</li> </ul> In\u00a0[39]: Copied! <pre>stats.ttest_ind(c.ctr,\n                t.ctr,\n                equal_var=False,\n                alternative='less').pvalue \n</pre> stats.ttest_ind(c.ctr,                 t.ctr,                 equal_var=False,                 alternative='less').pvalue  Out[39]: <pre>np.float64(0.7609688434562429)</pre> <p>\u041e\u0431\u0440\u0430\u0442\u043d\u0430\u044f \u0430\u043b\u0442\u0435\u0440\u043d\u0430\u0442\u0438\u0432\u043d\u0430\u044f \u0433\u0438\u043f\u043e\u0442\u0438\u0437\u0430 \u0421\u0442\u044c\u044e\u0434\u0435\u043d\u0442\u0430</p> <p>\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043e\u0431\u0440\u0430\u0442\u043d\u0443\u044e \u0430\u043b\u044c\u0442\u0435\u0440\u043d\u0430\u0442\u0438\u0432\u043d\u0443\u044e \u0433\u0438\u043f\u043e\u0442\u0435\u0437\u0443:</p> <ul> <li>$H_0$ - \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f CTR \u0432 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0438 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0430\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043fa\u0445 \u0440\u0430\u0432\u043d\u044b.</li> <li>$H_1$  - \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f CTR \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u0435 \u043d\u0438\u0436\u0435, \u0447\u0435\u043c \u0432 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439.</li> </ul> <ul> <li>\u041d\u0435 \u043c\u043e\u0436\u0435\u0442 \u043e\u0442\u0432\u0435\u0440\u0433\u043d\u0443\u0442\u044c \u043d\u0443\u043b\u0435\u0432\u0443\u044e \u0433\u0438\u043f\u043e\u0442\u0438\u0437\u0443, \u043d\u0435\u0442 \u0434\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044c\u0441\u0442\u0432 \u0442\u043e\u0433\u043e, \u0447\u0442\u043e \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 c.ctr \u0431\u043e\u043b\u044c\u0448\u0435, \u0447\u0435\u043c \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 t.ctr.</li> </ul> In\u00a0[40]: Copied! <pre>stats.ttest_ind(c.ctr,\n                t.ctr,\n                equal_var=False,\n                alternative='greater').pvalue \n</pre> stats.ttest_ind(c.ctr,                 t.ctr,                 equal_var=False,                 alternative='greater').pvalue  Out[40]: <pre>np.float64(0.23903115654375706)</pre> \u2871\u2877 \u0422\u0435\u0441\u0442 \u041c\u0430\u043d\u043d\u0430-\u0423\u0438\u0442\u043d\u0438 <p>\u0421\u043e\u0441\u0442\u0430\u0432\u0438\u043c \u0433\u0438\u043f\u043e\u0442\u0438\u0437\u044b</p> <ul> <li>$H_0$ - $P(X &gt; Y) = P(Y &gt; X)$ \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442, \u0447\u0442\u043e \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0435 \u0432\u0435\u043b\u0438\u0447\u0438\u043d\u044b $X$ \u0438 $Y$ \u0438\u043c\u0435\u044e\u0442 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435</li> <li>$H_1$ - $P(X &gt; Y) &gt; P(Y &gt; X)$ \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442, \u0447\u0442\u043e \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c $X$ \u0431\u044b\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u0435 $Y$ \u0431\u043e\u043b\u044c\u0448\u0435, \u0447\u0435\u043c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c $Y$ \u0431\u044b\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u0435 $X$ ('greater')</li> </ul> <p>\u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u0442\u0435\u0441\u0442 \u041c\u0430\u043d\u043d\u0430-\u0423\u0438\u0442\u043d\u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442, \u0435\u0441\u0442\u044c \u043b\u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0437\u043d\u0430\u0447\u0438\u043c\u0430\u044f \u0440\u0430\u0437\u043d\u0438\u0446\u0430 \u0432 \u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0438 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439 \u0434\u0432\u0443\u0445 \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u044b\u0445 \u0432\u044b\u0431\u043e\u0440\u043e\u043a $X$ \u0438 $Y$, \u0431\u0435\u0437 \u043f\u0440\u0435\u0434\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u043e \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0434\u0430\u043d\u043d\u044b\u0445.</p> <code>alternative</code> \u0410\u043b\u044c\u0442\u0435\u0440\u043d\u0430\u0442\u0438\u0432\u043d\u0430\u044f \u0433\u0438\u043f\u043e\u0442\u0435\u0437\u0430 H1 \u0427\u0442\u043e \u044d\u0442\u043e \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442 <code>'less'</code> P(X&lt;Y)&gt;P(X&gt;Y) \u041f\u0435\u0440\u0432\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 \u0438\u043c\u0435\u0435\u0442 \u043c\u0435\u043d\u044c\u0448\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f, \u0447\u0435\u043c \u0432\u0442\u043e\u0440\u0430\u044f <code>'greater'</code> P(X&gt;Y)&gt;P(X&lt;Y) \u041f\u0435\u0440\u0432\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 \u0438\u043c\u0435\u0435\u0442 \u0431\u043e\u043b\u044c\u0448\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f, \u0447\u0435\u043c \u0432\u0442\u043e\u0440\u0430\u044f <code>'two-sided'</code> \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0440\u0430\u0437\u043b\u0438\u0447\u0430\u044e\u0442\u0441\u044f (\u0431\u0435\u0437 \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0440\u0430\u0437\u043b\u0438\u0447\u0438\u044f) \u0415\u0441\u0442\u044c \u043e\u0442\u043b\u0438\u0447\u0438\u0435 \u0432 \u0434\u0432\u0443\u0445 \u0432\u044b\u0431\u043e\u0440\u043a\u0430\u0445, \u043d\u043e \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u043e \u0432 \u043a\u0430\u043a\u0443\u044e \u0441\u0442\u043e\u0440\u043e\u043d\u0443 In\u00a0[41]: Copied! <pre># random sample from c &gt; t\nstats.mannwhitneyu(c.ctr,\n                   t.ctr,\n                   alternative='greater').pvalue\n</pre> # random sample from c &gt; t stats.mannwhitneyu(c.ctr,                    t.ctr,                    alternative='greater').pvalue Out[41]: <pre>np.float64(3.0188242308889518e-56)</pre> <p>\u041e\u0431\u0440\u0430\u0442\u043d\u0430\u044f \u0430\u043b\u0442\u0435\u0440\u043d\u0430\u0442\u0438\u0432\u043d\u0430\u044f \u0433\u0438\u043f\u043e\u0442\u0438\u0437\u0430 \u041c\u0430\u043d\u043d\u0430-\u0423\u0438\u0442\u043d\u0438</p> In\u00a0[42]: Copied! <pre>stats.mannwhitneyu(c.ctr,\n                   t.ctr,\n                   alternative='less').pvalue\n</pre> stats.mannwhitneyu(c.ctr,                    t.ctr,                    alternative='less').pvalue Out[42]: <pre>np.float64(1.0)</pre> \u2871\u2877 \u0418\u0442\u043e\u0433\u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0442\u0435\u0441\u0442\u043e\u0432 <p>\u041f\u043e\u0434\u0432\u0435\u0434\u0435\u043c \u0438\u0442\u043e\u0433\u0438 \u0434\u0432\u0443\u0445 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0442\u0435\u0441\u0442\u043e\u0432</p> <ul> <li>\u0422\u0435\u0441\u0442 \u0421\u0442\u044c\u044e\u0434\u0435\u043d\u0442\u0430 \u043d\u0435 \u043f\u0440\u043e\u043a\u0440\u0430\u0441\u0438\u043b\u0441\u044f \u0438\u0437-\u0437\u0430 \u0437\u0430\u043b\u043e\u0436\u0435\u043d\u043d\u043e\u0433\u043e \u0432 t \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0443 \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f \u0441\u0440\u0435\u0434\u043d\u0435\u0433\u043e \u0432\u044b\u0431\u043e\u0440\u043e\u043a, \u0430 \u043e\u043d\u0438 \u0432 \u0434\u0432\u0443\u0445 \u0432\u044b\u0431\u043e\u0440\u043a\u0430\u0445 \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0438\u0434\u0435\u043d\u0442\u0438\u0447\u043d\u044b.</li> <li>\u0410 \u0432\u043e\u0442 \u0442\u0435\u0441\u0442 \u041c\u0430\u043d\u043d\u0430-\u0423\u0438\u0442\u043d\u0438 \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u0442 \u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432\u044b\u0431\u043e\u0440\u043e\u043a \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0434\u0440\u0443\u0433 \u0434\u0440\u0443\u0433\u0430 (\u043e\u0434\u043d\u043e\u0440\u043e\u0434\u043d\u043e\u0441\u0442\u044c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439), \u0430 \u043d\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432.</li> <li>\u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u043e\u043d \u0438 \u043d\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 \u0432\u044b\u0431\u043e\u0440\u043e\u043a \u0438 \u043f\u043e\u043a\u0430\u0437\u0430\u043b \u043d\u0430\u043b\u0438\u0447\u0438\u0435 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0437\u043d\u0430\u0447\u0438\u043c\u044b\u0445 \u0440\u0430\u0437\u043b\u0438\u0447\u0438\u0439 \u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0430\u0445.</li> <li>\u0427\u0442\u043e \u043a\u0430\u0441\u0430\u0435\u0442\u0441\u044f \u0433\u0438\u043f\u043e\u0442\u0435\u0437\u044b, \u0442\u043e \u043f\u043e\u0434\u0442\u0432\u0435\u0440\u0436\u0434\u0430\u0435\u0442\u0441\u044f <code>\u043e\u0431\u0440\u0430\u0442\u043d\u0430\u044f \u0430\u043b\u044c\u0442\u0435\u0440\u043d\u0430\u0442\u0438\u0432\u043d\u0430\u044f</code> \u0433\u0438\u043f\u043e\u0442\u0435\u0437\u0430:<ul> <li>\u0412\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u0447\u0438\u0441\u043b\u0430 CTR \u0438\u0437 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b \u0431\u043e\u043b\u044c\u0448\u0435\u0433\u043e \u043f\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044e, \u0447\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u0447\u0438\u0441\u043b\u0430 CTR \u0438\u0437 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b \u0432\u044b\u0448\u0435, \u0447\u0435\u043c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u0447\u0438\u0441\u043b\u0430 CTR \u0438\u0437 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b \u043c\u0435\u043d\u044c\u0448\u0435\u0433\u043e \u043f\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044e, \u0447\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u0447\u0438\u0441\u043b\u0430 CTR \u0438\u0437 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b.</li> </ul> </li> </ul> 6 | SMOOTHED CTR METRIC \u2871\u2877 \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432 <p>\u041f\u0440\u043e\u0432\u0435\u0434\u0435\u043c \u0435\u0449\u0435 \u043e\u0434\u0438\u043d \u0442\u0435\u0441\u0442</p> <p>\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e <code>CTR</code> \u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0430\u0445 \u043f\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0443 \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432 \u043d\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</p> In\u00a0[43]: Copied! <pre>df.groupby('exp_group')['views'].describe().T\n</pre> df.groupby('exp_group')['views'].describe().T Out[43]: exp_group 1 2 count 10079.000000 9952.000000 mean 66.299831 66.148513 std 45.043411 45.151210 min 2.000000 1.000000 25% 32.000000 32.000000 50% 56.000000 56.000000 75% 90.000000 91.000000 max 311.000000 321.000000 In\u00a0[178]: Copied! <pre># sns.set_palette(\"bright\")\n</pre> # sns.set_palette(\"bright\") In\u00a0[227]: Copied! <pre>sns.histplot(\n    df,\n    x=\"views\", \n    hue=\"exp_group\",\n    edgecolor=\".4\",\n    linewidth=.5,\n    palette=\"tab10\"\n)\n</pre> sns.histplot(     df,     x=\"views\",      hue=\"exp_group\",     edgecolor=\".4\",     linewidth=.5,     palette=\"tab10\" ) Out[227]: <pre>&lt;Axes: xlabel='views', ylabel='Count'&gt;</pre> <p>\u0412\u043e\u0437\u043d\u0438\u043a\u0430\u0435\u0442 \u0432\u0430\u0436\u043d\u044b\u0439 \u0432\u043e\u043f\u0440\u043e\u0441, \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0441\u043d\u043e \u043b\u0438 \u044d\u0442\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432</p> \u2871\u2877 \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u043a\u043e\u043b\u043e\u043d\u043a\u0443 \u0441 \u0441\u0433\u043b\u0430\u0436\u0435\u043d\u043d\u044b\u043c CTR <p>\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432 \u043d\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f</p> <ul> <li><code>CTR</code> \u0432 \u043e\u0431\u0435\u0438\u0445 \u0433\u0440\u0443\u043f\u043f\u0430\u0445 \u0441\u0438\u043b\u044c\u043d\u043e \u0437\u0430\u0448\u0443\u043c\u043b\u0435\u043d \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u043e\u0442 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0441 \u043d\u0438\u0437\u043a\u0438\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e\u043c \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432.</li> <li>\u0412 \u0434\u0432\u0443\u0445 \u0433\u0440\u0443\u043f\u043f\u0430\u0445 75-\u0439 \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u0438\u043b\u044c \u044d\u0442\u043e ~ 90 \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432 \u043d\u0430 \u044e\u0437\u0435\u0440\u0430!. \u0422\u0430\u043a\u043e\u0433\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432 \u043d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0434\u043b\u044f \u0442\u043e\u0447\u043d\u043e\u0439 \u043e\u0446\u0435\u043d\u043a\u0438 CTR.</li> <li>\u041f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u0446\u0435\u0434\u0443\u0440\u044b \u0441\u0433\u043b\u0430\u0436\u0438\u0432\u0430\u043d\u0438\u044f \u041b\u0430\u043f\u043b\u0430\u0441\u0430 \u043a \u043c\u0435\u0442\u0440\u0438\u043a\u0435 CTR \u043e\u0431\u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043e</li> </ul> $smoothed CTR_u = \\dfrac{clicks_u+\\alpha*globalCTR}{views_u + \\alpha}$ <p>\u0413\u0434\u0435 $\\alpha$ \u2014 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440.</p> <p>\u0418\u0434\u0435\u044f \u043f\u0440\u043e\u0441\u0442\u0430:</p> <ul> <li>\u041a\u043e\u0433\u0434\u0430 \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432 \u043c\u043d\u043e\u0433\u043e, \u0441\u0433\u043b\u0430\u0436\u0435\u043d\u043d\u044b\u0439 CTR \u043f\u043e\u0447\u0442\u0438 \u0440\u0430\u0432\u0435\u043d CTR \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f.</li> <li>\u041a\u043e\u0433\u0434\u0430 \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432 \u043c\u0430\u043b\u043e, \u0441\u0433\u043b\u0430\u0436\u0435\u043d\u043d\u044b\u0439 CTR \u043f\u043e\u0447\u0442\u0438 \u0440\u0430\u0432\u0435\u043d \u043e\u0431\u0449\u0435\u0433\u0440\u0443\u043f\u043f\u043e\u0432\u043e\u043c\u0443 CTR.</li> </ul> <p>\u0418\u043d\u0430\u0447\u0435 \u0433\u043e\u0432\u043e\u0440\u044f, \u0435\u0441\u043b\u0438 \u0443 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u043c\u043d\u043e\u0433\u043e \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432, \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0431\u044b\u0442\u044c \u0443\u0432\u0435\u0440\u0435\u043d\u044b, \u0447\u0442\u043e \u043a\u043b\u0438\u043a\u0438 / \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u044b \u2014 \u0445\u043e\u0440\u043e\u0448\u0430\u044f \u043e\u0446\u0435\u043d\u043a\u0430 \u0435\u0433\u043e CTR, \u0430 \u043a\u043e\u0433\u0434\u0430 \u0443 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u043e\u0432 \u043c\u0430\u043b\u043e, \u043c\u044b \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0446\u0435\u043d\u043a\u0438 \u043e\u0431\u0449\u0435\u0433\u0440\u0443\u043f\u043f\u043e\u0432\u043e\u0439 CTR</p> In\u00a0[45]: Copied! <pre>gctr1 = c['likes'].sum()/c['views'].sum()\ngctr2 = t['likes'].sum()/t['views'].sum()\n</pre> gctr1 = c['likes'].sum()/c['views'].sum() gctr2 = t['likes'].sum()/t['views'].sum() In\u00a0[46]: Copied! <pre>def get_smooth(x):\n    if x.exp_group == 1:\n        return (x.likes + 5 * gctr1) / (x.views + 5)\n    else:\n        return (x.likes + 5 * gctr2) / (x.views + 5)    \n\ndf['smooth_ctr'] = df.apply(get_smooth,axis=1)\n</pre> def get_smooth(x):     if x.exp_group == 1:         return (x.likes + 5 * gctr1) / (x.views + 5)     else:         return (x.likes + 5 * gctr2) / (x.views + 5)      df['smooth_ctr'] = df.apply(get_smooth,axis=1) In\u00a0[47]: Copied! <pre>c = df.query('exp_group == 1') # control \nt = df.query('exp_group == 2') # test \n</pre> c = df.query('exp_group == 1') # control  t = df.query('exp_group == 2') # test  \u2871\u2877 \u041f\u0440\u043e\u0432\u0435\u0434\u0435\u043c t-\u0442\u0435\u0441\u0442 \u0421\u0442\u044c\u044e\u0434\u0435\u043d\u0442\u0430 \u043d\u0430 \u0441\u0433\u043b\u0430\u0436\u0435\u043d\u043d\u044b\u0445 CTR <ul> <li>$H_0$ - \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f CTR \u0432 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0438 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0430\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043fa\u0445 \u0440\u0430\u0432\u043d\u044b</li> <li>$H_1$ - \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f CTR \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u0435 \u0432\u044b\u0448\u0435, \u0447\u0435\u043c \u0432 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439.</li> </ul> In\u00a0[228]: Copied! <pre>sns.displot(data=df,\n            x='ctr',\n            hue='exp_group',\n            kind='kde', \n            height=3.5, \n            aspect=1.6,\n            palette=\"tab10\")\n</pre> sns.displot(data=df,             x='ctr',             hue='exp_group',             kind='kde',              height=3.5,              aspect=1.6,             palette=\"tab10\") Out[228]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x798158802210&gt;</pre> In\u00a0[229]: Copied! <pre>sns.displot(data=df, \n            x='smooth_ctr',\n            hue='exp_group',\n            kind='kde', \n            height=3.5,\n            aspect=1.6,\n            palette=\"tab10\")\n</pre> sns.displot(data=df,              x='smooth_ctr',             hue='exp_group',             kind='kde',              height=3.5,             aspect=1.6,             palette=\"tab10\") Out[229]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7981588fd810&gt;</pre> In\u00a0[213]: Copied! <pre># t-test independent\nstats.ttest_ind(\n                c.smooth_ctr,\n                t.smooth_ctr,\n                equal_var=False,\n                alternative='less')\n</pre> # t-test independent stats.ttest_ind(                 c.smooth_ctr,                 t.smooth_ctr,                 equal_var=False,                 alternative='less') Out[213]: <pre>TtestResult(statistic=np.float64(2.2841320431616983), pvalue=np.float64(0.9888115092220721), df=np.float64(15791.866890424124))</pre> <p>\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043e\u0431\u0440\u0430\u0442\u043d\u0443\u044e \u0430\u043b\u044c\u0442\u0435\u0440\u043d\u0430\u0442\u0438\u0432\u043d\u0443\u044e \u0433\u0438\u043f\u043e\u0442\u0435\u0437\u0443:</p> <ul> <li>$H_0$  - \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f CTR \u0432 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0438 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0430\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043fa\u0445 \u0440\u0430\u0432\u043d\u044b.</li> <li>$H_1$  - \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f CTR \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u0435 \u043d\u0438\u0436\u0435, \u0447\u0435\u043c \u0432 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439.</li> </ul> In\u00a0[51]: Copied! <pre># can't reject \nstats.ttest_ind(\n                c.smooth_ctr,\n                t.smooth_ctr,\n                equal_var=False,\n                alternative='greater')\n</pre> # can't reject  stats.ttest_ind(                 c.smooth_ctr,                 t.smooth_ctr,                 equal_var=False,                 alternative='greater') Out[51]: <pre>TtestResult(statistic=np.float64(2.2841320431616983), pvalue=np.float64(0.011188490777927938), df=np.float64(15791.866890424124))</pre> <p>\u041e\u0442\u043b\u0438\u0447\u043d\u043e, \u0442\u0435\u0441\u0442 \u043f\u0440\u043e\u043a\u0440\u0430\u0441\u0438\u043b\u0441\u044f! pval &lt; 0.05</p> In\u00a0[52]: Copied! <pre># difference on means bw groups before after \nmean_ctr_before = df[df['exp_group']==1].ctr.mean()-df[df['exp_group']==2].ctr.mean()\nmean_ctr_after =  df[df['exp_group']==1].smooth_ctr.mean()-df[df['exp_group']==2].smooth_ctr.mean()\n\nprint(f'before: {round(mean_ctr_before,3)}, after: {round(mean_ctr_after,3)}')\n</pre> # difference on means bw groups before after  mean_ctr_before = df[df['exp_group']==1].ctr.mean()-df[df['exp_group']==2].ctr.mean() mean_ctr_after =  df[df['exp_group']==1].smooth_ctr.mean()-df[df['exp_group']==2].smooth_ctr.mean()  print(f'before: {round(mean_ctr_before,3)}, after: {round(mean_ctr_after,3)}') <pre>before: 0.001, after: 0.003\n</pre> In\u00a0[53]: Copied! <pre>round(mean_ctr_after / mean_ctr_before,3)\n</pre> round(mean_ctr_after / mean_ctr_before,3) Out[53]: <pre>np.float64(2.772)</pre> In\u00a0[54]: Copied! <pre>print(c.ctr.std() / c.smooth_ctr.std())\n</pre> print(c.ctr.std() / c.smooth_ctr.std()) <pre>1.1792320742547526\n</pre> <ul> <li>\u0420\u0430\u0437\u043d\u0438\u0446\u0430 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 CTR \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u043f\u043e\u0441\u043b\u0435 \u0441\u0433\u043b\u0430\u0436\u0438\u0432\u0430\u043d\u0438\u044f \u041b\u0430\u043f\u043b\u0430\u0441\u0430 \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u043b\u0430\u0441\u044c \u0432 2.7 \u0440\u0430\u0437!</li> <li>\u041f\u043e\u0441\u043b\u0435 \u0441\u0433\u043b\u0430\u0436\u0438\u0432\u0430\u043d\u0438\u044f \u041b\u0430\u043f\u043b\u0430\u0441\u0430 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0435 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0435 CTR \u0432 \u043e\u0431\u0435\u0438\u0445 \u0432\u044b\u0431\u043e\u0440\u043a\u0430\u0445 \u0443\u043c\u0435\u043d\u044c\u0448\u0438\u043b\u043e\u0441\u044c \u043d\u0430 ~ 17%</li> <li>\u0418\u0437-\u0437\u0430 \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u044f \u0440\u0430\u0437\u043d\u043e\u0441\u0442\u0438 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 \u0432 \u0442\u0440\u0438 \u0440\u0430\u0437\u0430 \u0438 \u0443\u043c\u0435\u043d\u044c\u0448\u0435\u043d\u0438\u044f \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0433\u043e \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u044f \u0443\u043c\u0435\u043d\u044c\u0448\u0438\u043b\u0438\u0441\u044c \u043e\u0446\u0435\u043d\u043a\u0438 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0438. \u042d\u0442\u043e \u0438 \u0434\u0430\u043b\u043e \u0443\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 t \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u044f \u0432 \u0442\u0440\u0438 \u0440\u0430\u0437\u0430, \u0447\u0442\u043e \u0438 \u0441\u043d\u0438\u0437\u0438\u043b\u043e p-value.</li> </ul> <p>\u0412\u044b\u0432\u043e\u0434: \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0441\u0433\u043b\u0430\u0436\u0438\u0432\u0430\u043d\u0438\u044f \u041b\u0430\u043f\u043b\u0430\u0441\u0430 \u0434\u043b\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0438 CTR \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u043b\u043e \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0442\u044c \u0447\u0443\u0432\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c t-\u043a\u0440\u0438\u0442\u0435\u0440\u0438\u044f \u0438 \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0438\u0442\u044c \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0437\u043d\u0430\u0447\u0438\u043c\u0443\u044e \u0440\u0430\u0437\u043d\u0438\u0446\u0443 \u0432 \u0433\u0440\u0443\u043f\u043f\u0430\u0445.</p> <p>\u041e\u0434\u043d\u0430\u043a\u043e \u0442\u0435\u043e\u0440\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0438, \u0447\u0442\u043e \u0435\u0433\u043e \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0441\u043e\u0432\u043f\u0430\u0434\u0430\u0435\u0442 \u0441 \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u044c\u044e \u043e\u0431\u0449\u0435\u0433\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u043b\u0430\u0439\u043a\u043e\u0432, \u043d\u0435\u0442 \u0438 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0442\u044c \u0442\u0430\u043a\u043e\u0439 \u043c\u0435\u0442\u043e\u0434 \u0441\u043b\u0435\u0434\u0443\u0435\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u043d\u0430 \u0441\u0432\u043e\u0439 \u0441\u0442\u0440\u0430\u0445 \u0438 \u0440\u0438\u0441\u043a.</p> \u2871\u2877 \u041f\u0440\u043e\u0432\u0435\u0434\u0435\u043c \u0422\u0435\u0441\u0442 \u041c\u0430\u043d\u043d\u0430-\u0423\u0438\u0442\u043d\u0438 \u043d\u0430 \u0441\u0433\u043b\u0430\u0436\u0435\u043d\u043d\u044b\u0445 CTR In\u00a0[55]: Copied! <pre># alternatie: random sample from control larger than random sample from test\nstats.mannwhitneyu(c.smooth_ctr,\n                   t.smooth_ctr,\n                   alternative='greater').pvalue\n</pre> # alternatie: random sample from control larger than random sample from test stats.mannwhitneyu(c.smooth_ctr,                    t.smooth_ctr,                    alternative='greater').pvalue Out[55]: <pre>np.float64(8.081982057135114e-62)</pre> In\u00a0[56]: Copied! <pre># alternative: random sample from control lower than random sample from control \nstats.mannwhitneyu(c.smooth_ctr,\n                   t.smooth_ctr,\n                   alternative='less').pvalue\n</pre> # alternative: random sample from control lower than random sample from control  stats.mannwhitneyu(c.smooth_ctr,                    t.smooth_ctr,                    alternative='less').pvalue Out[56]: <pre>np.float64(1.0)</pre> <ul> <li>\u041a\u0430\u043a \u0438 \u043e\u0436\u0438\u0434\u0430\u043b\u043e\u0441\u044c, \u0442\u0435\u0441\u0442 \u041c\u0430\u043d\u043d\u0430-\u0423\u0438\u0442\u043d\u0438 \u043f\u043e\u043a\u0430\u0437\u0430\u043b \u043d\u0430\u043b\u0438\u0447\u0438\u0435 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0437\u043d\u0430\u0447\u0438\u043c\u044b\u0445 \u0440\u0430\u0437\u043b\u0438\u0447\u0438\u0439 \u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0430\u0445 \u0441\u0433\u043b\u0430\u0436\u0435\u043d\u043d\u044b\u0445 <code>CTR</code></li> <li>\u0422\u0435\u0441\u0442 \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u0442 \u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432\u044b\u0431\u043e\u0440\u043e\u043a \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0434\u0440\u0443\u0433 \u0434\u0440\u0443\u0433\u0430 (\u043e\u0434\u043d\u043e\u0440\u043e\u0434\u043d\u043e\u0441\u0442\u044c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439), \u0430 \u043d\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432.</li> <li>\u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u043e\u043d \u0438 \u043d\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u0438\u0437\u043c\u0435\u043d\u0438\u0432\u0448\u0438\u0445\u0441\u044f \u043f\u043e\u0441\u043b\u0435 \u0441\u0433\u043b\u0430\u0436\u0438\u0432\u0430\u043d\u0438\u044f \u0441\u0440\u0435\u0434\u043d\u0438\u0445 \u0432\u044b\u0431\u043e\u0440\u043e\u043a \u0438 \u043d\u0430 \u0447\u0443\u0432\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0442\u0435\u0441\u0442\u0430 \u0442\u0430\u043a\u043e\u0435 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043d\u0435 \u043f\u043e\u0432\u043b\u0438\u044f\u043b\u043e.</li> </ul> 7 | NON-PARAMETRIC BOOTSTRAP <p>\u0421\u043e\u0437\u0434\u0430\u043b\u0438\u043c \u0434\u0432\u0435 \u043f\u0441\u0435\u0432\u0434\u043e\u0432\u044b\u0431\u043e\u0440\u043a\u0438</p> <ul> <li>\u0414\u043b\u044f \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f \u0441\u0440\u0435\u0434\u043d\u0435\u0433\u043e <code>CTR</code> \u0432 \u043f\u0435\u0440\u0432\u043e\u0439(\u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439) \u0438 \u0432\u0442\u043e\u0440\u043e\u0439(\u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445) \u0433\u0440\u0443\u043f\u043f\u0430\u0445, \u043f\u0440\u0438\u043c\u0435\u043d\u0438\u043c \u043d\u0435\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0431\u0443\u0442\u0441\u0442\u0440\u0435\u043f</li> <li>\u0411\u0443\u0434\u0435\u043c \u0440\u0430\u043d\u0434\u043e\u043c\u043d\u043e \u0433\u0435\u043d\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0441\u0435\u0432\u0434\u043e\u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0438\u0437 <code>CTR</code> \u0433\u0440\u0443\u043f\u043f 1 \u0438 2 \u0434\u043b\u0438\u043d\u043e\u0439 \u044d\u0442\u0438\u0445 \u0433\u0440\u0443\u043f\u043f \u0438 \u043a\u0430\u0436\u0434\u0443\u044e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u044e \u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u043f\u043e \u044d\u0442\u0438\u043c CTR \u0438 \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0442\u044c \u0438\u0445 \u0432 \u0441\u043f\u0438\u0441\u043e\u043a, \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043f\u043e\u0442\u043e\u043c \u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043c \u0430\u043d\u0430\u043b\u0438\u0437.</li> <li>\u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c <code>kde</code> \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0445\u0441\u044f \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 <code>CTR</code> \u0438\u0437 \u044d\u0442\u0438\u0445 100/1000 \u043f\u0441\u0435\u0432\u0434\u043e\u0432\u044b\u0431\u043e\u0440\u043e\u043a \u0438\u0437 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0438 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f.</li> <li>\u0412\u0438\u0437\u0443\u0430\u043b\u044c\u043d\u043e \u0437\u0430\u043c\u0435\u0442\u043d\u043e, \u043a\u0430\u043a \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f CTR \u0432 \u043f\u0441\u0435\u0432\u0434\u043e\u0432\u044b\u0431\u043e\u0440\u043a\u0430\u0445 \u0438\u0437 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b \u0431\u043e\u043b\u044c\u0448\u0435</li> </ul> In\u00a0[57]: Copied! <pre>import numpy as np\n\ndef boostrap_sample_mean(df,n_samples=10):\n\n    lst_means = []\n    for i in range(n_samples):\n        samples = np.random.choice(df['ctr'], \n                                   size=len(df), \n                                   replace=True)\n        lst_means.append(np.mean(samples))\n\n    return lst_means\n</pre> import numpy as np  def boostrap_sample_mean(df,n_samples=10):      lst_means = []     for i in range(n_samples):         samples = np.random.choice(df['ctr'],                                     size=len(df),                                     replace=True)         lst_means.append(np.mean(samples))      return lst_means In\u00a0[230]: Copied! <pre>cb = boostrap_sample_mean(c,100)\ntb = boostrap_sample_mean(t,100)\ncbs = pd.Series(cb,name='bootstap_ctl').to_frame()\ntbs = pd.Series(tb,name='bootstap_ctl').to_frame()\ncbs['exp_group'] = 0\ntbs['exp_group'] = 1\nm = pd.concat([cbs,tbs])\nsns.displot(data=m, \n            x='bootstap_ctl',\n            hue='exp_group',\n            kind='kde', \n            height=3.5, \n            aspect=1.6,\n            palette=\"tab10\")\n</pre> cb = boostrap_sample_mean(c,100) tb = boostrap_sample_mean(t,100) cbs = pd.Series(cb,name='bootstap_ctl').to_frame() tbs = pd.Series(tb,name='bootstap_ctl').to_frame() cbs['exp_group'] = 0 tbs['exp_group'] = 1 m = pd.concat([cbs,tbs]) sns.displot(data=m,              x='bootstap_ctl',             hue='exp_group',             kind='kde',              height=3.5,              aspect=1.6,             palette=\"tab10\") Out[230]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x798158799090&gt;</pre> In\u00a0[231]: Copied! <pre>cb = boostrap_sample_mean(c,1000)\ntb = boostrap_sample_mean(t,1000)\ncbs = pd.Series(cb,name='bootstap_ctl').to_frame()\ntbs = pd.Series(tb,name='bootstap_ctl').to_frame()\ncbs['exp_group'] = 0\ntbs['exp_group'] = 1\nm = pd.concat([cbs,tbs])\nsns.displot(data=m, \n            x='bootstap_ctl',\n            hue='exp_group',\n            kind='kde', \n            height=3.5,\n            aspect=1.6,\n            palette=\"tab10\")\n</pre> cb = boostrap_sample_mean(c,1000) tb = boostrap_sample_mean(t,1000) cbs = pd.Series(cb,name='bootstap_ctl').to_frame() tbs = pd.Series(tb,name='bootstap_ctl').to_frame() cbs['exp_group'] = 0 tbs['exp_group'] = 1 m = pd.concat([cbs,tbs]) sns.displot(data=m,              x='bootstap_ctl',             hue='exp_group',             kind='kde',              height=3.5,             aspect=1.6,             palette=\"tab10\") Out[231]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x79815873afd0&gt;</pre> <p>\u041f\u0440\u043e\u0432\u0435\u0434\u0435\u043c \u0442\u0435\u0441\u0442\u044b \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u043c \u043c\u0435\u0442\u043e\u0434\u043e\u043c \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c t-\u043a\u0440\u0438\u0442\u0435\u0440\u0438\u044f \u0421\u0442\u044c\u044e\u0434\u0435\u043d\u0442\u0430 \u0438 \u0435\u0433\u043e \u043d\u0435\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u043c \u0430\u043d\u0430\u043b\u043e\u0433\u043e\u043c - W \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0435\u043c \u0423\u0438\u043b\u043a\u043e\u043a\u0441\u043e\u043d\u0430 \u0432 \u0442\u0435\u0441\u0442\u0435 \u041c\u0430\u043d\u043d\u0430-\u0423\u0438\u0442\u043d\u0438 .</p> <ul> <li>$H_0$ - \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f <code>CTR</code> \u0432 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0438 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0430\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043fa\u0445 \u0440\u0430\u0432\u043d\u044b.</li> <li>$H_1$ - \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f <code>CTR</code> \u0432 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u0435 \u0432\u044b\u0448\u0435, \u0447\u0435\u043c \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439.</li> </ul> In\u00a0[60]: Copied! <pre>cbst = m.query('exp_group == 0')\ntbst = m.query('exp_group == 1')\n</pre> cbst = m.query('exp_group == 0') tbst = m.query('exp_group == 1') In\u00a0[61]: Copied! <pre>'''\n\nIn essence, this code tests whether the average of \u00a0cbst.bootstap_ctl\u00a0 is significantly greater than the average of tbst.bootstap_ctl\u00a0, without assuming equal variances between the groups. \n\nThe output \u00a0pvalue\u00a0 tells you the significance level of that test: a small p-value (e.g., less than 0.05) suggests strong evidence that \u00a0cbst.bootstap_ctl\u00a0 has a greater mean than \u00a0tbst.bootstap_ctl\u00a0.\n\nIn other words, there is very strong statistical evidence to reject the null hypothesis, you reject the null hypothesis in favor of the alternative hypothesis.\n\n'''\n\nstats.ttest_ind(cbst.bootstap_ctl, \n                tbst.bootstap_ctl, \n                equal_var=False,\n                alternative='greater').pvalue\n</pre> '''  In essence, this code tests whether the average of \u00a0cbst.bootstap_ctl\u00a0 is significantly greater than the average of tbst.bootstap_ctl\u00a0, without assuming equal variances between the groups.   The output \u00a0pvalue\u00a0 tells you the significance level of that test: a small p-value (e.g., less than 0.05) suggests strong evidence that \u00a0cbst.bootstap_ctl\u00a0 has a greater mean than \u00a0tbst.bootstap_ctl\u00a0.  In other words, there is very strong statistical evidence to reject the null hypothesis, you reject the null hypothesis in favor of the alternative hypothesis.  '''  stats.ttest_ind(cbst.bootstap_ctl,                  tbst.bootstap_ctl,                  equal_var=False,                 alternative='greater').pvalue Out[61]: <pre>np.float64(1.9380769874716835e-103)</pre> <ul> <li>\u041f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u043c \u043e\u0434\u043d\u043e\u0441\u0442\u043e\u0440\u043e\u043d\u044e\u044e \u0433\u0438\u043f\u043e\u0442\u0435\u0437\u0443 \u043e \u0431\u043e\u043b\u044c\u0448\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0438 \u0441\u0440\u0435\u0434\u043d\u0435\u0433\u043e \u0432 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u043b\u0435\u043d\u0438\u0438 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 <code>CTR</code> \u043f\u0441\u0435\u0434\u043e\u0432\u044b\u0431\u043e\u0440\u043e\u043a \u0438\u0437 \u0433\u0440\u0443\u043f\u043f\u044b 1, \u0442.\u0435. \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439, \u043e\u0442\u0432\u0435\u0440\u0433\u0430\u044f \u043d\u0443\u043b\u0435\u0432\u0443\u044e \u0433\u0438\u043f\u043e\u0442\u0438\u0437\u0443 \u0432 \u0441\u0442\u043e\u0440\u043e\u043d\u0443 \u0430\u043b\u044c\u0442\u0435\u0440\u043d\u0430\u0442\u0438\u0432\u043d\u043e\u0439</li> <li>\u041d\u043e\u0432\u044b\u0439 ML \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0432\u043e 2 \u0433\u0440\u0443\u043f\u043f\u0435 \u043f\u0440\u0438\u0432\u0435\u043b \u043a \u0441\u043d\u0438\u0436\u0435\u043d\u0438\u044e <code>CTR</code></li> </ul> In\u00a0[62]: Copied! <pre># random sample from control greater than random sample from test\nstats.mannwhitneyu(cbst.bootstap_ctl,\n                   tbst.bootstap_ctl, \n                   alternative='greater').pvalue\n</pre> # random sample from control greater than random sample from test stats.mannwhitneyu(cbst.bootstap_ctl,                    tbst.bootstap_ctl,                     alternative='greater').pvalue Out[62]: <pre>np.float64(1.9816531341532074e-96)</pre> \u2871\u2877 \u0412\u044b\u0432\u043e\u0434 <p>\u041f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u043c \u0433\u0438\u043f\u043e\u0442\u0435\u0437\u0443 \u043e \u0442\u043e\u043c:</p> <ul> <li>\u0412\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u0447\u0438\u0441\u043b\u0430 \u0441\u0440\u0435\u0434\u043d\u0435\u0433\u043e CTR \u0438\u0437 \u043f\u0441\u0435\u0432\u0434\u043e\u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b \u0431\u043e\u043b\u044c\u0448\u0435\u0433\u043e \u043f\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044e, \u0447\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u0447\u0438\u0441\u043b\u0430 \u0441\u0440\u0435\u0434\u043d\u0435\u0433\u043e CTR \u0438\u0437 \u043f\u0441\u0435\u0432\u0434\u043e\u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b \u0432\u044b\u0448\u0435, \u0447\u0435\u043c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u0447\u0438\u0441\u043b\u0430 \u0441\u0440\u0435\u0434\u0435\u043d\u0435\u0433\u043e CTR \u0438\u0437 \u043f\u0441\u0435\u0432\u0434\u043e\u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b \u043c\u0435\u043d\u044c\u0448\u0435\u0433\u043e \u043f\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044e, \u0447\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u0447\u0438\u0441\u043b\u0430 \u0441\u0440\u0435\u0434\u043d\u0435\u0433\u043e CTR \u0438\u0437 \u043f\u0441\u0435\u0432\u0434\u043e\u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b.</li> </ul> \u2871\u2877 \u041d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043e\u043a \u0431\u0443\u0442\u0441\u0442\u0440\u0435\u043f <p>\u041a \u043d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043a\u0443 \u0431\u0443\u0442\u0441\u0440\u0435\u043f\u0430 \u043c\u043e\u0436\u043d\u043e \u043e\u0442\u043d\u0435\u0441\u0442\u0438</p> <ul> <li>\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u043c\u0430\u043b\u043e\u0440\u0435\u0430\u043b\u0438\u0441\u0442\u0438\u0447\u043d\u043e\u0433\u043e \u043f\u0440\u0435\u0434\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u043e \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043f\u0435\u0440\u0435\u0432\u044b\u0431\u043e\u0440\u043e\u043a</li> <li>\u0438 \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0437\u0430\u0442\u0440\u0430\u0442\u044b \u043f\u0440\u0438 \u0438\u0445 \u043c\u043d\u043e\u0433\u043e\u043a\u0440\u0430\u0442\u043d\u043e\u043c \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0438.</li> </ul> 8 | POISSON BOOTSTRAP <p>\u0420\u0430\u0437\u043d\u043e\u0432\u0438\u0434\u043d\u043e\u0441\u0442\u044c \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0431\u0443\u0442\u0441\u0442\u0440\u0430\u043f\u0430 (bootstrap), \u043c\u0435\u0442\u043e\u0434\u0430 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0440\u0435\u0441\u0435\u043c\u043f\u043b\u0438\u043d\u0433\u0430. \u0412 \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u043e\u043c \u0431\u0443\u0442\u0441\u0442\u0440\u0430\u043f\u0435 \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u043d\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c \u0432\u044b\u0431\u0438\u0440\u0430\u044e\u0442\u0441\u044f \u0441 \u0432\u043e\u0437\u0432\u0440\u0430\u0442\u043e\u043c \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f \u0438\u0437 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u043f\u0440\u0438 \u044d\u0442\u043e\u043c \u0440\u0430\u0437\u043c\u0435\u0440 \u043a\u0430\u0436\u0434\u043e\u0439 \u0431\u0443\u0442\u0441\u0442\u0440\u0430\u043f-\u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0440\u0430\u0432\u0435\u043d \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u043c\u0443 \u043e\u0431\u044a\u0435\u043c\u0443. \u0412 Poisson bootstrap \u043f\u043e\u0434\u0445\u043e\u0434 \u043e\u0442\u043b\u0438\u0447\u0430\u0435\u0442\u0441\u044f \u043f\u043e \u0441\u043f\u043e\u0441\u043e\u0431\u0443 \u0432\u044b\u0431\u043e\u0440\u0430:</p> <ul> <li>\u041a\u0430\u0436\u0434\u043e\u043c\u0443 \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044e \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u043f\u0440\u0438\u0441\u0432\u0430\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u0447\u0438\u0441\u043b\u043e \u043f\u043e\u044f\u0432\u043b\u0435\u043d\u0438\u0439 (\u0432\u0435\u0441\u0430) \u0432 \u0431\u0443\u0442\u0441\u0442\u0440\u0430\u043f-\u0432\u044b\u0431\u043e\u0440\u043a\u0435, \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u0442\u0441\u044f \u0438\u0437 \u043f\u0443\u0430\u0441\u0441\u043e\u043d\u043e\u0432\u0441\u043a\u043e\u0433\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u043c 1</li> <li>\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0430\u0437, \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u043a\u0430\u0436\u0434\u043e\u0435 \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u0435 \u043f\u043e\u043f\u0430\u0434\u0430\u0435\u0442 \u0432 \u0431\u0443\u0442\u0441\u0442\u0440\u0430\u043f-\u0432\u044b\u0431\u043e\u0440\u043a\u0443, \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0439 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0439 \u0432\u0435\u043b\u0438\u0447\u0438\u043d\u043e\u0439.</li> <li>\u0420\u0430\u0437\u043c\u0435\u0440 \u0441\u0430\u043c\u043e\u0439 \u0431\u0443\u0442\u0441\u0442\u0440\u0430\u043f-\u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u0441\u044f \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0439 \u0432\u0435\u043b\u0438\u0447\u0438\u043d\u043e\u0439, \u0441 \u043e\u0436\u0438\u0434\u0430\u0435\u043c\u044b\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\u043c \u0440\u0430\u0432\u043d\u044b\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u043c\u0443 \u043e\u0431\u044a\u0435\u043c\u0443, \u043d\u043e \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043c\u043e\u0436\u0435\u0442 \u043e\u0442\u043b\u0438\u0447\u0430\u0442\u044c\u0441\u044f \u043e\u0442 \u043d\u0435\u0433\u043e.</li> <li>\u0412 \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u043e\u043c \u0431\u0443\u0442\u0441\u0442\u0440\u0430\u043f\u0435 \u0432\u0435\u0441\u0430 \u0441\u043b\u0435\u0434\u0443\u044e\u0442 \u043c\u0443\u043b\u044c\u0442\u0438\u043d\u043e\u043c\u0430\u043b\u044c\u043d\u043e\u043c\u0443 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044e \u0441 \u0441\u0443\u043c\u043c\u043e\u0439 \u0432\u0435\u0441\u043e\u0432 \u0444\u0438\u043a\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u0438 \u0440\u0430\u0432\u043d\u043e\u0439 \u0440\u0430\u0437\u043c\u0435\u0440\u0443 \u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u0430 \u0432 \u043f\u0443\u0430\u0441\u0441\u043e\u043d\u043e\u0432\u0441\u043a\u043e\u043c \u0432\u0435\u0441\u043e\u0432\u044b\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u044b.</li> </ul> In\u00a0[232]: Copied! <pre>def pboostrap_sample_mean(df,n_samples):\n    poisson_samples = stats.poisson(1).rvs((n_samples, len(df.likes.values))).astype(np.int64) # (n_samples,n_likes)\n    return (poisson_samples*df.likes.values).sum(axis=1)/(poisson_samples*df.views.values).sum(axis=1)\n\ncb = pboostrap_sample_mean(c,1000)\ntb = pboostrap_sample_mean(t,1000)\ncbs = pd.Series(cb,name='pbootstap_ctl').to_frame()\ntbs = pd.Series(tb,name='pbootstap_ctl').to_frame()\ncbs['exp_group'] = 0\ntbs['exp_group'] = 1\nm = pd.concat([cbs,tbs])\nsns.displot(data=m, \n            x='pbootstap_ctl',\n            hue='exp_group',\n            kind='kde', \n            height=3.5, \n            aspect=1.6,\n            palette=\"tab10\")\n</pre> def pboostrap_sample_mean(df,n_samples):     poisson_samples = stats.poisson(1).rvs((n_samples, len(df.likes.values))).astype(np.int64) # (n_samples,n_likes)     return (poisson_samples*df.likes.values).sum(axis=1)/(poisson_samples*df.views.values).sum(axis=1)  cb = pboostrap_sample_mean(c,1000) tb = pboostrap_sample_mean(t,1000) cbs = pd.Series(cb,name='pbootstap_ctl').to_frame() tbs = pd.Series(tb,name='pbootstap_ctl').to_frame() cbs['exp_group'] = 0 tbs['exp_group'] = 1 m = pd.concat([cbs,tbs]) sns.displot(data=m,              x='pbootstap_ctl',             hue='exp_group',             kind='kde',              height=3.5,              aspect=1.6,             palette=\"tab10\")  Out[232]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x798158601e50&gt;</pre> <ul> <li>\u041e\u0447\u0435\u0432\u0438\u0434\u043d\u043e, \u0447\u0442\u043e \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u044b\u0435 <code>CTR</code> \u0432 \u043f\u0441\u0435\u0432\u0434\u043e\u0432\u044b\u0431\u043e\u0440\u043a\u0430\u0445 \u0438\u0437 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b \u0431\u043e\u043b\u044c\u0448\u0435, \u0447\u0435\u043c \u0432 \u0442\u0435\u0441\u0442\u0432\u043e\u0439, \u0442\u0430\u043a \u043a\u0430\u043a \u0433\u0440\u0430\u0444\u0438\u043a\u0438 \u0438\u0445 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439 \u0434\u0430\u0436\u0435 \u043d\u0435 \u043f\u0435\u0440\u0435\u0441\u0435\u043a\u0430\u044e\u0442\u0441\u044f.</li> <li>\u041c\u043e\u0436\u043d\u043e \u0443\u0442\u0432\u0435\u0440\u0436\u0434\u0430\u0442\u044c, \u0447\u0442\u043e \u043d\u043e\u0432\u044b\u0439 ML \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c (\u0433\u0440\u0443\u043f\u043f\u0430 2) \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043ea\u0439 \u0433\u0440\u0443\u043f\u043f\u0435 2 (\u043a\u0440\u0430\u0441\u043d\u0430\u044f \u043d\u0430 \u0433\u0440\u0430\u0444\u0438\u043a\u0435 \u0432\u044b\u0448\u0435) \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u0442 \u043a \u0441\u043d\u0438\u0436\u0435\u043d\u0438\u044e \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u043e\u0433\u043e CTR</li> </ul> <p>\u041f\u0440\u043e\u0432\u0435\u0434\u0435\u043c \u0442\u0435\u0441\u0442 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u043c \u043c\u0435\u0442\u043e\u0434\u043e\u043c \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c t-\u043a\u0440\u0438\u0442\u0435\u0440\u0438\u044f \u0421\u0442\u044c\u044e\u0434\u0435\u043d\u0442\u0430.</p> In\u00a0[64]: Copied! <pre>stats.ttest_ind(cb, \n                tb, \n                equal_var=False,\n                alternative='greater')\n</pre> stats.ttest_ind(cb,                  tb,                  equal_var=False,                 alternative='greater') Out[64]: <pre>TtestResult(statistic=np.float64(200.72534507069847), pvalue=np.float64(0.0), df=np.float64(1505.7451048818073))</pre> <ul> <li>p-value = 0, \u0447\u0442\u043e \u043e\u0447\u0435\u0432\u0438\u0434\u043d\u043e \u043c\u0435\u043d\u044c\u0448\u0435 \u0443\u0440\u043e\u0432\u043d\u044f \u0437\u043d\u0430\u0447\u0438\u043c\u043e\u0441\u0442\u0438 \u0432 0.05.</li> <li>\u0421\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u043c\u043e\u0436\u0435\u043c \u043f\u0440\u0438\u043d\u044f\u0442\u044c \u0433\u0438\u043f\u043e\u0442\u0435\u0437\u0443, \u0447\u0442\u043e \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u044b\u0445 CTR \u043f\u0441\u0435\u0432\u0434\u043e\u0432\u044b\u0431\u043e\u0440\u043e\u043a \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b \u0431\u043e\u043b\u044c\u0448\u0435, \u0447\u0435\u043c \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u044b\u0445 CTR \u043f\u0441\u0435\u0432\u0434\u043e\u0432\u044b\u0431\u043e\u0440\u043e\u043a \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b.</li> <li>\u041d\u043e\u0432\u044b\u0439 ML \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0432\u043e 2 \u0433\u0440\u0443\u043f\u043f\u0435 \u043f\u0440\u0438\u0432\u0435\u043b \u043a \u0441\u043d\u0438\u0436\u0435\u043d\u0438\u044e CTR.</li> </ul> 9 | BUCKETISED STATISTICAL TESTS <p>\u041f\u043e\u0440\u044f\u0434\u043e\u043a \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0438:</p> <ul> <li>\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0430\u0437\u0431\u0438\u0442\u044c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043d\u0430 n \u00ab\u043a\u043e\u0440\u0437\u0438\u043d\u043e\u043a\u00bb (\u00ab\u0431\u0430\u043a\u0435\u0442\u043e\u0432\u00bb).</li> <li>\u041d\u0443\u0436\u043d\u043e \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c, \u0447\u0442\u043e \u043a\u0430\u0436\u0434\u044b\u0439 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u043f\u043e\u043f\u0430\u0434\u0435\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u0432 \u043e\u0434\u0438\u043d \u0431\u0430\u043a\u0435\u0442 \u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438 \u043f\u043e \u0431\u0430\u043a\u0435\u0442\u0430\u043c \u0431\u0443\u0434\u0443\u0442 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u044b \u0440\u0430\u0432\u043d\u043e\u043c\u0435\u0440\u043d\u043e.</li> <li>\u042d\u0442\u043e \u043c\u043e\u0436\u043d\u043e \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0442\u043e\u0433\u043e \u0436\u0435 \u0445\u0435\u0448\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0441 \u0441\u043e\u043b\u044c\u044e. \u041a\u0430\u0436\u0434\u044b\u0439 \u0431\u0430\u043a\u0435\u0442 \u0442\u0435\u043f\u0435\u0440\u044c \u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u0441\u044f \u043a\u0430\u043a \u0431\u044b \u00ab\u043c\u0435\u0442\u0430\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u043c\u00bb, \u0443\u0447\u0430\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u043c \u0432 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0435.</li> <li>\u0418 \u0432\u043d\u0443\u0442\u0440\u0438 \u0442\u0430\u043a\u043e\u0433\u043e \u00ab\u043c\u0435\u0442\u0430\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u00bb \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044c \u043d\u0443\u0436\u043d\u043e \u043a\u0430\u043a-\u0442\u043e \u0430\u0433\u0440\u0435\u0433\u0438\u0440\u043e\u0432\u0430\u0442\u044c.</li> <li>\u0411\u0443\u0434\u0435\u043c \u0432\u044b\u0447\u0435\u0441\u043b\u044f\u0442\u044c \u0441\u0440\u0435\u0434\u043d\u0435\u0435 CTR \u043f\u043e \u0431\u0430\u043a\u0435\u0442\u0443.</li> </ul> <p>\u0420\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u044b\u0435 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0435 \u0433\u0440\u0443\u043f\u043f\u044b \u043f\u043e 50 \u0431\u0430\u043a\u0435\u0442\u043e\u0432 \u0438 \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u0431\u0430\u043a\u0435\u0442\u0435 \u0441\u0440\u0435\u0434\u043d\u0435\u0435.</p> <p>\u0412 \u0438\u0442\u043e\u0433\u0435 \u043d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435 \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u0434\u0432\u0435 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0438\u0437 50 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 CTR \u043f\u043e \u044d\u0442\u0438\u043c \u00ab\u043c\u0435\u0442\u0430\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c\u00bb.</p> <p>\u042d\u0442\u043e \u043a\u0430\u043a \u0431\u0443\u0434\u0442\u043e \u043c\u044b \u0441\u0434\u0435\u043b\u0430\u043b\u0438 \u0431\u0443\u0442\u0441\u0442\u0440\u0435\u043f, \u043d\u043e \u0442\u043e\u043b\u044c\u043a\u043e \u043d\u0435 \u0441\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u0432 \u043f\u0441\u0435\u0432\u0434\u043e\u0432\u044b\u0431\u043e\u0440\u043a\u0443, \u0430 \u043e\u0442\u043a\u0443\u0441\u0438\u0432 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c \u043a\u0443\u0441\u043e\u043a \u0438\u0437 \u0438\u043c\u0435\u044e\u0449\u0435\u0439\u0441\u044f.</p> <p>\u0420\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u0434\u0432\u0435 \u0433\u0440\u0443\u043f\u043f\u044b \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u043d\u0430 50 \u0431\u0430\u043a\u0435\u0442\u043e\u0432 \u043f\u0440\u044f\u043c\u043e \u0432 ClickHouse \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0445\u044d\u0448\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f xxHash64:</p> <pre>query1 = \"\"\"\nselect \n    exp_group, \n    bucket,\n    sum(likes)/sum(views) as bucket_ctr\nfrom \n    (\n    select \n        exp_group, \n        xxHash64(user_id)%50 as bucket,\n        user_id,\n        sum(action = 'like') as likes,\n        sum(action = 'view') as views,\n        likes/views as ctr\n    from {db}.feed_actions \n    where toDate(time) between '2023-01-01' and '2023-01-07'\n          and exp_group in (1,2)\n    group by exp_group, bucket, user_id\n    )\ngroup by exp_group, bucket\n\"\"\"\n\ndf1 = ph.read_clickhouse(query1, \n                        connection = connection)\n</pre> <p>\u0412\u044b\u0433\u0440\u0443\u0437\u0438\u0432 \u0434\u0430\u043d\u043d\u044b\u0435, \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0438\u0445 \u0438\u0437 \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0444\u0430\u0438\u043b\u0430</p> In\u00a0[65]: Copied! <pre>import os; os.listdir()\nimport pandas as pd\n\nbu = pd.read_csv('bucket_ctr.csv',sep=',',index_col=[0])\nbu.head()\n</pre> import os; os.listdir() import pandas as pd  bu = pd.read_csv('bucket_ctr.csv',sep=',',index_col=[0]) bu.head() Out[65]: exp_group bucket bucket_ctr 0 1 0 0.196481 1 2 0 0.194262 2 1 1 0.211887 3 2 1 0.215328 4 1 2 0.206271 In\u00a0[66]: Copied! <pre># \u0434\u0435\u043b\u0438\u043c \u043d\u0430 \u0433\u0440\u0443\u043f\u043f\u044b\nbc = bu[bu['exp_group']==1]\nbt = bu[bu['exp_group']==2]\n</pre> # \u0434\u0435\u043b\u0438\u043c \u043d\u0430 \u0433\u0440\u0443\u043f\u043f\u044b bc = bu[bu['exp_group']==1] bt = bu[bu['exp_group']==2] In\u00a0[67]: Copied! <pre>round(stats.ttest_ind(bc.bucket_ctr,\n                bt.bucket_ctr,\n                equal_var=False, \n                alternative='less').pvalue,5)\n</pre> round(stats.ttest_ind(bc.bucket_ctr,                 bt.bucket_ctr,                 equal_var=False,                  alternative='less').pvalue,5) Out[67]: <pre>np.float64(1.0)</pre> <ul> <li>\u0422\u0430\u043a\u043e\u0439 \u0432\u044b\u0441\u043e\u043a\u0438\u0439 p-value \u0433\u043e\u0432\u043e\u0440\u0438\u0442 \u043e \u0431\u043e\u043b\u044c\u0448\u043e\u043c \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0440\u0435\u0447\u0438\u0438 \u0441 \u0430\u043b\u0442\u0435\u0440\u0435\u0430\u0442\u0438\u0432\u043d\u043e\u0439 \u0433\u0438\u043f\u043e\u0442\u0435\u0437\u044b; \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0438 \u0441\u0440\u0435\u043d\u0435\u0433\u043e \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b \u043b\u0438\u0431\u043e \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0438\u0437 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b \u0431\u043e\u043b\u044c\u0448\u0435</li> <li>\u041d\u0435\u0442 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u043e\u0431\u043e\u0441\u043d\u043e\u0432\u0430\u043d\u0438\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0438\u0437 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043a\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b \u043c\u0435\u043d\u044c\u0448\u0435, \u043e\u043f\u0440\u043e\u0432\u0435\u0440\u0433\u0430\u0435\u043c \u043d\u0443\u043b\u0435\u0432\u0443\u044e \u0433\u0438\u043f\u043e\u0442\u0438\u0437\u0443</li> </ul> In\u00a0[68]: Copied! <pre>round(stats.ttest_ind(bc.bucket_ctr,\n                bt.bucket_ctr,\n                equal_var=False, \n                alternative='greater').pvalue,5)\n</pre> round(stats.ttest_ind(bc.bucket_ctr,                 bt.bucket_ctr,                 equal_var=False,                  alternative='greater').pvalue,5) Out[68]: <pre>np.float64(0.0)</pre> <ul> <li>\u041e\u0442\u0432\u0435\u0440\u0433\u0430\u0435\u043c \u043d\u0443\u043b\u0435\u0432\u0443\u044e \u0433\u0438\u043f\u043e\u0442\u0438\u0437\u0443 \u0432 \u043f\u043e\u043b\u044c\u0437\u0443 \u0430\u043b\u044c\u0442\u0435\u0440\u043d\u0430\u0442\u0438\u0432\u043d\u043e\u0439</li> <li>\u042d\u0442\u043e\u0442 \u043f\u043e\u0434\u0445\u043e\u0434 \u0442\u0430\u043a \u0436\u0435 \u0433\u043e\u0432\u043e\u0440\u0438\u0442 \u043e \u0442\u043e\u043c \u0447\u0442\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0441\u0440\u0435\u0434\u043d\u0435\u0433\u043e \u0431\u043e\u043b\u044c\u0448\u0435 \u0432 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u0435</li> </ul> In\u00a0[69]: Copied! <pre>stats.mannwhitneyu(bc.bucket_ctr, \n                   bt.bucket_ctr, \n                   alternative='greater')\n</pre> stats.mannwhitneyu(bc.bucket_ctr,                     bt.bucket_ctr,                     alternative='greater') Out[69]: <pre>MannwhitneyuResult(statistic=np.float64(1995.0), pvalue=np.float64(1.429981968246323e-07))</pre> <ul> <li>\u041e\u0442\u043a\u043b\u043e\u043d\u044f\u0435\u043c \u043d\u0443\u043b\u0435\u0432\u0443\u044e \u0433\u0438\u043f\u043e\u0442\u0438\u0437\u0443 \u0432 \u043f\u043e\u043b\u044c\u0437\u0443 \u0430\u043b\u044c\u0442\u0435\u0440\u043d\u0430\u0442\u0438\u0432\u043d\u043e\u0439, \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u0442\u043e\u0433\u043e \u0447\u0442\u043e \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0438 \u0438\u0437 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043a\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b \u0431\u043e\u043b\u044c\u0448\u0435</li> </ul> 10 | CONCLUSION <p>\u0412\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u043e \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0433\u0438\u043f\u043e\u0442\u0435\u0437\u044b \u043e \u0442\u043e\u043c \u0447\u0442\u043e \u043d\u043e\u0432\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0432\u043e 2-\u0439 \u0433\u0440\u0443\u043f\u043f\u0435 \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u0442 \u043a \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u044e CTR \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0438 \u043d\u0435\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0438\u0447\u0441\u0435\u043a\u0438\u0445 \u043c\u0435\u0442\u043e\u0434\u043e\u0432:</p> <ul> <li>t-\u0442\u0435\u0441\u0442 \u043d\u0430 \u0432\u0441\u0435\u0445 \u0433\u0440\u0443\u043f\u043f\u0430\u0445 \u0431\u0435\u0437 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438</li> <li>\u0422\u0435\u0441\u0442 \u041c\u0430\u043d\u043d\u0430-\u0423\u0438\u0442\u043d\u0438 \u043d\u0430 \u0432\u0441\u0435\u0445 \u0433\u0440\u0443\u043f\u043f\u0430\u0445 \u0431\u0435\u0437 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438</li> <li>\u041d\u0435\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0431\u0443\u0442\u0441\u0442\u0440\u0435\u043f</li> <li>\u041f\u0443\u0430\u0441\u0441\u043e\u043d\u043e\u0432\u0441\u043a\u0438\u0439 \u0431\u0443\u0442\u0441\u0442\u0440\u0435\u043f</li> <li>t-\u0442\u0435\u0441\u0442 \u0438 \u0442\u0435\u0441\u0442 \u041c\u0430\u043d\u043d\u0430-\u0423\u0438\u0442\u043d\u0438 \u043d\u0430 \u0441\u0433\u043b\u0430\u0436\u0435\u043d\u043d\u043e\u043c ctr (\u03b1=5)</li> <li>t-\u0442\u0435\u0441\u0442 \u0438 \u0442\u0435\u0441\u0442 \u041c\u0430\u043d\u043d\u0430-\u0423\u0438\u0442\u043d\u0438 \u043f\u043e\u0432\u0435\u0440\u0445 \u0431\u0430\u043a\u0435\u0442\u043d\u043e\u0433\u043e \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f</li> </ul> <p>\u041f\u043e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0441\u0435\u0440\u0438\u0438 \u0442\u0435\u0441\u0442\u043e\u0432, \u0434\u0430\u043d\u043d\u0430\u044f \u0433\u0438\u043f\u043e\u0442\u0435\u0437\u0430 \u0431\u044b\u043b\u0430 \u043e\u0442\u0432\u0435\u0440\u0433\u043d\u0443\u0442\u0430 \u0438 \u043f\u0440\u0438\u043d\u044f\u0442\u0430 \u043e\u0431\u0440\u0430\u0442\u043d\u0430\u044f:</p> <ul> <li>\u043d\u043e\u0432\u044b\u0439 ML \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u0435 \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u0442 \u043a \u0441\u043d\u0438\u0436\u0435\u043d\u0438\u044e CTR \u043f\u043e \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044e \u0441 CTR \u0432 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439</li> <li>\u041e\u0434\u043d\u0430\u043a\u043e \u043d\u0435 \u0432\u0441\u0435 \u0442\u0435\u0441\u0442\u044b \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0438\u043b\u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0437\u043d\u0430\u0447\u0438\u043c\u043e\u0435 \u0440\u0430\u0437\u043b\u0438\u0447\u0438\u0435 \u0432 \u0433\u0440\u0443\u043f\u043f\u0430\u0445.</li> <li>\u0422\u0435\u0441\u0442 \u0421\u0442\u044c\u044e\u0434\u0435\u043d\u0442\u0430 \u043d\u0430 \u0433\u0440\u0443\u043f\u043f\u0430\u0445 \u0431\u0435\u0437 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u043d\u0435 \u043f\u0440\u043e\u043a\u0440\u0430\u0441\u0438\u043b\u0441\u044f \u0438\u0437-\u0437\u0430 \u0437\u0430\u043b\u043e\u0436\u0435\u043d\u043d\u043e\u0433\u043e \u0432 t \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0443 \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f \u0441\u0440\u0435\u0434\u043d\u0435\u0433\u043e \u0432\u044b\u0431\u043e\u0440\u043e\u043a, \u0430 \u043e\u043d\u0438 \u0432 \u0434\u0432\u0443\u0445 \u0432\u044b\u0431\u043e\u0440\u043a\u0430\u0445 \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0438\u0434\u0435\u043d\u0442\u0438\u0447\u043d\u044b. \u041d\u043e \u043f\u043e\u0441\u043b\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0441\u0433\u043b\u0430\u0436\u0438\u0432\u0430\u043d\u0438\u044f \u041b\u0430\u043f\u043b\u0430\u0441\u0430 \u0434\u043b\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0438 CTR, \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u043b\u043e \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0442\u044c \u0447\u0443\u0432\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c t \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u044f, \u0431\u044b\u043b\u0430 \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0430 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0437\u043d\u0430\u0447\u0438\u043c\u0430\u044f \u0440\u0430\u0437\u043d\u0438\u0446\u0430 \u0432 \u0433\u0440\u0443\u043f\u043f\u0430\u0445 \u0438 \u043f\u0440\u0438\u043d\u044f\u0442\u0430 \u043e\u0431\u0440\u0430\u0442\u043d\u0430\u044f \u0430\u043b\u044c\u0442\u0435\u0440\u043d\u0430\u0442\u0438\u0432\u043d\u0430\u044f \u0433\u0438\u043f\u043e\u0442\u0435\u0437\u0430: C\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f CTR \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u0435 \u043d\u0438\u0436\u0435, \u0447\u0435\u043c \u0432 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439.</li> </ul> \u0420\u0435\u043a\u043e\u043c\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u044f: <ul> <li>\u041d\u0435 \u0440\u0430\u0441\u043a\u0430\u0442\u044b\u0432\u0430\u0442\u044c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u043d\u0430 \u0432\u0441\u0435\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439. </li> <li>\u0412\u043e-\u043f\u0435\u0440\u0432\u044b\u0445, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u043f\u0443\u0430\u0441\u0441\u043e\u043d\u043e\u0432\u0441\u043a\u0438\u0439 \u0431\u0443\u0442\u0441\u0442\u0440\u0435\u043f \u043f\u043e\u043a\u0430\u0437\u0430\u043b, \u0447\u0442\u043e CTR \u0432 \u0442\u0430\u0440\u0433\u0435\u0442 \u0433\u0440\u0443\u043f\u043f\u0435 \u0441\u0442\u0430\u043b \u043d\u0438\u0436\u0435, \u0447\u0435\u043c \u0432 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u0435.</li> <li>\u0412\u043e-\u0432\u0442\u043e\u0440\u044b\u0445, \u0441\u0442\u043e\u0438\u0442 \u0442\u0430\u043a\u0436\u0435 \u0440\u0430\u0437\u043e\u0431\u0440\u0430\u0442\u044c\u0441\u044f, \u043f\u043e\u0447\u0435\u043c\u0443 CTR \u0441\u0442\u0430\u043b \u0431\u0438\u043c\u043e\u0434\u0430\u043b\u044c\u043d\u044b\u043c.</li></ul>"},{"location":"portfolio/projects/bcgx_powerco/task1.html","title":"Task1","text":""},{"location":"portfolio/projects/bcgx_powerco/task1.html#key-roles-and-responsibilities-of-a-data-scientist-at-bcg-x","title":"Key roles and responsibilities of a Data Scientist at BCG X\u00b6","text":""},{"location":"portfolio/projects/bcgx_powerco/task1.html#background","title":"Background\u00b6","text":"<p>BCG X is transforming businesses using data science to help companies generate competitive advantage. To do this, we typically follow a 5-step methodology:</p> <ol> <li><code>Business understanding</code> &amp; <code>problem framing</code>: what is the context of this problem and why are they trying to solve it?</li> <li><code>Exploratory data analysis</code> &amp; <code>data cleaning</code>: what data are we working with, what does it look like and how can we make it better?</li> <li><code>Feature engineering</code>: can we enrich this dataset using our own expertise or third party information?</li> <li><code>Modeling</code> and <code>evaluation</code>: can we use this dataset to accurately make predictions? If so, are they reliable?</li> <li><code>Insights</code> &amp; <code>Recommendations</code>: how we can communicate the value of these predictions by explaining them in a way that matters to the business?</li> </ol> <p>The tasks in this program will be focused on using different parts of this methodology at different times, so you\u2019ll get a taste of the overall process.</p> <p>It\u2019s a really exciting time to be working with BCG X as more clients are needing data to drive key decisions. So, let\u2019s check out what case you\u2019ll be working on!</p>"},{"location":"portfolio/projects/bcgx_powerco/task1.html#the-brief-from-powerco","title":"The brief from PowerCo\u00b6","text":"<p>The Associate Director (AD) of the Data Science team held a team meeting to discuss the client brief. You\u2019ll be working closely with Estelle Altazin, a senior data scientist on your team.</p> <p>Here are the key takeaways from the meeting:</p> <ul> <li>Your client is PowerCo - a major gas and electricity utility that supplies to small and medium sized enterprises</li> <li>The energy market has had a lot of change in recent years and there are more options than ever for customers to choose from</li> <li>PowerCo are concerned about their customers leaving for better offers from other energy providers</li> <li>When a customer leaves to use another service provider, this is called churn</li> <li>This is becoming a big issue for PowerCo and they have engaged BCG to help diagnose the reason why their customers are churning</li> </ul> <p>During the meeting your AD discussed some potential reasons for this churn, one being how \u201csensitive\u201d the price is.</p> <ul> <li>In other words, how much is price a factor in a customer\u2019s choice to stay with or leave PowerCo?</li> <li>So, now it\u2019s time for you to investigate this hypothesis</li> </ul>"},{"location":"portfolio/projects/bcgx_powerco/task1.html#your-task-we-need-to-understand-powercos-problem-in-detail","title":"Your task - we need to understand PowerCo\u2019s problem in detail\u00b6","text":"<p>First things first, you and Estelle need to understand the problem that PowerCo is facing at a deeper level and plan how you\u2019ll tackle it. If you recall the 5 steps in the Data Science methodology, this is called \u201cbusiness understanding &amp; problem framing\u201d.</p> <p>Your AD wants you and Estelle to email him by COB today outlining:</p> <ul> <li>the data that we\u2019ll need from the client, and</li> <li>the techniques we\u2019ll use to investigate the issue.</li> </ul> <p>Use the text field below to write your email, here\u2019s what you\u2019ll need to include:</p> <p>You must formulate PowerCo\u2019s issue as a problem using the 5 step data science process and lay out the major steps needed to test it.</p> <ul> <li>What do you think are the key reasons for a customer deciding to stay with or switch energy providers? For example: price, is it clean energy, customer service, location etc.</li> <li>What data do you think would be useful in order to investigate these key reasons? E.g. customer purchasing trends over past 5 years, location of business etc.</li> <li>If you were to get this data, how could you analyse or visualize it to test whether these reasons may have an impact on churn?</li> </ul>"},{"location":"portfolio/projects/bcgx_powerco/task1.html#email","title":"Email\u00b6","text":"<p>Hi [AD],</p> <p>In order to test the hypothesis of whether churn is driven by the customers\u2019 price sensitivity, we would need to model churn probabilities of customers, and derive the effect of prices on churn rates.</p> <p>We would need the following data to be able to build the models.</p> <ul> <li>Customer data - which should include characteristics of each client, for example, industry, historical electricity consumption, date joined as customer etc.</li> <li>Churn data - which should indicate if customer has churned</li> <li>Historical price data \u2013 which should indicate the prices the client charges to each customer for both electricity and gas at granular time intervals</li> </ul> <p>Once we have the data, the work plan would be:</p> <ul> <li>We need to define what price sensitivity is and calculate it</li> <li>We need to prepare the data and engineer features</li> <li>Then, we can test our hypothesis using a binary classification model (e.g. Logistic Regression, Random Forest, Gradient Boosted Machines to name a few)</li> <li>We would choose a model from one of the tested algorithms based on the model complexity, the explainability, and the accuracy of the models.</li> <li>With the trained model, we would be able to extrapolate the extent to which price sensitivity influences churn</li> </ul> <p>Regards, [Your name]</p>"},{"location":"portfolio/projects/bcgx_powerco/task2.html","title":"Task2","text":"In\u00a0[5]: Copied! <pre>import pandas as pd\nfrom pandas import option_context\nimport dask.dataframe as dd\nimport os\n</pre> import pandas as pd from pandas import option_context import dask.dataframe as dd import os In\u00a0[6]: Copied! <pre># !head -5 price_data.csv\n!head -5 client_data.csv\n</pre> # !head -5 price_data.csv !head -5 client_data.csv <pre>id,channel_sales,cons_12m,cons_gas_12m,cons_last_month,date_activ,date_end,date_modif_prod,date_renewal,forecast_cons_12m,forecast_cons_year,forecast_discount_energy,forecast_meter_rent_12m,forecast_price_energy_off_peak,forecast_price_energy_peak,forecast_price_pow_off_peak,has_gas,imp_cons,margin_gross_pow_ele,margin_net_pow_ele,nb_prod_act,net_margin,num_years_antig,origin_up,pow_max,churn\n24011ae4ebbe3035111d65fa7c15bc57,foosdfpfkusacimwkcsosbicdxkicaua,0,54946,0,2013-06-15,2016-06-15,2015-11-01,2015-06-23,0.0,0,0.0,1.78,0.114481,0.098142,40.606701,t,0.0,25.44,25.44,2,678.99,3,lxidpiddsbxsbosboudacockeimpuepw,43.648,1\nd29c2c54acc38ff3c0614d0a653813dd,MISSING,4660,0,0,2009-08-21,2016-08-30,2009-08-21,2015-08-31,189.95,0,0.0,16.27,0.1457109999999999,0.0,44.31137796,f,0.0,16.38,16.38,1,18.89,6,kamkkxfxxuwbdslkwifmmcsiusiuosws,13.8,0\n764c75f661154dac3a6c254cd082ea7d,foosdfpfkusacimwkcsosbicdxkicaua,544,0,0,2010-04-16,2016-04-16,2010-04-16,2015-04-17,47.96,0,0.0,38.72,0.165794,0.087899,44.31137796,f,0.0,28.6,28.6,1,6.6,6,kamkkxfxxuwbdslkwifmmcsiusiuosws,13.856,0\nbba03439a292a1e166f80264c16191cb,lmkebamcaaclubfxadlmueccxoimlema,1584,0,0,2010-03-30,2016-03-30,2010-03-30,2015-03-31,240.04,0,0.0,19.83,0.146694,0.0,44.31137796,f,0.0,30.22,30.22,1,25.46,6,kamkkxfxxuwbdslkwifmmcsiusiuosws,13.2,0\n</pre> In\u00a0[7]: Copied! <pre>!ls . -l -h\n</pre> !ls . -l -h <pre>total 23M\n-rw-rw-r-- 1 andrey andrey 3.4M Feb  8 17:14  client_data.csv\n-rw-rw-r-- 1 andrey andrey  18M Feb  8 17:14  price_data.csv\n-rw-rw-r-- 1 andrey andrey  70K Feb  8 18:23 'Task 2 - eda_starter.ipynb'\n-rw-rw-r-- 1 andrey andrey 285K Feb 10 00:47  Task2-edit.ipynb\n-rw-rw-r-- 1 andrey andrey 287K Feb 16 20:58  Task2.ipynb\n-rw-rw-r-- 1 andrey andrey 587K Feb  8 21:36 'Task 2 - Model Answer - EDA.ipynb'\n</pre> In\u00a0[8]: Copied! <pre>path = 'task2'\nclients_path = os.path.join('','client_data.csv')\nprices_path = os.path.join('','price_data.csv')\n</pre> path = 'task2' clients_path = os.path.join('','client_data.csv') prices_path = os.path.join('','price_data.csv') In\u00a0[9]: Copied! <pre># each row corresponds to data about the unique customer\nclient = dd.read_csv(clients_path,sep=',', blocksize=\"30M\")\n</pre> # each row corresponds to data about the unique customer client = dd.read_csv(clients_path,sep=',', blocksize=\"30M\") <p>The column data types are summarised below</p> In\u00a0[10]: Copied! <pre>client.dtypes\n</pre> client.dtypes Out[10]: <pre>id                                string[pyarrow]\nchannel_sales                     string[pyarrow]\ncons_12m                                    int64\ncons_gas_12m                                int64\ncons_last_month                             int64\ndate_activ                        string[pyarrow]\ndate_end                          string[pyarrow]\ndate_modif_prod                   string[pyarrow]\ndate_renewal                      string[pyarrow]\nforecast_cons_12m                         float64\nforecast_cons_year                          int64\nforecast_discount_energy                  float64\nforecast_meter_rent_12m                   float64\nforecast_price_energy_off_peak            float64\nforecast_price_energy_peak                float64\nforecast_price_pow_off_peak               float64\nhas_gas                           string[pyarrow]\nimp_cons                                  float64\nmargin_gross_pow_ele                      float64\nmargin_net_pow_ele                        float64\nnb_prod_act                                 int64\nnet_margin                                float64\nnum_years_antig                             int64\norigin_up                         string[pyarrow]\npow_max                                   float64\nchurn                                       int64\ndtype: object</pre> <p>We have a number of categorical string features which we can use to investigate why clients are churning</p> In\u00a0[11]: Copied! <pre># show all rows in the dataframe\nwith option_context('display.max_columns', None):\n    display(client.head(5))\n</pre> # show all rows in the dataframe with option_context('display.max_columns', None):     display(client.head(5)) id channel_sales cons_12m cons_gas_12m cons_last_month date_activ date_end date_modif_prod date_renewal forecast_cons_12m forecast_cons_year forecast_discount_energy forecast_meter_rent_12m forecast_price_energy_off_peak forecast_price_energy_peak forecast_price_pow_off_peak has_gas imp_cons margin_gross_pow_ele margin_net_pow_ele nb_prod_act net_margin num_years_antig origin_up pow_max churn 0 24011ae4ebbe3035111d65fa7c15bc57 foosdfpfkusacimwkcsosbicdxkicaua 0 54946 0 2013-06-15 2016-06-15 2015-11-01 2015-06-23 0.00 0 0.0 1.78 0.114481 0.098142 40.606701 t 0.00 25.44 25.44 2 678.99 3 lxidpiddsbxsbosboudacockeimpuepw 43.648 1 1 d29c2c54acc38ff3c0614d0a653813dd MISSING 4660 0 0 2009-08-21 2016-08-30 2009-08-21 2015-08-31 189.95 0 0.0 16.27 0.145711 0.000000 44.311378 f 0.00 16.38 16.38 1 18.89 6 kamkkxfxxuwbdslkwifmmcsiusiuosws 13.800 0 2 764c75f661154dac3a6c254cd082ea7d foosdfpfkusacimwkcsosbicdxkicaua 544 0 0 2010-04-16 2016-04-16 2010-04-16 2015-04-17 47.96 0 0.0 38.72 0.165794 0.087899 44.311378 f 0.00 28.60 28.60 1 6.60 6 kamkkxfxxuwbdslkwifmmcsiusiuosws 13.856 0 3 bba03439a292a1e166f80264c16191cb lmkebamcaaclubfxadlmueccxoimlema 1584 0 0 2010-03-30 2016-03-30 2010-03-30 2015-03-31 240.04 0 0.0 19.83 0.146694 0.000000 44.311378 f 0.00 30.22 30.22 1 25.46 6 kamkkxfxxuwbdslkwifmmcsiusiuosws 13.200 0 4 149d57cf92fc41cf94415803a877cb4b MISSING 4425 0 526 2010-01-13 2016-03-07 2010-01-13 2015-03-09 445.75 526 0.0 131.73 0.116900 0.100015 40.606701 f 52.32 44.91 44.91 1 47.98 6 kamkkxfxxuwbdslkwifmmcsiusiuosws 19.800 0 In\u00a0[12]: Copied! <pre># number of rows\nclient['id'].count().compute()\n</pre> # number of rows client['id'].count().compute() Out[12]: <pre>14606</pre> In\u00a0[13]: Copied! <pre>client[client['id'] == '038af19179925da21a25619c5a24b745'].compute()\n</pre> client[client['id'] == '038af19179925da21a25619c5a24b745'].compute() Out[13]: id channel_sales cons_12m cons_gas_12m cons_last_month date_activ date_end date_modif_prod date_renewal forecast_cons_12m ... has_gas imp_cons margin_gross_pow_ele margin_net_pow_ele nb_prod_act net_margin num_years_antig origin_up pow_max churn 11047 038af19179925da21a25619c5a24b745 foosdfpfkusacimwkcsosbicdxkicaua 3576 0 630 2013-06-03 2016-06-03 2015-04-29 2015-06-14 531.5 ... f 91.82 21.52 21.52 1 52.53 3 ldkssxwpmemidmecebumciepifcamkci 13.2 0 <p>1 rows \u00d7 26 columns</p> In\u00a0[14]: Copied! <pre>prices = dd.read_csv(prices_path,sep=',',blocksize=\"30M\")\nprices.head()\n</pre> prices = dd.read_csv(prices_path,sep=',',blocksize=\"30M\") prices.head() Out[14]: id price_date price_off_peak_var price_peak_var price_mid_peak_var price_off_peak_fix price_peak_fix price_mid_peak_fix 0 038af19179925da21a25619c5a24b745 2015-01-01 0.151367 0.0 0.0 44.266931 0.0 0.0 1 038af19179925da21a25619c5a24b745 2015-02-01 0.151367 0.0 0.0 44.266931 0.0 0.0 2 038af19179925da21a25619c5a24b745 2015-03-01 0.151367 0.0 0.0 44.266931 0.0 0.0 3 038af19179925da21a25619c5a24b745 2015-04-01 0.149626 0.0 0.0 44.266931 0.0 0.0 4 038af19179925da21a25619c5a24b745 2015-05-01 0.149626 0.0 0.0 44.266931 0.0 0.0 In\u00a0[15]: Copied! <pre>prices['id'].count().compute()\n</pre> prices['id'].count().compute() Out[15]: <pre>193002</pre> <p>Looks like we have pricing data for each of PowerCo's clients for the year 2015</p> In\u00a0[16]: Copied! <pre># prices for each customer by month\nprices[prices['id'] == '038af19179925da21a25619c5a24b745'].compute()\n</pre> # prices for each customer by month prices[prices['id'] == '038af19179925da21a25619c5a24b745'].compute() Out[16]: id price_date price_off_peak_var price_peak_var price_mid_peak_var price_off_peak_fix price_peak_fix price_mid_peak_fix 0 038af19179925da21a25619c5a24b745 2015-01-01 0.151367 0.0 0.0 44.266931 0.0 0.0 1 038af19179925da21a25619c5a24b745 2015-02-01 0.151367 0.0 0.0 44.266931 0.0 0.0 2 038af19179925da21a25619c5a24b745 2015-03-01 0.151367 0.0 0.0 44.266931 0.0 0.0 3 038af19179925da21a25619c5a24b745 2015-04-01 0.149626 0.0 0.0 44.266931 0.0 0.0 4 038af19179925da21a25619c5a24b745 2015-05-01 0.149626 0.0 0.0 44.266931 0.0 0.0 5 038af19179925da21a25619c5a24b745 2015-06-01 0.149626 0.0 0.0 44.266930 0.0 0.0 6 038af19179925da21a25619c5a24b745 2015-07-01 0.150321 0.0 0.0 44.444710 0.0 0.0 7 038af19179925da21a25619c5a24b745 2015-08-01 0.145859 0.0 0.0 44.444710 0.0 0.0 8 038af19179925da21a25619c5a24b745 2015-09-01 0.145859 0.0 0.0 44.444710 0.0 0.0 9 038af19179925da21a25619c5a24b745 2015-10-01 0.145859 0.0 0.0 44.444710 0.0 0.0 10 038af19179925da21a25619c5a24b745 2015-11-01 0.145859 0.0 0.0 44.444710 0.0 0.0 11 038af19179925da21a25619c5a24b745 2015-12-01 0.145859 0.0 0.0 44.444710 0.0 0.0 In\u00a0[50]: Copied! <pre>(client['churn'].value_counts(normalize=True)*100).compute()\n</pre> (client['churn'].value_counts(normalize=True)*100).compute() Out[50]: <pre>churn\n0    90.284814\n1     9.715186\nName: proportion, dtype: float64</pre> In\u00a0[17]: Copied! <pre>client.describe().compute().round(3)\n</pre> client.describe().compute().round(3) Out[17]: cons_12m cons_gas_12m cons_last_month forecast_cons_12m forecast_cons_year forecast_discount_energy forecast_meter_rent_12m forecast_price_energy_off_peak forecast_price_energy_peak forecast_price_pow_off_peak imp_cons margin_gross_pow_ele margin_net_pow_ele nb_prod_act net_margin num_years_antig pow_max churn count 14606.000 14606.000 14606.000 14606.000 14606.000 14606.000 14606.000 14606.000 14606.000 14606.000 14606.000 14606.000 14606.000 14606.000 14606.000 14606.000 14606.000 14606.000 mean 159220.286 28092.375 16090.270 1868.615 1399.763 0.967 63.087 0.137 0.050 43.130 152.787 24.565 24.563 1.292 189.265 4.998 18.135 0.097 std 573465.264 162973.059 64364.196 2387.572 3247.786 5.108 66.166 0.025 0.049 4.486 341.369 20.231 20.230 0.710 311.798 1.612 13.535 0.296 min 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1.000 0.000 1.000 3.300 0.000 25% 5674.750 0.000 0.000 494.995 0.000 0.000 16.180 0.116 0.000 40.607 0.000 14.280 14.280 1.000 50.712 4.000 12.500 0.000 50% 14115.500 0.000 792.500 1112.875 314.000 0.000 18.795 0.143 0.084 44.311 37.395 21.640 21.640 1.000 112.530 5.000 13.856 0.000 75% 40763.750 0.000 3383.000 2401.790 1745.750 0.000 131.030 0.146 0.099 44.311 193.980 29.880 29.880 1.000 243.098 6.000 19.172 0.000 max 6207104.000 4154590.000 771203.000 82902.830 175375.000 30.000 599.310 0.274 0.196 59.266 15042.790 374.640 374.640 32.000 24570.650 13.000 320.000 1.000 <ul> <li>If we look at the number of churned clients, they are quite low; 9.72%</li> <li>Nevertheless, our client has raised concer so we need to investigate what affects this churn</li> </ul> In\u00a0[18]: Copied! <pre>(client['churn'].value_counts(normalize=True).compute() * 100).round(2)\n</pre> (client['churn'].value_counts(normalize=True).compute() * 100).round(2) Out[18]: <pre>churn\n0    90.28\n1     9.72\nName: proportion, dtype: float64</pre> In\u00a0[19]: Copied! <pre>prices.describe().compute().round(3).T\n</pre> prices.describe().compute().round(3).T Out[19]: count mean std min 25% 50% 75% max price_off_peak_var 193002.0 0.141 0.025 0.0 0.126 0.146 0.152 0.281 price_peak_var 193002.0 0.055 0.050 0.0 0.000 0.085 0.102 0.230 price_mid_peak_var 193002.0 0.030 0.036 0.0 0.000 0.000 0.073 0.114 price_off_peak_fix 193002.0 43.334 5.410 0.0 40.729 44.267 44.445 59.445 price_peak_fix 193002.0 10.623 12.842 0.0 0.000 0.000 24.340 36.491 price_mid_peak_fix 193002.0 6.410 7.774 0.0 0.000 0.000 16.226 17.458 In\u00a0[20]: Copied! <pre>churned = client[client['churn'] == 1]\nnotchurn = client[client['churn'] == 0]\n</pre> churned = client[client['churn'] == 1] notchurn = client[client['churn'] == 0] <p>Lets look at the numbers relative to the churned clients</p> In\u00a0[21]: Copied! <pre>churn_counts = churned.groupby(['channel_sales']).agg(counts=('id','count')).compute()\nchurn_counts['count%'] = ((churn_counts['counts']/churn_counts['counts'].sum()) * 100).round(2)\nchurn_counts = churn_counts.reset_index()\nchurn_counts.sort_values(by='count%',ascending=False)\n</pre> churn_counts = churned.groupby(['channel_sales']).agg(counts=('id','count')).compute() churn_counts['count%'] = ((churn_counts['counts']/churn_counts['counts'].sum()) * 100).round(2) churn_counts = churn_counts.reset_index() churn_counts.sort_values(by='count%',ascending=False) Out[21]: channel_sales counts count% 0 foosdfpfkusacimwkcsosbicdxkicaua 820 57.79 2 MISSING 283 19.94 1 usilxuppasemubllopkaafesmlibmsdf 138 9.73 3 lmkebamcaaclubfxadlmueccxoimlema 103 7.26 4 ewpakwlliwisiwduibdlfmalxowmwpci 75 5.29 <ul> <li>Now lets look at relative to each group in channel_sales</li> <li>Lets also create a helper function which allows us to compare percentage wise the variation of churned to not churned clients</li> </ul> In\u00a0[22]: Copied! <pre>def compare_percentage(column:str,df:pd.DataFrame=client):\n\n    churn_counts = df.groupby([column,'churn']).agg(counts=('id','count')).compute()\n    churn_counts = churn_counts.reset_index()\n    all_counts = client.groupby([column]).agg(counts=('id','count')).compute()\n    \n    merged = churn_counts.merge(all_counts,left_on=column,right_index=True)\n    merged['percent%'] = ((merged['counts_x']/merged['counts_y'])*100).round(2)\n    merged = merged.groupby([column,'churn'])['percent%'].mean().to_frame().sort_index()\n    return merged\n\ncompare_percentage('channel_sales')\n</pre> def compare_percentage(column:str,df:pd.DataFrame=client):      churn_counts = df.groupby([column,'churn']).agg(counts=('id','count')).compute()     churn_counts = churn_counts.reset_index()     all_counts = client.groupby([column]).agg(counts=('id','count')).compute()          merged = churn_counts.merge(all_counts,left_on=column,right_index=True)     merged['percent%'] = ((merged['counts_x']/merged['counts_y'])*100).round(2)     merged = merged.groupby([column,'churn'])['percent%'].mean().to_frame().sort_index()     return merged  compare_percentage('channel_sales') Out[22]: percent% channel_sales churn MISSING 0 92.40 1 7.60 epumfxlbckeskwekxbiuasklxalciiuu 0 100.00 ewpakwlliwisiwduibdlfmalxowmwpci 0 91.60 1 8.40 fixdbufsefwooaasfcxdxadsiekoceaa 0 100.00 foosdfpfkusacimwkcsosbicdxkicaua 0 87.86 1 12.14 lmkebamcaaclubfxadlmueccxoimlema 0 94.41 1 5.59 sddiedcslfslkckwlfkdpoeeailfpeds 0 100.00 usilxuppasemubllopkaafesmlibmsdf 0 89.96 1 10.04 In\u00a0[23]: Copied! <pre>gas_counts = churned.groupby(['has_gas']).agg(counts=('id','count')).compute()\ngas_counts['count%'] = ((gas_counts['counts']/gas_counts['counts'].sum()) * 100).round(2)\ngas_counts.sort_values(by='count%',ascending=False)\n</pre> gas_counts = churned.groupby(['has_gas']).agg(counts=('id','count')).compute() gas_counts['count%'] = ((gas_counts['counts']/gas_counts['counts'].sum()) * 100).round(2) gas_counts.sort_values(by='count%',ascending=False) Out[23]: counts count% has_gas f 1202 84.71 t 217 15.29 In\u00a0[24]: Copied! <pre>compare_percentage('has_gas')\n</pre> compare_percentage('has_gas') Out[24]: percent% has_gas churn f 0 89.95 1 10.05 t 0 91.81 1 8.19 In\u00a0[25]: Copied! <pre>import warnings; warnings.filterwarnings('ignore')\n\nclient.select_dtypes(include=['float64','int64']).corr().round(2).compute().loc['churn'].to_frame()\n</pre> import warnings; warnings.filterwarnings('ignore')  client.select_dtypes(include=['float64','int64']).corr().round(2).compute().loc['churn'].to_frame() Out[25]: churn cons_12m -0.05 cons_gas_12m -0.04 cons_last_month -0.05 forecast_cons_12m 0.01 forecast_cons_year -0.00 forecast_discount_energy 0.02 forecast_meter_rent_12m 0.04 forecast_price_energy_off_peak -0.01 forecast_price_energy_peak 0.03 forecast_price_pow_off_peak 0.01 imp_cons -0.00 margin_gross_pow_ele 0.10 margin_net_pow_ele 0.10 nb_prod_act -0.01 net_margin 0.04 num_years_antig -0.07 pow_max 0.03 churn 1.00 <ul> <li>From the results we can see that most numeric features are very mildly linearly correlated to churn</li> <li>This metric doesn't quite give us enough detail about the effects of the number of products</li> </ul> In\u00a0[26]: Copied! <pre>client['nb_prod_act_cat'] = client['nb_prod_act'].astype('category')\ncompare_percentage('nb_prod_act_cat')\n</pre> client['nb_prod_act_cat'] = client['nb_prod_act'].astype('category') compare_percentage('nb_prod_act_cat') Out[26]: percent% nb_prod_act_cat churn 1 0 90.02 1 9.98 2 0 91.49 1 8.51 3 0 90.06 1 9.94 4 0 90.00 1 10.00 5 0 90.32 1 9.68 6 0 100.00 1 0.00 8 0 100.00 1 0.00 9 0 100.00 1 0.00 10 0 100.00 1 0.00 32 0 100.00 1 0.00 In\u00a0[27]: Copied! <pre>prices.head()\n</pre> prices.head() Out[27]: id price_date price_off_peak_var price_peak_var price_mid_peak_var price_off_peak_fix price_peak_fix price_mid_peak_fix 0 038af19179925da21a25619c5a24b745 2015-01-01 0.151367 0.0 0.0 44.266931 0.0 0.0 1 038af19179925da21a25619c5a24b745 2015-02-01 0.151367 0.0 0.0 44.266931 0.0 0.0 2 038af19179925da21a25619c5a24b745 2015-03-01 0.151367 0.0 0.0 44.266931 0.0 0.0 3 038af19179925da21a25619c5a24b745 2015-04-01 0.149626 0.0 0.0 44.266931 0.0 0.0 4 038af19179925da21a25619c5a24b745 2015-05-01 0.149626 0.0 0.0 44.266931 0.0 0.0 <p>Lets get the aggregations for the standard deviation metrics for each customer, which is an indicator of volatility</p> In\u00a0[28]: Copied! <pre>cols = ['price_off_peak_var','price_peak_var','price_mid_peak_var',\n        'price_off_peak_fix','price_peak_fix','price_mid_peak_fix']\n\ncols_diff = ['diff_price_off_peak_var','diff_price_peak_var','diff_price_mid_peak_var',\n             'diff_price_off_peak_fix','diff_price_peak_fix','diff_price_mid_peak_fix']\n\ncols_diff_cumsum = ['cdiff_price_off_peak_var','cdiff_price_peak_var','cdiff_price_mid_peak_var',\n                    'cdiff_price_off_peak_fix','cdiff_price_peak_fix','cdiff_price_mid_peak_fix']\n\nstd_prices = prices.groupby('id')[cols].std().compute()\n</pre> cols = ['price_off_peak_var','price_peak_var','price_mid_peak_var',         'price_off_peak_fix','price_peak_fix','price_mid_peak_fix']  cols_diff = ['diff_price_off_peak_var','diff_price_peak_var','diff_price_mid_peak_var',              'diff_price_off_peak_fix','diff_price_peak_fix','diff_price_mid_peak_fix']  cols_diff_cumsum = ['cdiff_price_off_peak_var','cdiff_price_peak_var','cdiff_price_mid_peak_var',                     'cdiff_price_off_peak_fix','cdiff_price_peak_fix','cdiff_price_mid_peak_fix']  std_prices = prices.groupby('id')[cols].std().compute() In\u00a0[29]: Copied! <pre>client_churn = client[['id','churn']].copy()\n</pre> client_churn = client[['id','churn']].copy() In\u00a0[30]: Copied! <pre>client_churn_stats = client_churn.merge(std_prices,left_on='id',right_index=True).compute()\nclient_churn_stats.index = client_churn_stats['id']\nclient_churn_stats = client_churn_stats.drop(['id'],axis=1)\n</pre> client_churn_stats = client_churn.merge(std_prices,left_on='id',right_index=True).compute() client_churn_stats.index = client_churn_stats['id'] client_churn_stats = client_churn_stats.drop(['id'],axis=1) In\u00a0[31]: Copied! <pre>client_churn_stats.corr().round(2).loc['churn'].to_frame()\n</pre> client_churn_stats.corr().round(2).loc['churn'].to_frame() Out[31]: churn churn 1.00 price_off_peak_var 0.04 price_peak_var 0.02 price_mid_peak_var 0.02 price_off_peak_fix 0.02 price_peak_fix 0.02 price_mid_peak_fix 0.01 In\u00a0[32]: Copied! <pre>for col_diff,col in zip(cols_diff,cols):\n    prices[col_diff] = prices[col].diff()\n</pre> for col_diff,col in zip(cols_diff,cols):     prices[col_diff] = prices[col].diff() In\u00a0[33]: Copied! <pre># count the number of times there have been price hi\ndiff_price_off_peak_var_counter = prices.groupby('id').apply(lambda x: (x['diff_price_off_peak_var'] &gt; 0).sum()).compute()\ndiff_price_peak_var_counter = prices.groupby('id').apply(lambda x: (x['diff_price_peak_var'] &gt; 0).sum()).compute()\ndiff_price_mid_peak_var_counter = prices.groupby('id').apply(lambda x: (x['diff_price_mid_peak_var'] &gt; 0).sum()).compute()\n\ndiff_price_off_peak_fix_counter = prices.groupby('id').apply(lambda x: (x['diff_price_off_peak_fix'] &gt; 0).sum()).compute()\ndiff_price_peak_fix_counter = prices.groupby('id').apply(lambda x: (x['diff_price_peak_fix'] &gt; 0).sum()).compute()\ndiff_price_mid_peak_fix_counter = prices.groupby('id').apply(lambda x: (x['diff_price_mid_peak_fix'] &gt; 0).sum()).compute()\n</pre> # count the number of times there have been price hi diff_price_off_peak_var_counter = prices.groupby('id').apply(lambda x: (x['diff_price_off_peak_var'] &gt; 0).sum()).compute() diff_price_peak_var_counter = prices.groupby('id').apply(lambda x: (x['diff_price_peak_var'] &gt; 0).sum()).compute() diff_price_mid_peak_var_counter = prices.groupby('id').apply(lambda x: (x['diff_price_mid_peak_var'] &gt; 0).sum()).compute()  diff_price_off_peak_fix_counter = prices.groupby('id').apply(lambda x: (x['diff_price_off_peak_fix'] &gt; 0).sum()).compute() diff_price_peak_fix_counter = prices.groupby('id').apply(lambda x: (x['diff_price_peak_fix'] &gt; 0).sum()).compute() diff_price_mid_peak_fix_counter = prices.groupby('id').apply(lambda x: (x['diff_price_mid_peak_fix'] &gt; 0).sum()).compute() In\u00a0[34]: Copied! <pre>diff_price_mid_peak_fix_counter\n</pre> diff_price_mid_peak_fix_counter Out[34]: <pre>id\n0002203ffbb812588b632b9e628cc38d    2\n0004351ebdd665e6ee664792efc4fd13    0\n0010bcc39e42b3c2131ed2ce55246e3c    0\n0010ee3855fdea87602a5b7aba8e42de    2\n00114d74e963e47177db89bc70108537    0\n                                   ..\nffef185810e44254c3a4c6395e6b4d8a    2\nfffac626da707b1b5ab11e8431a4d0a2    0\nfffc0cacd305dd51f316424bbb08d1bd    2\nfffe4f5646aa39c7f97f95ae2679ce64    2\nffff7fa066f1fb305ae285bb03bf325a    2\nLength: 16096, dtype: int64</pre> In\u00a0[35]: Copied! <pre>price_hikes = pd.concat([diff_price_off_peak_var_counter,diff_price_peak_var_counter,diff_price_mid_peak_var_counter,\n                           diff_price_off_peak_fix_counter,diff_price_peak_fix_counter,diff_price_mid_peak_fix_counter],axis=1)\nprice_hikes.columns = cols_diff\nprice_hikes\n</pre> price_hikes = pd.concat([diff_price_off_peak_var_counter,diff_price_peak_var_counter,diff_price_mid_peak_var_counter,                            diff_price_off_peak_fix_counter,diff_price_peak_fix_counter,diff_price_mid_peak_fix_counter],axis=1) price_hikes.columns = cols_diff price_hikes Out[35]: diff_price_off_peak_var diff_price_peak_var diff_price_mid_peak_var diff_price_off_peak_fix diff_price_peak_fix diff_price_mid_peak_fix id 0002203ffbb812588b632b9e628cc38d 1 2 2 1 2 2 0004351ebdd665e6ee664792efc4fd13 2 0 0 2 0 0 0010bcc39e42b3c2131ed2ce55246e3c 3 0 0 1 0 0 0010ee3855fdea87602a5b7aba8e42de 0 1 2 2 2 2 00114d74e963e47177db89bc70108537 2 0 0 1 0 0 ... ... ... ... ... ... ... ffef185810e44254c3a4c6395e6b4d8a 2 2 2 1 2 2 fffac626da707b1b5ab11e8431a4d0a2 2 0 0 1 0 0 fffc0cacd305dd51f316424bbb08d1bd 3 2 2 1 2 2 fffe4f5646aa39c7f97f95ae2679ce64 0 1 2 2 2 2 ffff7fa066f1fb305ae285bb03bf325a 0 1 2 2 2 2 <p>16096 rows \u00d7 6 columns</p> In\u00a0[36]: Copied! <pre>client_churn = client[['id','churn']].copy()\n</pre> client_churn = client[['id','churn']].copy() In\u00a0[37]: Copied! <pre>client_churn_stats = client_churn.merge(price_hikes,left_on='id',right_index=True).compute()\n</pre> client_churn_stats = client_churn.merge(price_hikes,left_on='id',right_index=True).compute() In\u00a0[38]: Copied! <pre>client_churn_stats['sum'] = client_churn_stats[cols_diff].min(axis=1)\nclient_churn_stats.index = client_churn_stats['id']\nclient_churn_stats = client_churn_stats.drop(['id'],axis=1)\n</pre> client_churn_stats['sum'] = client_churn_stats[cols_diff].min(axis=1) client_churn_stats.index = client_churn_stats['id'] client_churn_stats = client_churn_stats.drop(['id'],axis=1) In\u00a0[39]: Copied! <pre>client_churn_stats.corr().round(2).loc['churn'].to_frame()\n</pre> client_churn_stats.corr().round(2).loc['churn'].to_frame() Out[39]: churn churn 1.00 diff_price_off_peak_var -0.01 diff_price_peak_var 0.02 diff_price_mid_peak_var 0.05 diff_price_off_peak_fix 0.00 diff_price_peak_fix 0.04 diff_price_mid_peak_fix 0.04 sum 0.03 In\u00a0[40]: Copied! <pre>for cdiff,diff in zip(cols_diff_cumsum,cols_diff):\n    prices[cdiff] = prices[diff].cumsum()\n</pre> for cdiff,diff in zip(cols_diff_cumsum,cols_diff):     prices[cdiff] = prices[diff].cumsum() In\u00a0[41]: Copied! <pre>prices.columns\n</pre> prices.columns Out[41]: <pre>Index(['id', 'price_date', 'price_off_peak_var', 'price_peak_var',\n       'price_mid_peak_var', 'price_off_peak_fix', 'price_peak_fix',\n       'price_mid_peak_fix', 'diff_price_off_peak_var', 'diff_price_peak_var',\n       'diff_price_mid_peak_var', 'diff_price_off_peak_fix',\n       'diff_price_peak_fix', 'diff_price_mid_peak_fix',\n       'cdiff_price_off_peak_var', 'cdiff_price_peak_var',\n       'cdiff_price_mid_peak_var', 'cdiff_price_off_peak_fix',\n       'cdiff_price_peak_fix', 'cdiff_price_mid_peak_fix'],\n      dtype='object')</pre> <p>Accumulated difference in price variations for each customer</p> In\u00a0[42]: Copied! <pre>pdprices = prices.compute()\nclient_churn = client[['id','churn']].compute().copy()\n</pre> pdprices = prices.compute() client_churn = client[['id','churn']].compute().copy() In\u00a0[43]: Copied! <pre>pdprices\n</pre> pdprices Out[43]: id price_date price_off_peak_var price_peak_var price_mid_peak_var price_off_peak_fix price_peak_fix price_mid_peak_fix diff_price_off_peak_var diff_price_peak_var diff_price_mid_peak_var diff_price_off_peak_fix diff_price_peak_fix diff_price_mid_peak_fix cdiff_price_off_peak_var cdiff_price_peak_var cdiff_price_mid_peak_var cdiff_price_off_peak_fix cdiff_price_peak_fix cdiff_price_mid_peak_fix 0 038af19179925da21a25619c5a24b745 2015-01-01 0.151367 0.000000 0.000000 44.266931 0.00000 0.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 038af19179925da21a25619c5a24b745 2015-02-01 0.151367 0.000000 0.000000 44.266931 0.00000 0.000000 0.000000 0.000000 0.000000 0.0 0.0 0.0 0.000000 0.000000 0.000000 0.000000 0.00000 0.000000 2 038af19179925da21a25619c5a24b745 2015-03-01 0.151367 0.000000 0.000000 44.266931 0.00000 0.000000 0.000000 0.000000 0.000000 0.0 0.0 0.0 0.000000 0.000000 0.000000 0.000000 0.00000 0.000000 3 038af19179925da21a25619c5a24b745 2015-04-01 0.149626 0.000000 0.000000 44.266931 0.00000 0.000000 -0.001741 0.000000 0.000000 0.0 0.0 0.0 -0.001741 0.000000 0.000000 0.000000 0.00000 0.000000 4 038af19179925da21a25619c5a24b745 2015-05-01 0.149626 0.000000 0.000000 44.266931 0.00000 0.000000 0.000000 0.000000 0.000000 0.0 0.0 0.0 -0.001741 0.000000 0.000000 0.000000 0.00000 0.000000 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 192997 16f51cdc2baa19af0b940ee1b3dd17d5 2015-08-01 0.119916 0.102232 0.076257 40.728885 24.43733 16.291555 -0.008161 -0.004169 -0.000054 0.0 0.0 0.0 -0.031451 0.102232 0.076257 -3.538046 24.43733 16.291555 192998 16f51cdc2baa19af0b940ee1b3dd17d5 2015-09-01 0.119916 0.102232 0.076257 40.728885 24.43733 16.291555 0.000000 0.000000 0.000000 0.0 0.0 0.0 -0.031451 0.102232 0.076257 -3.538046 24.43733 16.291555 192999 16f51cdc2baa19af0b940ee1b3dd17d5 2015-10-01 0.119916 0.102232 0.076257 40.728885 24.43733 16.291555 0.000000 0.000000 0.000000 0.0 0.0 0.0 -0.031451 0.102232 0.076257 -3.538046 24.43733 16.291555 193000 16f51cdc2baa19af0b940ee1b3dd17d5 2015-11-01 0.119916 0.102232 0.076257 40.728885 24.43733 16.291555 0.000000 0.000000 0.000000 0.0 0.0 0.0 -0.031451 0.102232 0.076257 -3.538046 24.43733 16.291555 193001 16f51cdc2baa19af0b940ee1b3dd17d5 2015-12-01 0.119916 0.102232 0.076257 40.728885 24.43733 16.291555 0.000000 0.000000 0.000000 0.0 0.0 0.0 -0.031451 0.102232 0.076257 -3.538046 24.43733 16.291555 <p>193002 rows \u00d7 20 columns</p> In\u00a0[44]: Copied! <pre>diff_price_off_peak_var_cumsum = pdprices.groupby('id',as_index=True)[['cdiff_price_off_peak_var']].last()\ndiff_price_peak_var_cumsum = pdprices.groupby('id',as_index=True)[['cdiff_price_peak_var']].last()\ndiff_price_mid_peak_var_cumsum = pdprices.groupby('id',as_index=True)[['cdiff_price_mid_peak_var']].last()\n\ndiff_price_off_peak_fix_cumsum = pdprices.groupby('id',as_index=True)[['cdiff_price_off_peak_fix']].last()\ndiff_price_peak_fix_cumsum = pdprices.groupby('id',as_index=True)[['cdiff_price_peak_fix']].last()\ndiff_price_mid_peak_fix_cumsum = pdprices.groupby('id',as_index=True)[['cdiff_price_mid_peak_fix']].last()\n</pre> diff_price_off_peak_var_cumsum = pdprices.groupby('id',as_index=True)[['cdiff_price_off_peak_var']].last() diff_price_peak_var_cumsum = pdprices.groupby('id',as_index=True)[['cdiff_price_peak_var']].last() diff_price_mid_peak_var_cumsum = pdprices.groupby('id',as_index=True)[['cdiff_price_mid_peak_var']].last()  diff_price_off_peak_fix_cumsum = pdprices.groupby('id',as_index=True)[['cdiff_price_off_peak_fix']].last() diff_price_peak_fix_cumsum = pdprices.groupby('id',as_index=True)[['cdiff_price_peak_fix']].last() diff_price_mid_peak_fix_cumsum = pdprices.groupby('id',as_index=True)[['cdiff_price_mid_peak_fix']].last() In\u00a0[45]: Copied! <pre>price_accumulations = pd.concat([diff_price_off_peak_var_cumsum,diff_price_peak_var_cumsum,diff_price_mid_peak_var_cumsum,\n                                 diff_price_off_peak_fix_cumsum,diff_price_peak_fix_cumsum,diff_price_mid_peak_fix_cumsum],axis=1)\nprice_accumulations\n# price_accumulations.corr().round(2).loc['churn'].to_frame()\n</pre> price_accumulations = pd.concat([diff_price_off_peak_var_cumsum,diff_price_peak_var_cumsum,diff_price_mid_peak_var_cumsum,                                  diff_price_off_peak_fix_cumsum,diff_price_peak_fix_cumsum,diff_price_mid_peak_fix_cumsum],axis=1) price_accumulations # price_accumulations.corr().round(2).loc['churn'].to_frame() Out[45]: cdiff_price_off_peak_var cdiff_price_peak_var cdiff_price_mid_peak_var cdiff_price_off_peak_fix cdiff_price_peak_fix cdiff_price_mid_peak_fix id 0002203ffbb812588b632b9e628cc38d -0.031461 0.101673 0.073719 -3.538046 24.43733 16.291555 0004351ebdd665e6ee664792efc4fd13 -0.007424 0.000000 0.000000 0.177779 0.00000 0.000000 0010bcc39e42b3c2131ed2ce55246e3c 0.049913 0.000000 0.000000 1.677779 0.00000 0.000000 0010ee3855fdea87602a5b7aba8e42de -0.038299 0.095385 0.069409 -3.538046 24.43733 16.291555 00114d74e963e47177db89bc70108537 -0.005927 0.000000 0.000000 -0.000001 0.00000 0.000000 ... ... ... ... ... ... ... ffef185810e44254c3a4c6395e6b4d8a -0.038879 0.094804 0.068829 -3.538046 24.43733 16.291555 fffac626da707b1b5ab11e8431a4d0a2 -0.006320 0.000000 0.000000 0.177779 0.00000 0.000000 fffc0cacd305dd51f316424bbb08d1bd 0.000032 0.126871 0.091394 -3.038046 24.93733 16.791555 fffe4f5646aa39c7f97f95ae2679ce64 -0.033192 0.100491 0.074516 -3.538046 24.43733 16.291555 ffff7fa066f1fb305ae285bb03bf325a -0.031451 0.102232 0.076257 -3.538046 24.43733 16.291555 <p>16096 rows \u00d7 6 columns</p> In\u00a0[46]: Copied! <pre>client_churn_stats = client_churn.merge(price_accumulations,left_on='id',right_index=True)\nclient_churn_stats = client_churn_stats.drop(['id'],axis=1)\nclient_churn_stats\n</pre> client_churn_stats = client_churn.merge(price_accumulations,left_on='id',right_index=True) client_churn_stats = client_churn_stats.drop(['id'],axis=1) client_churn_stats Out[46]: churn cdiff_price_off_peak_var cdiff_price_peak_var cdiff_price_mid_peak_var cdiff_price_off_peak_fix cdiff_price_peak_fix cdiff_price_mid_peak_fix 0 1 -0.005334 0.085483 0.000000 -0.000001 0.000000 0.000000 1 0 -0.003767 0.000000 0.000000 0.177779 0.000000 0.000000 2 0 0.016431 0.088409 0.000000 0.177779 0.000000 0.000000 3 0 -0.002781 0.000000 0.000000 0.177779 0.000000 0.000000 4 0 -0.031461 0.101673 0.073719 -3.538046 24.437330 16.291555 ... ... ... ... ... ... ... ... 14601 0 -0.011195 0.000000 0.000000 0.177779 0.000000 0.000000 14602 1 -0.049015 0.093181 0.070990 14.906537 36.490689 8.367731 14603 1 -0.031461 0.101673 0.073719 -3.538046 24.437330 16.291555 14604 0 -0.003767 0.000000 0.000000 0.177779 0.000000 0.000000 14605 0 0.014595 0.086905 0.000000 -0.000001 0.000000 0.000000 <p>14606 rows \u00d7 7 columns</p> In\u00a0[47]: Copied! <pre>client_churn_stats.corr().round(2).loc['churn'].to_frame()\n</pre> client_churn_stats.corr().round(2).loc['churn'].to_frame() Out[47]: churn churn 1.00 cdiff_price_off_peak_var -0.01 cdiff_price_peak_var 0.03 cdiff_price_mid_peak_var 0.05 cdiff_price_off_peak_fix 0.02 cdiff_price_peak_fix 0.05 cdiff_price_mid_peak_fix 0.04"},{"location":"portfolio/projects/bcgx_powerco/task2.html#data-exploration","title":"Data Exploration\u00b6","text":""},{"location":"portfolio/projects/bcgx_powerco/task2.html#1-background","title":"1. Background\u00b6","text":""},{"location":"portfolio/projects/bcgx_powerco/task2.html#11-what-is-asked-of-us","title":"1.1 | What is asked of us\u00b6","text":"<p><code>Notebook Overview</code></p> <ul> <li>How to investigate whether price sensitivity is the most influential factor for a customer churning</li> <li>Conduct exploratory data analysis</li> </ul> <p><code>Price sensitivity</code> is the degree to which demand changes when the cost of a product or service changes</p> <ul> <li>In the context of PowerCo, the \u201cdemand\u201d refers to the demand for energy consumption</li> <li>Price sensitivity is commonly measured using the price elasticity of demand, which states that some consumers won't pay more if a lower-priced option is available</li> </ul> <p>What is <code>price elasticity</code> of demand?</p> <ul> <li>Price elasticity of demand is a measurement of the change in consumption of a product in relation to a change in its price</li> </ul>"},{"location":"portfolio/projects/bcgx_powerco/task2.html#12-what-the-client-is-handing-over","title":"1.2 | What the client is handing over\u00b6","text":"<p>Our client has sent over 3 data sets (shown below):</p> <ul> <li><code>Historical customer</code>: Customer data such as usage, sign up date, forecasted usage etc</li> <li><code>Historical pricing</code>: variable and fixed pricing data etc</li> <li><code>Churn indicator</code>: whether each customer has churned or not</li> </ul> <p>You need to analyze the following things:</p> <ul> <li>The data types of each column</li> <li>Descriptive statistics of the dataset</li> <li>Distributions of columns</li> </ul>"},{"location":"portfolio/projects/bcgx_powerco/task2.html#13-file-previews","title":"1.3 | File Previews\u00b6","text":""},{"location":"portfolio/projects/bcgx_powerco/task2.html#2-dataset-preview","title":"2. Dataset Preview\u00b6","text":""},{"location":"portfolio/projects/bcgx_powerco/task2.html#21-clients-dataset","title":"2.1 | Clients Dataset\u00b6","text":"<p>Basic problem setup is a customer churn one, we have information about each unique customers of PowerCo</p> <ul> <li><code>activity_new</code>: Category of the company\u2019s activity</li> <li><code>channel_sales</code>: Code of the sales channel</li> <li><code>cons_12m</code>: Electricity consumption of the past 12 months</li> <li><code>cons_gas_12m</code>: Gas consumption of the past 12 months</li> <li><code>cons_last_month</code>: Electricity consumption of the last month</li> <li><code>date_activ</code>: Date of activation of the contract</li> <li><code>date_end</code>: Registered date of the end of the contract</li> <li><code>date_modif_prod</code>: Date of the last modification of the product</li> <li><code>date_renewal</code>: Date of the next contract renewal</li> <li><code>forecast_cons_12m</code>: Forecasted electricity consumption for next 12 months</li> <li><code>forecast_cons_year</code>: Forecasted electricity consumption for the next calendar year</li> <li><code>forecast_discount_energy</code>: Forecasted value of current discount</li> <li><code>forecast_meter_rent_12m</code>: Forecasted bill of meter rental for the next 2 months</li> <li><code>forecast_price_energy_off_peak</code>: Forecasted energy price for 1st period (off peak)</li> <li><code>forecast_price_energy_peak</code>: Forecasted energy price for 2nd period (peak)</li> <li><code>forecast_price_pow_off_peak</code>: Forecasted power price for 1st period (off peak)</li> <li><code>has_gas</code>: Indicated if client is also a gas client</li> <li><code>imp_cons</code>: Current paid consumption</li> <li><code>margin_gross_pow_ele</code>: Gross margin on power subscription</li> <li><code>margin_net_pow_ele</code>: Net margin on power subscription</li> <li><code>nb_prod_act</code>: Number of active products and services</li> <li><code>net_margin</code>: Total net margin</li> <li><code>num_years_antig</code>: Antiquity of the client (in number of years)</li> <li><code>origin_up</code>: Code of the electricity campaign the customer first subscribed to</li> <li><code>pow_max</code>: Subscribed power</li> <li><code>churn</code>: Has the client churned over the next 3 months</li> </ul>"},{"location":"portfolio/projects/bcgx_powerco/task2.html#22-electiricty-prices-data","title":"2.2 | Electiricty Prices Data\u00b6","text":"<p>Dataset contains for each of PowerCo's clients, the pricing variation for each month</p> <ul> <li><code>id</code> = client company identifier</li> <li><code>price_date</code> = reference date</li> <li><code>price_off_peak_var</code> = price of energy for the 1st period (off peak)</li> <li><code>price_peak_var</code> = price of energy for the 2nd period (peak)</li> <li><code>price_mid_peak_var</code> = price of energy for the 3rd period (mid peak)</li> <li><code>price_off_peak_fix</code> = price of power for the 1st period (off peak)</li> <li><code>price_peak_fix</code> = price of power for the 2nd period (peak)</li> <li><code>price_mid_peak_fix</code> = price of power for the 3rd period (mid peak)</li> </ul> <p>This data will allow us to check the hypothesis that was asked of us, we'll do that in a later section</p>"},{"location":"portfolio/projects/bcgx_powerco/task2.html#3-descriptive-statistics","title":"3. Descriptive Statistics\u00b6","text":"<p>Lets look at some of the basic univariate statistics and some more in depth relations that we can find in the data</p>"},{"location":"portfolio/projects/bcgx_powerco/task2.html#31-what-well-do","title":"3.1 | What we'll do\u00b6","text":"<ul> <li>We want to first do some on the surface statistic investigations and ultimately,</li> <li>we'll need to answer questions regarding what influences <code>churn</code> of customers</li> <li>First lets check the percentage of clients who have churned in the next 3 months</li> </ul>"},{"location":"portfolio/projects/bcgx_powerco/task2.html#32-customer-data","title":"3.2 | Customer data\u00b6","text":""},{"location":"portfolio/projects/bcgx_powerco/task2.html#33-price-data","title":"3.3 | Price Data\u00b6","text":"<p>Lets look at the statistics for each of the pricing columns for all customers</p>"},{"location":"portfolio/projects/bcgx_powerco/task2.html#4-what-influences-churn","title":"4. What influences churn\u00b6","text":"<p>Lets investigate what factors affect the <code>churn</code>, and relay them via statistics and visualisations</p> <ul> <li>We'll be limiting our investigation to customer features only</li> </ul>"},{"location":"portfolio/projects/bcgx_powerco/task2.html#41-sales-channel","title":"4.1 | Sales Channel\u00b6","text":"<ul> <li>Sales channel doesn't seem to be a linear features that affects churn</li> <li>Some groups which affect churn more than others, notably group <code>foosdfpfkusacimwkcsosbicdxkicaua</code> make up about half of the churns</li> </ul>"},{"location":"portfolio/projects/bcgx_powerco/task2.html#42-gas-active-clients","title":"4.2 |  Gas Active Clients\u00b6","text":"<ul> <li>One other features we should look at specifies whether the client also a gas client</li> <li>We can note that if the clients are also gas customers, the likelihood of them churning is substantially smaller</li> <li>This would definitely be a point to raise to prevent current users from switching to other companies</li> </ul>"},{"location":"portfolio/projects/bcgx_powerco/task2.html#43-correlation-to-churn","title":"4.3 |  Correlation to Churn\u00b6","text":"<ul> <li>We can also compare how the number of existing customer products affects their choices amonstt other numeric columns</li> <li>Logically, the higher the number of products a customer has, the lower the changes of them churning</li> </ul>"},{"location":"portfolio/projects/bcgx_powerco/task2.html#44-number-of-products","title":"4.4 |  Number of Products\u00b6","text":"<p>Lets check is more detail, if indeed there is any relation to churn</p> <ul> <li>We can see that beyond 5 products, the churn percentage is 0</li> <li>Anything below this number is about the same</li> </ul>"},{"location":"portfolio/projects/bcgx_powerco/task2.html#5-hypothesis-investigation","title":"5. Hypothesis Investigation\u00b6","text":"<p>We looked at various factors that can affect <code>churn</code> from the customer dataset. Lets now check the pricing for each customer and check the customers sensitivity to pricing.</p>"},{"location":"portfolio/projects/bcgx_powerco/task2.html#51-standard-deviations","title":"5.1 |  Standard Deviations\u00b6","text":"<p>Since we have the consumption data for each of the companies for the year of 2015, we will create new features to measure price sensitivity</p>"},{"location":"portfolio/projects/bcgx_powerco/task2.html#52-price-hike-counters","title":"5.2 |  Price Hike Counters\u00b6","text":"<p>We can also find the difference between each month's value &amp; count how many times the prices went up for each of the metrics in the dataset for each of PowerCo's customers</p>"},{"location":"portfolio/projects/bcgx_powerco/task2.html#53-price-accumulation","title":"5.3 | Price Accumulation\u00b6","text":"<p>Lastly, lets check how the accumulated prices impact the churn</p>"},{"location":"portfolio/projects/bcgx_powerco/task2.html#6-takeaways","title":"6. Takeaways\u00b6","text":"<p>Having been introduced to our new client in the first part</p> <ul> <li>Coming out of the first meeting, we need to test a hypothesis raised during this meeting; \"how much is price a factor in a customer\u2019s choice\"</li> <li>We take a first look at the data that was presented to us by our client PowerCo, exploring both the data about the customers' clients as well as their pricings for the entire year 2015</li> <li>The client has expressed concern about customers leaving, it was found that only 9.7% of the clients' customers have churned. A small portion, but enough for the client to raise concern</li> </ul>"},{"location":"portfolio/projects/bcgx_powerco/task2.html#churn-investigation","title":"Churn Investigation\u00b6","text":"<p>Feature relations</p> <p><code>sales channel</code> showed that about 60% of churned users came from this subgroup foosdfpfkusacimwkcsosbicdxkicaua. Out of the entire subset, this portion equated to roughly 12% of users.</p> <p>84% of users who churned didn't have a gas contract <code>has_gas</code> with the same supplier</p> <p>Insight into <code>num_years_antig</code> showes that churn tends to drop the longer a customer is a client, likewise gross margin on power subscription net margin on power subscription showed one of the higher linear correlations to churn, ie. the higher the values tend to be the more likelihood of a churn outcome</p> <p>Overall, linear correlations of features tended to be quite weak to churn</p> <p>The number of products <code>nb_prod_act</code> with our client PowerCo was more insightful, it was found that if the users had 5 products, the churn was 0</p> <p>Hypothesis check for pricing</p> <p>To check the brought up hypothesis, we decided to check a few extra for the pricing</p> <ul> <li><code>standard deviations</code> of the yearly prices tended to show very small correlation to churn, all indicating that if the pricing is not stable, tendecies to churn increased</li> <li><code>Price hikes</code>, for which we counted the number of times prices increased over the duration of the year all showed that curstomers tended to churn with increasing number of times prices were increased</li> <li>Lastly, we checked the <code>accumulating price</code> sum for the entire year, overall the correlation didn't change too much either, showing a linear correlation of 0.05 at maximum</li> </ul> <p>These factors tend to show that pricing and volatility aren't the most critical factors in a customer's decision to churn</p>"},{"location":"portfolio/projects/bcgx_powerco/task3.html","title":"Task3","text":"In\u00a0[1]: Copied! <pre>import warnings; warnings.filterwarnings('ignore')\n</pre> import warnings; warnings.filterwarnings('ignore') In\u00a0[3]: Copied! <pre>!head -2 clean_data_after_eda.csv\n</pre> !head -2 clean_data_after_eda.csv <pre>id,channel_sales,cons_12m,cons_gas_12m,cons_last_month,date_activ,date_end,date_modif_prod,date_renewal,forecast_cons_12m,forecast_cons_year,forecast_discount_energy,forecast_meter_rent_12m,forecast_price_energy_off_peak,forecast_price_energy_peak,forecast_price_pow_off_peak,has_gas,imp_cons,margin_gross_pow_ele,margin_net_pow_ele,nb_prod_act,net_margin,num_years_antig,origin_up,pow_max,var_year_price_off_peak_var,var_year_price_peak_var,var_year_price_mid_peak_var,var_year_price_off_peak_fix,var_year_price_peak_fix,var_year_price_mid_peak_fix,var_year_price_off_peak,var_year_price_peak,var_year_price_mid_peak,var_6m_price_off_peak_var,var_6m_price_peak_var,var_6m_price_mid_peak_var,var_6m_price_off_peak_fix,var_6m_price_peak_fix,var_6m_price_mid_peak_fix,var_6m_price_off_peak,var_6m_price_peak,var_6m_price_mid_peak,churn\n24011ae4ebbe3035111d65fa7c15bc57,foosdfpfkusacimwkcsosbicdxkicaua,0,54946,0,2013-06-15,2016-06-15,2015-11-01,2015-06-23,0.0,0,0.0,1.78,0.114481,0.098142,40.606701,t,0.0,25.44,25.44,2,678.99,3,lxidpiddsbxsbosboudacockeimpuepw,43.648,6.129362790151515e-05,2.6276049696969715e-05,0.00044027625881060595,1.1027846956785217,49.55070278697598,22.022535052251545,1.102845989306423,49.55072906302568,22.022975328510356,0.00013145352656666664,4.10083842666666e-05,0.0009084736945666668,2.0862936898302333,99.53051658064774,44.235793835783426,2.0864251433568,99.53055758903201,44.23670230947799,1\n</pre> In\u00a0[4]: Copied! <pre>import pandas as pd\nfrom pandas import option_context\nimport dask.dataframe as dd\nimport numpy as np\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set(style='whitegrid')\n</pre> import pandas as pd from pandas import option_context import dask.dataframe as dd import numpy as np import os import numpy as np import matplotlib.pyplot as plt import seaborn as sns; sns.set(style='whitegrid') In\u00a0[8]: Copied! <pre>path = ''\npath_csv = os.path.join(path,'clean_data_after_eda.csv')\n\n# each row corresponds to data about the unique customer\nclients = dd.read_csv(path_csv,\n                    sep=',', \n                    blocksize=\"30M\")\n\n# standard processing\nclients['date_activ'] = dd.to_datetime(clients['date_activ'], format='%Y-%m-%d')\nclients['date_end'] = dd.to_datetime(clients['date_end'], format='%Y-%m-%d')\nclients['date_modif_prod'] = dd.to_datetime(clients['date_modif_prod'], format='%Y-%m-%d')\nclients['date_renewal'] = dd.to_datetime(clients['date_renewal'], format='%Y-%m-%d')\n\n# show all rows in the dataframe\nwith option_context('display.max_columns', None):\n    display(clients.head(5))\n</pre> path = '' path_csv = os.path.join(path,'clean_data_after_eda.csv')  # each row corresponds to data about the unique customer clients = dd.read_csv(path_csv,                     sep=',',                      blocksize=\"30M\")  # standard processing clients['date_activ'] = dd.to_datetime(clients['date_activ'], format='%Y-%m-%d') clients['date_end'] = dd.to_datetime(clients['date_end'], format='%Y-%m-%d') clients['date_modif_prod'] = dd.to_datetime(clients['date_modif_prod'], format='%Y-%m-%d') clients['date_renewal'] = dd.to_datetime(clients['date_renewal'], format='%Y-%m-%d')  # show all rows in the dataframe with option_context('display.max_columns', None):     display(clients.head(5)) id channel_sales cons_12m cons_gas_12m cons_last_month date_activ date_end date_modif_prod date_renewal forecast_cons_12m forecast_cons_year forecast_discount_energy forecast_meter_rent_12m forecast_price_energy_off_peak forecast_price_energy_peak forecast_price_pow_off_peak has_gas imp_cons margin_gross_pow_ele margin_net_pow_ele nb_prod_act net_margin num_years_antig origin_up pow_max var_year_price_off_peak_var var_year_price_peak_var var_year_price_mid_peak_var var_year_price_off_peak_fix var_year_price_peak_fix var_year_price_mid_peak_fix var_year_price_off_peak var_year_price_peak var_year_price_mid_peak var_6m_price_off_peak_var var_6m_price_peak_var var_6m_price_mid_peak_var var_6m_price_off_peak_fix var_6m_price_peak_fix var_6m_price_mid_peak_fix var_6m_price_off_peak var_6m_price_peak var_6m_price_mid_peak churn 0 24011ae4ebbe3035111d65fa7c15bc57 foosdfpfkusacimwkcsosbicdxkicaua 0 54946 0 2013-06-15 2016-06-15 2015-11-01 2015-06-23 0.00 0 0.0 1.78 0.114481 0.098142 40.606701 t 0.00 25.44 25.44 2 678.99 3 lxidpiddsbxsbosboudacockeimpuepw 43.648 0.000061 2.627605e-05 0.000440 1.102785 49.550703 22.022535 1.102846 4.955073e+01 22.022975 0.000131 4.100838e-05 9.084737e-04 2.086294 99.530517 44.235794 2.086425 9.953056e+01 4.423670e+01 1 1 d29c2c54acc38ff3c0614d0a653813dd MISSING 4660 0 0 2009-08-21 2016-08-30 2009-08-21 2015-08-31 189.95 0 0.0 16.27 0.145711 0.000000 44.311378 f 0.00 16.38 16.38 1 18.89 6 kamkkxfxxuwbdslkwifmmcsiusiuosws 13.800 0.000005 6.089453e-04 0.000000 0.006465 0.000000 0.000000 0.006470 6.089453e-04 0.000000 0.000003 1.217891e-03 0.000000e+00 0.009482 0.000000 0.000000 0.009485 1.217891e-03 0.000000e+00 0 2 764c75f661154dac3a6c254cd082ea7d foosdfpfkusacimwkcsosbicdxkicaua 544 0 0 2010-04-16 2016-04-16 2010-04-16 2015-04-17 47.96 0 0.0 38.72 0.165794 0.087899 44.311378 f 0.00 28.60 28.60 1 6.60 6 kamkkxfxxuwbdslkwifmmcsiusiuosws 13.856 0.000006 2.558511e-07 0.000000 0.007662 0.000000 0.000000 0.007668 2.558511e-07 0.000000 0.000004 9.450150e-08 0.000000e+00 0.000000 0.000000 0.000000 0.000004 9.450150e-08 0.000000e+00 0 3 bba03439a292a1e166f80264c16191cb lmkebamcaaclubfxadlmueccxoimlema 1584 0 0 2010-03-30 2016-03-30 2010-03-30 2015-03-31 240.04 0 0.0 19.83 0.146694 0.000000 44.311378 f 0.00 30.22 30.22 1 25.46 6 kamkkxfxxuwbdslkwifmmcsiusiuosws 13.200 0.000005 0.000000e+00 0.000000 0.006465 0.000000 0.000000 0.006470 0.000000e+00 0.000000 0.000003 0.000000e+00 0.000000e+00 0.000000 0.000000 0.000000 0.000003 0.000000e+00 0.000000e+00 0 4 149d57cf92fc41cf94415803a877cb4b MISSING 4425 0 526 2010-01-13 2016-03-07 2010-01-13 2015-03-09 445.75 526 0.0 131.73 0.116900 0.100015 40.606701 f 52.32 44.91 44.91 1 47.98 6 kamkkxfxxuwbdslkwifmmcsiusiuosws 19.800 0.000015 3.552481e-06 0.000003 0.005429 0.001954 0.000869 0.005444 1.957971e-03 0.000871 0.000011 2.896760e-06 4.860000e-10 0.000000 0.000000 0.000000 0.000011 2.896760e-06 4.860000e-10 0 <p>Lets check the data types in the dataset</p> In\u00a0[9]: Copied! <pre>clients.dtypes\n</pre> clients.dtypes Out[9]: <pre>id                                string[pyarrow]\nchannel_sales                     string[pyarrow]\ncons_12m                                    int64\ncons_gas_12m                                int64\ncons_last_month                             int64\ndate_activ                         datetime64[ns]\ndate_end                           datetime64[ns]\ndate_modif_prod                    datetime64[ns]\ndate_renewal                       datetime64[ns]\nforecast_cons_12m                         float64\nforecast_cons_year                          int64\nforecast_discount_energy                  float64\nforecast_meter_rent_12m                   float64\nforecast_price_energy_off_peak            float64\nforecast_price_energy_peak                float64\nforecast_price_pow_off_peak               float64\nhas_gas                           string[pyarrow]\nimp_cons                                  float64\nmargin_gross_pow_ele                      float64\nmargin_net_pow_ele                        float64\nnb_prod_act                                 int64\nnet_margin                                float64\nnum_years_antig                             int64\norigin_up                         string[pyarrow]\npow_max                                   float64\nvar_year_price_off_peak_var               float64\nvar_year_price_peak_var                   float64\nvar_year_price_mid_peak_var               float64\nvar_year_price_off_peak_fix               float64\nvar_year_price_peak_fix                   float64\nvar_year_price_mid_peak_fix               float64\nvar_year_price_off_peak                   float64\nvar_year_price_peak                       float64\nvar_year_price_mid_peak                   float64\nvar_6m_price_off_peak_var                 float64\nvar_6m_price_peak_var                     float64\nvar_6m_price_mid_peak_var                 float64\nvar_6m_price_off_peak_fix                 float64\nvar_6m_price_peak_fix                     float64\nvar_6m_price_mid_peak_fix                 float64\nvar_6m_price_off_peak                     float64\nvar_6m_price_peak                         float64\nvar_6m_price_mid_peak                     float64\nchurn                                       int64\ndtype: object</pre> Newly added features <p>Lets check the column feature data of our updated dataset</p> <p>The updated dataframe contains a fair number of new numerical features, which will be useful for feature engineeing</p> <pre><code>var_year_price_off_peak_var               float64\nvar_year_price_peak_var                   float64\nvar_year_price_mid_peak_var               float64\nvar_year_price_off_peak_fix               float64\nvar_year_price_peak_fix                   float64\nvar_year_price_mid_peak_fix               float64\nvar_year_price_off_peak                   float64\nvar_year_price_peak                       float64\nvar_year_price_mid_peak                   float64\nvar_6m_price_off_peak_var                 float64\nvar_6m_price_peak_var                     float64\nvar_6m_price_mid_peak_var                 float64\nvar_6m_price_off_peak_fix                 float64\nvar_6m_price_peak_fix                     float64\nvar_6m_price_mid_peak_fix                 float64\nvar_6m_price_off_peak                     float64\nvar_6m_price_peak                         float64\nvar_6m_price_mid_peak                     float64\n</code></pre> <p>These new features contain the yearly average and half a year averages of the relevant columns</p> In\u00a0[10]: Copied! <pre>prices_path = os.path.join('','price.csv')\n\nprices = dd.read_csv(prices_path,sep=',',blocksize=\"30M\")\nprices['price_date'] = dd.to_datetime(prices['price_date'], format='%Y-%m-%d')\nprices.head()\n</pre> prices_path = os.path.join('','price.csv')  prices = dd.read_csv(prices_path,sep=',',blocksize=\"30M\") prices['price_date'] = dd.to_datetime(prices['price_date'], format='%Y-%m-%d') prices.head() Out[10]: id price_date price_off_peak_var price_peak_var price_mid_peak_var price_off_peak_fix price_peak_fix price_mid_peak_fix 0 038af19179925da21a25619c5a24b745 2015-01-01 0.151367 0.0 0.0 44.266931 0.0 0.0 1 038af19179925da21a25619c5a24b745 2015-02-01 0.151367 0.0 0.0 44.266931 0.0 0.0 2 038af19179925da21a25619c5a24b745 2015-03-01 0.151367 0.0 0.0 44.266931 0.0 0.0 3 038af19179925da21a25619c5a24b745 2015-04-01 0.149626 0.0 0.0 44.266931 0.0 0.0 4 038af19179925da21a25619c5a24b745 2015-05-01 0.149626 0.0 0.0 44.266931 0.0 0.0 In\u00a0[11]: Copied! <pre>temp = prices.copy()\n</pre> temp = prices.copy() In\u00a0[12]: Copied! <pre># convert column to datetime\ncolumns = ['id','price_date','price_off_peak_var','price_off_peak_fix']\n\n# select both january and december rows  only\ntemp = temp[(temp['price_date'] == '2015-01-01') | (temp['price_date'] == '2015-12-01')][columns]\ntemp = temp.sort_values(by=['id','price_date'])\ntemp.head()\n</pre> # convert column to datetime columns = ['id','price_date','price_off_peak_var','price_off_peak_fix']  # select both january and december rows  only temp = temp[(temp['price_date'] == '2015-01-01') | (temp['price_date'] == '2015-12-01')][columns] temp = temp.sort_values(by=['id','price_date']) temp.head() Out[12]: id price_date price_off_peak_var price_off_peak_fix 72163 0002203ffbb812588b632b9e628cc38d 2015-01-01 0.126098 40.565969 72174 0002203ffbb812588b632b9e628cc38d 2015-12-01 0.119906 40.728885 157109 0004351ebdd665e6ee664792efc4fd13 2015-01-01 0.148047 44.266931 157120 0004351ebdd665e6ee664792efc4fd13 2015-12-01 0.143943 44.444710 168210 0010bcc39e42b3c2131ed2ce55246e3c 2015-01-01 0.150837 44.444710 In\u00a0[13]: Copied! <pre># find the difference between rows\ntemp['diff_price_off_peak_var'] = temp['price_off_peak_var'] - temp['price_off_peak_var'].shift(-1)\ntemp['diff_price_off_peak_fix'] = temp['price_off_peak_fix'] - temp['price_off_peak_fix'].shift(-1)\ntemp.head()\n</pre> # find the difference between rows temp['diff_price_off_peak_var'] = temp['price_off_peak_var'] - temp['price_off_peak_var'].shift(-1) temp['diff_price_off_peak_fix'] = temp['price_off_peak_fix'] - temp['price_off_peak_fix'].shift(-1) temp.head() Out[13]: id price_date price_off_peak_var price_off_peak_fix diff_price_off_peak_var diff_price_off_peak_fix 72163 0002203ffbb812588b632b9e628cc38d 2015-01-01 0.126098 40.565969 0.006192 -0.162916 72174 0002203ffbb812588b632b9e628cc38d 2015-12-01 0.119906 40.728885 -0.028141 -3.538046 157109 0004351ebdd665e6ee664792efc4fd13 2015-01-01 0.148047 44.266931 0.004104 -0.177779 157120 0004351ebdd665e6ee664792efc4fd13 2015-12-01 0.143943 44.444710 -0.006894 0.000000 168210 0010bcc39e42b3c2131ed2ce55246e3c 2015-01-01 0.150837 44.444710 -0.050443 -1.500000 <p>Now lets add these new features to the main datas of clients</p> In\u00a0[14]: Copied! <pre># update clients\nclients = clients.merge(temp,left_on='id',right_on='id')\nclients.head()\n</pre> # update clients clients = clients.merge(temp,left_on='id',right_on='id') clients.head() Out[14]: id channel_sales cons_12m cons_gas_12m cons_last_month date_activ date_end date_modif_prod date_renewal forecast_cons_12m ... var_6m_price_mid_peak_fix var_6m_price_off_peak var_6m_price_peak var_6m_price_mid_peak churn price_date price_off_peak_var price_off_peak_fix diff_price_off_peak_var diff_price_off_peak_fix 0 24011ae4ebbe3035111d65fa7c15bc57 foosdfpfkusacimwkcsosbicdxkicaua 0 54946 0 2013-06-15 2016-06-15 2015-11-01 2015-06-23 0.00 ... 44.235794 2.086425 9.953056e+01 44.236702 1 2015-01-01 0.125976 40.565969 -0.020057 -3.700961 1 24011ae4ebbe3035111d65fa7c15bc57 foosdfpfkusacimwkcsosbicdxkicaua 0 54946 0 2013-06-15 2016-06-15 2015-11-01 2015-06-23 0.00 ... 44.235794 2.086425 9.953056e+01 44.236702 1 2015-12-01 0.146033 44.266930 0.020057 3.700961 2 d29c2c54acc38ff3c0614d0a653813dd MISSING 4660 0 0 2009-08-21 2016-08-30 2009-08-21 2015-08-31 189.95 ... 0.000000 0.009485 1.217891e-03 0.000000 0 2015-01-01 0.151367 44.266931 0.003767 -0.177779 3 d29c2c54acc38ff3c0614d0a653813dd MISSING 4660 0 0 2009-08-21 2016-08-30 2009-08-21 2015-08-31 189.95 ... 0.000000 0.009485 1.217891e-03 0.000000 0 2015-12-01 0.147600 44.444710 -0.004845 0.177779 4 764c75f661154dac3a6c254cd082ea7d foosdfpfkusacimwkcsosbicdxkicaua 544 0 0 2010-04-16 2016-04-16 2010-04-16 2015-04-17 47.96 ... 0.000000 0.000004 9.450150e-08 0.000000 0 2015-01-01 0.172468 44.266931 0.004670 -0.177779 <p>5 rows \u00d7 49 columns</p> Check the correlation to churn! <p>What about the correlation of this feature to <code>churn</code></p> In\u00a0[15]: Copied! <pre>columns = ['diff_price_off_peak_var','diff_price_off_peak_fix','churn']\n\nclients.loc[:,columns].select_dtypes(include=['float64','int64']).corr().round(2).compute().loc['churn'].to_frame()\n</pre> columns = ['diff_price_off_peak_var','diff_price_off_peak_fix','churn']  clients.loc[:,columns].select_dtypes(include=['float64','int64']).corr().round(2).compute().loc['churn'].to_frame() Out[15]: churn diff_price_off_peak_var -0.00 diff_price_off_peak_fix 0.01 churn 1.00 In\u00a0[16]: Copied! <pre># Aggregate average prices per period by company\nmean_prices = prices.groupby(['id']).agg({\n'price_off_peak_var': 'mean', \n'price_peak_var': 'mean', \n'price_mid_peak_var': 'mean',\n'price_off_peak_fix': 'mean',\n'price_peak_fix': 'mean',\n'price_mid_peak_fix': 'mean'    \n}).reset_index()\n</pre> # Aggregate average prices per period by company mean_prices = prices.groupby(['id']).agg({ 'price_off_peak_var': 'mean',  'price_peak_var': 'mean',  'price_mid_peak_var': 'mean', 'price_off_peak_fix': 'mean', 'price_peak_fix': 'mean', 'price_mid_peak_fix': 'mean'     }).reset_index() In\u00a0[17]: Copied! <pre># Calculate the mean difference between consecutive periods\nmean_prices['off_peak_peak_var_mean_diff'] = mean_prices['price_off_peak_var'] - mean_prices['price_peak_var']\nmean_prices['peak_mid_peak_var_mean_diff'] = mean_prices['price_peak_var'] - mean_prices['price_mid_peak_var']\nmean_prices['off_peak_mid_peak_var_mean_diff'] = mean_prices['price_off_peak_var'] - mean_prices['price_mid_peak_var']\nmean_prices['off_peak_peak_fix_mean_diff'] = mean_prices['price_off_peak_fix'] - mean_prices['price_peak_fix']\nmean_prices['peak_mid_peak_fix_mean_diff'] = mean_prices['price_peak_fix'] - mean_prices['price_mid_peak_fix']\nmean_prices['off_peak_mid_peak_fix_mean_diff'] = mean_prices['price_off_peak_fix'] - mean_prices['price_mid_peak_fix']\nmean_prices.head()\n</pre> # Calculate the mean difference between consecutive periods mean_prices['off_peak_peak_var_mean_diff'] = mean_prices['price_off_peak_var'] - mean_prices['price_peak_var'] mean_prices['peak_mid_peak_var_mean_diff'] = mean_prices['price_peak_var'] - mean_prices['price_mid_peak_var'] mean_prices['off_peak_mid_peak_var_mean_diff'] = mean_prices['price_off_peak_var'] - mean_prices['price_mid_peak_var'] mean_prices['off_peak_peak_fix_mean_diff'] = mean_prices['price_off_peak_fix'] - mean_prices['price_peak_fix'] mean_prices['peak_mid_peak_fix_mean_diff'] = mean_prices['price_peak_fix'] - mean_prices['price_mid_peak_fix'] mean_prices['off_peak_mid_peak_fix_mean_diff'] = mean_prices['price_off_peak_fix'] - mean_prices['price_mid_peak_fix'] mean_prices.head() Out[17]: id price_off_peak_var price_peak_var price_mid_peak_var price_off_peak_fix price_peak_fix price_mid_peak_fix off_peak_peak_var_mean_diff peak_mid_peak_var_mean_diff off_peak_mid_peak_var_mean_diff off_peak_peak_fix_mean_diff peak_mid_peak_fix_mean_diff off_peak_mid_peak_fix_mean_diff 0 038af19179925da21a25619c5a24b745 0.148550 0.000000 0.000000 44.355820 0.000000 0.000000 0.148550 0.000000 0.148550 44.355820 0.000000 44.355820 1 31f2ce549924679a3cbb2d128ae9ea43 0.123027 0.102637 0.073525 40.661003 24.396601 16.264402 0.020390 0.029112 0.049502 16.264402 8.132199 24.396601 2 36b6352b4656216bfdb96f01e9a94b4e 0.118912 0.098372 0.068964 40.633851 24.380309 16.253540 0.020540 0.029408 0.049948 16.253542 8.126769 24.380311 3 48f3e6e86f7a8656b2c6b6ce2763055e 0.145552 0.000000 0.000000 44.400265 0.000000 0.000000 0.145552 0.000000 0.145552 44.400265 0.000000 44.400265 4 cce88c7d721430d8bd31f71ae686c91e 0.131729 0.112062 0.082026 40.854928 24.595955 16.466472 0.019667 0.030036 0.049703 16.258972 8.129484 24.388456 In\u00a0[18]: Copied! <pre>columns = [\n    'id', \n    'off_peak_peak_var_mean_diff',\n    'peak_mid_peak_var_mean_diff', \n    'off_peak_mid_peak_var_mean_diff',\n    'off_peak_peak_fix_mean_diff', \n    'peak_mid_peak_fix_mean_diff', \n    'off_peak_mid_peak_fix_mean_diff'\n]\n\n# update clients\nclients = clients.merge(mean_prices[columns], left_on='id', right_on='id')\n</pre> columns = [     'id',      'off_peak_peak_var_mean_diff',     'peak_mid_peak_var_mean_diff',      'off_peak_mid_peak_var_mean_diff',     'off_peak_peak_fix_mean_diff',      'peak_mid_peak_fix_mean_diff',      'off_peak_mid_peak_fix_mean_diff' ]  # update clients clients = clients.merge(mean_prices[columns], left_on='id', right_on='id') In\u00a0[19]: Copied! <pre>clients.head()\n</pre> clients.head() Out[19]: id channel_sales cons_12m cons_gas_12m cons_last_month date_activ date_end date_modif_prod date_renewal forecast_cons_12m ... price_off_peak_var price_off_peak_fix diff_price_off_peak_var diff_price_off_peak_fix off_peak_peak_var_mean_diff peak_mid_peak_var_mean_diff off_peak_mid_peak_var_mean_diff off_peak_peak_fix_mean_diff peak_mid_peak_fix_mean_diff off_peak_mid_peak_fix_mean_diff 0 24011ae4ebbe3035111d65fa7c15bc57 foosdfpfkusacimwkcsosbicdxkicaua 0 54946 0 2013-06-15 2016-06-15 2015-11-01 2015-06-23 0.00 ... 0.125976 40.565969 -0.020057 -3.700961 0.024038 0.034219 0.058257 18.590255 7.45067 26.040925 1 24011ae4ebbe3035111d65fa7c15bc57 foosdfpfkusacimwkcsosbicdxkicaua 0 54946 0 2013-06-15 2016-06-15 2015-11-01 2015-06-23 0.00 ... 0.146033 44.266930 0.020057 3.700961 0.024038 0.034219 0.058257 18.590255 7.45067 26.040925 2 d29c2c54acc38ff3c0614d0a653813dd MISSING 4660 0 0 2009-08-21 2016-08-30 2009-08-21 2015-08-31 189.95 ... 0.151367 44.266931 0.003767 -0.177779 0.142485 0.007124 0.149609 44.311375 0.00000 44.311375 3 d29c2c54acc38ff3c0614d0a653813dd MISSING 4660 0 0 2009-08-21 2016-08-30 2009-08-21 2015-08-31 189.95 ... 0.147600 44.444710 -0.004845 0.177779 0.142485 0.007124 0.149609 44.311375 0.00000 44.311375 4 764c75f661154dac3a6c254cd082ea7d foosdfpfkusacimwkcsosbicdxkicaua 544 0 0 2010-04-16 2016-04-16 2010-04-16 2015-04-17 47.96 ... 0.172468 44.266931 0.004670 -0.177779 0.082090 0.088421 0.170512 44.385450 0.00000 44.385450 <p>5 rows \u00d7 55 columns</p> <p>What these features add</p> <ul> <li>This feature may be useful because it adds more granularity to the existing feature that my colleague found to be useful (found in the provided dataset)</li> <li>Instead of looking at differences across an entire year, we have now created features that look at mean average price differences across different time periods (<code>off_peak</code>, <code>peak</code>, <code>mid_peak</code>).</li> <li>The dec-jan feature may reveal macro patterns that occur over an entire year, whereas inter-time-period features may reveal patterns on a micro scale between months.</li> </ul> In\u00a0[20]: Copied! <pre>clients.columns\n</pre> clients.columns Out[20]: <pre>Index(['id', 'channel_sales', 'cons_12m', 'cons_gas_12m', 'cons_last_month',\n       'date_activ', 'date_end', 'date_modif_prod', 'date_renewal',\n       'forecast_cons_12m', 'forecast_cons_year', 'forecast_discount_energy',\n       'forecast_meter_rent_12m', 'forecast_price_energy_off_peak',\n       'forecast_price_energy_peak', 'forecast_price_pow_off_peak', 'has_gas',\n       'imp_cons', 'margin_gross_pow_ele', 'margin_net_pow_ele', 'nb_prod_act',\n       'net_margin', 'num_years_antig', 'origin_up', 'pow_max',\n       'var_year_price_off_peak_var', 'var_year_price_peak_var',\n       'var_year_price_mid_peak_var', 'var_year_price_off_peak_fix',\n       'var_year_price_peak_fix', 'var_year_price_mid_peak_fix',\n       'var_year_price_off_peak', 'var_year_price_peak',\n       'var_year_price_mid_peak', 'var_6m_price_off_peak_var',\n       'var_6m_price_peak_var', 'var_6m_price_mid_peak_var',\n       'var_6m_price_off_peak_fix', 'var_6m_price_peak_fix',\n       'var_6m_price_mid_peak_fix', 'var_6m_price_off_peak',\n       'var_6m_price_peak', 'var_6m_price_mid_peak', 'churn', 'price_date',\n       'price_off_peak_var', 'price_off_peak_fix', 'diff_price_off_peak_var',\n       'diff_price_off_peak_fix', 'off_peak_peak_var_mean_diff',\n       'peak_mid_peak_var_mean_diff', 'off_peak_mid_peak_var_mean_diff',\n       'off_peak_peak_fix_mean_diff', 'peak_mid_peak_fix_mean_diff',\n       'off_peak_mid_peak_fix_mean_diff'],\n      dtype='object')</pre> Check the correlation to churn! <p>Lets also calculate the collation to churn</p> In\u00a0[22]: Copied! <pre>columns = [ \n    'off_peak_peak_var_mean_diff',\n    'peak_mid_peak_var_mean_diff', \n    'off_peak_mid_peak_var_mean_diff',\n    'off_peak_peak_fix_mean_diff', \n    'peak_mid_peak_fix_mean_diff', \n    'off_peak_mid_peak_fix_mean_diff',\n    'churn'\n]\n</pre> columns = [      'off_peak_peak_var_mean_diff',     'peak_mid_peak_var_mean_diff',      'off_peak_mid_peak_var_mean_diff',     'off_peak_peak_fix_mean_diff',      'peak_mid_peak_fix_mean_diff',      'off_peak_mid_peak_fix_mean_diff',     'churn' ] In\u00a0[23]: Copied! <pre>clients.loc[:,columns].select_dtypes(include=['float64','int64']).corr().round(2).compute().loc['churn'].to_frame()\n</pre> clients.loc[:,columns].select_dtypes(include=['float64','int64']).corr().round(2).compute().loc['churn'].to_frame() Out[23]: churn off_peak_peak_var_mean_diff -0.03 peak_mid_peak_var_mean_diff -0.01 off_peak_mid_peak_var_mean_diff -0.03 off_peak_peak_fix_mean_diff -0.04 peak_mid_peak_fix_mean_diff 0.05 off_peak_mid_peak_fix_mean_diff -0.03 churn 1.00 In\u00a0[24]: Copied! <pre># Aggregate average prices per period by company\nmean_prices_by_month = prices.groupby(['id', 'price_date']).agg({\n    'price_off_peak_var': 'mean', \n    'price_peak_var': 'mean', \n    'price_mid_peak_var': 'mean',\n    'price_off_peak_fix': 'mean',\n    'price_peak_fix': 'mean',\n    'price_mid_peak_fix': 'mean'    \n}).reset_index()\n</pre> # Aggregate average prices per period by company mean_prices_by_month = prices.groupby(['id', 'price_date']).agg({     'price_off_peak_var': 'mean',      'price_peak_var': 'mean',      'price_mid_peak_var': 'mean',     'price_off_peak_fix': 'mean',     'price_peak_fix': 'mean',     'price_mid_peak_fix': 'mean'     }).reset_index() <p>Calculate the difference between the prices of <code>off peak</code> and <code>peak</code> for each month per user</p> In\u00a0[25]: Copied! <pre># Calculate the mean difference between consecutive periods\nmean_prices_by_month['off_peak_peak_var_mean_diff'] = mean_prices_by_month['price_off_peak_var'] - mean_prices_by_month['price_peak_var']\nmean_prices_by_month['peak_mid_peak_var_mean_diff'] = mean_prices_by_month['price_peak_var'] - mean_prices_by_month['price_mid_peak_var']\nmean_prices_by_month['off_peak_mid_peak_var_mean_diff'] = mean_prices_by_month['price_off_peak_var'] - mean_prices_by_month['price_mid_peak_var']\nmean_prices_by_month['off_peak_peak_fix_mean_diff'] = mean_prices_by_month['price_off_peak_fix'] - mean_prices_by_month['price_peak_fix']\nmean_prices_by_month['peak_mid_peak_fix_mean_diff'] = mean_prices_by_month['price_peak_fix'] - mean_prices_by_month['price_mid_peak_fix']\nmean_prices_by_month['off_peak_mid_peak_fix_mean_diff'] = mean_prices_by_month['price_off_peak_fix'] - mean_prices_by_month['price_mid_peak_fix']\nmean_prices_by_month.head()\n</pre> # Calculate the mean difference between consecutive periods mean_prices_by_month['off_peak_peak_var_mean_diff'] = mean_prices_by_month['price_off_peak_var'] - mean_prices_by_month['price_peak_var'] mean_prices_by_month['peak_mid_peak_var_mean_diff'] = mean_prices_by_month['price_peak_var'] - mean_prices_by_month['price_mid_peak_var'] mean_prices_by_month['off_peak_mid_peak_var_mean_diff'] = mean_prices_by_month['price_off_peak_var'] - mean_prices_by_month['price_mid_peak_var'] mean_prices_by_month['off_peak_peak_fix_mean_diff'] = mean_prices_by_month['price_off_peak_fix'] - mean_prices_by_month['price_peak_fix'] mean_prices_by_month['peak_mid_peak_fix_mean_diff'] = mean_prices_by_month['price_peak_fix'] - mean_prices_by_month['price_mid_peak_fix'] mean_prices_by_month['off_peak_mid_peak_fix_mean_diff'] = mean_prices_by_month['price_off_peak_fix'] - mean_prices_by_month['price_mid_peak_fix'] mean_prices_by_month.head() Out[25]: id price_date price_off_peak_var price_peak_var price_mid_peak_var price_off_peak_fix price_peak_fix price_mid_peak_fix off_peak_peak_var_mean_diff peak_mid_peak_var_mean_diff off_peak_mid_peak_var_mean_diff off_peak_peak_fix_mean_diff peak_mid_peak_fix_mean_diff off_peak_mid_peak_fix_mean_diff 0 038af19179925da21a25619c5a24b745 2015-01-01 0.151367 0.0 0.0 44.266931 0.0 0.0 0.151367 0.0 0.151367 44.266931 0.0 44.266931 1 038af19179925da21a25619c5a24b745 2015-02-01 0.151367 0.0 0.0 44.266931 0.0 0.0 0.151367 0.0 0.151367 44.266931 0.0 44.266931 2 038af19179925da21a25619c5a24b745 2015-03-01 0.151367 0.0 0.0 44.266931 0.0 0.0 0.151367 0.0 0.151367 44.266931 0.0 44.266931 3 038af19179925da21a25619c5a24b745 2015-04-01 0.149626 0.0 0.0 44.266931 0.0 0.0 0.149626 0.0 0.149626 44.266931 0.0 44.266931 4 038af19179925da21a25619c5a24b745 2015-05-01 0.149626 0.0 0.0 44.266931 0.0 0.0 0.149626 0.0 0.149626 44.266931 0.0 44.266931 <p>Now from the above data, calculate the maximum difference between the <code>price_off_peak_fix</code> and <code>price_peak_var</code> to find the month where it maximum and get its value</p> In\u00a0[26]: Copied! <pre># Calculate the maximum monthly difference across time periods\nmax_diff_across_periods_months = mean_prices_by_month.groupby(['id']).agg({\n    'off_peak_peak_var_mean_diff': 'max',\n    'peak_mid_peak_var_mean_diff': 'max',\n    'off_peak_mid_peak_var_mean_diff': 'max',\n    'off_peak_peak_fix_mean_diff': 'max',\n    'peak_mid_peak_fix_mean_diff': 'max',\n    'off_peak_mid_peak_fix_mean_diff': 'max'\n}).reset_index().rename(\n    columns={\n        'off_peak_peak_var_mean_diff': 'off_peak_peak_var_max_monthly_diff',\n        'peak_mid_peak_var_mean_diff': 'peak_mid_peak_var_max_monthly_diff',\n        'off_peak_mid_peak_var_mean_diff': 'off_peak_mid_peak_var_max_monthly_diff',\n        'off_peak_peak_fix_mean_diff': 'off_peak_peak_fix_max_monthly_diff',\n        'peak_mid_peak_fix_mean_diff': 'peak_mid_peak_fix_max_monthly_diff',\n        'off_peak_mid_peak_fix_mean_diff': 'off_peak_mid_peak_fix_max_monthly_diff'\n    }\n)\n</pre> # Calculate the maximum monthly difference across time periods max_diff_across_periods_months = mean_prices_by_month.groupby(['id']).agg({     'off_peak_peak_var_mean_diff': 'max',     'peak_mid_peak_var_mean_diff': 'max',     'off_peak_mid_peak_var_mean_diff': 'max',     'off_peak_peak_fix_mean_diff': 'max',     'peak_mid_peak_fix_mean_diff': 'max',     'off_peak_mid_peak_fix_mean_diff': 'max' }).reset_index().rename(     columns={         'off_peak_peak_var_mean_diff': 'off_peak_peak_var_max_monthly_diff',         'peak_mid_peak_var_mean_diff': 'peak_mid_peak_var_max_monthly_diff',         'off_peak_mid_peak_var_mean_diff': 'off_peak_mid_peak_var_max_monthly_diff',         'off_peak_peak_fix_mean_diff': 'off_peak_peak_fix_max_monthly_diff',         'peak_mid_peak_fix_mean_diff': 'peak_mid_peak_fix_max_monthly_diff',         'off_peak_mid_peak_fix_mean_diff': 'off_peak_mid_peak_fix_max_monthly_diff'     } ) In\u00a0[27]: Copied! <pre>columns = [\n                            'id',\n                            'off_peak_peak_var_max_monthly_diff',\n                            'peak_mid_peak_var_max_monthly_diff',\n                            'off_peak_mid_peak_var_max_monthly_diff',\n                            'off_peak_peak_fix_max_monthly_diff',\n                            'peak_mid_peak_fix_max_monthly_diff',\n                            'off_peak_mid_peak_fix_max_monthly_diff'\n                        ]\n\n# update cients\nclients = clients.merge(max_diff_across_periods_months[columns], left_on='id', right_on='id')\n</pre> columns = [                             'id',                             'off_peak_peak_var_max_monthly_diff',                             'peak_mid_peak_var_max_monthly_diff',                             'off_peak_mid_peak_var_max_monthly_diff',                             'off_peak_peak_fix_max_monthly_diff',                             'peak_mid_peak_fix_max_monthly_diff',                             'off_peak_mid_peak_fix_max_monthly_diff'                         ]  # update cients clients = clients.merge(max_diff_across_periods_months[columns], left_on='id', right_on='id') In\u00a0[28]: Copied! <pre>clients.head()\n</pre> clients.head() Out[28]: id channel_sales cons_12m cons_gas_12m cons_last_month date_activ date_end date_modif_prod date_renewal forecast_cons_12m ... off_peak_mid_peak_var_mean_diff off_peak_peak_fix_mean_diff peak_mid_peak_fix_mean_diff off_peak_mid_peak_fix_mean_diff off_peak_peak_var_max_monthly_diff peak_mid_peak_var_max_monthly_diff off_peak_mid_peak_var_max_monthly_diff off_peak_peak_fix_max_monthly_diff peak_mid_peak_fix_max_monthly_diff off_peak_mid_peak_fix_max_monthly_diff 0 24011ae4ebbe3035111d65fa7c15bc57 foosdfpfkusacimwkcsosbicdxkicaua 0 54946 0 2013-06-15 2016-06-15 2015-11-01 2015-06-23 0.00 ... 0.058257 18.590255 7.45067 26.040925 0.060550 0.085483 0.146033 44.26693 8.145775 44.26693 1 24011ae4ebbe3035111d65fa7c15bc57 foosdfpfkusacimwkcsosbicdxkicaua 0 54946 0 2013-06-15 2016-06-15 2015-11-01 2015-06-23 0.00 ... 0.058257 18.590255 7.45067 26.040925 0.060550 0.085483 0.146033 44.26693 8.145775 44.26693 2 d29c2c54acc38ff3c0614d0a653813dd MISSING 4660 0 0 2009-08-21 2016-08-30 2009-08-21 2015-08-31 189.95 ... 0.149609 44.311375 0.00000 44.311375 0.151367 0.085483 0.151367 44.44471 0.000000 44.44471 3 d29c2c54acc38ff3c0614d0a653813dd MISSING 4660 0 0 2009-08-21 2016-08-30 2009-08-21 2015-08-31 189.95 ... 0.149609 44.311375 0.00000 44.311375 0.151367 0.085483 0.151367 44.44471 0.000000 44.44471 4 764c75f661154dac3a6c254cd082ea7d foosdfpfkusacimwkcsosbicdxkicaua 544 0 0 2010-04-16 2016-04-16 2010-04-16 2015-04-17 47.96 ... 0.170512 44.385450 0.00000 44.385450 0.084587 0.089162 0.172468 44.44471 0.000000 44.44471 <p>5 rows \u00d7 61 columns</p> In\u00a0[29]: Copied! <pre>clients.columns\n</pre> clients.columns Out[29]: <pre>Index(['id', 'channel_sales', 'cons_12m', 'cons_gas_12m', 'cons_last_month',\n       'date_activ', 'date_end', 'date_modif_prod', 'date_renewal',\n       'forecast_cons_12m', 'forecast_cons_year', 'forecast_discount_energy',\n       'forecast_meter_rent_12m', 'forecast_price_energy_off_peak',\n       'forecast_price_energy_peak', 'forecast_price_pow_off_peak', 'has_gas',\n       'imp_cons', 'margin_gross_pow_ele', 'margin_net_pow_ele', 'nb_prod_act',\n       'net_margin', 'num_years_antig', 'origin_up', 'pow_max',\n       'var_year_price_off_peak_var', 'var_year_price_peak_var',\n       'var_year_price_mid_peak_var', 'var_year_price_off_peak_fix',\n       'var_year_price_peak_fix', 'var_year_price_mid_peak_fix',\n       'var_year_price_off_peak', 'var_year_price_peak',\n       'var_year_price_mid_peak', 'var_6m_price_off_peak_var',\n       'var_6m_price_peak_var', 'var_6m_price_mid_peak_var',\n       'var_6m_price_off_peak_fix', 'var_6m_price_peak_fix',\n       'var_6m_price_mid_peak_fix', 'var_6m_price_off_peak',\n       'var_6m_price_peak', 'var_6m_price_mid_peak', 'churn', 'price_date',\n       'price_off_peak_var', 'price_off_peak_fix', 'diff_price_off_peak_var',\n       'diff_price_off_peak_fix', 'off_peak_peak_var_mean_diff',\n       'peak_mid_peak_var_mean_diff', 'off_peak_mid_peak_var_mean_diff',\n       'off_peak_peak_fix_mean_diff', 'peak_mid_peak_fix_mean_diff',\n       'off_peak_mid_peak_fix_mean_diff', 'off_peak_peak_var_max_monthly_diff',\n       'peak_mid_peak_var_max_monthly_diff',\n       'off_peak_mid_peak_var_max_monthly_diff',\n       'off_peak_peak_fix_max_monthly_diff',\n       'peak_mid_peak_fix_max_monthly_diff',\n       'off_peak_mid_peak_fix_max_monthly_diff'],\n      dtype='object')</pre> <p>What these features add</p> <ul> <li>Calculating the maximum price change of all the time periods would be a good feature to create because, as a utilities customer, there is nothing more annoying than sudden price changes between months, and a large increase in prices within a short time span would be an influencing factor in causing me to look at other utilities providers for a better deal</li> <li>Since we are trying to predict churn for this use case, I thought this would be an interesting feature to include.</li> </ul> Check the correlation to churn! <p>Lets also calculate the correlation to churn</p> In\u00a0[42]: Copied! <pre>columns = [\n                            'off_peak_peak_var_max_monthly_diff',\n                            'peak_mid_peak_var_max_monthly_diff',\n                            'off_peak_mid_peak_var_max_monthly_diff',\n                            'off_peak_peak_fix_max_monthly_diff',\n                            'peak_mid_peak_fix_max_monthly_diff',\n                            'off_peak_mid_peak_fix_max_monthly_diff',\n                            'churn'\n                        ]\n\ntest = clients[columns].copy()\n</pre> columns = [                             'off_peak_peak_var_max_monthly_diff',                             'peak_mid_peak_var_max_monthly_diff',                             'off_peak_mid_peak_var_max_monthly_diff',                             'off_peak_peak_fix_max_monthly_diff',                             'peak_mid_peak_fix_max_monthly_diff',                             'off_peak_mid_peak_fix_max_monthly_diff',                             'churn'                         ]  test = clients[columns].copy() In\u00a0[43]: Copied! <pre>test.select_dtypes(include=['float64','int64']).corr().round(2).compute().loc['churn'].to_frame()\n</pre> test.select_dtypes(include=['float64','int64']).corr().round(2).compute().loc['churn'].to_frame() Out[43]: churn off_peak_peak_var_max_monthly_diff -0.02 peak_mid_peak_var_max_monthly_diff 0.00 off_peak_mid_peak_var_max_monthly_diff -0.03 off_peak_peak_fix_max_monthly_diff -0.03 peak_mid_peak_fix_max_monthly_diff 0.05 off_peak_mid_peak_fix_max_monthly_diff -0.02 churn 1.00 <p>Not too bad, we may have found some features that help the model</p> In\u00a0[33]: Copied! <pre>clients['tenure'] = ((clients['date_end'] - clients['date_activ'])/pd.Timedelta('365 days')).astype(int)\nclients.head()\n</pre> clients['tenure'] = ((clients['date_end'] - clients['date_activ'])/pd.Timedelta('365 days')).astype(int) clients.head() Out[33]: id channel_sales cons_12m cons_gas_12m cons_last_month date_activ date_end date_modif_prod date_renewal forecast_cons_12m ... off_peak_peak_fix_mean_diff peak_mid_peak_fix_mean_diff off_peak_mid_peak_fix_mean_diff off_peak_peak_var_max_monthly_diff peak_mid_peak_var_max_monthly_diff off_peak_mid_peak_var_max_monthly_diff off_peak_peak_fix_max_monthly_diff peak_mid_peak_fix_max_monthly_diff off_peak_mid_peak_fix_max_monthly_diff tenure 0 24011ae4ebbe3035111d65fa7c15bc57 foosdfpfkusacimwkcsosbicdxkicaua 0 54946 0 2013-06-15 2016-06-15 2015-11-01 2015-06-23 0.00 ... 18.590255 7.45067 26.040925 0.060550 0.085483 0.146033 44.26693 8.145775 44.26693 3 1 24011ae4ebbe3035111d65fa7c15bc57 foosdfpfkusacimwkcsosbicdxkicaua 0 54946 0 2013-06-15 2016-06-15 2015-11-01 2015-06-23 0.00 ... 18.590255 7.45067 26.040925 0.060550 0.085483 0.146033 44.26693 8.145775 44.26693 3 2 d29c2c54acc38ff3c0614d0a653813dd MISSING 4660 0 0 2009-08-21 2016-08-30 2009-08-21 2015-08-31 189.95 ... 44.311375 0.00000 44.311375 0.151367 0.085483 0.151367 44.44471 0.000000 44.44471 7 3 d29c2c54acc38ff3c0614d0a653813dd MISSING 4660 0 0 2009-08-21 2016-08-30 2009-08-21 2015-08-31 189.95 ... 44.311375 0.00000 44.311375 0.151367 0.085483 0.151367 44.44471 0.000000 44.44471 7 4 764c75f661154dac3a6c254cd082ea7d foosdfpfkusacimwkcsosbicdxkicaua 544 0 0 2010-04-16 2016-04-16 2010-04-16 2015-04-17 47.96 ... 44.385450 0.00000 44.385450 0.084587 0.089162 0.172468 44.44471 0.000000 44.44471 6 <p>5 rows \u00d7 62 columns</p> <p>Lets evaluate the churn for each number of years a client been with the company</p> In\u00a0[34]: Copied! <pre>clients.groupby(['tenure']).agg({'churn': 'mean'}).sort_values(by='churn', ascending=False).compute()\n</pre> clients.groupby(['tenure']).agg({'churn': 'mean'}).sort_values(by='churn', ascending=False).compute() Out[34]: churn tenure 2 0.168317 3 0.144158 4 0.127493 13 0.095238 5 0.092031 12 0.083333 7 0.075500 6 0.075439 11 0.059783 8 0.047244 10 0.045455 9 0.012500 <ul> <li>We can see that the churn rate of customers in their first 4 years is relatively high</li> <li>Comparing that in their fifth year, the churn reduces quite substantially, which we found out in our EDA</li> </ul> In\u00a0[36]: Copied! <pre>skewed = [\n    'cons_12m', \n    'cons_gas_12m', \n    'cons_last_month',\n    'forecast_cons_12m', \n    'forecast_cons_year', \n    'forecast_discount_energy',\n    'forecast_meter_rent_12m', \n    'forecast_price_energy_off_peak',\n    'forecast_price_energy_peak', \n    'forecast_price_pow_off_peak'\n]\n\nclients[skewed].describe().compute()\n</pre> skewed = [     'cons_12m',      'cons_gas_12m',      'cons_last_month',     'forecast_cons_12m',      'forecast_cons_year',      'forecast_discount_energy',     'forecast_meter_rent_12m',      'forecast_price_energy_off_peak',     'forecast_price_energy_peak',      'forecast_price_pow_off_peak' ]  clients[skewed].describe().compute() Out[36]: cons_12m cons_gas_12m cons_last_month forecast_cons_12m forecast_cons_year forecast_discount_energy forecast_meter_rent_12m forecast_price_energy_off_peak forecast_price_energy_peak forecast_price_pow_off_peak count 2.919000e+04 2.919000e+04 29190.000000 29190.000000 29190.000000 29190.000000 29190.000000 29190.000000 29190.000000 29190.000000 mean 1.592285e+05 2.807021e+04 16092.905584 1868.220871 1400.011374 0.967283 63.086222 0.137286 0.050493 43.130243 std 5.734673e+05 1.628831e+05 64372.325502 2387.487112 3248.730182 5.110006 66.152296 0.024627 0.049036 4.487263 min 0.000000e+00 0.000000e+00 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 25% 5.677250e+03 0.000000e+00 0.000000 494.995000 0.000000 0.000000 16.180000 0.116340 0.000000 40.606701 50% 1.411600e+04 0.000000e+00 793.000000 1112.875000 314.000000 0.000000 18.800000 0.143166 0.084138 44.311378 75% 4.076300e+04 0.000000e+00 3383.000000 2400.240000 1745.750000 0.000000 131.030000 0.146348 0.098837 44.311378 max 6.207104e+06 4.154590e+06 771203.000000 82902.830000 175375.000000 30.000000 599.310000 0.273963 0.195975 59.266378 In\u00a0[37]: Copied! <pre># Apply log10 transformation\nclients[\"cons_12m\"] = np.log10(clients[\"cons_12m\"] + 1)\nclients[\"cons_gas_12m\"] = np.log10(clients[\"cons_gas_12m\"] + 1)\nclients[\"cons_last_month\"] = np.log10(clients[\"cons_last_month\"] + 1)\nclients[\"forecast_cons_12m\"] = np.log10(clients[\"forecast_cons_12m\"] + 1)\nclients[\"forecast_cons_year\"] = np.log10(clients[\"forecast_cons_year\"] + 1)\nclients[\"forecast_meter_rent_12m\"] = np.log10(clients[\"forecast_meter_rent_12m\"] + 1)\nclients[\"imp_cons\"] = np.log10(clients[\"imp_cons\"] + 1)\n</pre> # Apply log10 transformation clients[\"cons_12m\"] = np.log10(clients[\"cons_12m\"] + 1) clients[\"cons_gas_12m\"] = np.log10(clients[\"cons_gas_12m\"] + 1) clients[\"cons_last_month\"] = np.log10(clients[\"cons_last_month\"] + 1) clients[\"forecast_cons_12m\"] = np.log10(clients[\"forecast_cons_12m\"] + 1) clients[\"forecast_cons_year\"] = np.log10(clients[\"forecast_cons_year\"] + 1) clients[\"forecast_meter_rent_12m\"] = np.log10(clients[\"forecast_meter_rent_12m\"] + 1) clients[\"imp_cons\"] = np.log10(clients[\"imp_cons\"] + 1) <p>Check the statistics after we have applied transformations</p> In\u00a0[39]: Copied! <pre>clients[skewed].describe().compute()\n</pre> clients[skewed].describe().compute() Out[39]: cons_12m cons_gas_12m cons_last_month forecast_cons_12m forecast_cons_year forecast_discount_energy forecast_meter_rent_12m forecast_price_energy_off_peak forecast_price_energy_peak forecast_price_pow_off_peak count 29190.000000 29190.000000 29190.000000 29190.000000 29190.000000 29190.000000 29190.000000 29190.000000 29190.000000 29190.000000 mean 4.224066 0.779086 2.264751 2.962168 1.784583 0.967283 1.517289 0.137286 0.050493 43.130243 std 0.884419 1.716981 1.769310 0.683517 1.584982 5.110006 0.571349 0.024627 0.049036 4.487263 min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 25% 3.754215 0.000000 0.000000 2.695477 0.000000 0.000000 1.235023 0.116340 0.000000 40.606701 50% 4.149742 0.000000 2.899821 3.046836 2.498311 0.000000 1.296665 0.143166 0.084138 44.311378 75% 4.610277 0.000000 3.529430 3.380436 3.242231 0.000000 2.120673 0.146348 0.098837 44.311378 max 6.792889 6.618528 5.887169 4.918575 5.243970 30.000000 2.778376 0.273963 0.195975 59.266378 <ul> <li>Now we can see that for the majority of the features, their standard deviation is much lower after transformation.</li> <li>This is a good thing, it shows that these features are more stable and predictable now.</li> </ul> <p>Let's quickly check the distributions of some of these features too.</p> In\u00a0[41]: Copied! <pre>fig, axs = plt.subplots(nrows=3, figsize=(8,10))\n# Plot histograms\nsns.distplot((clients[\"cons_12m\"].dropna()), ax=axs[0])\nsns.distplot((clients[clients[\"has_gas\"]=='t'][\"cons_gas_12m\"].dropna()), ax=axs[1])\nsns.distplot((clients[\"cons_last_month\"].dropna()), ax=axs[2])\nplt.tight_layout()\nplt.show()\n</pre> fig, axs = plt.subplots(nrows=3, figsize=(8,10)) # Plot histograms sns.distplot((clients[\"cons_12m\"].dropna()), ax=axs[0]) sns.distplot((clients[clients[\"has_gas\"]=='t'][\"cons_gas_12m\"].dropna()), ax=axs[1]) sns.distplot((clients[\"cons_last_month\"].dropna()), ax=axs[2]) plt.tight_layout() plt.show()"},{"location":"portfolio/projects/bcgx_powerco/task3.html#data-engineering","title":"Data Engineering\u00b6","text":""},{"location":"portfolio/projects/bcgx_powerco/task3.html#1-background","title":"1. Background\u00b6","text":""},{"location":"portfolio/projects/bcgx_powerco/task3.html#now-its-time-for-feature-engineering","title":"Now it\u2019s time for feature engineering\u00b6","text":"<p>Estelle reviewed your work with the AD and Estelle has come up with an idea to enrich the dataset when trying to predict churn:</p> <p>\u201cI think that the difference between off-peak prices in December and January the preceding year could be a significant feature when predicting churn\u201d</p> <ul> <li>As the Data Scientist on the team, you need to investigate this question.</li> <li>So, in this task you\u2019ll be responsible for completing feature engineering for the dataset.</li> </ul>"},{"location":"portfolio/projects/bcgx_powerco/task3.html#what-is-feature-engineering","title":"What is feature engineering?\u00b6","text":"<p>Feature engineering refers to:</p> <ul> <li><code>Addition</code></li> <li><code>Deletion</code></li> <li><code>Combination</code></li> <li><code>Mutation</code></li> </ul> <p>of your data set to improve machine learning model training, leading to better performance and greater accuracy.</p> <ul> <li>In context of this task, feature engineering refers to the engineering of the price and client data to create new columns that will help us to predict churn more accurately.</li> <li>Effective feature engineering is based on sound knowledge of the business problem and the available data sources.</li> </ul>"},{"location":"portfolio/projects/bcgx_powerco/task3.html#what-can-we-do","title":"What can we do\u00b6","text":"<p>First - can we remove any of the columns in the datasets?</p> <ul> <li>There will almost always be columns in a dataset that can be removed, perhaps because they are not relevant to the analysis, or they only have 1 unique value.</li> </ul> <p>Second - can we expand the datasets and use existing columns to create new features?</p> <pre><code>For example, if you have \u201cdate\u201d columns, in their raw form they are not so useful. But if you were to extract month, day of month, day of year and year into individual columns, these could be more useful.</code></pre> <p>Third - can we combine some columns together to create \u201cbetter\u201d columns?</p> <ul> <li>How do we define a \u201cbetter\u201d column and how do we know which columns to combine? We\u2019re trying to accurately predict churn - so a \u201cbetter\u201d column could be a column that improves the accuracy of the model.</li> <li>And which columns to combine? This can sometimes be a matter of experimenting until you find something useful, or you may notice that 2 columns share very similar information so you want to combine them.</li> </ul> <p>Finally - can we combine these datasets and if so, how?</p> <ul> <li>To combine datasets, you need a column that features in both datasets that share the same values to join them on.</li> </ul>"},{"location":"portfolio/projects/bcgx_powerco/task3.html#2-reading-updated-customer-dataset","title":"2. Reading Updated Customer Dataset\u00b6","text":"<p>We have been given a newly updated dataset, which contains some new data columns</p> <ul> <li>We need to look into our newly update dataset and understand the changes</li> </ul>"},{"location":"portfolio/projects/bcgx_powerco/task3.html#3-reading-prices-dataset","title":"3. Reading Prices Dataset\u00b6","text":"<p>We have been suggested to utilise some data from the previously loaded dataset <code>prices</code></p> <ul> <li><code>prices</code> contains for each customer, the information about the pricing during the year</li> <li>Notably: \"difference between off-peak prices in December and January the preceding year\"</li> </ul> What do the columns mean <p>Lets remind our selves what each column represents in the <code>prices</code> dataset</p> <pre><code>- id = client company identifier\n- price_date = reference date\n- price_off_peak_var = price of energy for the 1st period (off peak)\n- price_peak_var = price of energy for the 2nd period (peak)\n- price_mid_peak_var = price of energy for the 3rd period (mid peak)\n- price_off_peak_fix = price of power for the 1st period (off peak)\n- price_peak_fix = price of power for the 2nd period (peak)\n- price_mid_peak_fix = price of power for the 3rd period (mid peak)\n</code></pre>"},{"location":"portfolio/projects/bcgx_powerco/task3.html#4-adding-features","title":"4. Adding Features\u00b6","text":""},{"location":"portfolio/projects/bcgx_powerco/task3.html#41-difference-in-offpeak-prices","title":"4.1 | Difference in offpeak prices\u00b6","text":"<p>Lets focus on adding the feature that we were told about first!</p> <ul> <li>Difference between off-peak prices in December and preceding January</li> <li>We can do this by selecting the relevant dates and finding the shift value differences</li> </ul>"},{"location":"portfolio/projects/bcgx_powerco/task3.html#42-difference-in-offpeak-prices","title":"4.2 | Difference in offpeak prices\u00b6","text":"<p>This new feature doesn't seem to be that significant, lets keep searching!</p> <ul> <li>Lets return to our <code>price</code> dataframe and extract some more features that we might find relevant and should try out for our churn model</li> <li>We are given the prices for each individual month, so if take the averages from this data, it should vary slightly from those that we were given in the updated dataset</li> </ul>"},{"location":"portfolio/projects/bcgx_powerco/task3.html#43-max-price-changes-across-periods-and-months","title":"4.3 | Max price changes across periods and months\u00b6","text":"<p>Remove the duplicates and get the mean value for the user, month combination</p>"},{"location":"portfolio/projects/bcgx_powerco/task3.html#5-further-feature-engineering","title":"5. Further feature engineering\u00b6","text":""},{"location":"portfolio/projects/bcgx_powerco/task3.html#51-tenure","title":"5.1 | Tenure\u00b6","text":"<p>Some other features that we can add are how long a client has been with PowerCo, we have two features that can help us with realising this</p> <pre><code>    \"date_activ\": [\"date of activation of the contract\"],\n    \"date_end\": [\"registered date of the end of the contract\"],\n</code></pre>"},{"location":"portfolio/projects/bcgx_powerco/task3.html#53-transforming-numerical-data","title":"5.3 | Transforming numerical data\u00b6","text":"Skewness <ul> <li>In the previous section we saw that some variables were highly skewed</li> <li>The reason why we need to treat skewness is because some predictive models have inherent assumptions about the distribution of the features that are being supplied to it</li> <li>Such models are called <code>parametric</code> models, and they typically assume that all variables are both independent and normally distributed.</li> </ul> Treatment <ul> <li><p>Skewness isn't always a bad thing, but as a rule of thumb it is always good practice to treat highly skewed variables because of the reason stated above, but also as it can improve the speed at which predictive models are able to converge to its best solution.</p> </li> <li><p>There are many ways that you can treat skewed variables. You can apply transformations such as: <code>Square root</code>, <code>Cubic root</code>, <code>Logarithm</code></p> </li> </ul> Selected columns <ul> <li>Let's apply logarithmic transformations only to a selected number of columns</li> </ul>"},{"location":"portfolio/projects/bcgx_powerco/task3.html#6-takeaways","title":"6. Takeaways\u00b6","text":"<p>Some takeaways from this section:</p> <ul> <li>Our colleagues had added some new features into the dataset, which gave us some ideas for further feature engineering</li> </ul> <ul> <li>Furthermore, we were asked to test another hypothesis regarding regarding off-peak prices for our client's customers, preliminary indicators based on linear correlation didn't suggest that it will be any more relevant than others we have already created, however we need to check this hypothesis when we will create our model</li> </ul> <ul> <li>Building upon these ideas, we utilised the <code>price</code> dataset, in order to extract other features such as the maximum difference between a customer's electricity pricings, something we didn't do in our EDA</li> </ul> <ul> <li>We also added another interesting feature, which represents the number of years the customer is a client; we noted that this could be a feature that helps the model differentiate between the two targets</li> </ul> <ul> <li>Lastly, we decided upon creating transformations for some of the columns that had highly skewed univariate data distributions, for models based on trees, this won't really be relevant, however it is still good practice</li> </ul> <p>It must be noted that our takaways don't really take us anywhere without an interation of modeling, which we need to do and asssess which features are more relevant since feature engineering is an iterative process of trial and error</p>"},{"location":"portfolio/projects/bcgx_powerco/task4.html","title":"Task4","text":"In\u00a0[18]: Copied! <pre>import dask.dataframe as dd\n\ndf = dd.read_parquet('data_for_predictions.parquet')\ndf.head()\n</pre> import dask.dataframe as dd  df = dd.read_parquet('data_for_predictions.parquet') df.head() Out[18]: id cons_12m cons_gas_12m cons_last_month forecast_cons_12m forecast_discount_energy forecast_meter_rent_12m forecast_price_energy_off_peak forecast_price_energy_peak forecast_price_pow_off_peak ... months_modif_prod months_renewal channel_MISSING channel_ewpakwlliwisiwduibdlfmalxowmwpci channel_foosdfpfkusacimwkcsosbicdxkicaua channel_lmkebamcaaclubfxadlmueccxoimlema channel_usilxuppasemubllopkaafesmlibmsdf origin_up_kamkkxfxxuwbdslkwifmmcsiusiuosws origin_up_ldkssxwpmemidmecebumciepifcamkci origin_up_lxidpiddsbxsbosboudacockeimpuepw 0 24011ae4ebbe3035111d65fa7c15bc57 0.000000 4.739944 0.000000 0.000000 0.0 0.444045 0.114481 0.098142 40.606701 ... 2 6 0 0 1 0 0 0 0 1 1 d29c2c54acc38ff3c0614d0a653813dd 3.668479 0.000000 0.000000 2.280920 0.0 1.237292 0.145711 0.000000 44.311378 ... 76 4 1 0 0 0 0 1 0 0 2 764c75f661154dac3a6c254cd082ea7d 2.736397 0.000000 0.000000 1.689841 0.0 1.599009 0.165794 0.087899 44.311378 ... 68 8 0 0 1 0 0 1 0 0 3 bba03439a292a1e166f80264c16191cb 3.200029 0.000000 0.000000 2.382089 0.0 1.318689 0.146694 0.000000 44.311378 ... 69 9 0 0 0 1 0 1 0 0 4 149d57cf92fc41cf94415803a877cb4b 3.646011 0.000000 2.721811 2.650065 0.0 2.122969 0.116900 0.100015 40.606701 ... 71 9 1 0 0 0 0 1 0 0 <p>5 rows \u00d7 63 columns</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"portfolio/projects/bcgx_powerco/task4.html#feature-engineering-and-modelling","title":"Feature Engineering and Modelling\u00b6","text":""},{"location":"portfolio/projects/bcgx_powerco/task4.html#1-background","title":"1. Background\u00b6","text":""},{"location":"portfolio/projects/bcgx_powerco/task4.html#final-dataset-refinement","title":"Final dataset refinement\u00b6","text":"<p>Estelle conducted a further review and has provided you with a final dataset to use this for this task, named \u201cdata_for_predictions.csv\u201d.</p> <p>The outputs of your work will be shared with the AD and Estelle has given you a few points to include within the notebook:</p> <ul> <li>Why did you choose the evaluation metrics that you used? Please elaborate on your choices.</li> <li>Do you think that the model performance is satisfactory? Give justification for your answer.</li> <li>Make sure that your work is presented clearly with comments and explanations</li> </ul>"},{"location":"portfolio/projects/bcgx_powerco/task4.html#binary-classifier","title":"Binary Classifier\u00b6","text":"<p>From our business requirements, we know that we need to understand why our client's customers are churning. We were given data that corresponds to a binary outcome for each of their customer, which represens the fact of churn in three months (either churn, target = 1 or stayed, taget = 0)</p> <p>We also noted that various features which we triend to engineer all have very weak linear correlation to the target variable, which suggests that to undestand why customers churn we need to turn to a model which is able to capture the complexity of our non linearity in the dataset</p> <p>For this reason we decide to treat this problem as as binary classifiation problem in which our model will learn to differentiate between the two target outcomes (churn or stayed)</p>"},{"location":"portfolio/projects/bcgx_powerco/task4.html#descriptive-model","title":"Descriptive Model\u00b6","text":"<p>Our requirement for our model is that it needs to be able to explain which features we pass into it is relevant and those which are not, for this reason we can turn our attention to <code>Random Forest</code></p>"},{"location":"portfolio/projects/bcgx_powerco/task4.html#7-finally-lets-create-a-quick-summary-for-the-client","title":"7. Finally, let's create a quick summary for the client\u00b6","text":"<p>Before we finish up, the client wants a quick update on the project progress. Your AD wants you to draft an abstract (executive summary) of your findings so far.</p>"},{"location":"portfolio/projects/bcgx_powerco/task4.html#here-is-your-task","title":"Here is your task:\u00b6","text":"<p>Develop an abstract slide synthesizing all the findings from the project so far, keeping in mind that this will be for the key stakeholders meeting which the Head of the SME division, as well as other various stakeholders, will be attending.</p> <p>Note: a steering committee meeting is a meeting where the BCG team presents key findings and recommendations (and/or project progress) to key client stakeholders.</p> <p>A few things to think about for this abstract include:</p> <ul> <li>What is the most important number or metric to share with the client?</li> <li>What impact would the model have on the client\u2019s bottom line?</li> </ul> <p>Please note, there are multiple ways to approach the task and that the sample answer is just one way to do it.</p> <p>If you are stuck:</p> <ul> <li>What do you think the client wants to hear? How much detail should you go into, especially with the technical details of your work?</li> <li>Always test what you write with the \u201cso what?\u201d test, i.e. sharing a fact, even an interesting one, only matters if the client can actually do something useful with it.</li> <li>E.g. 60% of your customers are from City A is pointless, but customers in City A should be prioritized for giving discount as they are among your most valuable ones, if true, is an actionable finding.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/index.html","title":"Lloyd's Banking Group Data Science","text":""},{"location":"portfolio/projects/lloyds_ds/index.html#welcome-to-the-data-science-job-simulation","title":"Welcome to the\u00a0Data Science Job Simulation","text":""},{"location":"portfolio/projects/lloyds_ds/index.html#your-role","title":"Your Role","text":"<ul> <li>You are a data science graduate at Lloyds Banking Group.</li> <li>You are part of the Data Science &amp; Analytics team, where each member contributes uniquely to projects. </li> <li>Your team has been assigned the task of enhancing customer retention through analytics, focusing on predicting customer churn. </li> <li>As a recent graduate eager to apply your data science skills, you see this project as an excellent opportunity to make a meaningful impact.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/index.html#your-goal","title":"Your Goal","text":"<ul> <li>Your primary task is to collaborate with the team to gather and analyse data relevant to customer churn.</li> <li>You need to perform exploratory data analysis to uncover patterns and insights, clean and preprocess the data, and build a predictive model for customer churn.</li> <li>Additionally, you are expected to suggest ways to evaluate and measure the model's performance.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/index.html#project-briefing","title":"Project briefing","text":""},{"location":"portfolio/projects/lloyds_ds/index.html#project-brief","title":"Project brief","text":"<p>Welcome to the Data Science &amp; Analytics team at Lloyds Banking Group. As a new data science graduate, you have been entrusted with a critical project that could significantly impact our customer retention strategies. Li, our senior data scientist, has specifically chosen you to take over this project due to your strong analytical skills and enthusiasm for solving real-world business problems. This is an exciting opportunity for you to apply your knowledge and make a real impact within our team.</p>"},{"location":"portfolio/projects/lloyds_ds/index.html#context","title":"Context","text":"<p>The project you are about to embark on is the \"Customer Retention Enhancement through Predictive Analytics\" initiative. This project arose from an urgent need to address declining retention rates within certain segments of our customer base. Over the past few months, we've noticed a worrying trend of increased customer churn, particularly among young professionals and small business owners. This poses a substantial threat to our market position and long-term profitability.</p> <p>Our fictional client, SmartBank, a subsidiary of Lloyds, has reported that a substantial portion of their customer base is at risk of moving to competitors offering more personalised banking solutions. SmartBank has tasked our team with developing a predictive model to identify at-risk customers and propose targeted interventions to retain them.</p>"},{"location":"portfolio/projects/lloyds_ds/index.html#key-concepts","title":"Key concepts","text":"<p>Before you begin, it's essential to understand a few key concepts:</p> <ol> <li>Customer churn:\u00a0The process by which customers stop doing business with a company. Identifying and preventing churn is crucial for maintaining a stable customer base.</li> <li>Predictive analytics:\u00a0Techniques that use historical data to forecast future possibilities. In this project, you'll use predictive analytics to predict which customers are likely to churn.</li> <li>Exploratory data analysis (EDA):\u00a0A method of analysing data sets to summarise their primary characteristics, often using visual strategies. EDA is crucial for understanding the data before building predictive models.</li> <li>Machine learning models:\u00a0Algorithms that let computers learn from and make predictions or decisions based on data. You'll be building a classification model to predict customer churn.</li> </ol>"},{"location":"portfolio/projects/lloyds_ds/index.html#project-requirements","title":"Project requirements","text":"<p>Your task involves two main phases.</p>"},{"location":"portfolio/projects/lloyds_ds/index.html#phase-1-data-gathering-and-exploratory-analysis","title":"Phase 1: Data gathering and exploratory analysis","text":"<ul> <li>Objective:\u00a0Collect relevant data, perform exploratory data analysis (EDA), and prepare the data for modelling.</li> <li>Steps:<ul> <li>Identify and gather data from provided sources relevant to predicting customer churn.</li> <li>Conduct EDA to understand the data, identify key features, and uncover patterns and insights.</li> <li>Clean and preprocess the data to ensure it is ready for model building.</li> </ul> </li> <li>Deliverable:\u00a0Submit a report detailing your exploratory data analysis and key findings, including the cleaned and preprocessed data set.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/index.html#phase-2-building-a-machine-learning-model","title":"Phase 2: Building a machine learning model","text":"<ul> <li>Objective:\u00a0Develop a predictive model to identify customers at risk of churning and propose ways to measure the model\u2019s performance.</li> <li>Steps:<ul> <li>Choose a suitable machine learning algorithm for the classification task.</li> <li>Build and train the model to predict customer churn.</li> <li>Suggest ways to evaluate and measure the model\u2019s performance, ensuring its reliability and accuracy.</li> </ul> </li> <li>Deliverable:\u00a0Submit a report, including the trained machine learning model and your proposed methods for evaluating and measuring the model\u2019s performance.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/index.html#the-importance-of-this-project","title":"The importance of this project","text":"<p>Addressing customer churn is critical for maintaining SmartBank's competitive edge and ensuring long-term sustainability. The board of directors is strongly encouraging the team to deliver a solution promptly, as timely delivery will help secure our market position and capitalize on opportunities. The team is feeling the heat, but they are also highly motivated and determined to turn this challenge into an opportunity. Your role is pivotal in this effort, and your contributions will be closely monitored by senior management.</p> <p>Embrace this chance to showcase your skills and make a meaningful impact. It\u2019s time to dive in and get to work!</p>"},{"location":"portfolio/projects/lloyds_ds/index.html#tasks-notebooks","title":"Tasks &amp; Notebooks","text":"<p>The task descriptions and jupyter notebooks for this project</p> <ul> <li>Task 1: Description</li> <li>Task 1: Notebook</li> <li>Task 2: Description</li> <li>Task 2: Notebook</li> </ul> <p>Any questions or comments about the above post can be addressed on the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>"},{"location":"portfolio/projects/lloyds_ds/task1.html","title":"Let's get started","text":"<p>As you step into the role of a data science graduate at Lloyds Banking Group, you're immediately thrust into a real-world scenario with significant implications for our business. The Data Science &amp; Analytics team, under the leadership of Li, a seasoned senior data scientist, is currently grappling with a critical project: predicting customer churn to enhance retention strategies.</p> <p>Li briefs you on the situation, \"We've observed a worrying trend of customers, particularly young professionals and small business owners, leaving for competitors. This project aims to reverse that trend by identifying at-risk customers and implementing targeted interventions.\"</p> <p>Your task is crucial. You'll begin by gathering and analysing data to understand the factors contributing to customer churn to uncover actionable insights that can inform strategic decisions. The pressure is on, as SmartBank, a key subsidiary, has reported a decline in retention rates, and there's mounting pressure from senior management to deliver solutions swiftly.</p> <p>Li emphasises the importance of this task, noting, \"Our findings will directly impact the strategies we deploy to retain our customers. We need accurate, insightful analysis to inform these strategies.\"</p> <p>You're not alone in this; the team is here to guide and support you. This is your opportunity to apply your skills, contributing to a project that could shape the future of customer engagement at Lloyds.</p> <p></p>"},{"location":"portfolio/projects/lloyds_ds/task1.html#techniques-for-identifying-and-collecting-data","title":"Techniques for identifying and collecting data","text":""},{"location":"portfolio/projects/lloyds_ds/task1.html#understanding-the-data-landscape","title":"Understanding the data landscape","text":"<p>The first step in data collection is to understand the landscape of available data. In a corporate environment, data can come from a variety of sources, including internal databases, customer relationship management (CRM) systems, financial records, web analytics, and external data sets. Knowing where data resides and how it can be accessed is crucial for gathering relevant information efficiently.</p>"},{"location":"portfolio/projects/lloyds_ds/task1.html#defining-data-requirements","title":"Defining data requirements","text":"<p>Clearly define what you need to know before you begin. To predict customer churn, identify key variables such as customer demographics, transaction history, customer service interactions, and usage patterns. This will help focus your efforts on collecting the most relevant and useful data for your analysis.</p>"},{"location":"portfolio/projects/lloyds_ds/task1.html#data-collection-methods","title":"Data collection methods","text":"<p>Data can be collected through several methods:</p> <ul> <li>Primary data collection: This involves gathering data directly from the source. For example, conducting surveys and interviews, or direct observation. While primary data is specific and relevant, it can be time-consuming and costly to obtain.</li> <li>Secondary data collection: Involves using existing data collected for another purpose. This could include historical data, third-party data sets, or data obtained from public sources. Secondary data is often easier and quicker to access but may require more careful consideration to ensure its relevance and accuracy.</li> <li>Automated data collection: Using tools such as web scraping, application programming interfaces (APIs), or data integration platforms to automatically gather data from online sources or databases. This method is efficient for collecting large volumes of data and keeping it up-to-date.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task1.html#evaluating-data-quality","title":"Evaluating data quality","text":"<p>Once data is collected, it's essential to evaluate its quality. Consider the following aspects:</p> <ul> <li>Accuracy: Ensure that the data accurately reflects the real-world scenarios you are studying.</li> <li>Completeness: Check for missing values or incomplete records that might skew your analysis.</li> <li>Consistency: Ensure that data is consistent across different sources and time periods.</li> <li>Timeliness: Data should be current and relevant to the time frame of your analysis.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task1.html#data-integration-and-storage","title":"Data integration and storage","text":"<p>After collecting data from various sources, the next step is to integrate it into a cohesive data set. This may involve merging data sets, transforming data formats, or cleaning data to ensure uniformity. Once integrated, store the data securely, ensuring that it is organised and easily accessible for analysis.</p> <p>By mastering these techniques, you'll be well-prepared to gather and utilise data effectively, setting the stage for a successful data analysis project.</p> <p></p>"},{"location":"portfolio/projects/lloyds_ds/task1.html#methods-for-performing-exploratory-data-analysis-to-uncover-patterns-and-insights","title":"Methods for performing exploratory data analysis to uncover patterns and insights","text":"<p>Exploratory data analysis (EDA) is a crucial stage in the data science process. It allows you to understand the underlying configuration of your data and identify key patterns and relationships. EDA involves using statistical techniques and data visualisation to summarise the main characteristics of the data set.</p>"},{"location":"portfolio/projects/lloyds_ds/task1.html#descriptive-statistics","title":"Descriptive statistics","text":"<p>Begin your EDA by calculating descriptive statistics, which provide a summary of the basic features of your data. Key metrics include:</p> <ul> <li>Mean, median, and mode: These measures of central tendency help you understand the typical value in your data set.</li> <li>Standard deviation and variance: These metrics indicate the spread or dispersion of your data, showing how much variation exists from the average.</li> <li>Min, max, and range: These values provide insights into the bounds of your data, highlighting any potential outliers.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task1.html#data-visualisation","title":"Data visualisation","text":"<p>Visualisation is a powerful tool in EDA, helping you to quickly grasp complex data distributions and relationships. Common visualisation techniques include:</p> <ul> <li>Histograms and density plots: Useful for understanding the distribution of a single variable, including its central tendency and spread.</li> <li>Box plots: Help identify the spread and outliers in your data by showing the quartiles and median.</li> <li>Scatter plots: Ideal for examining relationships between two continuous variables, helping you identify potential correlations or trends.</li> <li>Bar charts and heatmaps: Useful for categorical data, showing the frequency or proportion of categories and the relationships between categorical variables.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task1.html#correlation-analysis","title":"Correlation analysis","text":"<p>To uncover relationships between variables, perform a correlation analysis. This involves calculating correlation coefficients, such as Pearson\u2019s or Spearman\u2019s, which quantify the strength and direction of the relationship between variables. Understanding these correlations can help you identify key predictors of customer churn.</p>"},{"location":"portfolio/projects/lloyds_ds/task1.html#data-profiling-and-anomaly-detection","title":"Data profiling and anomaly detection","text":"<p>Data profiling involves examining data for anomalies, missing values, or inconsistencies. This phase is essential for ensuring data quality before proceeding to model building. Look for patterns in missing data, which could indicate underlying data collection issues or important missing information.</p>"},{"location":"portfolio/projects/lloyds_ds/task1.html#hypothesis-generation","title":"Hypothesis generation","text":"<p>Based on the insights gained from descriptive statistics and visualisations, formulate hypotheses about your data. For example, you might hypothesise that high customer service interaction frequency is related to increased churn rates. These hypotheses can guide your subsequent analysis and model-building efforts.</p> <p>Employing these methods can help you uncover critical insights from your data, guiding your understanding and informing your decision-making process as you prepare for more advanced data analysis stages.</p> <p></p>"},{"location":"portfolio/projects/lloyds_ds/task1.html#best-practices-for-cleaning-and-preparing-data-for-machine-learning-models","title":"Best practices for cleaning and preparing data for machine learning models","text":"<p>Properly cleaning and preparing data is essential for building reliable and accurate machine learning models. This process ensures that the data used in model training is of high quality, which directly impacts the model's performance.</p>"},{"location":"portfolio/projects/lloyds_ds/task1.html#handling-missing-data","title":"Handling missing data","text":"<p>Missing data is a typical problem that can significantly affect your model's accuracy. Here are a few strategies to address this problem:</p> <ul> <li>Imputation: Replace missing values with a statistical measure such as the mean, median, or mode of the column. For categorical variables, the most frequent category can be used.</li> <li>Deletion: Remove rows or columns with missing values, particularly if the proportion of missing data is small. However, this should be done cautiously to avoid losing valuable information.</li> <li>Flagging: Create a new binary column that flags whether data was missing in the original data set. This can help your model learn if the absence of data is itself informative.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task1.html#outlier-detection-and-treatment","title":"Outlier detection and treatment","text":"<p>Outliers can skew the results of your machine-learning model. Use visualisation techniques like box plots or statistical methods to detect outliers. Once identified, you can:</p> <ul> <li>Remove outliers: If they are caused by data entry errors or are not relevant to the analysis.</li> <li>Cap outliers: Set a threshold beyond which data is capped. This technique minimises the influence of extreme values without removing data points entirely.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task1.html#normalisation-and-standardisation","title":"Normalisation and standardisation","text":"<p>Data normalisation and standardisation are techniques used to ensure that numerical features contribute equally to the model's learning process. These processes involve:</p> <ul> <li>Normalisation: Rescaling the values of numeric features to a common scale, typically [0, 1]. This is useful when features have different units or scales.</li> <li>Standardisation: Transforming data to have a mean of zero and a standard deviation of one. This process is particularly useful when the data follows a Gaussian distribution.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task1.html#encoding-categorical-variables","title":"Encoding categorical variables","text":"<p>Machine learning models require numerical input, making it necessary to convert categorical data into numerical form. Common methods include:</p> <ul> <li>One-hot encoding: Creating binary columns for each category in a categorical feature. This method prevents the model from assuming any ordinal relationship between categories.</li> <li>Label encoding: Converting each category to a numerical value. This method is simpler but should be used with caution as it can imply an ordinal relationship where none exists.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task1.html#feature-engineering-and-selection","title":"Feature engineering and selection","text":"<p>Creating new features from the existing data (feature engineering) and selecting the most relevant features (feature selection) can significantly improve model performance. Techniques include:</p> <ul> <li>Creating interaction features: Combining two or more features to capture interactions.</li> <li>Feature scaling: Adjusting the range of features to ensure they contribute equally to the model.</li> <li>Dimensionality reduction: Using methods such as principal component analysis (PCA) to reduce the number of features, which can improve model performance and reduce overfitting.</li> </ul> <p>By adhering to these best practices, you'll ensure that your data set is clean, well-prepared, and optimised for building effective machine learning models. This foundational work is crucial for achieving accurate and reliable predictions in your project.</p> <p></p>"},{"location":"portfolio/projects/lloyds_ds/task1.html#data-gathering-and-exploratory-analysis","title":"Data gathering and exploratory analysis","text":"<p>Now that you've been introduced to the scenario and the importance of this project, it's time to roll up your sleeves and get started. This task will challenge you to apply your data science skills in a real-world context, helping you connect theoretical knowledge and practical application.</p>"},{"location":"portfolio/projects/lloyds_ds/task1.html#data-collection-start-with-relevance","title":"Data collection: start with relevance","text":"<p>Your first step is to identify and gather relevant data that will provide insights into customer churn. Focus on data that can help you understand customer behaviour, such as demographics, transaction history, and customer service interactions. Remember, the goal is to collect data that is pertinent to the problem at hand. Approach this step with a critical eye, considering how each piece of data might contribute to understanding why customers are leaving.</p>"},{"location":"portfolio/projects/lloyds_ds/task1.html#eda-discover-patterns-and-insights","title":"EDA: discover patterns and insights","text":"<p>Once you've collected the data, the next step is to perform EDA. This phase is crucial as it helps you uncover patterns, identify anomalies, and understand the data's structure. Use visual tools such as histograms, scatter plots, and heat maps to explore relationships between variables. Pay close attention to trends that might indicate early signs of churn, such as decreased usage frequency or increased interaction with customer service.</p> <p>EDA is not just a technical exercise; it's an opportunity to hypothesise the underlying causes of churn. For example, if you notice that customers who engage less with your digital services are more likely to churn, this insight could inform targeted retention strategies. Your analysis should be thorough and well-documented, providing a clear narrative that connects your findings to potential business actions.</p>"},{"location":"portfolio/projects/lloyds_ds/task1.html#data-cleaning-and-preparation-ensuring-quality","title":"Data cleaning and preparation: ensuring quality","text":"<p>The quality of your data affects the reliability of your predictive model. In this stage, focus on cleaning and preparing your data set. Handle missing values appropriately, either through imputation or removal, and ensure that all variables are in a consistent format. Normalising or standardising your data may be necessary, especially if the data includes variables with different scales. This step is about precision and care; small errors can lead to significant inaccuracies in your model.</p> <p>As you work through these activities, keep the broader project goals in mind. Your findings from EDA and your cleaned data set will form the foundation for building a robust predictive model. This model will help SmartBank not only understand current churn rates but also anticipate and mitigate future risks, directly influencing customer retention strategies.</p>"},{"location":"portfolio/projects/lloyds_ds/task1.html#why-accuracy-and-thorough-understanding-matter","title":"Why accuracy and thorough understanding matter","text":"<p>Your work in this task is not just about completing an assignment; it's about developing a deep, practical understanding of data science in action. Accuracy and thorough understanding are crucial, as the insights you derive will inform strategic decisions at SmartBank. The quality of your analysis could mean the difference between retaining valuable customers and losing them to competitors.</p> <p>Approach each step with diligence and an analytical mindset. This is your opportunity to make a tangible impact on a real-world problem, honing your skills in the process. Embrace the challenge, knowing that your contributions are vital to the project's success.</p> <p></p>"},{"location":"portfolio/projects/lloyds_ds/task1.html#task-instructions","title":"Task instructions","text":"<p>Introduction</p> <p>In this task, you will take the first critical steps toward building a predictive model for customer churn. Your work will involve gathering relevant data, conducting EDA, and preparing the data set for model development. These activities are foundational for ensuring the accuracy and reliability of your subsequent analysis and predictions.</p> <p>Instructions</p> <p>Identify and gather data:</p> <ul> <li>Review the provided data sources and select those most relevant for predicting customer churn. Focus on key areas such as customer demographics, transaction history, and customer service interactions.</li> <li>Document your selection criteria and rationale for choosing each data set, ensuring that the data will provide meaningful insights into customer behaviour.</li> </ul> <p>Perform EDA:</p> <ul> <li>Use statistical techniques and data visualisation tools to explore the data sets. Create visualisations such as histograms, scatter plots, and box plots to understand distributions, trends, and relationships between variables.</li> <li>Identify key features that may influence customer churn, paying special attention to patterns or anomalies that could be significant.</li> </ul> <p>Clean and preprocess the data:</p> <ul> <li>Handle missing values by choosing appropriate methods such as imputation, removal, or flagging. Justify your chosen method based on the data and context.</li> <li>Detect and address outliers that could skew the analysis or predictions. Decide whether to cap, transform, or remove outliers based on their nature and potential impact.</li> <li>Standardise or normalise numerical features to ensure consistent scales across variables. This step is crucial for preparing the data for machine learning algorithms.</li> <li>Encode categorical variables using techniques like one-hot encoding to transform them into a numerical form appropriate for analysis.</li> </ul> <p>Deliverable:</p> <ul> <li>File submission: Submit a comprehensive report detailing your data gathering, EDA, and data cleaning processes. The report should include:<ul> <li>A summary of the data sets selected and the rationale for their inclusion</li> <li>Visualisations and statistical summaries from the EDA</li> <li>A description of the data cleaning and preprocessing steps taken</li> <li>The cleaned and preprocessed data set ready for model building</li> </ul> </li> </ul> <p>Ensure that your report is clear, concise, and well-organised, as it will be a key component of the project's success, guiding future analysis and model development.</p> <p>Any questions or comments about the above post can be addressed on the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>"},{"location":"portfolio/projects/lloyds_ds/task1_nb.html","title":"Task1 nb","text":"In\u00a0[2]: Copied! <pre>import pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings; warnings.filterwarnings('ignore')\nfrom IPython.display import Image\n</pre> import pandas as pd import plotly.express as px import matplotlib.pyplot as plt import seaborn as sns import warnings; warnings.filterwarnings('ignore') from IPython.display import Image In\u00a0[3]: Copied! <pre>ds = pd.read_excel('Customer_Churn_Data_Large.xlsx',sheet_name=None)\nds.keys()\n</pre> ds = pd.read_excel('Customer_Churn_Data_Large.xlsx',sheet_name=None) ds.keys() Out[3]: <pre>dict_keys(['Customer_Demographics', 'Transaction_History', 'Customer_Service', 'Online_Activity', 'Churn_Status'])</pre> In\u00a0[4]: Copied! <pre>demographic = ds['Customer_Demographics']\ntransaction = ds['Transaction_History']\nservice = ds['Customer_Service']\nactivity = ds['Online_Activity']\nstatus = ds['Churn_Status']\nstatus['ChurnStatus'] = pd.Categorical(status['ChurnStatus'],[0,1])\n</pre> demographic = ds['Customer_Demographics'] transaction = ds['Transaction_History'] service = ds['Customer_Service'] activity = ds['Online_Activity'] status = ds['Churn_Status'] status['ChurnStatus'] = pd.Categorical(status['ChurnStatus'],[0,1]) In\u00a0[5]: Copied! <pre>demographic.head()\n</pre> demographic.head() Out[5]: CustomerID Age Gender MaritalStatus IncomeLevel 0 1 62 M Single Low 1 2 65 M Married Low 2 3 18 M Single Low 3 4 21 M Widowed Low 4 5 21 M Divorced Medium In\u00a0[6]: Copied! <pre>transaction.head()\n</pre> transaction.head() Out[6]: CustomerID TransactionID TransactionDate AmountSpent ProductCategory 0 1 7194 2022-03-27 416.50 Electronics 1 2 7250 2022-08-08 54.96 Clothing 2 2 9660 2022-07-25 197.50 Electronics 3 2 2998 2022-01-25 101.31 Furniture 4 2 1228 2022-07-24 397.37 Clothing In\u00a0[7]: Copied! <pre>service.head()\n</pre> service.head() Out[7]: CustomerID InteractionID InteractionDate InteractionType ResolutionStatus 0 1 6363 2022-03-31 Inquiry Resolved 1 2 3329 2022-03-17 Inquiry Resolved 2 3 9976 2022-08-24 Inquiry Resolved 3 4 7354 2022-11-18 Inquiry Resolved 4 4 5393 2022-07-03 Inquiry Unresolved In\u00a0[8]: Copied! <pre>activity.head()\n</pre> activity.head() Out[8]: CustomerID LastLoginDate LoginFrequency ServiceUsage 0 1 2023-10-21 34 Mobile App 1 2 2023-12-05 5 Website 2 3 2023-11-15 3 Website 3 4 2023-08-25 2 Website 4 5 2023-10-27 41 Website In\u00a0[9]: Copied! <pre>status.head()\n</pre> status.head() Out[9]: CustomerID ChurnStatus 0 1 0 1 2 1 2 3 0 3 4 0 4 5 0 In\u00a0[10]: Copied! <pre>numbers = demographic.merge(status,left_on='CustomerID',right_on='CustomerID',how='left').groupby(['ChurnStatus'])['CustomerID'].describe()['count']\nnumbers/numbers.sum()\n</pre> numbers = demographic.merge(status,left_on='CustomerID',right_on='CustomerID',how='left').groupby(['ChurnStatus'])['CustomerID'].describe()['count'] numbers/numbers.sum() Out[10]: <pre>ChurnStatus\n0    0.796\n1    0.204\nName: count, dtype: float64</pre> In\u00a0[11]: Copied! <pre>demo_churn = demographic.merge(status,left_on='CustomerID',right_on='CustomerID')\n</pre> demo_churn = demographic.merge(status,left_on='CustomerID',right_on='CustomerID') In\u00a0[12]: Copied! <pre>age_churn = demo_churn.groupby(['Age','ChurnStatus']).agg(Counts=('Age','count')).reset_index()\ntots = demo_churn.groupby(['Age'],as_index=False).agg(Counts=('Age','count'))\nmerged = age_churn.merge(tots,left_on='Age',right_on='Age')\nmerged['Percent'] = ((merged['Counts_x']/merged['Counts_y'])*100).round(2)\n</pre> age_churn = demo_churn.groupby(['Age','ChurnStatus']).agg(Counts=('Age','count')).reset_index() tots = demo_churn.groupby(['Age'],as_index=False).agg(Counts=('Age','count')) merged = age_churn.merge(tots,left_on='Age',right_on='Age') merged['Percent'] = ((merged['Counts_x']/merged['Counts_y'])*100).round(2) In\u00a0[13]: Copied! <pre>merged[merged['ChurnStatus'] == 1]['Counts_x'].sum()/merged['Counts_x'].sum()\n</pre> merged[merged['ChurnStatus'] == 1]['Counts_x'].sum()/merged['Counts_x'].sum() Out[13]: <pre>np.float64(0.204)</pre> In\u00a0[47]: Copied! <pre># age bins of customers who churned\nmerged['Age_bins'] = pd.cut(merged['Age'],[10,20,30,40,50,60,70])\nmerged['Age_bins'] = merged['Age_bins'].astype('string')\nprint(merged[merged['ChurnStatus'] == 1].groupby(['Age_bins'])['Percent'].describe().to_markdown(tablefmt='simple'))\n</pre> # age bins of customers who churned merged['Age_bins'] = pd.cut(merged['Age'],[10,20,30,40,50,60,70]) merged['Age_bins'] = merged['Age_bins'].astype('string') print(merged[merged['ChurnStatus'] == 1].groupby(['Age_bins'])['Percent'].describe().to_markdown(tablefmt='simple')) <pre>Age_bins      count     mean       std    min      25%     50%      75%    max\n----------  -------  -------  --------  -----  -------  ------  -------  -----\n(10, 20]          3  21.16     3.6861   17.65  19.24    20.83   22.915   25\n(20, 30]         10  19.288    7.67809   9.09  12.7075  21.11   23.6275  31.58\n(30, 40]         10  18.298   12.0951    0     10.2775  16.325  26.7025  38.46\n(40, 50]         10  20.87     8.92221   5.26  15.23    21.11   25.99    33.33\n(50, 60]         10  22.324    8.66058  11.11  15.79    22.115  29.2     35\n(60, 70]          9  20.4722   9.27481   7.14  14.29    16.67   28.57    33.33\n</pre> In\u00a0[46]: Copied! <pre>print(merged[merged['ChurnStatus'] == 0].groupby(['Age_bins'])['Percent'].describe().to_markdown(tablefmt='simple'))\n</pre> print(merged[merged['ChurnStatus'] == 0].groupby(['Age_bins'])['Percent'].describe().to_markdown(tablefmt='simple')) <pre>Age_bins      count     mean       std    min      25%     50%      75%     max\n----------  -------  -------  --------  -----  -------  ------  -------  ------\n(10, 20]          3  78.84     3.6861   75     77.085   79.17   80.76     82.35\n(20, 30]         10  80.712    7.67809  68.42  76.3725  78.89   87.2925   90.91\n(30, 40]         10  81.702   12.0951   61.54  73.2975  83.675  89.7225  100\n(40, 50]         10  79.13     8.92221  66.67  74.01    78.89   84.77     94.74\n(50, 60]         10  77.676    8.66058  65     70.8     77.885  84.21     88.89\n(60, 70]          9  79.5278   9.27481  66.67  71.43    83.33   85.71     92.86\n</pre> In\u00a0[39]: Copied! <pre>def m(x):\n    return print(x.to_markdown(tablefmt='simple'))\n</pre> def m(x):     return print(x.to_markdown(tablefmt='simple')) In\u00a0[45]: Copied! <pre>gender_churn = demo_churn.groupby(['Gender','ChurnStatus']).agg(counts=('Age','count'))\ngender_churn['percentage'] = 100 * gender_churn['counts'] / gender_churn['counts'].sum()\nm(gender_churn)\n</pre> gender_churn = demo_churn.groupby(['Gender','ChurnStatus']).agg(counts=('Age','count')) gender_churn['percentage'] = 100 * gender_churn['counts'] / gender_churn['counts'].sum() m(gender_churn) <pre>            counts    percentage\n--------  --------  ------------\n('F', 0)       412          41.2\n('F', 1)       101          10.1\n('M', 0)       384          38.4\n('M', 1)       103          10.3\n</pre> In\u00a0[42]: Copied! <pre>marital_churn = demo_churn.groupby(['MaritalStatus','ChurnStatus']).agg(counts=('Age','count')) \nmarital_churn['percentage'] = 100 * marital_churn['counts'] / marital_churn['counts'].sum()\nm(marital_churn)\n</pre> marital_churn = demo_churn.groupby(['MaritalStatus','ChurnStatus']).agg(counts=('Age','count'))  marital_churn['percentage'] = 100 * marital_churn['counts'] / marital_churn['counts'].sum() m(marital_churn) <pre>                   counts    percentage\n---------------  --------  ------------\n('Divorced', 0)       202          20.2\n('Divorced', 1)        46           4.6\n('Married', 0)        201          20.1\n('Married', 1)         60           6\n('Single', 0)         171          17.1\n('Single', 1)          44           4.4\n('Widowed', 0)        222          22.2\n('Widowed', 1)         54           5.4\n</pre> In\u00a0[43]: Copied! <pre>income_churn = demo_churn.groupby(['IncomeLevel','ChurnStatus']).agg(counts=('Age','count'))\nincome_churn['percentage'] = 100 * income_churn['counts'] / income_churn['counts'].sum()\nm(income_churn)\n</pre> income_churn = demo_churn.groupby(['IncomeLevel','ChurnStatus']).agg(counts=('Age','count')) income_churn['percentage'] = 100 * income_churn['counts'] / income_churn['counts'].sum() m(income_churn) <pre>                 counts    percentage\n-------------  --------  ------------\n('High', 0)         282          28.2\n('High', 1)          67           6.7\n('Low', 0)          253          25.3\n('Low', 1)           72           7.2\n('Medium', 0)       261          26.1\n('Medium', 1)        65           6.5\n</pre> In\u00a0[20]: Copied! <pre># average number of transactions\ntransaction_churn = transaction.groupby(['CustomerID'],as_index=False).agg(Counts=('TransactionID','count')).merge(status,left_on='CustomerID',right_on='CustomerID')\ntransaction_churn['Active'] = transaction_churn['Counts'].apply(lambda x: x &gt; 0)\ntransaction_churn.groupby(['ChurnStatus'])['Counts'].median()\n</pre> # average number of transactions transaction_churn = transaction.groupby(['CustomerID'],as_index=False).agg(Counts=('TransactionID','count')).merge(status,left_on='CustomerID',right_on='CustomerID') transaction_churn['Active'] = transaction_churn['Counts'].apply(lambda x: x &gt; 0) transaction_churn.groupby(['ChurnStatus'])['Counts'].median() Out[20]: <pre>ChurnStatus\n0    5.0\n1    5.0\nName: Counts, dtype: float64</pre> In\u00a0[21]: Copied! <pre># average transaction by user\ntransaction_data = transaction.merge(status,left_on='CustomerID',right_on='CustomerID')\n</pre> # average transaction by user transaction_data = transaction.merge(status,left_on='CustomerID',right_on='CustomerID') <p>Lets check how each groups tends to spend on different product categories</p> In\u00a0[38]: Copied! <pre>trx_churn_prods_vals = transaction_data.groupby(['ProductCategory','ChurnStatus'])['AmountSpent'].describe().round(2)\nprint(trx_churn_prods_vals.to_markdown(tablefmt='simple'))\n</pre> trx_churn_prods_vals = transaction_data.groupby(['ProductCategory','ChurnStatus'])['AmountSpent'].describe().round(2) print(trx_churn_prods_vals.to_markdown(tablefmt='simple')) <pre>                      count    mean     std    min     25%     50%     75%     max\n------------------  -------  ------  ------  -----  ------  ------  ------  ------\n('Books', 0)            844  248.96  141.55   5.18  127.01  248.5   365.49  495.85\n('Books', 1)            197  250.99  146.68   6.26  118.64  260.79  381.84  499.86\n('Clothing', 0)         813  253     142.76   5.76  129.07  256.79  372.13  499.06\n('Clothing', 1)         187  250.05  153.31   5.65  110.12  238.97  393.88  496.88\n('Electronics', 0)      776  248.27  139.61   5.53  129.58  244.38  366.77  498.03\n('Electronics', 1)      225  251.02  140.06   6.3   125.88  255.14  371.73  490.6\n('Furniture', 0)        779  247.99  140.56   6.79  129.37  243.71  365.36  499.7\n('Furniture', 1)        213  242.27  140.58   5.86  119.98  224.83  366.66  499.51\n('Groceries', 0)        820  251.88  145.05   5.2   126.24  249.44  381.39  499.57\n('Groceries', 1)        200  272.95  138.11   5.92  161.76  277.58  389.38  494.9\n</pre> In\u00a0[23]: Copied! <pre>churn_trans_average_all = transaction_data.set_index('TransactionDate').groupby('ChurnStatus').resample('7d')['AmountSpent'].mean().reset_index()\nchurn_trans_average_max = transaction_data.set_index('TransactionDate').groupby('ChurnStatus').resample('7d')['AmountSpent'].max().reset_index()\nchurn_trans_average_groc = transaction_data[transaction_data['ProductCategory'] == 'Groceries'].set_index('TransactionDate').groupby('ChurnStatus').resample('7d')['AmountSpent'].mean().reset_index()\n</pre> churn_trans_average_all = transaction_data.set_index('TransactionDate').groupby('ChurnStatus').resample('7d')['AmountSpent'].mean().reset_index() churn_trans_average_max = transaction_data.set_index('TransactionDate').groupby('ChurnStatus').resample('7d')['AmountSpent'].max().reset_index() churn_trans_average_groc = transaction_data[transaction_data['ProductCategory'] == 'Groceries'].set_index('TransactionDate').groupby('ChurnStatus').resample('7d')['AmountSpent'].mean().reset_index() In\u00a0[34]: Copied! <pre>fig = px.line(churn_trans_average_all,\n        x='TransactionDate',\n        y='AmountSpent',\n        color='ChurnStatus',\n        markers=True,\n        template='plotly_white',\n        title='All category average spending',\n        color_discrete_sequence=[\"#456987\", \"#7ed2bc\"]\n        )\n\n# Update layout for transparent background and white fonts\nfig.update_layout(\n    paper_bgcolor='rgba(0,0,0,0)',  # Transparent outer background[2][3][8]\n    plot_bgcolor='rgba(0,0,0,0)',   # Transparent plotting area background[2][3][8]\n    title_font=dict(color='white'),  # White plot title\n    xaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),\n    yaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),\n\n    legend=dict(\n        font=dict(color='white'),  # White legend text\n        bgcolor='rgba(0,0,0,0)'   # Transparent legend background\n    ),\n    title=dict(\n        text=\"All category average spending\",\n        font=dict(\n            color='white',\n            size=20,\n            family='Courier New, monospace'  # Use a bold font family\n        )\n    )\n)\n\n# Update x and y axes for white lines, ticks, labels, and 20% opacity grid\nfig.update_xaxes(\n    showline=True,         # Show axis line\n    linecolor='white',     # White axis line\n    tickfont=dict(color='white'),  # White tick labels\n    gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity\n    linewidth=2\n)\nfig.update_yaxes(\n    showline=True,\n    linecolor='white',\n    tickfont=dict(color='white'),\n    gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity\n    linewidth=2\n)\n\nfig.show('png',width=1200,height=400)\n</pre> fig = px.line(churn_trans_average_all,         x='TransactionDate',         y='AmountSpent',         color='ChurnStatus',         markers=True,         template='plotly_white',         title='All category average spending',         color_discrete_sequence=[\"#456987\", \"#7ed2bc\"]         )  # Update layout for transparent background and white fonts fig.update_layout(     paper_bgcolor='rgba(0,0,0,0)',  # Transparent outer background[2][3][8]     plot_bgcolor='rgba(0,0,0,0)',   # Transparent plotting area background[2][3][8]     title_font=dict(color='white'),  # White plot title     xaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),     yaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),      legend=dict(         font=dict(color='white'),  # White legend text         bgcolor='rgba(0,0,0,0)'   # Transparent legend background     ),     title=dict(         text=\"All category average spending\",         font=dict(             color='white',             size=20,             family='Courier New, monospace'  # Use a bold font family         )     ) )  # Update x and y axes for white lines, ticks, labels, and 20% opacity grid fig.update_xaxes(     showline=True,         # Show axis line     linecolor='white',     # White axis line     tickfont=dict(color='white'),  # White tick labels     gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity     linewidth=2 ) fig.update_yaxes(     showline=True,     linecolor='white',     tickfont=dict(color='white'),     gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity     linewidth=2 )  fig.show('png',width=1200,height=400) In\u00a0[33]: Copied! <pre>fig = px.line(churn_trans_average_max,\n        x='TransactionDate',\n        y='AmountSpent',\n        color='ChurnStatus',\n        markers=True,\n        template='plotly_white',\n        color_discrete_sequence=[\"#456987\", \"#7ed2bc\"]\n)\n\n# Update layout for transparent background and white fonts\nfig.update_layout(\n    paper_bgcolor='rgba(0,0,0,0)',  # Transparent outer background[2][3][8]\n    plot_bgcolor='rgba(0,0,0,0)',   # Transparent plotting area background[2][3][8]\n    title_font=dict(color='white'),  # White plot title\n    xaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),\n    yaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),\n\n    legend=dict(\n        font=dict(color='white'),  # White legend text\n        bgcolor='rgba(0,0,0,0)'   # Transparent legend background\n    ),\n    title=dict(\n        text=\"Maximum spending for purchases\",\n        font=dict(\n            color='white',\n            size=20,\n            family='Courier New, monospace'  # Use a bold font family\n        )\n    )\n)\n\n# Update x and y axes for white lines, ticks, labels, and 20% opacity grid\nfig.update_xaxes(\n    showline=True,         # Show axis line\n    linecolor='white',     # White axis line\n    tickfont=dict(color='white'),  # White tick labels\n    gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity\n    linewidth=2\n)\nfig.update_yaxes(\n    showline=True,\n    linecolor='white',\n    tickfont=dict(color='white'),\n    gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity\n    linewidth=2\n)\n\n\nfig.show('png',width=1200,height=400)\n</pre> fig = px.line(churn_trans_average_max,         x='TransactionDate',         y='AmountSpent',         color='ChurnStatus',         markers=True,         template='plotly_white',         color_discrete_sequence=[\"#456987\", \"#7ed2bc\"] )  # Update layout for transparent background and white fonts fig.update_layout(     paper_bgcolor='rgba(0,0,0,0)',  # Transparent outer background[2][3][8]     plot_bgcolor='rgba(0,0,0,0)',   # Transparent plotting area background[2][3][8]     title_font=dict(color='white'),  # White plot title     xaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),     yaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),      legend=dict(         font=dict(color='white'),  # White legend text         bgcolor='rgba(0,0,0,0)'   # Transparent legend background     ),     title=dict(         text=\"Maximum spending for purchases\",         font=dict(             color='white',             size=20,             family='Courier New, monospace'  # Use a bold font family         )     ) )  # Update x and y axes for white lines, ticks, labels, and 20% opacity grid fig.update_xaxes(     showline=True,         # Show axis line     linecolor='white',     # White axis line     tickfont=dict(color='white'),  # White tick labels     gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity     linewidth=2 ) fig.update_yaxes(     showline=True,     linecolor='white',     tickfont=dict(color='white'),     gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity     linewidth=2 )   fig.show('png',width=1200,height=400)  In\u00a0[31]: Copied! <pre>fig = px.line(churn_trans_average_groc,\n        x='TransactionDate',\n        y='AmountSpent',\n        color='ChurnStatus',\n        template='plotly_white',\n        color_discrete_sequence=[\"#456987\", \"#7ed2bc\"],\n        markers=True)\n\n# Update layout for transparent background and white fonts\nfig.update_layout(\n    paper_bgcolor='rgba(0,0,0,0)',  # Transparent outer background[2][3][8]\n    plot_bgcolor='rgba(0,0,0,0)',   # Transparent plotting area background[2][3][8]\n    title_font=dict(color='white'),  # White plot title\n    xaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),\n    yaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),\n\n    legend=dict(\n        font=dict(color='white'),  # White legend text\n        bgcolor='rgba(0,0,0,0)'   # Transparent legend background\n    ),\n    title=dict(\n        text=\"Groceries average spending\",\n        font=dict(\n            color='white',\n            size=20,\n            family='Courier New, monospace'  # Use a bold font family\n        )\n    )\n)\n\n# Update x and y axes for white lines, ticks, labels, and 20% opacity grid\nfig.update_xaxes(\n    showline=True,         # Show axis line\n    linecolor='white',     # White axis line\n    tickfont=dict(color='white'),  # White tick labels\n    gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity\n    linewidth=2\n)\nfig.update_yaxes(\n    showline=True,\n    linecolor='white',\n    tickfont=dict(color='white'),\n    gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity\n    linewidth=2\n)\n\n\nfig.show('png',width=1200,height=400)\n</pre> fig = px.line(churn_trans_average_groc,         x='TransactionDate',         y='AmountSpent',         color='ChurnStatus',         template='plotly_white',         color_discrete_sequence=[\"#456987\", \"#7ed2bc\"],         markers=True)  # Update layout for transparent background and white fonts fig.update_layout(     paper_bgcolor='rgba(0,0,0,0)',  # Transparent outer background[2][3][8]     plot_bgcolor='rgba(0,0,0,0)',   # Transparent plotting area background[2][3][8]     title_font=dict(color='white'),  # White plot title     xaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),     yaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),      legend=dict(         font=dict(color='white'),  # White legend text         bgcolor='rgba(0,0,0,0)'   # Transparent legend background     ),     title=dict(         text=\"Groceries average spending\",         font=dict(             color='white',             size=20,             family='Courier New, monospace'  # Use a bold font family         )     ) )  # Update x and y axes for white lines, ticks, labels, and 20% opacity grid fig.update_xaxes(     showline=True,         # Show axis line     linecolor='white',     # White axis line     tickfont=dict(color='white'),  # White tick labels     gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity     linewidth=2 ) fig.update_yaxes(     showline=True,     linecolor='white',     tickfont=dict(color='white'),     gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity     linewidth=2 )   fig.show('png',width=1200,height=400) In\u00a0[49]: Copied! <pre>service_churn = service.merge(status,left_on='CustomerID',right_on='CustomerID',how='left')\n\n# resolved / unresolved statistics for both groups\nserv_churn = service_churn.groupby(['InteractionType','ResolutionStatus','ChurnStatus'])['CustomerID'].count().reset_index()\nserv_churn_stats = pd.pivot(serv_churn,index=['ChurnStatus','InteractionType'],columns='ResolutionStatus',values='CustomerID')\nserv_churn_stats['Total'] = serv_churn_stats['Resolved'] + serv_churn_stats['Unresolved']\nserv_churn_stats['Resolved/Total'] = ((serv_churn_stats['Resolved']/serv_churn_stats['Total'])*100).round(2)\nprint(serv_churn_stats.to_markdown(tablefmt='simple'))\n</pre> service_churn = service.merge(status,left_on='CustomerID',right_on='CustomerID',how='left')  # resolved / unresolved statistics for both groups serv_churn = service_churn.groupby(['InteractionType','ResolutionStatus','ChurnStatus'])['CustomerID'].count().reset_index() serv_churn_stats = pd.pivot(serv_churn,index=['ChurnStatus','InteractionType'],columns='ResolutionStatus',values='CustomerID') serv_churn_stats['Total'] = serv_churn_stats['Resolved'] + serv_churn_stats['Unresolved'] serv_churn_stats['Resolved/Total'] = ((serv_churn_stats['Resolved']/serv_churn_stats['Total'])*100).round(2) print(serv_churn_stats.to_markdown(tablefmt='simple')) <pre>                    Resolved    Unresolved    Total    Resolved/Total\n----------------  ----------  ------------  -------  ----------------\n(0, 'Complaint')         124           139      263             47.15\n(0, 'Feedback')          155           129      284             54.58\n(0, 'Inquiry')           137           112      249             55.02\n(1, 'Complaint')          32            40       72             44.44\n(1, 'Feedback')           44            32       76             57.89\n(1, 'Inquiry')            31            27       58             53.45\n</pre> In\u00a0[50]: Copied! <pre># general activity statistics &amp; relation to churn\nactivity_churn = activity.merge(status,left_on='CustomerID',right_on='CustomerID',how='left')\n\ntotal_activity = activity_churn.groupby(['ServiceUsage'])['LoginFrequency'].sum()\nactivity_churn_stat = activity_churn[['ServiceUsage','ChurnStatus','LoginFrequency']].groupby(['ServiceUsage','ChurnStatus']).sum()\nactivity_churn_stat_merge = activity_churn_stat.reset_index().merge(total_activity,left_on='ServiceUsage',right_on='ServiceUsage')\nactivity_churn_stat_merge['serviceusage_ratio'] = ((activity_churn_stat_merge['LoginFrequency_x']/activity_churn_stat_merge['LoginFrequency_y'])*100).round(2)\nprint(activity_churn_stat_merge.to_markdown(tablefmt='simple'))\n</pre> # general activity statistics &amp; relation to churn activity_churn = activity.merge(status,left_on='CustomerID',right_on='CustomerID',how='left')  total_activity = activity_churn.groupby(['ServiceUsage'])['LoginFrequency'].sum() activity_churn_stat = activity_churn[['ServiceUsage','ChurnStatus','LoginFrequency']].groupby(['ServiceUsage','ChurnStatus']).sum() activity_churn_stat_merge = activity_churn_stat.reset_index().merge(total_activity,left_on='ServiceUsage',right_on='ServiceUsage') activity_churn_stat_merge['serviceusage_ratio'] = ((activity_churn_stat_merge['LoginFrequency_x']/activity_churn_stat_merge['LoginFrequency_y'])*100).round(2) print(activity_churn_stat_merge.to_markdown(tablefmt='simple')) <pre>    ServiceUsage      ChurnStatus    LoginFrequency_x    LoginFrequency_y    serviceusage_ratio\n--  --------------  -------------  ------------------  ------------------  --------------------\n 0  Mobile App                  0                6768                8565                 79.02\n 1  Mobile App                  1                1797                8565                 20.98\n 2  Online Banking              0                7667                9375                 81.78\n 3  Online Banking              1                1708                9375                 18.22\n 4  Website                     0                6653                7972                 83.45\n 5  Website                     1                1319                7972                 16.55\n</pre> In\u00a0[32]: Copied! <pre>activity_churn_stat = activity_churn.set_index('LastLoginDate').groupby(['ChurnStatus']).resample('15d')['LoginFrequency'].sum().reset_index()\nactivity_churn_stat = activity_churn_stat.merge(numbers,left_on='ChurnStatus',right_on='ChurnStatus')\nactivity_churn_stat['norm_LoginFrequency'] = (activity_churn_stat['LoginFrequency']/activity_churn_stat['count'])*100\nactivity_churn_stat['LastLoginDate'] = pd.Categorical(activity_churn_stat['LastLoginDate'])\n\nfig = px.line(activity_churn_stat,\n             x='LastLoginDate',\n             y='norm_LoginFrequency',\n             color='ChurnStatus',\n             template='plotly_white',\n             color_discrete_sequence=[\"#456987\", \"#7ed2bc\"],\n             markers=True)\n\n# Update layout for transparent background and white fonts\nfig.update_layout(\n    paper_bgcolor='rgba(0,0,0,0)',  # Transparent outer background[2][3][8]\n    plot_bgcolor='rgba(0,0,0,0)',   # Transparent plotting area background[2][3][8]\n    title_font=dict(color='white'),  # White plot title\n    xaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),\n    yaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),\n\n    legend=dict(\n        font=dict(color='white'),  # White legend text\n        bgcolor='rgba(0,0,0,0)'   # Transparent legend background\n    ),\n    title=dict(\n        text=\"Online activity per user\",\n        font=dict(\n            color='white',\n            size=20,\n            family='Courier New, monospace'  # Use a bold font family\n        )\n    )\n)\n\n# Update x and y axes for white lines, ticks, labels, and 20% opacity grid\nfig.update_xaxes(\n    showline=True,         # Show axis line\n    linecolor='white',     # White axis line\n    tickfont=dict(color='white'),  # White tick labels\n    gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity\n    linewidth=2\n)\nfig.update_yaxes(\n    showline=True,\n    linecolor='white',\n    tickfont=dict(color='white'),\n    gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity\n    linewidth=2\n)\n\n\nfig.show('png',width=1200,height=400)\n</pre> activity_churn_stat = activity_churn.set_index('LastLoginDate').groupby(['ChurnStatus']).resample('15d')['LoginFrequency'].sum().reset_index() activity_churn_stat = activity_churn_stat.merge(numbers,left_on='ChurnStatus',right_on='ChurnStatus') activity_churn_stat['norm_LoginFrequency'] = (activity_churn_stat['LoginFrequency']/activity_churn_stat['count'])*100 activity_churn_stat['LastLoginDate'] = pd.Categorical(activity_churn_stat['LastLoginDate'])  fig = px.line(activity_churn_stat,              x='LastLoginDate',              y='norm_LoginFrequency',              color='ChurnStatus',              template='plotly_white',              color_discrete_sequence=[\"#456987\", \"#7ed2bc\"],              markers=True)  # Update layout for transparent background and white fonts fig.update_layout(     paper_bgcolor='rgba(0,0,0,0)',  # Transparent outer background[2][3][8]     plot_bgcolor='rgba(0,0,0,0)',   # Transparent plotting area background[2][3][8]     title_font=dict(color='white'),  # White plot title     xaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),     yaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),      legend=dict(         font=dict(color='white'),  # White legend text         bgcolor='rgba(0,0,0,0)'   # Transparent legend background     ),     title=dict(         text=\"Online activity per user\",         font=dict(             color='white',             size=20,             family='Courier New, monospace'  # Use a bold font family         )     ) )  # Update x and y axes for white lines, ticks, labels, and 20% opacity grid fig.update_xaxes(     showline=True,         # Show axis line     linecolor='white',     # White axis line     tickfont=dict(color='white'),  # White tick labels     gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity     linewidth=2 ) fig.update_yaxes(     showline=True,     linecolor='white',     tickfont=dict(color='white'),     gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity     linewidth=2 )   fig.show('png',width=1200,height=400) \ud83d\udcca EDA | Summary <p>Lets review what we learned from this segment</p> <ul> <li>20.4% of customers churned, which is significant and should be investigated</li> <li>Demographic feature relation to churn is very minimal, but still useful for </li> <li>The number of purchases of customers is not a factor for churn</li> <li>Purchase history data for groceries and other subsets was found to be a feature to differentiate churned customers</li> <li>Service interaction data didn't reveal a major variation between groups, however unresolved inquiries and complaints were higher for churned customers</li> <li>Online activity data indicated that churned customers were slightly less active in the last couple of months</li> </ul> <p>Not all data sources available to us showed meaningful data related to customer churn, however it should be noted that we can't rule out any source of data without constructing our model pipeline and evaluating some model metrics</p>"},{"location":"portfolio/projects/lloyds_ds/task1_nb.html#lloyds-banking-group-internship-data-science","title":"Lloyds Banking Group Internship Data Science\u00b6","text":""},{"location":"portfolio/projects/lloyds_ds/task1_nb.html#phase-1-data-gathering-and-exploratory-analysis","title":"\ud83d\udcca Phase 1: Data gathering and exploratory analysis\u00b6","text":""},{"location":"portfolio/projects/lloyds_ds/task1_nb.html#project-brief","title":"Project brief\u00b6","text":"<ul> <li><p>Welcome to the Data Science &amp; Analytics team at Lloyds Banking Group. As a new data science graduate, you have been entrusted with a critical project that could significantly impact our customer retention strategies. Li, our senior data scientist, has specifically chosen you to take over this project due to your strong analytical skills and enthusiasm for solving real-world business problems. This is an exciting opportunity for you to apply your knowledge and make a real impact within our team. Context</p> </li> <li><p>The project you are about to embark on is the \"Customer Retention Enhancement through Predictive Analytics\" initiative. This project arose from an urgent need to address declining retention rates within certain segments of our customer base. Over the past few months, we've noticed a worrying trend of increased customer churn, particularly among young professionals and small business owners. This poses a substantial threat to our market position and long-term profitability.</p> </li> <li><p>Our fictional client, SmartBank, a subsidiary of Lloyds, has reported that a substantial portion of their customer base is at risk of moving to competitors offering more personalised banking solutions. SmartBank has tasked our team with developing a predictive model to identify at-risk customers and propose targeted interventions to retain them. Key concepts</p> </li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task1_nb.html#key-concepts","title":"Key Concepts\u00b6","text":"<p>Before you begin, it's essential to understand a few key concepts:</p> <ul> <li>Customer churn: The process by which customers stop doing business with a company. Identifying and preventing churn is crucial for maintaining a stable customer base. Predictive analytics: Techniques that use historical data to forecast future possibilities. In this project, you'll use predictive analytics to predict which customers are likely to churn. Exploratory data analysis (EDA): A method of analysing data sets to summarise their primary characteristics, often using visual strategies. EDA is crucial for understanding the data before building predictive models. Machine learning models: Algorithms that let computers learn from and make predictions or decisions based on data. You'll be building a classification model to predict customer churn.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task1_nb.html#project-requirements","title":"Project requirements\u00b6","text":"<p>Your task involves two main phases.</p> <ul> <li><p><code>Phase 1: Data gathering and exploratory analysis</code></p> <ul> <li>Objective: Collect relevant data, perform exploratory data analysis (EDA), and prepare the data for modelling. Steps: Identify and gather data from provided sources relevant to predicting customer churn. Conduct EDA to understand the data, identify key features, and uncover patterns and insights. Clean and preprocess the data to ensure it is ready for model building. Deliverable: Submit a report detailing your exploratory data analysis and key findings, including the cleaned and preprocessed data set.</li> </ul> </li> <li><p>Phase 2: Building a machine learning model</p> <ul> <li>Objective: Develop a predictive model to identify customers at risk of churning and propose ways to measure the model\u2019s performance. Steps: Choose a suitable machine learning algorithm for the classification task. Build and train the model to predict customer churn. Suggest ways to evaluate and measure the model\u2019s performance, ensuring its reliability and accuracy. Deliverable: Submit a report, including the trained machine learning model and your proposed methods for evaluating and measuring the model\u2019s performance.</li> </ul> </li> </ul> <p>The importance of this project</p> <p>Addressing customer churn is critical for maintaining SmartBank's competitive edge and ensuring long-term sustainability. The board of directors is strongly encouraging the team to deliver a solution promptly, as timely delivery will help secure our market position and capitalize on opportunities. The team is feeling the heat, but they are also highly motivated and determined to turn this challenge into an opportunity. Your role is pivotal in this effort, and your contributions will be closely monitored by senior management.</p> <p>Embrace this chance to showcase your skills and make a meaningful impact. It\u2019s time to dive in and get to work!</p>"},{"location":"portfolio/projects/lloyds_ds/task1_nb.html#read-dataset","title":"Read Dataset\u00b6","text":""},{"location":"portfolio/projects/lloyds_ds/task1_nb.html#eda","title":"EDA\u00b6","text":""},{"location":"portfolio/projects/lloyds_ds/task1_nb.html#1-customerid-numbers","title":"1. CustomerID numbers\u00b6","text":"<p>Lets look at how many customers are churning ~20% of the sampled data</p>"},{"location":"portfolio/projects/lloyds_ds/task1_nb.html#2-demographic-relation-to-churn","title":"2. Demographic relation to churn\u00b6","text":"<p>From a demographic point if view the data tells us that no one feature shows the root causes of churn</p> <ul> <li><p>For age groups churned customer variation:</p> <ul> <li>The mean churn for each age is evenly distribted</li> <li>While age group 50-60 has a slightly higher mean of 22%</li> <li>we look at maximum values 30-40 had an age at which 38.4% churned</li> <li>The minimum churn values dont differ to much either, all in the region of 5-10%</li> </ul> </li> <li><p>Gender doesnt seem to be critical either, with churned customers being about 50/50 male and female</p> </li> <li><p>Marital status showes that a slightly higher number of clients who churned were married or widowed (6, 5.4%) as opposed to single and divorced (4.4, 4.6%), which also indicates that there is no clear pattern responsible for churn</p> </li> <li><p>The income group also didnt really add any significant insigh with all three groups sharing about the same distribution of 6.5 - 7.2%</p> </li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task1_nb.html#3-user-transaction-relation-to-churn","title":"3. User transaction relation to churn\u00b6","text":"<p>For transactional data, we have more features to differentiate both groups</p> <ul> <li>Whilst there is no difference in the number of purchases of each group</li> <li>We start to see some clear variation in spending in various ProductCategory; especially evident in statistics of grocery purchases.</li> <li>The mean purchases of customers who churned is much higher (mean,25% &amp; 50% subsets)</li> </ul> <pre>\t                count\tmean\tstd\tmin\t25%\t50%\t75%\tmax\nGroceries\t0\t820.0\t251.88\t145.05\t5.20\t126.24\t249.44\t381.39\t499.57\n                1\t200.0\t272.95\t138.11\t5.92\t161.76\t277.58\t389.38\t494.90\n</pre> <p>Historical transaction data:</p> <ul> <li>Average/Mean spending across all categories is about the same over time</li> <li>Maximum spending of churned customers is noticably less than those who didn't churn, especially in the last 6 months</li> <li>For groceries, historical data also confirms that churned customers tend to spend much more on groceries, on multiple occasions, the difference between these two groups is very significant (1.5 - 2 times)</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task1_nb.html#4-service-relation-to-churn","title":"4. Service relation to churn\u00b6","text":"<p>We also have information about the customer interaction with the banks' customer service department</p> <ul> <li><p>On first impression, bad customer service and unresolved issues of customer users can be a reason why a customer churned</p> </li> <li><p>As with other data, variation between both groups is quite minimal, however:</p> <ul> <li>complaints and Inquiries, churned customers had a smaller resolved percentage, as opposed to non churned customers</li> </ul> </li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task1_nb.html#5-activity-relation-to-churn","title":"5. Activity relation to churn\u00b6","text":"<p>The last group of data is related to the user's online activity</p> <ul> <li><p>For churned users, percentage tended to increase for users who utilised Mobile App (21%) more than Online Banking (18.22%) and Website (16.55%) for users who churned</p> </li> <li><p>For Non churned users, the usage tended to be quite even for all three groups (79 - 83%)</p> </li> <li><p>If we look at the timeline</p> <ul> <li>Up to October 2023, churned and non churned users tended to have similar activity</li> <li>Activity of churned users after October 2023 tended to reduce quite a bit compared to non churned users</li> </ul> </li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task2.html","title":"Let's get started","text":"<p>You've already laid a solid foundation by exploring and preparing the data. Now, it's time to apply machine learning techniques to build a predictive model for customer churn, a crucial step in enhancing SmartBank's customer retention strategies.</p> <p>In this task, your focus will shift from data preparation to model development. The challenge lies in selecting the right machine learning algorithm and fine-tuning it to accurately predict which customers are at risk of leaving. This model will provide actionable insights, enabling the team to develop targeted interventions to retain valuable customers.</p> <p>Li, your mentor, has emphasised the importance of accuracy and precision in this phase. \"The insights from this model will drive our retention strategies. It\u2019s crucial that we build a model that not only predicts churn but also provides clear indicators of why customers are leaving,\" she explains. This guidance underscores the practical implications of your work; the model you build must be both accurate and interpretable.</p> <p>Your task involves choosing an appropriate algorithm, training the model, and evaluating its performance. Remember, the goal is to create a model that can be easily understood and acted on by business stakeholders. As you work through this task, consider the factors that could influence churn, such as spending habits, service usage, and demographic characteristics.</p> <p>This is a chance to showcase your data science expertise in a real-world scenario. Your efforts will not only enhance your skills but also contribute significantly to the team's understanding of customer behaviour. As you begin, keep in mind the practical impact of your model on SmartBank's strategic decisions. Let's get started and bring your analysis to life!</p> <p></p>"},{"location":"portfolio/projects/lloyds_ds/task2.html#approaches-to-selecting-appropriate-machine-learning-algorithms","title":"Approaches to selecting appropriate machine learning algorithms","text":"<p>Selecting the right machine learning algorithm is crucial for building a robust predictive model. Given the complexity of customer churn prediction, where the target variable is categorical, you need to consider several factors that influence the choice of the model. Here are some approaches to help guide your selection.</p>"},{"location":"portfolio/projects/lloyds_ds/task2.html#understanding-the-problem-type-and-data-characteristics","title":"Understanding the problem type and data characteristics","text":"<p>In churn prediction, you're dealing with a\u00a0binary classification problem. Key considerations include:</p> <ul> <li>Imbalance in the data set:\u00a0Customer churn data sets often have an imbalance, where the number of churned customers is significantly less than non-churned. Techniques like\u00a0resampling,\u00a0SMOTE (synthetic minority over-sampling technique), or\u00a0adjusted class weights\u00a0in algorithms are crucial for handling this imbalance effectively.</li> <li>Feature engineering:\u00a0Advanced feature engineering techniques, such as\u00a0interaction terms,\u00a0polynomial features, and\u00a0dimensionality reduction (e.g., principal component analysis), can significantly influence the performance of algorithms, especially those sensitive to multicollinearity and high-dimensional spaces, like logistic regression and support vector machines (SVMs).</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task2.html#algorithm-selection-and-considerations","title":"Algorithm selection and considerations","text":"<ul> <li>Logistic regression:\u00a0Preferred for its simplicity and interpretability, logistic regression can be enhanced with\u00a0regularisation techniques (L1, L2)\u00a0to prevent overfitting, especially in high-dimensional data sets.</li> <li>Decision trees and random forests:\u00a0These are powerful for capturing non-linear relationships and interactions between features. Random forests, an ensemble of decision trees, provide robustness against overfitting and allow for\u00a0feature importance analysis, which can be crucial in understanding which factors contribute most to churn.</li> <li>SVMs:\u00a0Effective in high-dimensional spaces and when the decision boundary is not linear. The use of\u00a0kernel tricks (e.g., RBF, polynomial)\u00a0allows SVMs to handle non-linear relationships, but they require careful tuning of hyperparameters such as\u00a0C (regularisation parameter)\u00a0and\u00a0gamma.</li> <li>Neural networks:\u00a0While potentially offering high accuracy, especially with complex data patterns, they require large amounts of data and computational power. Techniques like\u00a0dropout,\u00a0batch normalisation, and\u00a0early stopping\u00a0are essential to prevent overfitting.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task2.html#model-evaluation-and-tuning","title":"Model evaluation and tuning","text":"<ul> <li>Cross-validation:\u00a0Advanced cross-validation techniques, such as\u00a0stratified k-fold, ensure that each fold has a representative distribution of the target class, crucial for imbalanced data sets.</li> <li>Hyperparameter tuning:\u00a0Employ\u00a0grid search\u00a0or\u00a0random search\u00a0for systematic exploration of the hyperparameter space. For more efficient optimisation, consider using\u00a0Bayesian optimisation\u00a0or\u00a0automated machine learning (AutoML)\u00a0tools.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task2.html#scalability-and-practical-considerations","title":"Scalability and practical considerations","text":"<ul> <li>Model deployment:\u00a0Consider the model's scalability and integration into the business workflow. This includes\u00a0real-time prediction capabilities, ease of updating the model with new data, and computational efficiency.</li> <li>Interpretability vs. accuracy trade-offs:\u00a0In practice, balancing interpretability with predictive power is often necessary, especially when model decisions need to be transparent to stakeholders.</li> </ul> <p>By delving into these advanced considerations, you'll be better equipped to select and fine-tune machine learning algorithms that are both accurate and aligned with the practical needs of the business context in which they will be deployed.</p> <p></p>"},{"location":"portfolio/projects/lloyds_ds/task2.html#approaches-to-selecting-and-building-machine-learning-models-for-classification-tasks","title":"Approaches to selecting and building machine learning models for classification tasks","text":"<p>Building a machine learning model for classification tasks, such as predicting customer churn, requires a deep understanding of both the algorithmic foundation and the practical nuances of implementation. Here are some advanced approaches to guide you through this process.</p>"},{"location":"portfolio/projects/lloyds_ds/task2.html#feature-selection-and-engineering","title":"Feature selection and engineering","text":"<ul> <li>Dimensionality reduction:\u00a0Techniques such as\u00a0principal component analysis (PCA)\u00a0or\u00a0t-distributed stochastic neighbour embedding (t-SNE)\u00a0can be used to reduce the feature space, mitigating the curse of dimensionality and enhancing model performance.</li> <li>Feature importance analysis:\u00a0Algorithms like random forests provide intrinsic measures of feature importance, which can guide the selection of the most predictive features. This step is crucial for simplifying the model and improving interpretability without sacrificing accuracy.</li> <li>Interaction terms and polynomial features:\u00a0Introducing interaction terms and polynomial features can capture non-linear relationships between variables, which are often missed in linear models. This is particularly useful in models like logistic regression, where extending the feature space can significantly enhance predictive capability.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task2.html#model-selection-and-evaluation","title":"Model selection and evaluation","text":"<p>Choosing the right model involves balancing several factors:</p> <ul> <li>Algorithm suitability:\u00a0While logistic regression and decision trees offer simplicity and interpretability, they may lack the predictive power of more complex models like\u00a0gradient boosting machines (GBMs),\u00a0XGBoost, or\u00a0neural networks. The choice often depends on the trade-off between model performance and explainability.</li> <li>Model evaluation metrics:\u00a0In the context of imbalanced data sets, traditional metrics like accuracy are often misleading. Use metrics such as\u00a0precision, recall, F1-score,\u00a0and\u00a0ROC-AUC\u00a0to get a more accurate picture of model performance. Additionally, the\u00a0confusion matrix\u00a0provides detailed insights into the true positives, false positives, true negatives, and false negatives, which are critical for understanding model behaviour.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task2.html#advanced-model-tuning-techniques","title":"Advanced model tuning techniques","text":"<p>Optimising model performance involves fine-tuning hyperparameters:</p> <ul> <li>Grid search and random search:\u00a0These methods are standard for hyperparameter optimisation but can be computationally expensive. Grid search is exhaustive, covering all combinations of specified hyperparameters, while random search samples a wide range but in a more computationally efficient manner.</li> <li>Bayesian optimisation:\u00a0For more efficient hyperparameter tuning, Bayesian optimisation offers a probabilistic approach to finding the optimal parameters, often outperforming traditional methods in terms of both accuracy and computational cost.</li> <li>Cross-validation:\u00a0Use\u00a0stratified k-fold cross-validation\u00a0to ensure that each fold has the same proportion of classes as the original data set, which is crucial for imbalanced classification tasks. This approach helps in validating that the model generalises well to unseen data.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task2.html#model-implementation-and-scalability","title":"Model implementation and scalability","text":"<p>Once a model is selected and tuned, consider its deployment and scalability:</p> <ul> <li>Pipeline integration:\u00a0Incorporate the model into a robust data pipeline, ensuring it can handle real-time data streams and integrate seamlessly with existing systems. This includes automating data preprocessing, model prediction, and output generation.</li> <li>Model monitoring and maintenance:\u00a0Post-deployment, continuously monitor model performance to detect drifts in data distribution or declines in accuracy. Implementing\u00a0version control\u00a0for models, along with retraining strategies, ensures the model remains accurate and relevant as new data becomes available.</li> </ul> <p>By integrating these advanced techniques, you'll build a classification model that is not only accurate and robust but also scalable and maintainable, ensuring long-term value for the business.</p> <p></p>"},{"location":"portfolio/projects/lloyds_ds/task2.html#techniques-for-suggesting-ways-to-evaluate-and-measure-the-models-performance","title":"Techniques for suggesting ways to evaluate and measure the model\u2019s performance","text":"<p>Evaluating and measuring the performance of a machine learning model, especially in classification tasks like predicting customer churn, is crucial for understanding its effectiveness and reliability. Here are some advanced techniques and metrics to ensure comprehensive evaluation.</p>"},{"location":"portfolio/projects/lloyds_ds/task2.html#choosing-the-right-evaluation-metrics","title":"Choosing the right evaluation metrics","text":"<p>Selecting appropriate metrics depends on the specific characteristics of the data set and the business objectives:</p> <ul> <li>Precision and recall:\u00a0These metrics are particularly important in imbalanced data sets where false positives and false negatives carry different costs.\u00a0Precision\u00a0measures the proportion of true positive predictions among all positive predictions, while\u00a0recallmeasures the proportion of true positives identified out of all actual positives.</li> <li>F1 score:\u00a0The F1 score balances precision and recall, offering a single metric that accounts for both false positives and false negatives. This is particularly useful when the costs of these errors are similar.</li> <li>ROC-AUC (receiver operating characteristic - area under curve):\u00a0The ROC-AUC score evaluates the trade-off between true positive rates and false positive rates across different threshold settings. A higher AUC indicates better model performance across various decision thresholds.</li> <li>Confusion matrix:\u00a0This matrix offers a detailed breakdown of the true positives, false positives, true negatives, and false negatives. It is a fundamental tool for understanding model performance, especially in terms of misclassification types.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task2.html#model-calibration-and-validation","title":"Model calibration and validation","text":"<ul> <li>Calibration curves:\u00a0To assess how well predicted probabilities align with actual outcomes, use calibration curves. These curves compare predicted probabilities with actual outcome frequencies, helping to adjust the model to improve probability estimation.</li> <li>Cross-validation:\u00a0Beyond simple training-validation splits,\u00a0k-fold cross-validationensures that the model's performance is consistently evaluated across different subsets of the data. This technique reduces the likelihood of overfitting and ensures that the model generalises well to unseen data.</li> <li>Bootstrapping:\u00a0This statistical method involves repeatedly resampling the data set with replacement to estimate the distribution of model performance metrics. Bootstrapping provides insights into the variability and robustness of the model's predictions.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task2.html#post-model-analysis","title":"Post-model analysis","text":"<ul> <li>Feature importance and SHAP values:\u00a0Understanding why a model makes certain predictions is crucial, especially in business contexts where decisions must be justified.\u00a0Feature importance\u00a0metrics in models like random forests and\u00a0SHAP (SHapley Additive exPlanations) values\u00a0provide insights into how each feature contributes to the model\u2019s decisions.</li> <li>Error analysis:\u00a0Conduct a thorough analysis of the model's errors, focusing on cases where the model performs poorly. This analysis can reveal data patterns that the model misses, leading to insights for further feature engineering or model adjustments.</li> <li>Business impact analysis:\u00a0Beyond statistical metrics, evaluate the model's performance in terms of business outcomes. For example, measure the impact of the model on customer retention rates or revenue. This analysis helps assess the model's practical value.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task2.html#continuous-monitoring-and-reassessment","title":"Continuous monitoring and reassessment","text":"<ul> <li>Model drift detection:\u00a0Implement systems to detect model drift, which occurs when the data distribution changes over time, leading to a decline in model performance. Techniques like monitoring prediction probabilities or feature distributions can help the early detection of drift.</li> <li>Retraining strategies:\u00a0Based on performance monitoring, establish criteria for retraining the model. This could involve periodic retraining or retraining triggered by specific performance thresholds or detected drifts.</li> </ul> <p>By employing these advanced techniques for model evaluation and measurement, you ensure that the predictive model not only performs well statistically but also aligns with business goals, providing actionable insights and reliable predictions.</p> <p></p>"},{"location":"portfolio/projects/lloyds_ds/task2.html#crafting-the-predictive-blueprint","title":"Crafting the predictive blueprint","text":"<p>As you delve deeper into the intricacies of data science, Task 2 offers a critical opportunity to influence the strategic direction of Lloyds Banking Group. Your work in this phase will pivot from EDA to the construction of a predictive model that could significantly shape the bank's customer retention strategies.</p> <p>The urgency and importance of this task cannot be overstated. Lloyds has seen a subtle but concerning trend in customer attrition, which, if not addressed, could have substantial implications for its market standing and profitability. This project is a real-world application where your findings could directly influence business decisions and outcomes.</p> <p>Team collaboration and strategic impact</p> <p>Working closely with Li and the Data Science &amp; Analytics team, you are stepping into a role where your technical expertise merges with strategic business needs. The team has identified key areas where predictive insights could allow for proactive interventions, thus reducing churn and enhancing customer loyalty. Li underscores the strategic importance of your task, noting, \"Our ability to predict churn allows us to personalise our engagement strategies, tailoring our approach to meet the needs and preferences of our customers. This is critical for maintaining a competitive edge.\"</p> <p>Integrating analytical insights into business strategies</p> <p>Your task involves selecting the most appropriate machine learning algorithm, which balances predictive accuracy with interpretability. This balance is crucial; while complex models may offer high accuracy, they can be challenging to explain to non-technical stakeholders who need to trust and act on these insights. Therefore, the choice of algorithm must consider not only the statistical performance but also the business context and usability.</p> <p>Building the model is where your analytical skills will shine. Using the preprocessed data from Task 1, you'll train the model to identify patterns indicative of potential churn. This involves iterating on various models, tuning hyperparameters, and validating the model to ensure it generalises well to new data.</p> <p>Evaluating and communicating model performance</p> <p>Beyond building the model, it's essential to suggest robust evaluation metrics. The goal is to provide a comprehensive view of the model's performance, highlighting its strengths and identifying any limitations. Metrics like precision, recall, and F1 score will be key, especially in the context of imbalanced data sets where simple accuracy might be misleading. Furthermore, explaining the model's predictions through feature importance or other interpretability tools will be vital for aligning the model's outputs with business decisions.</p> <p>Delivering actionable insights</p> <p>The culmination of your work will be a detailed report. This report should present the technical aspects of the model and translate these findings into actionable business insights. Your ability to communicate complex data science concepts in a clear and actionable manner will be crucial in ensuring that the strategic implications of your work are understood and implemented by the business.</p> <p>This task is a critical component of Lloyds' broader strategy to harness data-driven insights for business growth. Your contributions will play a pivotal role in shaping how the bank understands and responds to customer needs, ultimately driving customer satisfaction and loyalty. As you embark on this task, remember that your work has the potential to make a significant impact, both analytically and strategically.</p> <p></p>"},{"location":"portfolio/projects/lloyds_ds/task2.html#task-instructions","title":"Task instructions","text":"<p>Introduction</p> <p>In this task, you will focus on developing a robust machine learning model to predict customer churn. Your objective is to select an appropriate algorithm, train and validate the model, and propose evaluation metrics that will help assess its performance. This task is pivotal for providing actionable insights that can inform business strategies at Lloyds Banking Group.</p> <p>Instructions</p> <p>Select an appropriate machine learning algorithm:</p> <ul> <li>Review the characteristics of the data set and the nature of the churn prediction problem.</li> <li>Consider algorithms such as logistic regression, decision trees, random forests, gradient boosting machines, or neural networks.</li> <li>Choose an algorithm that balances accuracy and interpretability, suitable for the business context.</li> </ul> <p>Build and train the model:</p> <ul> <li>Use the preprocessed data set from Task 1 to train your chosen model.</li> <li>Implement techniques like cross-validation to ensure the model generalises well to unseen data.</li> <li>Perform hyperparameter tuning to optimise the model\u2019s performance.</li> </ul> <p>Evaluate model performance:</p> <ul> <li>Select appropriate metrics to evaluate the model's performance, such as precision, recall, F1 score, ROC-AUC, and confusion matrix analysis.</li> <li>Consider the implications of each metric in the context of imbalanced data sets, ensuring that the evaluation provides a comprehensive view of the model's effectiveness.</li> </ul> <p>Suggest ways to improve and utilise the model:</p> <ul> <li>Provide recommendations on how the model can be used by the business to identify at-risk customers and develop retention strategies.</li> <li>Discuss any potential improvements or adjustments to the model that could enhance its accuracy or applicability in different business scenarios.</li> </ul> <p>Deliverable:</p> <ul> <li>Report submission:\u00a0Compile a comprehensive report that includes:<ul> <li>A detailed description of the selected algorithm and the rationale behind its choice.</li> <li>The trained model, along with performance metrics and evaluation results.</li> <li>Suggested ways to utilise the model's predictions for business decision-making and potential areas for improvement.</li> </ul> </li> </ul> <p>Ensure that your report is clear, concise, and well-organised, effectively communicating both the technical aspects of the model and its practical applications for the business. This report will be a critical tool for stakeholders to understand and leverage the predictive insights generated by your model.</p> <p>Any questions or comments about the above post can be addressed on the  mldsai-info channel or to me directly  shtrauss2, on  shtrausslearning or  shtrausslearning</p>"},{"location":"portfolio/projects/lloyds_ds/task2_nb.html","title":"Task2 nb","text":"In\u00a0[25]: Copied! <pre>import pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings; warnings.filterwarnings('ignore')\nfrom IPython.display import Image\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n</pre> import pandas as pd import plotly.express as px import matplotlib.pyplot as plt import seaborn as sns import warnings; warnings.filterwarnings('ignore') from IPython.display import Image  from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import roc_auc_score from sklearn.metrics import classification_report from sklearn.model_selection import train_test_split from sklearn.model_selection import StratifiedShuffleSplit  In\u00a0[26]: Copied! <pre>ds = pd.read_excel('Customer_Churn_Data_Large.xlsx',sheet_name=None)\nds.keys()\n</pre> ds = pd.read_excel('Customer_Churn_Data_Large.xlsx',sheet_name=None) ds.keys() Out[26]: <pre>dict_keys(['Customer_Demographics', 'Transaction_History', 'Customer_Service', 'Online_Activity', 'Churn_Status'])</pre> In\u00a0[27]: Copied! <pre>demographic = ds['Customer_Demographics']\ntransaction = ds['Transaction_History']\nservice = ds['Customer_Service']\nactivity = ds['Online_Activity']\nstatus = ds['Churn_Status']\nstatus['ChurnStatus'] = pd.Categorical(status['ChurnStatus'],[0,1])\n</pre> demographic = ds['Customer_Demographics'] transaction = ds['Transaction_History'] service = ds['Customer_Service'] activity = ds['Online_Activity'] status = ds['Churn_Status'] status['ChurnStatus'] = pd.Categorical(status['ChurnStatus'],[0,1]) In\u00a0[28]: Copied! <pre>demographic.head()\n</pre> demographic.head() Out[28]: CustomerID Age Gender MaritalStatus IncomeLevel 0 1 62 M Single Low 1 2 65 M Married Low 2 3 18 M Single Low 3 4 21 M Widowed Low 4 5 21 M Divorced Medium In\u00a0[29]: Copied! <pre>transaction.head()\n</pre> transaction.head() Out[29]: CustomerID TransactionID TransactionDate AmountSpent ProductCategory 0 1 7194 2022-03-27 416.50 Electronics 1 2 7250 2022-08-08 54.96 Clothing 2 2 9660 2022-07-25 197.50 Electronics 3 2 2998 2022-01-25 101.31 Furniture 4 2 1228 2022-07-24 397.37 Clothing <p>The number of rows of transactional data is much higher than the other user aggregated data</p> In\u00a0[30]: Copied! <pre>transaction.shape\n</pre> transaction.shape Out[30]: <pre>(5054, 5)</pre> In\u00a0[31]: Copied! <pre>service.head()\n</pre> service.head() Out[31]: CustomerID InteractionID InteractionDate InteractionType ResolutionStatus 0 1 6363 2022-03-31 Inquiry Resolved 1 2 3329 2022-03-17 Inquiry Resolved 2 3 9976 2022-08-24 Inquiry Resolved 3 4 7354 2022-11-18 Inquiry Resolved 4 4 5393 2022-07-03 Inquiry Unresolved In\u00a0[32]: Copied! <pre>activity.head()\n</pre> activity.head() Out[32]: CustomerID LastLoginDate LoginFrequency ServiceUsage 0 1 2023-10-21 34 Mobile App 1 2 2023-12-05 5 Website 2 3 2023-11-15 3 Website 3 4 2023-08-25 2 Website 4 5 2023-10-27 41 Website In\u00a0[33]: Copied! <pre>status.head()\n</pre> status.head() Out[33]: CustomerID ChurnStatus 0 1 0 1 2 1 2 3 0 3 4 0 4 5 0 In\u00a0[34]: Copied! <pre>status.shape\n</pre> status.shape Out[34]: <pre>(1000, 2)</pre> In\u00a0[35]: Copied! <pre>baseline = demographic.merge(status,left_on='CustomerID',right_on='CustomerID',how='left')\nbaseline.drop('CustomerID',axis=1,inplace=True)\n\nX = baseline[['Age','Gender','MaritalStatus','IncomeLevel']]\nX = pd.get_dummies(X)\ny = baseline['ChurnStatus']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,\n                                                    test_size=0.2,\n                                                    stratify=y,\n                                                    random_state=32)\n</pre> baseline = demographic.merge(status,left_on='CustomerID',right_on='CustomerID',how='left') baseline.drop('CustomerID',axis=1,inplace=True)  X = baseline[['Age','Gender','MaritalStatus','IncomeLevel']] X = pd.get_dummies(X) y = baseline['ChurnStatus']  X_train, X_test, y_train, y_test = train_test_split(X,y,                                                     test_size=0.2,                                                     stratify=y,                                                     random_state=32) In\u00a0[36]: Copied! <pre>model = DecisionTreeClassifier(\n                                max_depth=None,\n                                # min_samples_split=2,\n                                # min_samples_leaf=5,\n                                # criterion='entropy',\n                               class_weight='balanced',\n                            #    class_weight={1:0.9,0:0.1}\n                                random_state=32\n                               )\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\n\nprint(classification_report(y_test,y_pred))\nprint('roc_auc',round(roc_auc_score(y_test,y_pred),2))\n</pre>  model = DecisionTreeClassifier(                                 max_depth=None,                                 # min_samples_split=2,                                 # min_samples_leaf=5,                                 # criterion='entropy',                                class_weight='balanced',                             #    class_weight={1:0.9,0:0.1}                                 random_state=32                                ) model.fit(X_train,y_train) y_pred = model.predict(X_test)  print(classification_report(y_test,y_pred)) print('roc_auc',round(roc_auc_score(y_test,y_pred),2)) <pre>              precision    recall  f1-score   support\n\n           0       0.78      0.72      0.75       159\n           1       0.15      0.20      0.17        41\n\n    accuracy                           0.61       200\n   macro avg       0.47      0.46      0.46       200\nweighted avg       0.65      0.61      0.63       200\n\nroc_auc 0.46\n</pre> <p>Whilst, the model is not very accurate, it still somewhat confirms that age is an important feature, but we will need to improve our model for this to be confirmed for more accurate models</p> In\u00a0[37]: Copied! <pre>import numpy as np\n\nfeat_import = list(map(float,np.round(model.feature_importances_,3)))\ndict(zip(X_train.columns,feat_import))\n</pre> import numpy as np  feat_import = list(map(float,np.round(model.feature_importances_,3))) dict(zip(X_train.columns,feat_import)) Out[37]: <pre>{'Age': 0.539,\n 'Gender_F': 0.027,\n 'Gender_M': 0.055,\n 'MaritalStatus_Divorced': 0.056,\n 'MaritalStatus_Married': 0.04,\n 'MaritalStatus_Single': 0.031,\n 'MaritalStatus_Widowed': 0.058,\n 'IncomeLevel_High': 0.078,\n 'IncomeLevel_Low': 0.054,\n 'IncomeLevel_Medium': 0.061}</pre> In\u00a0[38]: Copied! <pre>customID = pd.Series(demographic['CustomerID'].unique()).to_frame()\ncustomID.columns = ['CustomerID']\n\nactivity_churn = activity.merge(status,left_on='CustomerID',right_on='CustomerID',how='left')\nlatest_activity = activity_churn[activity_churn['LastLoginDate'] &gt; '2023-10-18']\n\nlatest_activity_agg = latest_activity.groupby('CustomerID')['LoginFrequency'].sum()\n\ncustomID = customID.merge(latest_activity_agg,left_on='CustomerID',right_on='CustomerID',how='left').fillna(0)\ncustomID = customID.merge(status,left_on='CustomerID',right_on='CustomerID')\n\ncustomID = demographic[['CustomerID','Age']].merge(customID,left_on='CustomerID',right_on='CustomerID',how='left')\ncustomID.head()\n</pre> customID = pd.Series(demographic['CustomerID'].unique()).to_frame() customID.columns = ['CustomerID']  activity_churn = activity.merge(status,left_on='CustomerID',right_on='CustomerID',how='left') latest_activity = activity_churn[activity_churn['LastLoginDate'] &gt; '2023-10-18']  latest_activity_agg = latest_activity.groupby('CustomerID')['LoginFrequency'].sum()  customID = customID.merge(latest_activity_agg,left_on='CustomerID',right_on='CustomerID',how='left').fillna(0) customID = customID.merge(status,left_on='CustomerID',right_on='CustomerID')  customID = demographic[['CustomerID','Age']].merge(customID,left_on='CustomerID',right_on='CustomerID',how='left') customID.head() Out[38]: CustomerID Age LoginFrequency ChurnStatus 0 1 62 34.0 0 1 2 65 5.0 1 2 3 18 3.0 0 3 4 21 0.0 0 4 5 21 41.0 0 In\u00a0[39]: Copied! <pre>def get_metrics(df:pd.DataFrame,return_model=False):\n\n   y = df['ChurnStatus']\n   X = df.drop(['ChurnStatus','CustomerID'],axis=1)\n   X = pd.get_dummies(X)\n\n   X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=32)\n\n   model = DecisionTreeClassifier(\n                                 max_depth=None,\n                                 # min_samples_split=2,\n                                 # min_samples_leaf=5,\n                                 # criterion='entropy',\n                                 class_weight='balanced',\n                              #    class_weight={1:0.9,0:0.1}\n                                 random_state=32\n                                 )\n   model.fit(X_train,y_train)\n   y_pred = model.predict(X_test)\n\n   print(classification_report(y_test,y_pred))\n   print('roc_auc',round(roc_auc_score(y_test,y_pred),2))\n\n   if(return_model):\n      return model,X_train.columns\n\nget_metrics(customID)\n</pre> def get_metrics(df:pd.DataFrame,return_model=False):     y = df['ChurnStatus']    X = df.drop(['ChurnStatus','CustomerID'],axis=1)    X = pd.get_dummies(X)     X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=32)     model = DecisionTreeClassifier(                                  max_depth=None,                                  # min_samples_split=2,                                  # min_samples_leaf=5,                                  # criterion='entropy',                                  class_weight='balanced',                               #    class_weight={1:0.9,0:0.1}                                  random_state=32                                  )    model.fit(X_train,y_train)    y_pred = model.predict(X_test)     print(classification_report(y_test,y_pred))    print('roc_auc',round(roc_auc_score(y_test,y_pred),2))     if(return_model):       return model,X_train.columns  get_metrics(customID) <pre>              precision    recall  f1-score   support\n\n           0       0.81      0.58      0.67       159\n           1       0.22      0.46      0.30        41\n\n    accuracy                           0.56       200\n   macro avg       0.51      0.52      0.49       200\nweighted avg       0.69      0.56      0.60       200\n\nroc_auc 0.52\n</pre> In\u00a0[40]: Copied! <pre># average transaction by user\ntransaction_data = transaction.merge(status,left_on='CustomerID',right_on='CustomerID')\n\n# not working \nspend_stats = transaction_data.groupby('CustomerID').agg(mean_spent=('AmountSpent','mean'),\n                                                        max_spent=('AmountSpent','max'),\n                                                        min_spent=('AmountSpent','min'),\n                                                        std_spent=('AmountSpent','std'),\n                                                        shop_count=('AmountSpent','count'),\n                                                        total_spent=('AmountSpent','sum'))\nspend_stats['std_spent'] = spend_stats['std_spent'].fillna(0.0)\nspend_stats = spend_stats.reset_index()\ncustomID2 = customID.merge(spend_stats,left_on='CustomerID',right_on='CustomerID',how='left')\nget_metrics(customID2)\n</pre> # average transaction by user transaction_data = transaction.merge(status,left_on='CustomerID',right_on='CustomerID')  # not working  spend_stats = transaction_data.groupby('CustomerID').agg(mean_spent=('AmountSpent','mean'),                                                         max_spent=('AmountSpent','max'),                                                         min_spent=('AmountSpent','min'),                                                         std_spent=('AmountSpent','std'),                                                         shop_count=('AmountSpent','count'),                                                         total_spent=('AmountSpent','sum')) spend_stats['std_spent'] = spend_stats['std_spent'].fillna(0.0) spend_stats = spend_stats.reset_index() customID2 = customID.merge(spend_stats,left_on='CustomerID',right_on='CustomerID',how='left') get_metrics(customID2)  <pre>              precision    recall  f1-score   support\n\n           0       0.78      0.82      0.80       159\n           1       0.12      0.10      0.11        41\n\n    accuracy                           0.68       200\n   macro avg       0.45      0.46      0.46       200\nweighted avg       0.65      0.68      0.66       200\n\nroc_auc 0.46\n</pre> In\u00a0[41]: Copied! <pre>activity_churn['month'] = activity_churn['LastLoginDate'].dt.month\n\nfrom sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\nencoded_service = encoder.fit_transform(activity_churn['ServiceUsage'])\n\nactivity_churn['ServiceUsage'] = encoded_service\n\nactivity_churn['last_active'] = (activity['LastLoginDate'].max() - activity_churn['LastLoginDate']).dt.days\n# activity_churn = activity_churn[['CustomerID','ServiceUsage','month','last_active']]\n# activity_churn = activity_churn[['CustomerID','last_active']]\nactivity_churn = activity_churn[['CustomerID','month']]\n</pre> activity_churn['month'] = activity_churn['LastLoginDate'].dt.month  from sklearn.preprocessing import LabelEncoder  encoder = LabelEncoder() encoded_service = encoder.fit_transform(activity_churn['ServiceUsage'])  activity_churn['ServiceUsage'] = encoded_service  activity_churn['last_active'] = (activity['LastLoginDate'].max() - activity_churn['LastLoginDate']).dt.days # activity_churn = activity_churn[['CustomerID','ServiceUsage','month','last_active']] # activity_churn = activity_churn[['CustomerID','last_active']] activity_churn = activity_churn[['CustomerID','month']] In\u00a0[42]: Copied! <pre>customID3 = customID.merge(activity_churn,left_on='CustomerID',right_on='CustomerID',how='left')\nget_metrics(customID3)\n</pre> customID3 = customID.merge(activity_churn,left_on='CustomerID',right_on='CustomerID',how='left') get_metrics(customID3) <pre>              precision    recall  f1-score   support\n\n           0       0.82      0.69      0.75       159\n           1       0.25      0.41      0.31        41\n\n    accuracy                           0.63       200\n   macro avg       0.54      0.55      0.53       200\nweighted avg       0.70      0.63      0.66       200\n\nroc_auc 0.55\n</pre> In\u00a0[43]: Copied! <pre># tr_counts = transaction_data.groupby(['CustomerID','ProductCategory']).agg(purchase_counts=('AmountSpent','sum')).reset_index()\n# tr_counts = transaction_data.groupby(['CustomerID','ProductCategory']).agg(purchase_counts=('AmountSpent','mean')).reset_index()\n# tr_counts = transaction_data.groupby(['CustomerID','ProductCategory']).agg(purchase_counts=('AmountSpent','min')).reset_index()\n# tr_counts = transaction_data.groupby(['CustomerID','ProductCategory']).agg(purchase_counts=('AmountSpent','std')).reset_index()\ntr_counts = transaction_data.groupby(['CustomerID','ProductCategory']).agg(purchase_counts=('AmountSpent','count')).reset_index()\n\ntr_counts_pivot = pd.pivot_table(tr_counts,index='CustomerID',columns='ProductCategory',values='purchase_counts',fill_value=0).reset_index()\ntr_counts_pivot = tr_counts_pivot[['CustomerID','Books','Clothing','Electronics','Furniture','Groceries']]\n</pre> # tr_counts = transaction_data.groupby(['CustomerID','ProductCategory']).agg(purchase_counts=('AmountSpent','sum')).reset_index() # tr_counts = transaction_data.groupby(['CustomerID','ProductCategory']).agg(purchase_counts=('AmountSpent','mean')).reset_index() # tr_counts = transaction_data.groupby(['CustomerID','ProductCategory']).agg(purchase_counts=('AmountSpent','min')).reset_index() # tr_counts = transaction_data.groupby(['CustomerID','ProductCategory']).agg(purchase_counts=('AmountSpent','std')).reset_index() tr_counts = transaction_data.groupby(['CustomerID','ProductCategory']).agg(purchase_counts=('AmountSpent','count')).reset_index()  tr_counts_pivot = pd.pivot_table(tr_counts,index='CustomerID',columns='ProductCategory',values='purchase_counts',fill_value=0).reset_index() tr_counts_pivot = tr_counts_pivot[['CustomerID','Books','Clothing','Electronics','Furniture','Groceries']] In\u00a0[54]: Copied! <pre>customID4 = customID3.merge(tr_counts_pivot,left_on='CustomerID',right_on='CustomerID',how='left')\ncustomID4 = customID4.fillna(0.0)\ncustomID4\n</pre> customID4 = customID3.merge(tr_counts_pivot,left_on='CustomerID',right_on='CustomerID',how='left') customID4 = customID4.fillna(0.0) customID4 Out[54]: CustomerID Age LoginFrequency ChurnStatus month Books Clothing Electronics Furniture Groceries 0 1 62 34.0 0 10 0.0 0.0 1.0 0.0 0.0 1 2 65 5.0 1 12 0.0 2.0 3.0 1.0 1.0 2 3 18 3.0 0 11 1.0 1.0 0.0 2.0 2.0 3 4 21 0.0 0 8 0.0 1.0 2.0 1.0 1.0 4 5 21 41.0 0 10 0.0 0.0 3.0 2.0 3.0 ... ... ... ... ... ... ... ... ... ... ... 995 996 54 0.0 0 1 1.0 0.0 0.0 0.0 0.0 996 997 19 0.0 0 4 0.0 0.0 1.0 1.0 0.0 997 998 47 0.0 0 7 1.0 0.0 0.0 0.0 0.0 998 999 23 0.0 0 1 1.0 0.0 2.0 4.0 2.0 999 1000 34 0.0 0 8 2.0 0.0 0.0 2.0 2.0 <p>1000 rows \u00d7 10 columns</p> In\u00a0[45]: Copied! <pre>get_metrics(customID4)\n</pre> get_metrics(customID4) <pre>              precision    recall  f1-score   support\n\n           0       0.83      0.80      0.81       159\n           1       0.32      0.37      0.34        41\n\n    accuracy                           0.71       200\n   macro avg       0.57      0.58      0.58       200\nweighted avg       0.73      0.71      0.72       200\n\nroc_auc 0.58\n</pre> In\u00a0[46]: Copied! <pre>transaction = transaction.merge(demographic,left_on='CustomerID',right_on='CustomerID')\ntransaction = transaction.merge(service,left_on='CustomerID',right_on='CustomerID')\ntransaction = transaction.merge(activity,left_on='CustomerID',right_on='CustomerID')\ntransaction = transaction.merge(status,left_on='CustomerID',right_on='CustomerID')\n\ntransaction = transaction[['CustomerID','AmountSpent','ProductCategory',\n                           'Age','Gender','MaritalStatus','IncomeLevel',\n                           'InteractionType','ResolutionStatus',\n                           'LoginFrequency','ServiceUsage','ChurnStatus']]\n</pre> transaction = transaction.merge(demographic,left_on='CustomerID',right_on='CustomerID') transaction = transaction.merge(service,left_on='CustomerID',right_on='CustomerID') transaction = transaction.merge(activity,left_on='CustomerID',right_on='CustomerID') transaction = transaction.merge(status,left_on='CustomerID',right_on='CustomerID')  transaction = transaction[['CustomerID','AmountSpent','ProductCategory',                            'Age','Gender','MaritalStatus','IncomeLevel',                            'InteractionType','ResolutionStatus',                            'LoginFrequency','ServiceUsage','ChurnStatus']] In\u00a0[47]: Copied! <pre>model,features = get_metrics(transaction,return_model=True)\n</pre> model,features = get_metrics(transaction,return_model=True) <pre>              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99       827\n           1       0.95      0.97      0.96       214\n\n    accuracy                           0.98      1041\n   macro avg       0.97      0.98      0.98      1041\nweighted avg       0.98      0.98      0.98      1041\n\nroc_auc 0.98\n</pre> In\u00a0[48]: Copied! <pre>feat_import = list(map(float,np.round(model.feature_importances_,3)))\nfig = px.bar(pd.Series(dict(zip(features,feat_import)),\n                       name='feature_importance',),template='plotly_white')\n\n\n# Update layout for transparent background and white fonts\nfig.update_layout(\n    paper_bgcolor='rgba(0,0,0,0)',  # Transparent outer background[2][3][8]\n    plot_bgcolor='rgba(0,0,0,0)',   # Transparent plotting area background[2][3][8]\n    title_font=dict(color='white'),  # White plot title\n    xaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),\n    yaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),\n\n    legend=dict(\n        font=dict(color='white'),  # White legend text\n        bgcolor='rgba(0,0,0,0)'   # Transparent legend background\n    ),\n    title=dict(\n        text=\"Feature Importance of Decision Tree\",\n        font=dict(\n            color='white',\n            size=20,\n            family='Courier New, monospace'  # Use a bold font family\n        )\n    )\n)\n\n\n# Update x and y axes for white lines, ticks, labels, and 20% opacity grid\nfig.update_xaxes(\n    showline=True,         # Show axis line\n    linecolor='white',     # White axis line\n    tickfont=dict(color='white'),  # White tick labels\n    gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity\n    linewidth=2\n)\nfig.update_yaxes(\n    showline=True,\n    linecolor='white',\n    tickfont=dict(color='white'),\n    gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity\n    linewidth=2\n)\n\nfig.update_traces(\n    marker=dict(\n        line=dict(color=\"white\", width=1)  # black outline with width 2\n    )\n)\n\nfig.show('png',width=1200,height=400)\n</pre> feat_import = list(map(float,np.round(model.feature_importances_,3))) fig = px.bar(pd.Series(dict(zip(features,feat_import)),                        name='feature_importance',),template='plotly_white')   # Update layout for transparent background and white fonts fig.update_layout(     paper_bgcolor='rgba(0,0,0,0)',  # Transparent outer background[2][3][8]     plot_bgcolor='rgba(0,0,0,0)',   # Transparent plotting area background[2][3][8]     title_font=dict(color='white'),  # White plot title     xaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),     yaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),      legend=dict(         font=dict(color='white'),  # White legend text         bgcolor='rgba(0,0,0,0)'   # Transparent legend background     ),     title=dict(         text=\"Feature Importance of Decision Tree\",         font=dict(             color='white',             size=20,             family='Courier New, monospace'  # Use a bold font family         )     ) )   # Update x and y axes for white lines, ticks, labels, and 20% opacity grid fig.update_xaxes(     showline=True,         # Show axis line     linecolor='white',     # White axis line     tickfont=dict(color='white'),  # White tick labels     gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity     linewidth=2 ) fig.update_yaxes(     showline=True,     linecolor='white',     tickfont=dict(color='white'),     gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity     linewidth=2 )  fig.update_traces(     marker=dict(         line=dict(color=\"white\", width=1)  # black outline with width 2     ) )  fig.show('png',width=1200,height=400) <p>Important Features</p> <p>Now that we have an accurate model, which features are important to predicting customer churn</p> <ul> <li>Age, login frequency, marital status features are amongst the highest contributors to the model predictions</li> </ul> In\u00a0[49]: Copied! <pre>from collections import Counter\nfrom imblearn.over_sampling import SMOTE\n\n\n# Assume X, y are your features and labels\nprint(\"Original class distribution:\", Counter(y))\n\ny = customID4['ChurnStatus']\nX = customID4.drop(['ChurnStatus'],axis=1)\n</pre> from collections import Counter from imblearn.over_sampling import SMOTE   # Assume X, y are your features and labels print(\"Original class distribution:\", Counter(y))  y = customID4['ChurnStatus'] X = customID4.drop(['ChurnStatus'],axis=1) <pre>Original class distribution: Counter({0: 796, 1: 204})\n</pre> In\u00a0[50]: Copied! <pre>smote = SMOTE(sampling_strategy=0.75, random_state=42)\n\nX_resampled, y_resampled = smote.fit_resample(X, y)\n</pre> smote = SMOTE(sampling_strategy=0.75, random_state=42)  X_resampled, y_resampled = smote.fit_resample(X, y) In\u00a0[51]: Copied! <pre>from collections import Counter\n\n# Assume X, y are your features and labels\nprint(\"Original class distribution:\", Counter(y_resampled))\n</pre> from collections import Counter  # Assume X, y are your features and labels print(\"Original class distribution:\", Counter(y_resampled)) <pre>Original class distribution: Counter({0: 796, 1: 597})\n</pre> In\u00a0[52]: Copied! <pre>X_resampled = pd.get_dummies(X_resampled)\n\nX_train, X_test, y_train, y_test = train_test_split(X_resampled,\n                                                    y_resampled,\n                                                    test_size=0.2,\n                                                    stratify=y_resampled,random_state=32)\n\nmodel = DecisionTreeClassifier(\n                                max_depth=None,\n                                # min_samples_split=2,\n                                # min_samples_leaf=5,\n                                # criterion='entropy',\n                                class_weight='balanced',\n                            #    class_weight={1:0.9,0:0.1}\n                                random_state=32\n                                )\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\n\nprint(classification_report(y_test,y_pred))\nprint('roc_auc',round(roc_auc_score(y_test,y_pred),2))\n</pre> X_resampled = pd.get_dummies(X_resampled)  X_train, X_test, y_train, y_test = train_test_split(X_resampled,                                                     y_resampled,                                                     test_size=0.2,                                                     stratify=y_resampled,random_state=32)  model = DecisionTreeClassifier(                                 max_depth=None,                                 # min_samples_split=2,                                 # min_samples_leaf=5,                                 # criterion='entropy',                                 class_weight='balanced',                             #    class_weight={1:0.9,0:0.1}                                 random_state=32                                 ) model.fit(X_train,y_train) y_pred = model.predict(X_test)  print(classification_report(y_test,y_pred)) print('roc_auc',round(roc_auc_score(y_test,y_pred),2)) <pre>              precision    recall  f1-score   support\n\n           0       0.77      0.71      0.74       159\n           1       0.65      0.72      0.69       120\n\n    accuracy                           0.72       279\n   macro avg       0.71      0.72      0.71       279\nweighted avg       0.72      0.72      0.72       279\n\nroc_auc 0.72\n</pre> In\u00a0[53]: Copied! <pre>feat_import = list(map(float,np.round(model.feature_importances_,3)))\nfig = px.bar(pd.Series(dict(zip(features,feat_import)),\n                       name='feature_importance',),template='plotly_white')\n\n\n# Update layout for transparent background and white fonts\nfig.update_layout(\n    paper_bgcolor='rgba(0,0,0,0)',  # Transparent outer background[2][3][8]\n    plot_bgcolor='rgba(0,0,0,0)',   # Transparent plotting area background[2][3][8]\n    title_font=dict(color='white'),  # White plot title\n    xaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),\n    yaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),\n\n    legend=dict(\n        font=dict(color='white'),  # White legend text\n        bgcolor='rgba(0,0,0,0)'   # Transparent legend background\n    ),\n    title=dict(\n        text=\"Feature Importance of Decision Tree\",\n        font=dict(\n            color='white',\n            size=20,\n            family='Courier New, monospace'  # Use a bold font family\n        )\n    )\n)\n\n\n# Update x and y axes for white lines, ticks, labels, and 20% opacity grid\nfig.update_xaxes(\n    showline=True,         # Show axis line\n    linecolor='white',     # White axis line\n    tickfont=dict(color='white'),  # White tick labels\n    gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity\n    linewidth=2\n)\nfig.update_yaxes(\n    showline=True,\n    linecolor='white',\n    tickfont=dict(color='white'),\n    gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity\n    linewidth=2\n)\n\nfig.update_traces(\n    marker=dict(\n        line=dict(color=\"white\", width=1)  # black outline with width 2\n    )\n)\n\nfig.show('png',width=1000,height=400)\n</pre> feat_import = list(map(float,np.round(model.feature_importances_,3))) fig = px.bar(pd.Series(dict(zip(features,feat_import)),                        name='feature_importance',),template='plotly_white')   # Update layout for transparent background and white fonts fig.update_layout(     paper_bgcolor='rgba(0,0,0,0)',  # Transparent outer background[2][3][8]     plot_bgcolor='rgba(0,0,0,0)',   # Transparent plotting area background[2][3][8]     title_font=dict(color='white'),  # White plot title     xaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),     yaxis_title_font=dict(family=\"Arial Black, Arial, sans-serif\", size=16, color=\"white\"),      legend=dict(         font=dict(color='white'),  # White legend text         bgcolor='rgba(0,0,0,0)'   # Transparent legend background     ),     title=dict(         text=\"Feature Importance of Decision Tree\",         font=dict(             color='white',             size=20,             family='Courier New, monospace'  # Use a bold font family         )     ) )   # Update x and y axes for white lines, ticks, labels, and 20% opacity grid fig.update_xaxes(     showline=True,         # Show axis line     linecolor='white',     # White axis line     tickfont=dict(color='white'),  # White tick labels     gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity     linewidth=2 ) fig.update_yaxes(     showline=True,     linecolor='white',     tickfont=dict(color='white'),     gridcolor='rgba(255,255,255,0.1)',  # White grid lines with 20% opacity     linewidth=2 )  fig.update_traces(     marker=dict(         line=dict(color=\"white\", width=1)  # black outline with width 2     ) )  fig.show('png',width=1000,height=400) <p>Taking the upsampling approach:</p> <ul> <li><p>We can see that the model tends to rely on a slightly different features to make preditions to the one above</p> <ul> <li><p>Amount spent for all transactions, age, and furnature category purchase</p> </li> <li><p>Surprising that grocery product count feature was not amongst the highest contributed features since the EDA clearly showed that there is a big difference in purchases made by both groups for this specific subset</p> </li> </ul> </li> </ul> \ud83d\udcca Feature Engineering | Summary <p>Lets review what we learned from this segment</p> <p>The amount of data provided for customers is very limited, creating a challenging problem</p> <ul> <li>The binary classification problem was found to have a high imblance of class samples</li> <li>Specific features were shown the have positive impact, improving the model, however these features were not sufficient enough to create a good working model which generalises well on new data on the customer aggreated data</li> <li>Both SMOTE and merging of transactional data together with all other available data created an artificial increase in data (mostly for the positive class), which allowed the model to show better roc_auc results</li> <li>Age and specific purchase categories behaviour also are important features in classifying churn</li> <li>Amount spend on transactions play an important role in a models ability to classify churn</li> <li>Marital status for this set of customers was shown the have some impact on churn clasification</li> </ul> <p>Some key takaways are that we have successfully created models that allow us to predict customers which are prone to churn. This allows us to automate the process of targeting specific customers before they actually do decide to churn. </p>"},{"location":"portfolio/projects/lloyds_ds/task2_nb.html#lloyds-banking-group-internship-data-science","title":"Lloyds Banking Group Internship Data Science\u00b6","text":""},{"location":"portfolio/projects/lloyds_ds/task2_nb.html#phase-2-building-a-machine-learning-model","title":"\ud83d\udcca Phase 2: Building a machine learning model\u00b6","text":""},{"location":"portfolio/projects/lloyds_ds/task2_nb.html#project-brief","title":"Project brief\u00b6","text":"<ul> <li><p>Welcome to the Data Science &amp; Analytics team at Lloyds Banking Group. As a new data science graduate, you have been entrusted with a critical project that could significantly impact our customer retention strategies. Li, our senior data scientist, has specifically chosen you to take over this project due to your strong analytical skills and enthusiasm for solving real-world business problems. This is an exciting opportunity for you to apply your knowledge and make a real impact within our team. Context</p> </li> <li><p>The project you are about to embark on is the \"Customer Retention Enhancement through Predictive Analytics\" initiative. This project arose from an urgent need to address declining retention rates within certain segments of our customer base. Over the past few months, we've noticed a worrying trend of increased customer churn, particularly among young professionals and small business owners. This poses a substantial threat to our market position and long-term profitability.</p> </li> <li><p>Our fictional client, SmartBank, a subsidiary of Lloyds, has reported that a substantial portion of their customer base is at risk of moving to competitors offering more personalised banking solutions. SmartBank has tasked our team with developing a predictive model to identify at-risk customers and propose targeted interventions to retain them. Key concepts</p> </li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task2_nb.html#key-concepts","title":"Key Concepts\u00b6","text":"<p>Before you begin, it's essential to understand a few key concepts:</p> <ul> <li>Customer churn: The process by which customers stop doing business with a company. Identifying and preventing churn is crucial for maintaining a stable customer base. Predictive analytics: Techniques that use historical data to forecast future possibilities. In this project, you'll use predictive analytics to predict which customers are likely to churn. Exploratory data analysis (EDA): A method of analysing data sets to summarise their primary characteristics, often using visual strategies. EDA is crucial for understanding the data before building predictive models. Machine learning models: Algorithms that let computers learn from and make predictions or decisions based on data. You'll be building a classification model to predict customer churn.</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task2_nb.html#project-requirements","title":"Project requirements\u00b6","text":"<ul> <li><p><code>Phase 2: Building a machine learning model</code></p> <ul> <li>Objective: Develop a predictive model to identify customers at risk of churning and propose ways to measure the model\u2019s performance. Steps: Choose a suitable machine learning algorithm for the classification task. Build and train the model to predict customer churn. Suggest ways to evaluate and measure the model\u2019s performance, ensuring its reliability and accuracy. Deliverable: Submit a report, including the trained machine learning model and your proposed methods for evaluating and measuring the model\u2019s performance.</li> </ul> </li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task2_nb.html#1-read-dataset","title":"1 | Read Dataset\u00b6","text":"<p>We're working with the same dataset as in Phase1.</p> <ul> <li><p>Having done an exploratory data analysis, we have some ideas of which data we can try to utilise in order to create a churn binary classifier model</p> </li> <li><p>Demographic features such as age is definitely worth trying out, as we saw some variation in mean &amp; median values for both subgroups</p> </li> <li><p>Some form of patterns in transactional data was observed, so given sufficient data, the models should be able to extract some insign from the patterns which will allow the model to classify between the two classes</p> </li> <li><p>Insight into user online activity for both groups also suggested some variability between different types of login types for churned and non-churned customers, which suggests that these features also need to be explored as possible feature candidates for improving the model metrics</p> </li> <li><p>Customer service interactions for both groups also showed some minor differences which the model might pick up on</p> </li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task2_nb.html#2-feature-engineering","title":"2 | Feature Engineering\u00b6","text":""},{"location":"portfolio/projects/lloyds_ds/task2_nb.html#things-to-note","title":"Things to note\u00b6","text":"<ul> <li><p>The next step for us is to determine which features are to be used to model the customer churn</p> </li> <li><p>Churn data for each customer CustomerID is found in data status</p> <ul> <li>Each label is defined for a single customer, for whom we have information about whether they churned or not, which is only 1000 customers</li> </ul> </li> <li><p>From our exploratory data analysis, we found that the ratio of churned to non-churned customers is quite low, which implies that upon training our model we are facing an imbalanced classification problem, which is made worse by the fact that we have very little data to work with</p> </li> <li><p>Another point that should be kept in might is interpretability, so we can convey some insights of what factors influence churn to the buisness, on top of those we found in our EDA</p> </li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task2_nb.html#1baseline","title":"1.Baseline\u00b6","text":"<p>Lets define some form of baseline of features, which will be used to create a binary classification model</p> <ul> <li><p>Our baseline model based on decision trees utilising demographic features doesn't show very good results</p> <ul> <li><p>Roc_auc of 0.46 with a recall of 0.20 for the positive churn class, imbalance is something we should pay attention to</p> </li> <li><p>Most likely the features themselves alone not relevant for solving this classification problem</p> </li> <li><p>Nevertheless we have a baseline model and metric to work from</p> </li> </ul> </li> </ul> <p>About the model approach taken</p> <ul> <li><p>To focus more on the feature assembly aspect, we'll use the same approach for all features</p> </li> <li><p>The dataset is split into two subset, one for training, the other for testing, we'll be evaluating the metrics on the test set</p> </li> <li><p>The split occurs utilising stratification of the target variable with a ratio of 0.8/0.2 for the train/test split</p> </li> <li><p>Categorical features are processed using One-Hot-Encoding</p> </li> <li><p>A decision tree classifier model is trained without a depth restriction &amp; class weight set to treat samples which occure less frequent by multiplying the gini/entropy measures by a specific weight, rather than just sample counts</p> </li> <li><p>Metrics are evaluated on the test set using classification report and roc_auc metric</p> </li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task2_nb.html#2-added-activity-feature","title":"2. Added Activity Feature\u00b6","text":"<p>Lets turn our attention to LoginFrequency, summing the total ammount of of times a user has logged in across all platforms</p> <ul> <li>As per exploratory data analysis, lets take only the period after the middle of October, as that is where the most variation between the two groups existed</li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task2_nb.html#3-what-didnt-really-work","title":"3. What didn't really work\u00b6","text":"<p>It is also important to note the different approaches which didn't result in any improvement in metrics</p>"},{"location":"portfolio/projects/lloyds_ds/task2_nb.html#transactional-data","title":"Transactional data\u00b6","text":"<p>Statistics of user spending doesn't lead to improvements in our metric</p>"},{"location":"portfolio/projects/lloyds_ds/task2_nb.html#user-activity","title":"User Activity\u00b6","text":"<p>Several variations of features from user online activity</p> <ul> <li><p>'CustomerID','ServiceUsage','month','last_active' (didn't improve metric)</p> </li> <li><p>'CustomerID','last_active' (didn't improve metric)</p> </li> <li><p>'CustomerID','month' (month of last activity provided slight increase in metrics)</p> <ul> <li>Raising the recall of the positive class to 0.41, which resulted in an improved roc_auc of 0.55</li> </ul> </li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task2_nb.html#more-transactional-data","title":"More Transactional data\u00b6","text":"<ul> <li><p>Sum,mean,min,std statistics of spending on all product categories ProductCategory all didn't improve the metrics</p> </li> <li><p>Number of products (count) bought for each category slightly increased the metrics to a roc_auc of 0.58</p> <ul> <li>Mostly for the more accuracy predictions of the negative class (0)</li> </ul> </li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task2_nb.html#4-what-worked","title":"4. What worked\u00b6","text":""},{"location":"portfolio/projects/lloyds_ds/task2_nb.html#transaction-upsampling","title":"Transaction Upsampling\u00b6","text":"<p>It seems evident from the data that there are no magic features that correlate well to customer</p> <ul> <li><p>The provided CustomerID aggregation centric features (only 1000 labels) seem very insufficient to resolve this problem</p> </li> <li><p>One of the previous attempted study cases (ANZ Customer transaction predictive analytics) had a similar problem of insufficient provided data</p> </li> <li><p>A workaround was to utilise the transactional data of customers and merge additional features to this data,</p> <ul> <li>Which creates a larger datset (5000+) compared to the CustomerID (1000) aggregation from merging all available data</li> </ul> </li> </ul>"},{"location":"portfolio/projects/lloyds_ds/task2_nb.html#smote-upsampling","title":"SMOTE Upsampling\u00b6","text":"<p>A common approach to upsampling is called SMOTE; which generates similar data observed in the positive class artificially, allowing a more evenly balances label ratio to be generated</p> <ul> <li><p>So lets return to the dataset in which we have a single label for each CustomerID, <code>customID4</code></p> </li> <li><p>Instead we'll be artificially upsampling the positve class a little, from 204 labels to 597</p> </li> <li><p>Such an approach has increased the roc_auc metrics to 0.72, which seems like a big indicator that our models lack data for extracting insights</p> </li> </ul>"},{"location":"portfolio/sfml/index.html","title":"Machine Learning","text":""},{"location":"portfolio/sfml/index.html#_1","title":"\u041b\u0438\u043d\u0435\u0439\u043d\u044b\u0435 \u041c\u043e\u0434\u0435\u043b\u0438","text":"<p>\u0414\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u0430\u043a\u0442\u0438\u043a\u0438 \u0441 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u043c\u0438 \u043c\u043e\u0434\u0435\u043b\u044f\u043c\u0438 (\u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u0438 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f)</p> <ul> <li> <p>\u041b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0420\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430</p> <p>\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0438 \u043a\u0432\u0430\u0440\u0442\u0438\u0440\u044b \u0432 \u041c\u043e\u0441\u043a\u0432\u0435 \u0438 \u0436\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443 \u0434\u0430\u043d\u043d\u044b\u0445, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f <code>sklearn</code></p> </li> <li> <p>\u041b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0420\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430 1</p> <p>\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0438 \u043d\u0435\u0434\u0432\u0438\u0436\u0438\u043c\u043e\u0441\u0442\u0438 \u0432 \u0411\u043e\u0441\u0442\u043e\u043d\u0435, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0441\u0430\u043c\u043e\u043f\u0438\u0441\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0441 <code>numpy</code> \u0438 \u0440\u0435\u0448\u0430\u0435\u043c \u0437\u0430\u0434\u0430\u0447\u0443 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u0439 \u0438 \u043f\u043e\u0434\u0445\u043e\u0434 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430</p> </li> <li> <p>\u041b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0420\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430 2</p> <p>\u041e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u0430\u044f \u0437\u0430\u0434\u0430\u0447\u0430 \u043a\u0430\u043a \u0438 \u0432 <code>\u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430 1</code>, \u043d\u043e \u0432 \u044d\u0442\u043e\u0442 \u0440\u0430\u0437 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0431\u043e\u043b\u0435\u0435 \u043f\u0440\u043e\u0434\u0432\u0438\u043d\u0443\u0442\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 OOP, \u0434\u043b\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0442\u0438\u043f\u0430 \u043c\u0435\u0442\u043e\u0434\u0430.</p> </li> <li> <p>\u041b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0420\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u041c\u0435\u0442\u0440\u0438\u043a\u0438 \u041a\u0430\u0447\u0435\u0441\u0442\u0432\u0430</p> <p>\u0411\u0430\u0437\u043e\u0432\u044b\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043e\u0446\u0435\u043d\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 <code>accuracy</code>, <code>precision</code>, <code>recall</code> \u0438 <code>F1 score</code>, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0438\u0437 <code>sklearn</code></p> </li> <li> <p>\u041b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0420\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430 1</p> <p>\u0420\u0435\u0448\u0430\u0435\u043c \u0437\u0430\u0434\u0430\u0447\u0443 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0434\u043e\u0440\u043e\u0433\u0438\u0445 \u043e\u0442 \u0434\u0435\u0448\u0435\u0432\u044b\u0445 \u0441\u043c\u0430\u0440\u0442\u0444\u043e\u043d\u043e\u0432, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u043e\u0446\u0435\u043d\u043a\u0443 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0437 <code>sklearn</code> </p> </li> <li> <p>\u041b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0420\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430 2</p> <p>\u0420\u0435\u0448\u0430\u0435\u043c \u0437\u0430\u0434\u0430\u0447\u0443 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0434\u043e\u0445\u043e\u0434\u043e\u0432 \u043b\u044e\u0434\u0435\u0439, \u0434\u043b\u044f \u044d\u0442\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 \u043c\u044b \u043d\u0430\u0447\u0438\u043d\u0430\u0435\u043c \u0441 \u0441\u0430\u043c\u043e\u043f\u0438\u0441\u043d\u044b\u043c\u0438 \u0444\u0443\u043d\u043a\u0446\u0438\u044f\u043c\u0438 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0441 \u043b\u043e\u0433\u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438, \u0440\u0435\u0448\u0430\u044f \u0435\u0435 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u043c \u0441\u043f\u0443\u0441\u043a\u043e\u043c. \u041e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u0443\u044e \u043c\u0435\u0442\u0440\u0438\u043a\u0443 <code>ROC-AUC</code>. \u0414\u0430\u043b\u0435\u0435 \u044d\u0442\u0438 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u0431\u0435\u0440\u0442\u044b\u0432\u0430\u0435\u043c \u0432 \u043c\u0435\u0442\u043e\u0434\u044b \u043a\u043b\u0430\u0441\u0441\u0430 OOP, \u0438 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u043a\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0440\u0435\u0433\u0443\u043b\u0438\u0440\u043e\u0432\u043a\u0438 \u0440\u0435\u0433\u0443\u0440\u0435\u043b\u0438\u0437\u0430\u0443\u0438\u0438 \u0434\u043b\u044f \u043e\u0431\u043e\u0438\u0445 \u0432\u0438\u0434\u043e\u0432 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438. </p> </li> </ul>"},{"location":"portfolio/sfml/index.html#_2","title":"\u041a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f","text":"<p>\u0418\u0437\u0443\u0447\u0430\u0435\u043c \u043c\u0435\u0442\u043e\u0434\u044b \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445, \u043a\u0430\u043a\u0438\u0435 \u0443 \u0440\u0430\u0437\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b, \u043a\u0430\u043a \u043e\u043d\u0438 \u043e\u0442\u043b\u0438\u0447\u0430\u044e\u0442\u0441\u044f \u0434\u0440\u0443\u0433 \u043e\u0442 \u0434\u0440\u0443\u0433\u0430 \u0438 \u043a\u0430\u043a\u0438\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438, \u0432 \u043a\u043e\u043d\u0446\u0435 \u0434\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438</p> <ul> <li> <p>K-means \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430</p> <p>\u041e\u0437\u043d\u0430\u043a\u0430\u043c\u043b\u0438\u0432\u0430\u0435\u043c\u0441\u044f \u0441 <code>KMeans</code> \u0438\u0437 <code>sklearn</code>, \u043a\u0430\u043a\u0438\u0435 \u0443 \u043d\u0435\u0435 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0438 \u043c\u0435\u0442\u043e\u0434\u044b \u043a\u043b\u0430\u0441\u0441\u0430 \u043d\u0430 \u043f\u0440\u043e\u0441\u0442\u044b\u0445 \u0441\u0438\u043d\u0442\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445</p> </li> <li> <p>EM-\u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430</p> <p>\u041e\u0437\u043d\u0430\u043a\u0430\u043c\u043b\u0438\u0432\u0430\u0435\u043c\u0441\u044f \u0441 <code>GaussianMixture</code> \u0438\u0437 <code>sklearn</code>, \u043a\u0430\u043a\u0438\u0435 \u0443 \u043d\u0435\u0435 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0438 \u043c\u0435\u0442\u043e\u0434\u044b \u043a\u043b\u0430\u0441\u0441\u0430 \u043d\u0430 \u043f\u0440\u043e\u0441\u0442\u044b\u0445 \u0441\u0438\u043d\u0442\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445</p> </li> <li> <p>\u0410\u0433\u043b\u043e\u043c\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430</p> <p>\u041e\u0437\u043d\u0430\u043a\u0430\u043c\u043b\u0438\u0432\u0430\u0435\u043c\u0441\u044f \u0441 <code>AgglomerativeClustering</code> \u0438\u0437 <code>sklearn</code>, \u043a\u0430\u043a\u0438\u0435 \u0443 \u043d\u0435\u0435 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0438 \u043c\u0435\u0442\u043e\u0434\u044b \u043a\u043b\u0430\u0441\u0441\u0430 \u043d\u0430 \u043f\u0440\u043e\u0441\u0442\u044b\u0445 \u0441\u0438\u043d\u0442\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445</p> </li> <li> <p>DBSCAN. \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430</p> <p>\u041e\u0437\u043d\u0430\u043a\u0430\u043c\u043b\u0438\u0432\u0430\u0435\u043c\u0441\u044f \u0441 <code>DBSCAN</code> \u0438\u0437 <code>sklearn</code>, \u043a\u0430\u043a\u0438\u0435 \u0443 \u043d\u0435\u0435 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0438 \u043c\u0435\u0442\u043e\u0434\u044b \u043a\u043b\u0430\u0441\u0441\u0430 \u043d\u0430 \u043f\u0440\u043e\u0441\u0442\u044b\u0445 \u0441\u0438\u043d\u0442\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445</p> </li> <li> <p>\u041a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u0441\u0438\u043b\u0443\u044d\u0442\u0430 \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430</p> <p>\u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430 \u043f\u043e \u043e\u0446\u0435\u043d\u043a\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e \u043c\u0435\u0442\u0440\u0438\u043a\u0443 <code>silhouette_score</code> \u043d\u0430 \u043f\u0440\u043e\u0441\u0442\u043e\u043c \u043f\u0440\u0438\u043c\u0435\u0440\u0435 </p> </li> <li> <p>\u041e\u0434\u043d\u043e\u0440\u043e\u0434\u043d\u043e\u0441\u0442\u044c \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430</p> <p>\u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430 \u043f\u043e \u043e\u0446\u0435\u043d\u043a\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e \u043c\u0435\u0442\u0440\u0438\u043a\u0443 <code>homogeneity_score</code> \u043d\u0430 \u043f\u0440\u043e\u0441\u0442\u043e\u043c \u043f\u0440\u0438\u043c\u0435\u0440\u0435</p> </li> <li> <p>\u041f\u043e\u043b\u043d\u043e\u0442\u0430 \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430</p> <p>\u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430 \u043f\u043e \u043e\u0446\u0435\u043d\u043a\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e \u043c\u0435\u0442\u0440\u0438\u043a\u0443 <code>completeness_score</code> \u043d\u0430 \u043f\u0440\u043e\u0441\u0442\u043e\u043c \u043f\u0440\u0438\u043c\u0435\u0440\u0435</p> </li> <li> <p>V-\u043c\u0435\u0440\u0430 \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430</p> <p>\u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430 \u043f\u043e \u043e\u0446\u0435\u043d\u043a\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e \u043c\u0435\u0442\u0440\u0438\u043a\u0443 <code>v_measure_score</code> \u043d\u0430 \u043f\u0440\u043e\u0441\u0442\u043e\u043c \u043f\u0440\u0438\u043c\u0435\u0440\u0435</p> </li> <li> <p>\u041a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u041d\u043e\u0432\u043e\u0441\u0442\u0435\u0439</p> <p>\u0420\u0435\u0448\u0430\u0435\u043c \u0437\u0430\u0434\u0430\u0447\u0443 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432. \u0414\u043b\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0442\u0435\u043a\u0441\u0442\u0430, \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438 \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u0441\u0442\u043e\u043f \u0441\u043b\u043e\u0432\u0430 \u0441 <code>CountVectorizer</code>. \u041d\u0430 \u0444\u0438\u043b\u044c\u0442\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0442\u043e\u043a\u0435\u043d\u044b \u0432 \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f <code>word2vec</code> \u0438 \u0443\u0441\u0440\u0435\u0434\u043d\u044f\u0435\u043c \u0434\u043b\u044f \u0432\u0441\u0435\u0433\u043e \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f. \u041d\u0430 \u044d\u0442\u0438\u0445 \u0444\u0438\u0447\u0430\u0445 \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044e \u0438 \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0438.</p> </li> </ul>"},{"location":"portfolio/sfml/index.html#_3","title":"\u0420\u0435\u0448\u0430\u044e\u0449\u0438\u0435 \u0434\u0435\u0440\u0435\u0432\u044c\u044f \u0438 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441","text":"<p>\u0418\u0437\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432, \u0438 \u0432\u0430\u0440\u0438\u0430\u0446\u0438\u0438 \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u044d\u0442\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u043c \u0431\u044d\u0433\u0433\u0438\u043d\u0433\u0430 \u0438 \u043c\u0435\u0442\u043e\u0434\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0445 \u043f\u043e\u0434\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432 </p> <ul> <li> <p>\u0420\u0435\u0448\u0430\u044e\u0448\u0438\u0435 \u0414\u0435\u0440\u0435\u0432\u044c\u044f</p> <p>\u041e\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c, \u043a\u0430\u043a \u043e\u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f, \u0447\u0442\u043e \u0432 \u043d\u0435\u0439 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043f\u0440\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438. \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u044f\u0432\u043d\u044b\u0439 \u043f\u0440\u0438\u043c\u0435\u0440 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, \u0438 \u043e\u0442\u043c\u0435\u0442\u0438\u043c \u043a\u0430\u043a \u0432 \u044d\u0442\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043c\u043e\u0436\u043d\u043e \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u043e\u0432\u0430\u0442\u044c.</p> </li> <li> <p>\u0420\u0435\u0448\u0430\u044e\u0448\u0438\u0435 \u0414\u0435\u0440\u0435\u0432\u044c\u044f \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430</p> <p>\u041f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\u0435\u043c \u043d\u0430 \u043f\u0440\u043e\u0441\u0442\u043e\u043c \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043a\u0430\u043a \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0433\u043b\u0443\u0431\u0438\u043d\u044b \u0432\u043b\u0438\u044f\u0435\u0442 \u043d\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f. \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u043a\u043e\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u0443\u0435\u0442 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435. </p> </li> <li> <p>\u0410\u043d\u0441\u0430\u043c\u0431\u043b\u0438 \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430</p> <p>\u041d\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u0431\u0438\u043d\u0430\u0440\u043d\u043e\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438, \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u0433\u043b\u0430\u0432\u043d\u044b\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0438\u0438 \u0432 \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u0440\u0438\u0432\u0435\u0434\u0443\u0442 \u043d\u0430\u0441 \u043a \u043c\u043e\u0434\u0435\u043b\u0438 <code>\u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441</code>. \u041d\u0430\u0447\u0438\u043d\u0430\u044f \u043f\u0440\u043e\u0441\u0442\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u0435\u0440\u0435\u0432\u044c\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u0438, \u043e\u0431\u0435\u0440\u043d\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0432 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u0431\u044d\u0433\u0433\u0438\u043d\u0433\u0430 <code>BaggingClassifier</code>. \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043f\u0440\u0438\u043c\u0435\u0440 \u043a\u0430\u043a \u044d\u0442\u0443 \u043c\u043e\u0434\u0435\u043b\u044c \u043c\u043e\u0436\u043d\u043e \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u043f\u043e\u0434\u0431\u043e\u0440\u043e\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440. \u0418\u0437\u0443\u0447\u0438\u043c \u043f\u043e \u043a\u0430\u043a\u0438\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442 \u0440\u0430\u043d\u0434\u043e\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u0432 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u043c \u043b\u0435\u0441\u0435 <code>RandomForestClassifier</code>, \u0447\u0442\u043e \u0442\u0430\u043a\u043e\u0435 <code>OOB</code> \u043c\u0435\u0442\u0440\u0438\u043a\u0430, \u0441\u0440\u0430\u0432\u043d\u0438\u043c \u0431\u044d\u0433\u0433\u0438\u043d\u0433 \u0441 \u0434\u0435\u0440\u0435\u0432\u044c\u044f\u043c\u0438 \u0438 \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u043c\u0438 \u043c\u043e\u0434\u0435\u043b\u044f\u043c\u0438.</p> </li> </ul>"},{"location":"portfolio/sfml/index.html#_4","title":"\u0411\u0443\u0441\u0442\u0438\u043d\u0433. \u0421\u0442\u0435\u043a\u0438\u043d\u0433","text":"<p>\u041f\u043e\u0434\u0445\u043e\u0434\u044b \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u0430 \u0438 \u0441\u0442\u0435\u043a\u0438\u043d\u0433 \u043c\u043e\u0434\u0435\u043b\u0435\u0439</p> <ul> <li> <p>\u041a\u043e\u0433\u0434\u0430 \u0446\u0435\u043b\u043e\u0433\u043e \u043b\u0435\u0441\u0430 \u043c\u0430\u043b\u043e</p> <p>\u041e\u0442\u043c\u0435\u0442\u0438\u043c \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u044e\u0442 \u0432 \u043c\u043e\u0434\u0435\u043b\u0438 \u0430\u043d\u0441\u0430\u0431\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441, \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u043d\u0430 \u043f\u0440\u043e\u0441\u0442\u043e\u043c \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u0440\u0430\u0437\u043d\u0438\u0446\u0443 \u043c\u0435\u0436\u0434\u0443 \u043d\u0435 \u0433\u043b\u0443\u0431\u043e\u043a\u043e\u0439 \u0438 \u0433\u043b\u0443\u0431\u043e\u043a\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u044c\u044e</p> </li> <li> <p>\u0411\u0443\u0441\u0442\u0438\u043d\u0433 \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430</p> <p>\u041d\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u0437\u0430\u0434\u0430\u0447\u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0438 \u0436\u0438\u043b\u044c\u044f \u0432 \u0431\u043e\u0441\u0442\u043e\u043d\u0435,   </p> </li> </ul>"},{"location":"portfolio/sfml/index.html#_5","title":"\u0412\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0440\u044f\u0434\u044b \u0432 \u041c\u041e","text":"<p>Time series based machine learning </p> <ul> <li>\u041e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043a \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u043c \u0440\u044f\u0434\u0430\u043c</li> </ul>"},{"location":"portfolio/sfml/index.html#projects","title":"Projects","text":"<p>Hackathon competition projects</p> <ul> <li>Mid-Term Hackathon Project</li> <li>Final-Term Hackathon Project</li> <li>Feature Importances</li> </ul>"},{"location":"portfolio/sfml/feature_importance.html","title":"Feature importance","text":"In\u00a0[\u00a0]: Copied! <pre># \u0423\u0431\u0435\u0440\u0435\u043c \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f, \u0447\u0442\u043e\u0431\u044b \u043e\u043d\u0438 \u043d\u0435 \u0437\u0430\u0433\u0440\u043e\u043c\u043e\u0436\u0434\u0430\u043b\u0438 \u0432\u044b\u0432\u043e\u0434\n\nimport warning|s \nwarnings.filterwarnings('ignore')\n</pre> # \u0423\u0431\u0435\u0440\u0435\u043c \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f, \u0447\u0442\u043e\u0431\u044b \u043e\u043d\u0438 \u043d\u0435 \u0437\u0430\u0433\u0440\u043e\u043c\u043e\u0436\u0434\u0430\u043b\u0438 \u0432\u044b\u0432\u043e\u0434  import warning|s  warnings.filterwarnings('ignore') In\u00a0[\u00a0]: Copied! <pre># \u0423\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043c \u0440\u0430\u0437\u043c\u0435\u0440\u044b \u0448\u0440\u0438\u0444\u0442\u043e\u0432 \u0434\u043b\u044f \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=16)          # \u0440\u0430\u0437\u043c\u0435\u0440\u044b \u0442\u0435\u043a\u0441\u0442\u0430 \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e\nplt.rc('axes', titlesize=20)     # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u0430 \u043e\u0441\u0435\u0439\nplt.rc('axes', labelsize=18)     # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u043e\u0441\u0435\u0439\nplt.rc('xtick', labelsize=18)    # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u043c\u0435\u0442\u043e\u043a \u043f\u043e \u043e\u0441\u0438 x\nplt.rc('ytick', labelsize=18)    # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u043c\u0435\u0442\u043e\u043a \u043f\u043e \u043e\u0441\u0438 y\nplt.rc('legend', fontsize=16)    # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u043b\u0435\u0433\u0435\u043d\u0434\u044b\nplt.rc('figure', titlesize=20)   # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u0430 \u0432\u0441\u0435\u0433\u043e \u0433\u0440\u0430\u0444\u0438\u043a\u0430\n</pre> # \u0423\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043c \u0440\u0430\u0437\u043c\u0435\u0440\u044b \u0448\u0440\u0438\u0444\u0442\u043e\u0432 \u0434\u043b\u044f \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432 import matplotlib.pyplot as plt  plt.rc('font', size=16)          # \u0440\u0430\u0437\u043c\u0435\u0440\u044b \u0442\u0435\u043a\u0441\u0442\u0430 \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e plt.rc('axes', titlesize=20)     # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u0430 \u043e\u0441\u0435\u0439 plt.rc('axes', labelsize=18)     # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u043e\u0441\u0435\u0439 plt.rc('xtick', labelsize=18)    # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u043c\u0435\u0442\u043e\u043a \u043f\u043e \u043e\u0441\u0438 x plt.rc('ytick', labelsize=18)    # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u043c\u0435\u0442\u043e\u043a \u043f\u043e \u043e\u0441\u0438 y plt.rc('legend', fontsize=16)    # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u043b\u0435\u0433\u0435\u043d\u0434\u044b plt.rc('figure', titlesize=20)   # \u0440\u0430\u0437\u043c\u0435\u0440 \u0448\u0440\u0438\u0444\u0442\u0430 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u0430 \u0432\u0441\u0435\u0433\u043e \u0433\u0440\u0430\u0444\u0438\u043a\u0430 In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\ndf = pd.read_csv(\"diabetes.csv\")\nprint(df.shape)\ndf.head()\n</pre> import pandas as pd  df = pd.read_csv(\"diabetes.csv\") print(df.shape) df.head() <pre>(768, 9)\n</pre> Out[\u00a0]: Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome 0 6 148 72 35 0 33.6 0.627 50 1 1 1 85 66 29 0 26.6 0.351 31 0 2 8 183 64 0 0 23.3 0.672 32 1 3 1 89 66 23 94 28.1 0.167 21 0 4 0 137 40 35 168 43.1 2.288 33 1 In\u00a0[\u00a0]: Copied! <pre>target = df[\"Outcome\"]\n\ndf = df.drop(\"Outcome\", axis=1)\ndf.head()\n</pre> target = df[\"Outcome\"]  df = df.drop(\"Outcome\", axis=1) df.head() Out[\u00a0]: Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age 0 6 148 72 35 0 33.6 0.627 50 1 1 85 66 29 0 26.6 0.351 31 2 8 183 64 0 0 23.3 0.672 32 3 1 89 66 23 94 28.1 0.167 21 4 0 137 40 35 168 43.1 2.288 33 <p>\u041a\u0430\u0436\u0435\u0442\u0441\u044f, \u0447\u0442\u043e \u0432\u0441\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0447\u0438\u0441\u043b\u0435\u043d\u043d\u044b\u0435. \u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u044d\u0442\u043e:</p> In\u00a0[\u00a0]: Copied! <pre>numerical_cols = df.select_dtypes(exclude=[\"object\"]).columns.tolist()\nlen(numerical_cols) == df.shape[1]\n</pre> numerical_cols = df.select_dtypes(exclude=[\"object\"]).columns.tolist() len(numerical_cols) == df.shape[1] Out[\u00a0]: <pre>True</pre> In\u00a0[\u00a0]: Copied! <pre>df.isna().sum()\n</pre> df.isna().sum() Out[\u00a0]: <pre>Pregnancies                 0\nGlucose                     0\nBloodPressure               0\nSkinThickness               0\nInsulin                     0\nBMI                         0\nDiabetesPedigreeFunction    0\nAge                         0\ndtype: int64</pre> <p>\u041f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0435\u0442, \u0432\u0441\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0447\u0438\u0441\u043b\u0435\u043d\u043d\u044b\u0435, \u0438 \u043c\u044b \u0441\u043e\u0431\u0438\u0440\u0430\u0435\u043c\u0441\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u0438 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043f\u0440\u043e\u0432\u043e\u0434\u0438\u0442\u044c \u043d\u0435 \u0431\u0443\u0434\u0435\u043c.</p> In\u00a0[\u00a0]: Copied! <pre>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    df,\n    target,\n    test_size=0.33,\n    random_state=42,\n    stratify=target, # \u043f\u043e\u0434\u0435\u043b\u0438\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u0442\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u0447\u0442\u043e\u0431\u044b \u0438 \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439, \u0438 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0431\u044b\u043b\u043e \u0442\u0430\u043a\u043e\u0435 \u0436\u0435 \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432, \u043a\u0430\u043a \u0438 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\n)\nX_train.shape, X_test.shape\n</pre> from sklearn.model_selection import train_test_split  X_train, X_test, y_train, y_test = train_test_split(     df,     target,     test_size=0.33,     random_state=42,     stratify=target, # \u043f\u043e\u0434\u0435\u043b\u0438\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u0442\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u0447\u0442\u043e\u0431\u044b \u0438 \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439, \u0438 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0431\u044b\u043b\u043e \u0442\u0430\u043a\u043e\u0435 \u0436\u0435 \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432, \u043a\u0430\u043a \u0438 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 ) X_train.shape, X_test.shape Out[\u00a0]: <pre>((514, 8), (254, 8))</pre> In\u00a0[\u00a0]: Copied! <pre># \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0431\u0430\u043b\u0430\u043d\u0441 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \n\nval_count_init = pd.Series(target).value_counts().to_dict()\nval_count_train = y_train.value_counts().to_dict()\nval_count_test = y_test.value_counts().to_dict()\n\nget_relation = lambda x: round(x[1] / x[0], 2)\n\nprint(\n    \"\"\"\n    \u0421\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435:  {}\\t(\u043a\u043b\u0430\u0441\u0441 1 / \u043a\u043b\u0430\u0441\u0441 0 = {})\n    \u0421\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435: {}\\t(\u043a\u043b\u0430\u0441\u0441 1 / \u043a\u043b\u0430\u0441\u0441 0 = {})\n    \u0421\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435:  {}\\t(\u043a\u043b\u0430\u0441\u0441 1 / \u043a\u043b\u0430\u0441\u0441 0 = {})\n    \"\"\".format(\n        val_count_init, get_relation(val_count_init),\n        val_count_train, get_relation(val_count_train),\n        val_count_test, get_relation(val_count_test),\n    )\n)\n</pre> # \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0431\u0430\u043b\u0430\u043d\u0441 \u043a\u043b\u0430\u0441\u0441\u043e\u0432   val_count_init = pd.Series(target).value_counts().to_dict() val_count_train = y_train.value_counts().to_dict() val_count_test = y_test.value_counts().to_dict()  get_relation = lambda x: round(x[1] / x[0], 2)  print(     \"\"\"     \u0421\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435:  {}\\t(\u043a\u043b\u0430\u0441\u0441 1 / \u043a\u043b\u0430\u0441\u0441 0 = {})     \u0421\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435: {}\\t(\u043a\u043b\u0430\u0441\u0441 1 / \u043a\u043b\u0430\u0441\u0441 0 = {})     \u0421\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435:  {}\\t(\u043a\u043b\u0430\u0441\u0441 1 / \u043a\u043b\u0430\u0441\u0441 0 = {})     \"\"\".format(         val_count_init, get_relation(val_count_init),         val_count_train, get_relation(val_count_train),         val_count_test, get_relation(val_count_test),     ) ) <pre>\n    \u0421\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435:  {0: 500, 1: 268}\t(\u043a\u043b\u0430\u0441\u0441 1 / \u043a\u043b\u0430\u0441\u0441 0 = 0.54)\n    \u0421\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435: {0: 335, 1: 179}\t(\u043a\u043b\u0430\u0441\u0441 1 / \u043a\u043b\u0430\u0441\u0441 0 = 0.53)\n    \u0421\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435:  {0: 165, 1: 89}\t(\u043a\u043b\u0430\u0441\u0441 1 / \u043a\u043b\u0430\u0441\u0441 0 = 0.54)\n    \n</pre> <p>\u041f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0443 \u043d\u0430\u0441 \u043d\u0435\u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435, \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c F1-\u043c\u0435\u0440\u0443 \u0438 PR AUC.</p> In\u00a0[\u00a0]: Copied! <pre>from sklearn.metrics import (\n    f1_score, \n    average_precision_score,\n)\n</pre> from sklearn.metrics import (     f1_score,      average_precision_score, ) <p>\u0414\u043b\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u0438 \u0432\u044b\u0432\u0435\u0434\u0435\u043d\u0438\u044f F1-\u043c\u0435\u0440\u044b \u0438 PR AUC \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0438\u0437 \u044f\u0447\u0435\u0439\u043a\u0438 \u043d\u0438\u0436\u0435:</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\ndef get_all_metrics(\n    true_labels, \n    predictions, \n    probabilities, \n    print_metrics=False,\n):\n    \"\"\"\u041f\u043e\u0434\u0441\u0447\u0451\u0442 \u0438 \u0432\u044b\u0432\u043e\u0434 \u0432\u0441\u0435\u0445 \u043c\u0435\u0442\u0440\u0438\u043a.\n\n    :param true_labels: \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043a\u0438 \u043a\u043b\u0430\u0441\u0441\u043e\u0432\n    :param predictions: \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f (\u043c\u0435\u0442\u043a\u0438 \u043a\u043b\u0430\u0441\u0441\u043e\u0432)\n    :param probabilities: \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f (\u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u043a \u043a\u0430\u0436\u0434\u043e\u043c\u0443 \u043a\u043b\u0430\u0441\u0441\u0443)\n    :param print_metrics: \u0435\u0441\u043b\u0438 True, \u0442\u043e \u043f\u0435\u0447\u0430\u0442\u0430\u0435\u0442 \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0435\u0441\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0438\n    :return: \u0441\u043b\u043e\u0432\u0430\u0440\u044c, \u043a\u043b\u044e\u0447\u0438 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e -- \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u043c\u0435\u0442\u0440\u0438\u043a, \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f -- \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0435\u0441\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0438   \n    \"\"\"\n    f1 = f1_score(true_labels, predictions)\n    pr_auc =  average_precision_score(true_labels, probabilities)\n    \n    if print_metrics:\n        print(\n            \"F1-\u043c\u0435\u0440\u0430 = {}\\nPR AUC = {}\".format(\n                round(f1, 3), round(pr_auc, 3)\n            )\n        )\n    return {\n        \"F1-score\": f1, \n        \"PR AUC\": pr_auc,\n    }\n</pre> import numpy as np  def get_all_metrics(     true_labels,      predictions,      probabilities,      print_metrics=False, ):     \"\"\"\u041f\u043e\u0434\u0441\u0447\u0451\u0442 \u0438 \u0432\u044b\u0432\u043e\u0434 \u0432\u0441\u0435\u0445 \u043c\u0435\u0442\u0440\u0438\u043a.      :param true_labels: \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043a\u0438 \u043a\u043b\u0430\u0441\u0441\u043e\u0432     :param predictions: \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f (\u043c\u0435\u0442\u043a\u0438 \u043a\u043b\u0430\u0441\u0441\u043e\u0432)     :param probabilities: \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f (\u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u043a \u043a\u0430\u0436\u0434\u043e\u043c\u0443 \u043a\u043b\u0430\u0441\u0441\u0443)     :param print_metrics: \u0435\u0441\u043b\u0438 True, \u0442\u043e \u043f\u0435\u0447\u0430\u0442\u0430\u0435\u0442 \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0435\u0441\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0438     :return: \u0441\u043b\u043e\u0432\u0430\u0440\u044c, \u043a\u043b\u044e\u0447\u0438 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e -- \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u043c\u0435\u0442\u0440\u0438\u043a, \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f -- \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0435\u0441\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0438        \"\"\"     f1 = f1_score(true_labels, predictions)     pr_auc =  average_precision_score(true_labels, probabilities)          if print_metrics:         print(             \"F1-\u043c\u0435\u0440\u0430 = {}\\nPR AUC = {}\".format(                 round(f1, 3), round(pr_auc, 3)             )         )     return {         \"F1-score\": f1,          \"PR AUC\": pr_auc,     } In\u00a0[\u00a0]: Copied! <pre>from xgboost import XGBClassifier                   # \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433 \nfrom sklearn.ensemble import (\n    AdaBoostClassifier,                             # \u0430\u0434\u0430\u043f\u0442\u0438\u0432\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433\n    RandomForestClassifier,                         # \u0431\u044d\u0433\u0433\u0438\u043d\u0433\n    StackingClassifier,                             # \u0441\u0442\u0435\u043a\u0438\u043d\u0433\n    VotingClassifier,                               # \u0432\u043e\u0442\u0438\u043d\u0433\n)\n\nfrom sklearn.tree import DecisionTreeClassifier\n</pre> from xgboost import XGBClassifier                   # \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433  from sklearn.ensemble import (     AdaBoostClassifier,                             # \u0430\u0434\u0430\u043f\u0442\u0438\u0432\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433     RandomForestClassifier,                         # \u0431\u044d\u0433\u0433\u0438\u043d\u0433     StackingClassifier,                             # \u0441\u0442\u0435\u043a\u0438\u043d\u0433     VotingClassifier,                               # \u0432\u043e\u0442\u0438\u043d\u0433 )  from sklearn.tree import DecisionTreeClassifier In\u00a0[\u00a0]: Copied! <pre>from sklearn.linear_model import LogisticRegression\n</pre> from sklearn.linear_model import LogisticRegression <p>\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0432 \u044f\u0447\u0435\u0439\u043a\u0435 \u043d\u0438\u0436\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u043c\u0435\u0442\u0440\u0438\u043a \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430.</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\ndef model_train_test(X_train, y_train, X_test, y_test, model):\n    \"\"\"\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e.\n\n    :param X_train: \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\n    :param y_train: \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\n    :param X_test: \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\n    :param y_test: \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\n    :param model: \u043c\u043e\u0434\u0435\u043b\u044c, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u0443\u0435\u0442\u0441\u044f\n    :return: \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f -- \u043a\u043b\u0430\u0441\u0441\u044b \u0438 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043a \u043a\u0430\u0436\u0434\u043e\u043c\u0443 \u043a\u043b\u0430\u0441\u0441\u0443, \u0432\u0441\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438  \n    \"\"\"\n    model.fit(X_train, y_train)\n\n    pred = model.predict(X_test)\n    proba = model.predict_proba(X_test)\n\n    print(str(model), end=\"\\n\\n\")\n\n    all_metrics = get_all_metrics(\n        y_test, \n        pred, \n        proba[:, 1], \n        print_metrics=True,\n    )\n    return model, pred, proba, all_metrics\n</pre> import matplotlib.pyplot as plt  def model_train_test(X_train, y_train, X_test, y_test, model):     \"\"\"\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e.      :param X_train: \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438     :param y_train: \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438     :param X_test: \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438     :param y_test: \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438     :param model: \u043c\u043e\u0434\u0435\u043b\u044c, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u0443\u0435\u0442\u0441\u044f     :return: \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f -- \u043a\u043b\u0430\u0441\u0441\u044b \u0438 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043a \u043a\u0430\u0436\u0434\u043e\u043c\u0443 \u043a\u043b\u0430\u0441\u0441\u0443, \u0432\u0441\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438       \"\"\"     model.fit(X_train, y_train)      pred = model.predict(X_test)     proba = model.predict_proba(X_test)      print(str(model), end=\"\\n\\n\")      all_metrics = get_all_metrics(         y_test,          pred,          proba[:, 1],          print_metrics=True,     )     return model, pred, proba, all_metrics <p>\u0411\u0443\u0434\u0435\u043c \u0441\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0442\u044c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u044b \u0441 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e (\u043f\u043e\u0434\u0431\u043e\u0440 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0434\u0435\u043b\u0430\u0442\u044c \u043d\u0435 \u0431\u0443\u0434\u0435\u043c).</p> In\u00a0[\u00a0]: Copied! <pre>model_logreg, pred_logreg, proba_logreg, all_metrics_logreg = model_train_test(\n    X_train, y_train,\n    X_test, y_test,\n    LogisticRegression(),\n)\n</pre> model_logreg, pred_logreg, proba_logreg, all_metrics_logreg = model_train_test(     X_train, y_train,     X_test, y_test,     LogisticRegression(), ) <pre>LogisticRegression()\n\nF1-\u043c\u0435\u0440\u0430 = 0.596\nPR AUC = 0.728\n</pre> In\u00a0[\u00a0]: Copied! <pre>model_xgb, pred_xgb, proba_xgb, all_metrics_xgb = model_train_test(\n    X_train, y_train,\n    X_test, y_test,\n    XGBClassifier(),\n)\n</pre> model_xgb, pred_xgb, proba_xgb, all_metrics_xgb = model_train_test(     X_train, y_train,     X_test, y_test,     XGBClassifier(), ) <pre>XGBClassifier()\n\nF1-\u043c\u0435\u0440\u0430 = 0.617\nPR AUC = 0.697\n</pre> In\u00a0[\u00a0]: Copied! <pre>model_adab, pred_adab, proba_adab, all_metrics_adab = model_train_test(\n    X_train, y_train,\n    X_test, y_test,\n    AdaBoostClassifier(\n        base_estimator=DecisionTreeClassifier()\n    ),\n)\n</pre> model_adab, pred_adab, proba_adab, all_metrics_adab = model_train_test(     X_train, y_train,     X_test, y_test,     AdaBoostClassifier(         base_estimator=DecisionTreeClassifier()     ), ) <pre>AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n\nF1-\u043c\u0435\u0440\u0430 = 0.473\nPR AUC = 0.418\n</pre> In\u00a0[\u00a0]: Copied! <pre>model_rf, pred_rf, proba_rf, all_metrics_rf = model_train_test(\n    X_train, y_train,\n    X_test, y_test,\n    RandomForestClassifier(),\n)\n</pre> model_rf, pred_rf, proba_rf, all_metrics_rf = model_train_test(     X_train, y_train,     X_test, y_test,     RandomForestClassifier(), ) <pre>RandomForestClassifier()\n\nF1-\u043c\u0435\u0440\u0430 = 0.625\nPR AUC = 0.698\n</pre> In\u00a0[\u00a0]: Copied! <pre>model_trees_stacking, pred_trees_stacking, proba_trees_stacking, all_metrics_trees_stacking = model_train_test(\n    X_train, y_train,\n    X_test, y_test,\n    StackingClassifier(\n        estimators=[\n            (\"decision_tree_{}\".format(i+1), DecisionTreeClassifier()) for i in range(10)\n        ],\n        final_estimator=DecisionTreeClassifier()\n    ),\n)\n</pre> model_trees_stacking, pred_trees_stacking, proba_trees_stacking, all_metrics_trees_stacking = model_train_test(     X_train, y_train,     X_test, y_test,     StackingClassifier(         estimators=[             (\"decision_tree_{}\".format(i+1), DecisionTreeClassifier()) for i in range(10)         ],         final_estimator=DecisionTreeClassifier()     ), ) <pre>StackingClassifier(estimators=[('decision_tree_1', DecisionTreeClassifier()),\n                               ('decision_tree_2', DecisionTreeClassifier()),\n                               ('decision_tree_3', DecisionTreeClassifier()),\n                               ('decision_tree_4', DecisionTreeClassifier()),\n                               ('decision_tree_5', DecisionTreeClassifier()),\n                               ('decision_tree_6', DecisionTreeClassifier()),\n                               ('decision_tree_7', DecisionTreeClassifier()),\n                               ('decision_tree_8', DecisionTreeClassifier()),\n                               ('decision_tree_9', DecisionTreeClassifier()),\n                               ('decision_tree_10', DecisionTreeClassifier())],\n                   final_estimator=DecisionTreeClassifier())\n\nF1-\u043c\u0435\u0440\u0430 = 0.527\nPR AUC = 0.445\n</pre> In\u00a0[\u00a0]: Copied! <pre>model_trees_voting, pred_trees_voting, proba_trees_voting, all_metrics_trees_voting = model_train_test(\n    X_train, y_train,\n    X_test, y_test,\n    VotingClassifier(\n        estimators=[\n            (\"decision_tree_{}\".format(i+1), DecisionTreeClassifier()) for i in range(10)\n        ],\n        voting=\"soft\"\n    ),\n)\n</pre> model_trees_voting, pred_trees_voting, proba_trees_voting, all_metrics_trees_voting = model_train_test(     X_train, y_train,     X_test, y_test,     VotingClassifier(         estimators=[             (\"decision_tree_{}\".format(i+1), DecisionTreeClassifier()) for i in range(10)         ],         voting=\"soft\"     ), ) <pre>VotingClassifier(estimators=[('decision_tree_1', DecisionTreeClassifier()),\n                             ('decision_tree_2', DecisionTreeClassifier()),\n                             ('decision_tree_3', DecisionTreeClassifier()),\n                             ('decision_tree_4', DecisionTreeClassifier()),\n                             ('decision_tree_5', DecisionTreeClassifier()),\n                             ('decision_tree_6', DecisionTreeClassifier()),\n                             ('decision_tree_7', DecisionTreeClassifier()),\n                             ('decision_tree_8', DecisionTreeClassifier()),\n                             ('decision_tree_9', DecisionTreeClassifier()),\n                             ('decision_tree_10', DecisionTreeClassifier())],\n                 voting='soft')\n\nF1-\u043c\u0435\u0440\u0430 = 0.529\nPR AUC = 0.495\n</pre> <p>\u0414\u043b\u044f \u0443\u0434\u043e\u0431\u0441\u0442\u0432\u0430 \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0445\u0441\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0441\u0430\u0433\u0440\u0435\u0433\u0438\u0440\u0443\u0435\u043c \u0438\u0445.</p> In\u00a0[\u00a0]: Copied! <pre>metrics_df = pd.DataFrame(\n    [all_metrics_logreg, all_metrics_adab, all_metrics_xgb, all_metrics_rf, all_metrics_trees_stacking, all_metrics_trees_voting], \n    index=[\"Logistic Regression\", \"AdaBoost\", \"XGBoost\", \"RandomForest\", \"Stacking\", \"Voting\"]\n)\nmetrics_df.sort_values(by=[\"F1-score\", \"PR AUC\"], ascending=False)\n</pre> metrics_df = pd.DataFrame(     [all_metrics_logreg, all_metrics_adab, all_metrics_xgb, all_metrics_rf, all_metrics_trees_stacking, all_metrics_trees_voting],      index=[\"Logistic Regression\", \"AdaBoost\", \"XGBoost\", \"RandomForest\", \"Stacking\", \"Voting\"] ) metrics_df.sort_values(by=[\"F1-score\", \"PR AUC\"], ascending=False) Out[\u00a0]: F1-score PR AUC RandomForest 0.625000 0.697630 XGBoost 0.617284 0.697381 Logistic Regression 0.596273 0.727906 Voting 0.529412 0.495224 Stacking 0.526946 0.445035 AdaBoost 0.473373 0.417632 <p>\u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u0441\u0443\u0434\u044f \u043f\u043e F1-\u043c\u0435\u0440\u0435 \u0438 PR AUC, \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u043c\u0438 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430\u043c\u0438 \u0432 \u0434\u0430\u043d\u043d\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441 \u0438 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f. \u041e\u0434\u043d\u0430\u043a\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438\u0441\u044c \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043d\u0435\u0432\u044b\u0441\u043e\u043a\u0438\u0435.</p> <p>\u0421\u043d\u0430\u0447\u0430\u043b\u0430 \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438. \u0414\u043b\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u043f\u0440\u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0435.</p> In\u00a0[\u00a0]: Copied! <pre>logreg_coefs = np.abs(model_logreg.coef_).squeeze()\n</pre> logreg_coefs = np.abs(model_logreg.coef_).squeeze() In\u00a0[\u00a0]: Copied! <pre>print(\n    \"\u041d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a: {}\\n\u041d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a: {}\".format(\n        X_train.columns[np.argmax(logreg_coefs)],\n        X_train.columns[np.argmin(logreg_coefs)]\n    )\n)\n</pre> print(     \"\u041d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a: {}\\n\u041d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a: {}\".format(         X_train.columns[np.argmax(logreg_coefs)],         X_train.columns[np.argmin(logreg_coefs)]     ) ) <pre>\u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a: DiabetesPedigreeFunction\n\u041d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a: Insulin\n</pre> <p>\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0432 \u043f\u043e\u0440\u044f\u0434\u043a\u0435 \u0438\u0445 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438.</p> In\u00a0[\u00a0]: Copied! <pre>sorted_idx = np.argsort(logreg_coefs)\nsorted_cols = X_train.columns[sorted_idx].tolist()\nsorted_fi = logreg_coefs[sorted_idx]\n</pre> sorted_idx = np.argsort(logreg_coefs) sorted_cols = X_train.columns[sorted_idx].tolist() sorted_fi = logreg_coefs[sorted_idx] In\u00a0[\u00a0]: Copied! <pre>plt.barh(sorted_cols, sorted_fi);\n</pre> plt.barh(sorted_cols, sorted_fi); <p>\u041f\u0435\u0440\u0435\u0439\u0434\u0435\u043c \u043a \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u044f\u043c:</p> In\u00a0[\u00a0]: Copied! <pre># \u0427\u0442\u043e\u0431\u044b \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u044c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443 shap, \u0440\u0430\u0441\u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0438\u0440\u0443\u0439\u0442\u0435 \u0441\u0442\u0440\u043e\u043a\u0443 \u043d\u0438\u0436\u0435\n\n# !pip install shap\n</pre> # \u0427\u0442\u043e\u0431\u044b \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u044c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443 shap, \u0440\u0430\u0441\u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0438\u0440\u0443\u0439\u0442\u0435 \u0441\u0442\u0440\u043e\u043a\u0443 \u043d\u0438\u0436\u0435  # !pip install shap In\u00a0[\u00a0]: Copied! <pre>import shap\n</pre> import shap <p>\u0414\u043b\u044f \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0438\u0437 \u044f\u0447\u0435\u0439\u043a\u0438 \u043d\u0438\u0436\u0435:</p> In\u00a0[\u00a0]: Copied! <pre>def get_model_name_fi(model, fi_type, X_train=None):\n    \"\"\"\u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432.\n\n    :param model: \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c\n    :param fi_type: \u0442\u0438\u043f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432: 'builded' -- \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u044b\u0439 \u0432 \u043a\u043b\u0430\u0441\u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u043b\u0438 'shap' -- \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e shap-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439\n    :param X_train: \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 (\u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u0430 \u043f\u0440\u0438 fi_type='shap')\n    :return: \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    \"\"\"\n    model_name = str(model)[:str(model).find(\"(\")]\n\n    if fi_type == \"builded\":\n        if \"xgbclassifier\" in model_name.lower() or \"randomforest\" in model_name.lower():\n            fi = model.feature_importances_\n        else:\n            # AdaBoostClassifier, StackingClassifier \u0438 VotingClassifier \u043d\u0435 \u0438\u043c\u0435\u044e\u0442 \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432,\n            # \u043d\u043e \u0442.\u043a. \u0443 \u043d\u0430\u0441 \u0432 \u043e\u0441\u043d\u0432\u043e\u0435 \u043b\u0435\u0436\u0430\u0442 \u0434\u0435\u0440\u0435\u0432\u044c\u044f, \u0442\u043e \u043c\u044b \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u043e \u0432\u0441\u0435\u043c \u0434\u0435\u0440\u0435\u0432\u044c\u044f\u043c\n            fi = np.concatenate(\n                [\n                    dt.feature_importances_.reshape((1, -1)) for dt in model.estimators_\n                ]\n            ).mean(axis=0)\n    elif fi_type == \"shap\":\n        explainer = shap.TreeExplainer(model)\n        fi = explainer.shap_values(X_train)\n    return model_name, fi\n</pre> def get_model_name_fi(model, fi_type, X_train=None):     \"\"\"\u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432.      :param model: \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c     :param fi_type: \u0442\u0438\u043f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432: 'builded' -- \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u044b\u0439 \u0432 \u043a\u043b\u0430\u0441\u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u043b\u0438 'shap' -- \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e shap-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439     :param X_train: \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 (\u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u0430 \u043f\u0440\u0438 fi_type='shap')     :return: \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432     \"\"\"     model_name = str(model)[:str(model).find(\"(\")]      if fi_type == \"builded\":         if \"xgbclassifier\" in model_name.lower() or \"randomforest\" in model_name.lower():             fi = model.feature_importances_         else:             # AdaBoostClassifier, StackingClassifier \u0438 VotingClassifier \u043d\u0435 \u0438\u043c\u0435\u044e\u0442 \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432,             # \u043d\u043e \u0442.\u043a. \u0443 \u043d\u0430\u0441 \u0432 \u043e\u0441\u043d\u0432\u043e\u0435 \u043b\u0435\u0436\u0430\u0442 \u0434\u0435\u0440\u0435\u0432\u044c\u044f, \u0442\u043e \u043c\u044b \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u043e \u0432\u0441\u0435\u043c \u0434\u0435\u0440\u0435\u0432\u044c\u044f\u043c             fi = np.concatenate(                 [                     dt.feature_importances_.reshape((1, -1)) for dt in model.estimators_                 ]             ).mean(axis=0)     elif fi_type == \"shap\":         explainer = shap.TreeExplainer(model)         fi = explainer.shap_values(X_train)     return model_name, fi <p>\u0422\u0430\u043a\u0436\u0435 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0434\u043b\u044f \u0440\u0430\u0437\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c:</p> In\u00a0[\u00a0]: Copied! <pre>def plot_several_models_feature_importances(models, features, fi_type, X_train=None):\n    \"\"\"\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u0442 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432. \n\n    :param models: \u0441\u043f\u0438\u0441\u043e\u043a \u043e\u0431\u0443\u0447\u043d\u0435\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \n    :param features: \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    :param fi_type: \u0442\u0438\u043f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432: 'builded' -- \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u044b\u0439 \u0432 \u043a\u043b\u0430\u0441\u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u043b\u0438 'shap' -- \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e shap-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439\n    :param X_train: \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 (\u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u0430 \u043f\u0440\u0438 fi_type='shap')\n    \"\"\"\n    N = len(models)\n    plt.figure(figsize=(10 * N, 8 * (N // 2 + 1)))\n\n    for i, model in enumerate(models):\n        model_name, fi = get_model_name_fi(model, fi_type, X_train=X_train)\n\n        if fi_type == \"builded\":\n            plt.subplot(2, N // 2 + 1, i + 1)\n            sorted_idx = np.argsort(models[0].feature_importances_)[::-1]\n            plt.barh(features[sorted_idx], fi[sorted_idx])\n            plt.title(model_name);\n        elif fi_type == \"shap\":\n            shap.summary_plot(fi, X_train, plot_type=\"bar\")\n    \n    if fi_type == \"builded\":\n        plt.subplot(2, N // 2 + 1, i + 2)\n        for i, model in enumerate(models):\n            model_name, fi = get_model_name_fi(model, fi_type, X_train=X_train)\n            plt.barh(features[sorted_idx], fi[sorted_idx], alpha=0.5, label=model_name)\n        plt.legend()\n        plt.title(\"All models\");\n\n    plt.show()\n</pre> def plot_several_models_feature_importances(models, features, fi_type, X_train=None):     \"\"\"\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u0442 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432.       :param models: \u0441\u043f\u0438\u0441\u043e\u043a \u043e\u0431\u0443\u0447\u043d\u0435\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439      :param features: \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432     :param fi_type: \u0442\u0438\u043f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432: 'builded' -- \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u044b\u0439 \u0432 \u043a\u043b\u0430\u0441\u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u043b\u0438 'shap' -- \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e shap-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439     :param X_train: \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 (\u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u0430 \u043f\u0440\u0438 fi_type='shap')     \"\"\"     N = len(models)     plt.figure(figsize=(10 * N, 8 * (N // 2 + 1)))      for i, model in enumerate(models):         model_name, fi = get_model_name_fi(model, fi_type, X_train=X_train)          if fi_type == \"builded\":             plt.subplot(2, N // 2 + 1, i + 1)             sorted_idx = np.argsort(models[0].feature_importances_)[::-1]             plt.barh(features[sorted_idx], fi[sorted_idx])             plt.title(model_name);         elif fi_type == \"shap\":             shap.summary_plot(fi, X_train, plot_type=\"bar\")          if fi_type == \"builded\":         plt.subplot(2, N // 2 + 1, i + 2)         for i, model in enumerate(models):             model_name, fi = get_model_name_fi(model, fi_type, X_train=X_train)             plt.barh(features[sorted_idx], fi[sorted_idx], alpha=0.5, label=model_name)         plt.legend()         plt.title(\"All models\");      plt.show() In\u00a0[\u00a0]: Copied! <pre>plot_several_models_feature_importances(\n    models=[\n        model_adab,\n        model_xgb,\n        model_rf,\n        model_trees_stacking,\n        model_trees_voting,\n    ], \n    features=X_train.columns,\n    fi_type=\"builded\",\n)\n</pre> plot_several_models_feature_importances(     models=[         model_adab,         model_xgb,         model_rf,         model_trees_stacking,         model_trees_voting,     ],      features=X_train.columns,     fi_type=\"builded\", ) <p>\u0412\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0434\u043b\u044f \u0440\u0430\u0437\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438\u0441\u044c \u0440\u0430\u0437\u043d\u044b\u0435, \u043d\u043e \u0441\u043b\u0435\u0434\u0443\u0435\u0442 \u0437\u0430\u043c\u0435\u0442\u0438\u0442\u044c, \u0447\u0442\u043e \u0432\u0441\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043b\u0438 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u043c \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0433\u043b\u044e\u043a\u043e\u0437\u044b <code>Glucose</code>.</p> In\u00a0[\u00a0]: Copied! <pre>plot_several_models_feature_importances(\n    models=[\n        model_xgb,\n        model_rf, \n        # \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0435 \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0435\u043c\u044b\u0435 \u043d\u0430\u043c\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0435 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u044e\u0442\u0441\u044f shap.TreeExplainer, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u044d\u0442\u0438 \u0434\u0432\u0435 \u043c\u043e\u0434\u0435\u043b\u0438\n    ], \n    features=X_train.columns,\n    fi_type=\"shap\",\n    X_train=X_train,\n)\n</pre> plot_several_models_feature_importances(     models=[         model_xgb,         model_rf,          # \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0435 \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0435\u043c\u044b\u0435 \u043d\u0430\u043c\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0435 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u044e\u0442\u0441\u044f shap.TreeExplainer, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u044d\u0442\u0438 \u0434\u0432\u0435 \u043c\u043e\u0434\u0435\u043b\u0438     ],      features=X_train.columns,     fi_type=\"shap\",     X_train=X_train, ) <p>\u041a\u0430\u043a \u0432\u0438\u0434\u043d\u043e \u0438\u0437 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432 \u0432\u044b\u0448\u0435, \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u0430\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043e\u0442\u043b\u0438\u0447\u0430\u0435\u0442\u0441\u044f \u043e\u0442 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e <code>shap</code>-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439.</p> <p>\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 <code>XGBoost</code>:</p> <ol> <li>\u0412\u043e\u0437\u044c\u043c\u0435\u043c \u0442\u043e\u043f-5 \u043b\u0443\u0447\u0448\u0438\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043f\u043e \u043c\u043d\u0435\u043d\u0438\u044e <code>xgboost</code> \u0438 \u043e\u0431\u0443\u0447\u0438\u043c \u043d\u0430 \u043d\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u044c</li> <li>\u0412\u043e\u0437\u044c\u043c\u0435\u043c \u0442\u043e\u043f-5 \u0445\u0443\u0434\u0448\u0438\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0438 \u043e\u0431\u0443\u0447\u0438\u043c \u043d\u0430 \u043d\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u044c</li> </ol> <p>\u041e\u0436\u0438\u0434\u0430\u0435\u0442\u0441\u044f, \u0447\u0442\u043e \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0432 \u043f\u0435\u0440\u0432\u043e\u043c \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0435 \u0431\u0443\u0434\u0435\u0442 \u0441\u0438\u043b\u044c\u043d\u043e \u0432\u044b\u0448\u0435, \u0447\u0435\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u043e \u0432\u0442\u043e\u0440\u043e\u043c \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0435. \u0410\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u044b\u0435 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u044b \u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043c \u0441 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c\u044e, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e <code>shap</code>-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0438 <code>forward-backward-\u043e\u0442\u0431\u043e\u0440\u0430</code>.</p> In\u00a0[\u00a0]: Copied! <pre>def train_test_n_most_worst_important(n, sorted_idx, columns, model, X_train, y_train, X_test, y_test):\n    \"\"\"\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 n \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0438 \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445.\n\n    :param n: \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    :param sorted_idx: \u0438\u043d\u0434\u0435\u043a\u0441\u044b \u043e\u0442\u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u043f\u043e \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0430\u043d\u0438\u044e \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0435\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \n    :param columns: \u043a\u043e\u043b\u043e\u043d\u043a\u0438 (\u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438) \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u043c \u043f\u043e\u0440\u044f\u0434\u043a\u0435 \n    :param model: \u043c\u043e\u0434\u0435\u043b\u044c\n    :param X_train: \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 (\u0431\u0435\u0437 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439)\n    :param y_train: \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\n    :param X_test: \u0442\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 (\u0431\u0435\u0437 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439)\n    :param y_test: \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\n    :return: \u043a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0445 \u043a\u043e\u0440\u0442\u0435\u0436\u0435\u0439: \n             1: \n                - n \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n                - \u043c\u043e\u0434\u0435\u043b\u044c, \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0430\u044f \u043d\u0430 n \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445\n                - \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 (\u043c\u0435\u0442\u043a\u0438 \u043a\u043b\u0430\u0441\u0441\u043e\u0432)\n                - \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 (\u0432\u0435\u0440\u043e\u0447\u0442\u043d\u043e\u044f\u0442\u0438 \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043a \u043a\u043b\u0430\u0441\u0441\u0430\u043c)\n                - \u043c\u0435\u0442\u0440\u0438\u043a\u0438: F1-\u043c\u0435\u0440\u0430 \u0438 PR AUC\n            2: \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u044b\u0439 \u043d\u0430\u0431\u043e\u0440, \u043d\u043e \u0434\u043b\u044f n \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    \"\"\"\n    # \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\n    top_n_features = columns[sorted_idx[:-n-1:-1]].tolist()\n    print(\n        \"{} \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\\n{}\".format(\n            n, top_n_features\n        )\n    )\n    model_top_n, pred_top_n, proba_top_n, all_metrics_top_n = model_train_test(\n        X_train[top_n_features], y_train,\n        X_test[top_n_features], y_test,\n        model,\n    )\n    print(\"-\"*100)\n    # \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\n    bottom_n_features = columns[sorted_idx[:n]].tolist()\n    print(\n        \"{} \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\\n{}\".format(\n            n, bottom_n_features\n        )\n    )\n    model_bottom_n, pred_bottom_n, proba_bottom_n, all_metrics_bottom_n = model_train_test(\n        X_train[bottom_n_features], y_train,\n        X_test[bottom_n_features], y_test,\n        model,\n    )\n    return (\n        (\n           top_n_features,\n           model_top_n,\n           pred_top_n,\n           proba_top_n,\n           all_metrics_top_n\n       ), (\n            bottom_n_features,\n            model_bottom_n,\n            pred_bottom_n,\n            proba_bottom_n,\n            all_metrics_bottom_n\n        )\n    )\n</pre> def train_test_n_most_worst_important(n, sorted_idx, columns, model, X_train, y_train, X_test, y_test):     \"\"\"\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 n \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0438 \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445.      :param n: \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432     :param sorted_idx: \u0438\u043d\u0434\u0435\u043a\u0441\u044b \u043e\u0442\u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u043f\u043e \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0430\u043d\u0438\u044e \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0435\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432      :param columns: \u043a\u043e\u043b\u043e\u043d\u043a\u0438 (\u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438) \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u043c \u043f\u043e\u0440\u044f\u0434\u043a\u0435      :param model: \u043c\u043e\u0434\u0435\u043b\u044c     :param X_train: \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 (\u0431\u0435\u0437 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439)     :param y_train: \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438     :param X_test: \u0442\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 (\u0431\u0435\u0437 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439)     :param y_test: \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438     :return: \u043a\u043e\u0440\u0442\u0435\u0436 \u0438\u0437 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0445 \u043a\u043e\u0440\u0442\u0435\u0436\u0435\u0439:               1:                  - n \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432                 - \u043c\u043e\u0434\u0435\u043b\u044c, \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0430\u044f \u043d\u0430 n \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445                 - \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 (\u043c\u0435\u0442\u043a\u0438 \u043a\u043b\u0430\u0441\u0441\u043e\u0432)                 - \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 (\u0432\u0435\u0440\u043e\u0447\u0442\u043d\u043e\u044f\u0442\u0438 \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043a \u043a\u043b\u0430\u0441\u0441\u0430\u043c)                 - \u043c\u0435\u0442\u0440\u0438\u043a\u0438: F1-\u043c\u0435\u0440\u0430 \u0438 PR AUC             2: \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u044b\u0439 \u043d\u0430\u0431\u043e\u0440, \u043d\u043e \u0434\u043b\u044f n \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432     \"\"\"     # \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438     top_n_features = columns[sorted_idx[:-n-1:-1]].tolist()     print(         \"{} \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\\n{}\".format(             n, top_n_features         )     )     model_top_n, pred_top_n, proba_top_n, all_metrics_top_n = model_train_test(         X_train[top_n_features], y_train,         X_test[top_n_features], y_test,         model,     )     print(\"-\"*100)     # \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438     bottom_n_features = columns[sorted_idx[:n]].tolist()     print(         \"{} \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\\n{}\".format(             n, bottom_n_features         )     )     model_bottom_n, pred_bottom_n, proba_bottom_n, all_metrics_bottom_n = model_train_test(         X_train[bottom_n_features], y_train,         X_test[bottom_n_features], y_test,         model,     )     return (         (            top_n_features,            model_top_n,            pred_top_n,            proba_top_n,            all_metrics_top_n        ), (             bottom_n_features,             model_bottom_n,             pred_bottom_n,             proba_bottom_n,             all_metrics_bottom_n         )     ) In\u00a0[\u00a0]: Copied! <pre># \u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0438\u0437 \u0430\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u0431\u0443\u0434\u0443\u0442 \u043c\u0435\u043d\u044f\u0442\u044c\u0441\u044f \u043f\u0440\u0438 \u0432\u044b\u0437\u043e\u0432\u0430\u0445 \n# \u0444\u0443\u043d\u043a\u0446\u0438\u0438 `train_test_n_most_worst_important`\n\nCONFIG = {\n    \"n\": 5,\n    \"columns\": X_train.columns, \n    \"model\": XGBClassifier(), \n    \"X_train\": X_train, \n    \"y_train\": y_train, \n    \"X_test\": X_test, \n    \"y_test\": y_test,\n}\n</pre> # \u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0438\u0437 \u0430\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u0431\u0443\u0434\u0443\u0442 \u043c\u0435\u043d\u044f\u0442\u044c\u0441\u044f \u043f\u0440\u0438 \u0432\u044b\u0437\u043e\u0432\u0430\u0445  # \u0444\u0443\u043d\u043a\u0446\u0438\u0438 `train_test_n_most_worst_important`  CONFIG = {     \"n\": 5,     \"columns\": X_train.columns,      \"model\": XGBClassifier(),      \"X_train\": X_train,      \"y_train\": y_train,      \"X_test\": X_test,      \"y_test\": y_test, } In\u00a0[\u00a0]: Copied! <pre>_, fi_xgb = get_model_name_fi(model_xgb, fi_type=\"builded\")\nsorted_idx_xgb = np.argsort(fi_xgb)\n\nbest_xgb, worst_xgb = train_test_n_most_worst_important(\n    sorted_idx=sorted_idx_xgb, \n    **CONFIG\n)\n</pre> _, fi_xgb = get_model_name_fi(model_xgb, fi_type=\"builded\") sorted_idx_xgb = np.argsort(fi_xgb)  best_xgb, worst_xgb = train_test_n_most_worst_important(     sorted_idx=sorted_idx_xgb,      **CONFIG ) <pre>5 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\n['Glucose', 'BMI', 'Age', 'Pregnancies', 'BloodPressure']\nXGBClassifier()\n\nF1-\u043c\u0435\u0440\u0430 = 0.643\nPR AUC = 0.68\n----------------------------------------------------------------------------------------------------\n5 \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\n['SkinThickness', 'Insulin', 'DiabetesPedigreeFunction', 'BloodPressure', 'Pregnancies']\nXGBClassifier()\n\nF1-\u043c\u0435\u0440\u0430 = 0.368\nPR AUC = 0.473\n</pre> In\u00a0[\u00a0]: Copied! <pre>_, fi_xgb_shap = get_model_name_fi(model_xgb, fi_type=\"shap\", X_train=X_train)\n\n# \u0412 `fi_xgb_shap` \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u0442.\u0435. \n# \u0440\u0430\u0437\u043c\u0435\u0440 `fi_xgb_shap`: &lt;\u043a\u043e\u043b-\u0432\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0435&gt; x &lt;\u043a\u043e\u043b-\u0432\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432&gt;. \n# \u041f\u0440\u0438\u0447\u0435\u043c \u0435\u0441\u043b\u0438 \u0447\u0430\u0441\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u043d\u0430\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \n# \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u043f\u043e \u043c\u043e\u0434\u0443\u043b\u044e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 -- \u044d\u0442\u043e \u0447\u0438\u0441\u043b\u043e \u0438 \u0431\u0443\u0434\u0435\u0442 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c\u044e \u0442\u043e\u0433\u043e \u0438\u043b\u0438 \u0438\u043d\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430. \n\nfi_xgb_shap = np.abs(fi_xgb_shap).mean(axis=0)\n\nsorted_idx_xgb_shap = np.argsort(fi_xgb_shap)\n\nbest_xgb_shap, worst_xgb_shap = train_test_n_most_worst_important(\n    sorted_idx=sorted_idx_xgb_shap, \n    **CONFIG\n)\n</pre> _, fi_xgb_shap = get_model_name_fi(model_xgb, fi_type=\"shap\", X_train=X_train)  # \u0412 `fi_xgb_shap` \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u0442.\u0435.  # \u0440\u0430\u0437\u043c\u0435\u0440 `fi_xgb_shap`: &lt;\u043a\u043e\u043b-\u0432\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0435&gt; x &lt;\u043a\u043e\u043b-\u0432\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432&gt;.  # \u041f\u0440\u0438\u0447\u0435\u043c \u0435\u0441\u043b\u0438 \u0447\u0430\u0441\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u043d\u0430\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430  # \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u043f\u043e \u043c\u043e\u0434\u0443\u043b\u044e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 -- \u044d\u0442\u043e \u0447\u0438\u0441\u043b\u043e \u0438 \u0431\u0443\u0434\u0435\u0442 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c\u044e \u0442\u043e\u0433\u043e \u0438\u043b\u0438 \u0438\u043d\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430.   fi_xgb_shap = np.abs(fi_xgb_shap).mean(axis=0)  sorted_idx_xgb_shap = np.argsort(fi_xgb_shap)  best_xgb_shap, worst_xgb_shap = train_test_n_most_worst_important(     sorted_idx=sorted_idx_xgb_shap,      **CONFIG ) <pre>5 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\n['Glucose', 'BMI', 'DiabetesPedigreeFunction', 'Pregnancies', 'Age']\nXGBClassifier()\n\nF1-\u043c\u0435\u0440\u0430 = 0.605\nPR AUC = 0.692\n----------------------------------------------------------------------------------------------------\n5 \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\n['SkinThickness', 'BloodPressure', 'Insulin', 'Age', 'Pregnancies']\nXGBClassifier()\n\nF1-\u043c\u0435\u0440\u0430 = 0.362\nPR AUC = 0.49\n</pre> In\u00a0[\u00a0]: Copied! <pre>from sklearn.feature_selection import SequentialFeatureSelector\n\nfbs = SequentialFeatureSelector(XGBClassifier(), n_features_to_select=5)\nfbs.fit(X_train, y_train)\ntop_5_features_xgb_fbs = X_train.columns[fbs.support_]\n\nprint(\"5 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\\n\", top_5_features_xgb_fbs)\n\nbest_xgb_fbs = model_train_test(\n    X_train[top_5_features_xgb_fbs], y_train,\n    X_test[top_5_features_xgb_fbs], y_test,\n    XGBClassifier(),\n)\n</pre> from sklearn.feature_selection import SequentialFeatureSelector  fbs = SequentialFeatureSelector(XGBClassifier(), n_features_to_select=5) fbs.fit(X_train, y_train) top_5_features_xgb_fbs = X_train.columns[fbs.support_]  print(\"5 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\\n\", top_5_features_xgb_fbs)  best_xgb_fbs = model_train_test(     X_train[top_5_features_xgb_fbs], y_train,     X_test[top_5_features_xgb_fbs], y_test,     XGBClassifier(), ) <pre>5 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:\n Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'BMI'], dtype='object')\nXGBClassifier()\n\nF1-\u043c\u0435\u0440\u0430 = 0.593\nPR AUC = 0.668\n</pre> In\u00a0[\u00a0]: Copied! <pre># \u041f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 SequentialFeatureSelector \u043d\u0435 \u0432\u044b\u0434\u0430\u0435\u0442 \u0447\u0438\u0441\u043b\u0435\u043d\u043d\u0443\u044e \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c, \u0442\u043e \u0432\u044b\u0431\u0435\u0440\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \n# \u043d\u0435 \u043f\u043e\u043f\u0430\u0432\u0448\u0438\u0435 \u0432 \u0441\u043f\u0438\u0441\u043e\u043a \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043a\u0430\u043a \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0435\n\nbottom_5_features_xgb_fbs = X_train.columns[~fbs.support_] \n# \u0442\u0430\u043a\u0438\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0431\u0443\u0434\u0435\u0442 \u0432\u0441\u0435\u0433\u043e 3, \u043f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 5 \u0443\u0436\u0435 \u043e\u0442\u043e\u0431\u0440\u0430\u043d\u044b \u043a\u0430\u043a \u043b\u0443\u0447\u0448\u0438\u0435, \u0430 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0432\u0441\u0435\u0433\u043e 8 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n\nprint(\"3 \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430:\\n\", bottom_5_features_xgb_fbs)\n\nworst_xgb_fbs = model_train_test(\n    X_train[bottom_5_features_xgb_fbs], y_train,\n    X_test[bottom_5_features_xgb_fbs], y_test,\n    XGBClassifier(),\n)\n</pre> # \u041f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 SequentialFeatureSelector \u043d\u0435 \u0432\u044b\u0434\u0430\u0435\u0442 \u0447\u0438\u0441\u043b\u0435\u043d\u043d\u0443\u044e \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c, \u0442\u043e \u0432\u044b\u0431\u0435\u0440\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438,  # \u043d\u0435 \u043f\u043e\u043f\u0430\u0432\u0448\u0438\u0435 \u0432 \u0441\u043f\u0438\u0441\u043e\u043a \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043a\u0430\u043a \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0435  bottom_5_features_xgb_fbs = X_train.columns[~fbs.support_]  # \u0442\u0430\u043a\u0438\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0431\u0443\u0434\u0435\u0442 \u0432\u0441\u0435\u0433\u043e 3, \u043f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 5 \u0443\u0436\u0435 \u043e\u0442\u043e\u0431\u0440\u0430\u043d\u044b \u043a\u0430\u043a \u043b\u0443\u0447\u0448\u0438\u0435, \u0430 \u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0432\u0441\u0435\u0433\u043e 8 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432  print(\"3 \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430:\\n\", bottom_5_features_xgb_fbs)  worst_xgb_fbs = model_train_test(     X_train[bottom_5_features_xgb_fbs], y_train,     X_test[bottom_5_features_xgb_fbs], y_test,     XGBClassifier(), ) <pre>3 \u043d\u0430\u0438\u043c\u0435\u043d\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430:\n Index(['Insulin', 'DiabetesPedigreeFunction', 'Age'], dtype='object')\nXGBClassifier()\n\nF1-\u043c\u0435\u0440\u0430 = 0.493\nPR AUC = 0.591\n</pre> In\u00a0[\u00a0]: Copied! <pre>metrics_df = pd.DataFrame(\n    [\n        best_xgb[-1], worst_xgb[-1],\n        best_xgb_shap[-1], worst_xgb_shap[-1],\n        best_xgb_fbs[-1], worst_xgb_fbs[-1]\n    ], \n    index=[\n        \"XGBoost top 5 feats\", \"XGBoost bottom 5 feats\", \n        \"SHAP top 5 feats\", \"SHAP bottom 5 feats\", \n        \"FBS top 5 feats\", \"FBS bottom 5 feats\"\n    ]\n)\nmetrics_df \n</pre> metrics_df = pd.DataFrame(     [         best_xgb[-1], worst_xgb[-1],         best_xgb_shap[-1], worst_xgb_shap[-1],         best_xgb_fbs[-1], worst_xgb_fbs[-1]     ],      index=[         \"XGBoost top 5 feats\", \"XGBoost bottom 5 feats\",          \"SHAP top 5 feats\", \"SHAP bottom 5 feats\",          \"FBS top 5 feats\", \"FBS bottom 5 feats\"     ] ) metrics_df  Out[\u00a0]: F1-score PR AUC XGBoost top 5 feats 0.642857 0.679527 XGBoost bottom 5 feats 0.368421 0.473027 SHAP top 5 feats 0.604938 0.692427 SHAP bottom 5 feats 0.362416 0.490260 FBS top 5 feats 0.592593 0.668465 FBS bottom 5 feats 0.493333 0.590893 <p>\u041a\u0430\u043a \u0438 \u043e\u0436\u0438\u0434\u0430\u043b\u043e\u0441\u044c, \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043f\u0440\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0438 5 \u00ab\u0445\u0443\u0434\u0448\u0438\u0445\u00bb \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043d\u0438\u0436\u0435, \u0447\u0435\u043c \u043f\u0440\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0438 5 \u00ab\u043b\u0443\u0447\u0448\u0438\u0445\u00bb \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432. \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u00ab\u043b\u0443\u0447\u0448\u0438\u0435\u00bb \u0438 \u00ab\u0445\u0443\u0434\u0448\u0438\u0435\u00bb \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u043b\u044f \u0440\u0430\u0437\u043d\u044b\u0445 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u0438 \u0441\u0440\u0430\u0432\u043d\u0438\u0442\u044c, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u0435\u0440\u0435\u0441\u0435\u043a\u0430\u044e\u0442\u0441\u044f \u044d\u0442\u0438 \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u0430.</p> In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(16, 8))\nplt.title(\"Top-5 features\")\nplt.hist(\n    [best_xgb[0], best_xgb_shap[0], top_5_features_xgb_fbs], \n    label=[\"Builded FI\", \"SHAP\", \"Forward-backward selection\"]\n)\nplt.xticks(rotation=45);\nplt.legend();\n</pre> plt.figure(figsize=(16, 8)) plt.title(\"Top-5 features\") plt.hist(     [best_xgb[0], best_xgb_shap[0], top_5_features_xgb_fbs],      label=[\"Builded FI\", \"SHAP\", \"Forward-backward selection\"] ) plt.xticks(rotation=45); plt.legend(); In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(16, 8))\nplt.title(\"Bottom-10 features\")\nplt.hist(\n    [worst_xgb[0], worst_xgb_shap[0], bottom_5_features_xgb_fbs],\n    label=[\"Builded FI\", \"SHAP\", \"Forward-backward selection\"]\n)\nplt.xticks(rotation=45);\nplt.legend();\n</pre> plt.figure(figsize=(16, 8)) plt.title(\"Bottom-10 features\") plt.hist(     [worst_xgb[0], worst_xgb_shap[0], bottom_5_features_xgb_fbs],     label=[\"Builded FI\", \"SHAP\", \"Forward-backward selection\"] ) plt.xticks(rotation=45); plt.legend(); <p>\u041a\u0430\u043a \u0432\u0438\u0434\u043d\u043e \u0438\u0437 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432 \u0432\u044b\u0448\u0435, \u043f\u0435\u0440\u0435\u0441\u0435\u0447\u0435\u043d\u0438\u044f \u0435\u0441\u0442\u044c, \u0438 \u0438\u0445 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043c\u043d\u043e\u0433\u043e.</p> In\u00a0[\u00a0]: Copied! <pre>metrics_df.sort_values(by=[\"F1-score\", \"PR AUC\"], ascending=False)\n</pre> metrics_df.sort_values(by=[\"F1-score\", \"PR AUC\"], ascending=False) Out[\u00a0]: F1-score PR AUC XGBoost top 5 feats 0.642857 0.679527 SHAP top 5 feats 0.604938 0.692427 FBS top 5 feats 0.592593 0.668465 FBS bottom 5 feats 0.493333 0.590893 XGBoost bottom 5 feats 0.368421 0.473027 SHAP bottom 5 feats 0.362416 0.490260 <p>\u0418\u0437 \u0442\u0430\u0431\u043b\u0438\u0446\u044b \u0432\u044b\u0448\u0435 \u0441\u043b\u0435\u0434\u0443\u0435\u0442, \u0447\u0442\u043e \u0432 \u043d\u0430\u0448\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u0435 \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u0430\u044e\u0442 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u044b\u0441\u043e\u043a\u0438\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438, \u043f\u043e\u0434\u043e\u0431\u0440\u0430\u043b\u0438 XGBoost  \u0441\u043e \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u043e\u0439 \u043e\u0446\u0435\u043d\u043a\u043e\u0439 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (\u0441\u0443\u0434\u044f \u043f\u043e F1-\u043c\u0435\u0440\u0435) \u0438 SHAP (\u0441\u0443\u0434\u044f \u043f\u043e PR AUC), \u043e\u0442\u0431\u043e\u0440 forward-backward \u043f\u043e\u043a\u0430\u0437\u0430\u043b \u0447\u0443\u0442\u044c \u043c\u0435\u043d\u0435\u0435 \u0445\u043e\u0440\u043e\u0448\u0438\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442. \u041e\u0434\u043d\u0430\u043a\u043e \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432 \u0432\u044b\u0431\u043e\u0440\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043f\u0440\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 top-5 \u0438\u043b\u0438 bottom-5  \u043f\u043e\u043a\u0430\u0437\u0430\u043b\u0438 \u043d\u0435 \u0441\u0438\u043b\u044c\u043d\u043e \u043e\u0442\u043b\u0438\u0447\u0430\u044e\u0449\u0438\u0435\u0441\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b.</p> <p>\u041f\u043e\u043c\u0438\u043c\u043e \u043f\u0440\u043e\u0447\u0435\u0433\u043e, \u0432 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0435 <code>shap</code> \u0435\u0441\u0442\u044c \u043c\u043d\u043e\u0433\u043e \u0445\u043e\u0440\u043e\u0448\u0438\u0445 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u043c\u043e\u0433\u0430\u044e\u0442 \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0432\u044b\u0432\u043e\u0434 \u043e \u043f\u0440\u0438\u0447\u0438\u043d\u0430\u0445 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0438 \u0442\u0435\u043c \u0441\u0430\u043c\u044b\u043c \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0438\u0440\u0443\u0435\u043c\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0435\u0439. \u041f\u0440\u0438\u0432\u0435\u0434\u0435\u043c \u043f\u0440\u0438\u043c\u0435\u0440\u044b \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0439:</p> In\u00a0[\u00a0]: Copied! <pre>explainer = shap.TreeExplainer(model_xgb)\nshap_values = explainer.shap_values(X_train)\nshap.summary_plot(shap_values, X_train, plot_type=\"bar\") # \u0440\u0430\u043d\u0435\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u044b\n</pre> explainer = shap.TreeExplainer(model_xgb) shap_values = explainer.shap_values(X_train) shap.summary_plot(shap_values, X_train, plot_type=\"bar\") # \u0440\u0430\u043d\u0435\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u044b <p>\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c, \u043a\u0430\u043a \u043a\u0430\u043a\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u0432\u043b\u0438\u044f\u044e\u0442 \u043d\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 (\u0433\u0440\u0430\u0444\u0438\u043a \u0432 \u044f\u0447\u0435\u0439\u043a\u0435 \u043d\u0438\u0436\u0435). \u041a\u0430\u0436\u0434\u0430\u044f \u0442\u043e\u0447\u043a\u0430 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0441\u043e\u0431\u043e\u0439 \u043e\u0434\u043d\u043e SHAP-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0438 \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430. \u041a\u0440\u0430\u0441\u043d\u044b\u0439 \u0446\u0432\u0435\u0442 \u0442\u043e\u0447\u043a\u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0432\u044b\u0441\u043e\u043a\u043e\u0439 \u0446\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043e\u0431\u044a\u0435\u043a\u0442\u0430, \u0441\u0438\u043d\u0438\u0439 \u2014 \u043d\u0438\u0437\u043a\u043e\u0439. \u041d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043a\u0440\u0430\u0441\u043d\u044b\u0445 \u0438 \u0441\u0438\u043d\u0438\u0445 \u0442\u043e\u0447\u0435\u043a \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043e \u0432\u043b\u0438\u044f\u043d\u0438\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430.</p> <p>\u0412 \u043d\u0430\u0448\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u044f\u0445 \u0447\u0435\u043c \u043d\u0438\u0436\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0438 \u0433\u043b\u044e\u043a\u043e\u0437\u044b, \u0442\u0435\u043c \u043d\u0438\u0436\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u0434\u0438\u0430\u0431\u0435\u0442\u0430, \u0438 \u0447\u0435\u043c \u0432\u044b\u0448\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044c \u0433\u043b\u044e\u043a\u043e\u0437\u044b, \u0442\u0435\u043c \u0432\u044b\u0448\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c, \u0447\u0442\u043e \u0443 \u043f\u0430\u0446\u0438\u0435\u043d\u0442\u0430 \u0434\u0438\u0430\u0431\u0435\u0442.</p> In\u00a0[\u00a0]: Copied! <pre>shap.summary_plot(shap_values, X_train)\n</pre> shap.summary_plot(shap_values, X_train) <p>\u0418\u0437 \u0433\u0440\u0430\u0444\u0438\u043a\u0430 \u0432\u044b\u0448\u0435 \u0442\u0430\u043a\u0436\u0435 \u0441\u043b\u0435\u0434\u0443\u0435\u0442, \u0447\u0442\u043e <code>Glucose</code> \u2014 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a.</p> <p>\u0422\u0430\u043a\u0436\u0435 \u043c\u043e\u0436\u043d\u043e \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c, \u043a\u0430\u043a \u0432\u044b\u0431\u0440\u0430\u043d\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0432\u043b\u0438\u044f\u044e\u0442 \u043d\u0430 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0428\u0435\u043f\u043b\u0438. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c, \u043a\u0430\u043a \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0433\u043b\u044e\u043a\u043e\u0437\u044b <code>Glucose</code> \u0432\u043b\u0438\u044f\u0435\u0442 \u043d\u0430 \u043d\u0430\u043b\u0438\u0447\u0438\u0435 \u0434\u0438\u0430\u0431\u0435\u0442\u0430. \u0414\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u0437\u0430\u0434\u0430\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a, \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0435 \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043c\u044b \u0445\u043e\u0442\u0438\u043c \u043f\u0440\u043e\u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u0443\u0436\u0435 \u0432\u044b\u0431\u0440\u0430\u043d\u043d\u043e\u0433\u043e \u043d\u0430\u043c\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 <code>Glucose</code>, \u043d\u0443\u0436\u043d\u043e \u043f\u0435\u0440\u0435\u0434\u0430\u0442\u044c \u0434\u0440\u0443\u0433\u043e\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430 <code>interaction_index</code> (\u0435\u0441\u043b\u0438 \u044d\u0442\u043e\u0442 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u043d\u0435 \u0431\u0443\u0434\u0435\u0442 \u0437\u0430\u0434\u0430\u043d, \u0442\u043e \u0434\u0440\u0443\u0433\u043e\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a \u0431\u0443\u0434\u0435\u0442 \u0432\u044b\u0431\u0440\u0430\u043d \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438).</p> <p>\u0413\u0440\u0430\u0444\u0438\u043a \u0438\u0437 \u044f\u0447\u0435\u0439\u043a\u0438 \u043d\u0438\u0436\u0435 \u0441\u0432\u0438\u0434\u0435\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u0443\u0435\u0442 \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0433\u043b\u044e\u043a\u043e\u0437\u044b \u0438\u043c\u0435\u0435\u0442 \u043c\u0435\u043d\u044c\u0448\u0435\u0435 \u0432\u043b\u0438\u044f\u043d\u0438\u0435 \u043d\u0430 \u0440\u0435\u0448\u0435\u043d\u0438\u0435 \u043e \u0434\u0438\u0430\u0431\u0435\u0442\u0435 \u043f\u0440\u0438 \u043c\u043e\u043b\u043e\u0434\u043e\u043c \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0435 (&lt; 30 \u043b\u0435\u0442).</p> In\u00a0[\u00a0]: Copied! <pre>shap.dependence_plot(\"Glucose\", shap_values, X_train, interaction_index=\"Age\")\n</pre> shap.dependence_plot(\"Glucose\", shap_values, X_train, interaction_index=\"Age\") In\u00a0[\u00a0]: Copied! <pre># \u0413\u0440\u0430\u0444\u0438\u043a\u0438 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c:\n\nfor feature in X_train.columns:\n    shap.dependence_plot(feature, shap_values, X_train, display_features=X_train) \n</pre> # \u0413\u0440\u0430\u0444\u0438\u043a\u0438 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c:  for feature in X_train.columns:     shap.dependence_plot(feature, shap_values, X_train, display_features=X_train)  <p>\u0414\u0430\u043d\u043d\u044b\u0439 \u0442\u0438\u043f \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043d\u0430 \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u044b\u0435 \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f: \u043a\u0430\u043a \u0440\u0430\u0437\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0432\u043b\u0438\u044f\u044e\u0442 \u043d\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438.</p> In\u00a0[\u00a0]: Copied! <pre># \u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f \u0432\u043e \u0432\u0441\u0435\u043c \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\n\nshap.initjs() # \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438\u043d\u0442\u0435\u0440\u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0433\u043e \u0433\u0440\u0430\u0444\u0438\u043a\u0430\nshap.force_plot(explainer.expected_value, shap_values, features=X_train) \n</pre> # \u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f \u0432\u043e \u0432\u0441\u0435\u043c \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445  shap.initjs() # \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438\u043d\u0442\u0435\u0440\u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0433\u043e \u0433\u0440\u0430\u0444\u0438\u043a\u0430 shap.force_plot(explainer.expected_value, shap_values, features=X_train)  Out[\u00a0]: Visualization omitted, Javascript library not loaded!   Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.  In\u00a0[\u00a0]: Copied! <pre># \u0414\u043b\u044f \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u0433\u043e \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f \u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445 \n# (\u0432 \u044d\u0442\u043e\u0439 \u044f\u0447\u0435\u0439\u043a\u0435 \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u043f\u0435\u0440\u0432\u043e\u0435 \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u0435)\n\nshap.initjs() # \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438\u043d\u0442\u0435\u0440\u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0433\u043e \u0433\u0440\u0430\u0444\u0438\u043a\u0430\nshap.force_plot(explainer.expected_value, shap_values[0, :], X_train.iloc[0])\n</pre> # \u0414\u043b\u044f \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u0433\u043e \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f \u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445  # (\u0432 \u044d\u0442\u043e\u0439 \u044f\u0447\u0435\u0439\u043a\u0435 \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u043f\u0435\u0440\u0432\u043e\u0435 \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u0435)  shap.initjs() # \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438\u043d\u0442\u0435\u0440\u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0433\u043e \u0433\u0440\u0430\u0444\u0438\u043a\u0430 shap.force_plot(explainer.expected_value, shap_values[0, :], X_train.iloc[0]) Out[\u00a0]: Visualization omitted, Javascript library not loaded!   Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.  <p>\u041d\u0430 \u0433\u0440\u0430\u0444\u0438\u043a\u0435 \u0432\u044b\u0448\u0435 <code>base_value</code> \u043f\u043e\u043c\u0435\u0447\u0435\u043d\u043e \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0435 \u043f\u0440\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438, \u0430 \u0436\u0438\u0440\u043d\u044b\u043c \u0432\u044b\u0434\u0435\u043b\u0435\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435.</p> In\u00a0[\u00a0]: Copied! <pre>shap.initjs() # \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438\u043d\u0442\u0435\u0440\u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0433\u043e \u0433\u0440\u0430\u0444\u0438\u043a\u0430\nshap.force_plot(explainer.expected_value, shap_values[1, :], X_train.iloc[1])\n</pre> shap.initjs() # \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438\u043d\u0442\u0435\u0440\u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0433\u043e \u0433\u0440\u0430\u0444\u0438\u043a\u0430 shap.force_plot(explainer.expected_value, shap_values[1, :], X_train.iloc[1]) Out[\u00a0]: Visualization omitted, Javascript library not loaded!   Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.  <p>\u041a\u043e\u043d\u0435\u0447\u043d\u043e, \u043d\u0438 \u043e\u0434\u0438\u043d \u0438\u0437 \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0435\u043c\u044b\u0445 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u043e\u0446\u0435\u043d\u043a\u0438 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043d\u0435 \u0438\u0434\u0435\u0430\u043b\u0435\u043d, \u043e\u0434\u043d\u0430\u043a\u043e, \u043a\u0430\u043a \u043f\u043e\u043a\u0430\u0437\u0430\u043b \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442, \u0432\u0441\u0435 \u044d\u0442\u0438 \u043c\u0435\u0442\u043e\u0434\u044b \u043f\u043e\u043c\u043e\u0433\u0430\u044e\u0442 \u0432\u044b\u0431\u0440\u0430\u0442\u044c \u0442\u0430\u043a\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u043f\u0440\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u044c \u0431\u0443\u0434\u0435\u0442 \u0445\u043e\u0440\u043e\u0448\u043e \u043e\u0431\u0443\u0447\u0430\u0442\u044c\u0441\u044f.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"portfolio/sfml/feature_importance.html","title":"\u0410\u043d\u0441\u0430\u043c\u0431\u043b\u0438 \u0440\u0435\u0448\u0430\u044e\u0449\u0438\u0445 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432: \u043e\u0446\u0435\u043d\u043a\u0430 \u0437\u043d\u0430\u0447\u0438\u043c\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\u00b6","text":""},{"location":"portfolio/sfml/feature_importance.html","title":"\u0422\u0435\u043e\u0440\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0447\u0430\u0441\u0442\u044c\u00b6","text":""},{"location":"portfolio/sfml/feature_importance.html","title":"\u041e\u0446\u0435\u043d\u043a\u0430 \u0437\u043d\u0430\u0447\u0438\u043c\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u0435\u0439 \u0440\u0435\u0448\u0430\u044e\u0449\u0438\u0445 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432\u00b6","text":"<p>\u041d\u0430 \u043f\u0440\u0430\u043a\u0442\u0438\u043a\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c\u0438 \u0441\u043f\u043e\u0441\u043e\u0431\u0430\u043c\u0438:</p> <ul> <li>\u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u044b\u0445 \u0444\u0443\u043d\u043a\u0446\u0438\u0439 \u0432 \u0442\u0443 \u0438\u043b\u0438 \u0438\u043d\u0443\u044e \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443. \u0412 \u0434\u0430\u043d\u043d\u043e\u043c \u0441\u0435\u043c\u0438\u043d\u0430\u0440\u0435 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0442\u044c \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u0438 \u0440\u0435\u0448\u0430\u044e\u0449\u0438\u0445 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a <code>xgboost</code> \u0438 <code>sklearn</code>, \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0443\u0436\u0435 \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u0430 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043f\u043e\u0441\u043b\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438</li> <li>\u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e SHAP-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439. \u041f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043e\u0437\u043d\u0430\u0447\u0430\u044e\u0442 \u00ab\u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435\u00bb \u0432\u043b\u0438\u044f\u043d\u0438\u0435 \u043d\u0430 \u043f\u0440\u043e\u0433\u043d\u043e\u0437, \u0442. \u0435. \u0447\u0442\u043e \u0434\u0430\u043d\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u0442 \u043c\u043e\u0434\u0435\u043b\u044c \u043a \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044e \u043a\u043b\u0430\u0441\u0441\u0430 $1$; \u043e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u2014 \u00ab\u043e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0435\u00bb \u0432\u043b\u0438\u044f\u043d\u0438\u0435 \u043d\u0430 \u043f\u0440\u043e\u0433\u043d\u043e\u0437 \u2014 \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u0442 \u043c\u043e\u0434\u0435\u043b\u044c \u043a \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044e \u043a\u043b\u0430\u0441\u0441\u0430 $0$. \u041f\u0440\u043e SHAP-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0431\u0443\u0434\u0435\u0442 \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u0430\u043d\u043e \u0434\u0430\u043b\u0435\u0435 \u0432 \u043a\u0443\u0440\u0441\u0435. \u0414\u043b\u044f \u0438\u0445 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443 <code>shap</code></li> <li>Forward, backward selection, forward-backward \u043e\u0442\u0431\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432:<ul> <li>\u043f\u0440\u044f\u043c\u043e\u0439 \u043e\u0442\u0431\u043e\u0440 (Forward selection): \u043d\u0430\u0447\u0438\u043d\u0430\u0435\u043c \u0441 \u043f\u0443\u0441\u0442\u043e\u0433\u043e \u043d\u0430\u0431\u043e\u0440\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u0430 \u0437\u0430\u0442\u0435\u043c \u0438\u0442\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u043e \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u044e\u0449\u0438\u0435 \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u0439 \u043f\u0440\u0438\u0440\u043e\u0441\u0442 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043c\u043e\u0434\u0435\u043b\u0435\u0439</li> <li>\u043e\u0431\u0440\u0430\u0442\u043d\u044b\u0439 \u043e\u0442\u0431\u043e\u0440 (Backward selection): \u043d\u0430\u0447\u0438\u043d\u0430\u0435\u043c \u0441 \u043d\u0430\u0431\u043e\u0440\u0430, \u0441\u043e\u0441\u0442\u043e\u044f\u0449\u0435\u0433\u043e \u0438\u0437 \u0432\u0441\u0435\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u0434\u0430\u043b\u0435\u0435 \u043d\u0430 \u043a\u0430\u0436\u0434\u043e\u0439 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438 \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u00ab\u0445\u0443\u0434\u0448\u0438\u0439\u00bb \u043f\u0440\u0438\u0437\u043d\u0430\u043a</li> <li>Forward-backward selection: \u043e\u0441\u043d\u043e\u0432\u043d\u0430\u044f \u0438\u0434\u0435\u044f \u0437\u0430\u043a\u043b\u044e\u0447\u0430\u0435\u0442\u0441\u044f \u0432 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0438 \u0438\u043b\u0438 \u0443\u0434\u0430\u043b\u0435\u043d\u0438\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0438 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0438 \u0442\u043e\u0433\u043e, \u043a\u0430\u043a \u044d\u0442\u043e \u0443\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u0432\u043b\u0438\u044f\u0435\u0442 \u043d\u0430 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c \u0438\u043b\u0438 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0434\u0440\u0443\u0433\u0438\u0445 \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0435\u0432. \u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u043c\u0435\u0442\u043e\u0434\u0430 \u0432 <code>sklearn</code>. \u041f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u043e\u043d \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043b\u0438\u0448\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0432 \u043f\u043e\u0440\u044f\u0434\u043a\u0435 \u0438\u0445 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438, \u0442\u043e \u0431\u0443\u0434\u0435\u043c \u0441\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0442\u044c \u043e\u0442\u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0432 \u043f\u043e\u0440\u044f\u0434\u043a\u0435 \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0430\u043d\u0438\u044f/\u0443\u0431\u044b\u0432\u0430\u043d\u0438\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u0441\u043f\u0438\u0441\u043a\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0441 \u0434\u0430\u043d\u043d\u044b\u043c \u043c\u0435\u0442\u043e\u0434\u043e\u043c (\u0430 \u043f\u0435\u0440\u0432\u044b\u0435 \u0434\u0432\u0430 \u043c\u0435\u0442\u043e\u0434\u0430 \u043e\u0446\u0435\u043d\u043a\u0438 \u0437\u043d\u0430\u0447\u0438\u043c\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0431\u0443\u0434\u0435\u043c \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c)</li> </ul> </li> </ul>"},{"location":"portfolio/sfml/feature_importance.html","title":"\u041e\u0446\u0435\u043d\u043a\u0430 \u0437\u043d\u0430\u0447\u0438\u043c\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0432 \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u044f\u0445\u00b6","text":""},{"location":"portfolio/sfml/feature_importance.html","title":"\u041f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0447\u0430\u0441\u0442\u044c\u00b6","text":"<p>\u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043e\u0442\u0441\u044e\u0434\u0430. \u0414\u0430\u043d\u043d\u044b\u0435 \u0431\u044b\u043b\u0438 \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u041d\u0430\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u043c \u0438\u043d\u0441\u0442\u0438\u0442\u0443\u0442\u043e\u043c \u0434\u0438\u0430\u0431\u0435\u0442\u0430, \u0431\u043e\u043b\u0435\u0437\u043d\u0435\u0439 \u043e\u0440\u0433\u0430\u043d\u043e\u0432 \u043f\u0438\u0449\u0435\u0432\u0430\u0440\u0435\u043d\u0438\u044f \u0438 \u043f\u043e\u0447\u0435\u043a. \u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c, \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0449\u0443\u044e, \u0435\u0441\u0442\u044c \u043b\u0438 \u0443 \u043f\u0430\u0446\u0438\u0435\u043d\u0442\u0430 \u0434\u0438\u0430\u0431\u0435\u0442, \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u044b\u0445 \u0434\u0438\u0430\u0433\u043d\u043e\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0438\u0437\u043c\u0435\u0440\u0435\u043d\u0438\u0439.</p>"},{"location":"portfolio/sfml/feature_importance.html","title":"\u0421\u043a\u0430\u0447\u0438\u0432\u0430\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":""},{"location":"portfolio/sfml/feature_importance.html","title":"\u0420\u0430\u0437\u0432\u0435\u0434\u043e\u0447\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u0438 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\u00b6","text":""},{"location":"portfolio/sfml/feature_importance.html","title":"\u0414\u0435\u043b\u0435\u043d\u0438\u0435 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0443\u044e \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e\u00b6","text":""},{"location":"portfolio/sfml/feature_importance.html","title":"\u0412\u044b\u0431\u043e\u0440 \u043c\u0435\u0442\u0440\u0438\u043a \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430\u00b6","text":""},{"location":"portfolio/sfml/feature_importance.html","title":"\u0412\u044b\u0431\u043e\u0440, \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0435\u0439\u00b6","text":"<p>\u0411\u0443\u0434\u0435\u043c \u0441\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0442\u044c \u0432\u0441\u0435 \u0442\u0438\u043f\u044b \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u0435\u0439 \u043c\u043e\u0434\u0435\u043b\u0435\u0439, \u0432 \u043e\u0441\u043d\u043e\u0432\u0435 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043b\u0435\u0436\u0438\u0442 \u0440\u0435\u0448\u0430\u044e\u0449\u0435\u0435 \u0434\u0435\u0440\u0435\u0432\u043e.</p>"},{"location":"portfolio/sfml/feature_importance.html","title":"\u0412\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\u00b6","text":"<p>\u0418\u0437\u043d\u0430\u0447\u0430\u043b\u044c\u043d\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0428\u0435\u043f\u043b\u0438 (SHAP-\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f, SHAP-values) \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u043b\u0438\u0441\u044c \u0432 \u0442\u0435\u043e\u0440\u0438\u0438 \u0438\u0433\u0440 \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u043d\u0438\u044f \u0432\u043a\u043b\u0430\u0434\u0430 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0437 \u0443\u0447\u0430\u0441\u0442\u043d\u0438\u043a\u043e\u0432 \u043a\u043e\u043e\u043f\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u043e\u0439 \u0438\u0433\u0440\u044b (\u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0438\u0433\u0440\u043e\u043a\u0438 \u0434\u0435\u0439\u0441\u0442\u0432\u0443\u044e\u0442 \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u043d\u043e).</p> <p>\u0411\u043e\u043b\u0435\u0435 \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u043e\u0435 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u043e \u0432 \u043b\u043e\u043d\u0433\u0440\u0438\u0434\u0435. \u0421 \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0442\u043e\u0447\u043a\u0438 \u0437\u0440\u0435\u043d\u0438\u044f \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0428\u0435\u043f\u043b\u0438 (\u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 <code>shap</code>) \u043c\u043e\u0433\u0443\u0442 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0442\u044c\u0441\u044f \u0434\u043b\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0442\u0430\u0431\u043b\u0438\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 (\u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0442, \u043a\u0430\u043a \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u0432\u043b\u0438\u044f\u044e\u0442 \u043d\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f), \u043f\u0440\u0438 \u0440\u0430\u0431\u043e\u0442\u0435 \u0441 \u0442\u0435\u043a\u0441\u0442\u0430\u043c\u0438 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043c\u043e\u0433\u0443\u0442 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0442\u044c\u0441\u044f \u0434\u043b\u044f \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u044f \u0432\u043b\u0438\u044f\u043d\u0438\u044f \u0441\u043b\u043e\u0432 \u043f\u0440\u0438 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432 \u043f\u043e \u044d\u043c\u043e\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u043c\u0443 \u043e\u043a\u0440\u0430\u0441\u0443) \u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c\u0438 (\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0428\u0435\u043f\u043b\u0438 \u0431\u0443\u0434\u0443\u0442 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c, \u043a\u0430\u043a \u043f\u043e\u0432\u043b\u0438\u044f\u043b \u043d\u0430 \u043f\u0440\u0438\u043d\u044f\u0442\u0438\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u044c\u044e \u0442\u043e\u0442 \u0438\u043b\u0438 \u0438\u043d\u043e\u0439 \u043f\u0438\u043a\u0441\u0435\u043b\u044c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f).</p>"},{"location":"portfolio/sfml/feature_importance.html#shap","title":"\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u0438\u0434\u044b \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432 \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 <code>shap</code>\u00b6","text":""},{"location":"portfolio/sfml/feature_importance.html#shapsummary_plot","title":"<code>shap.summary_plot</code>\u00b6","text":""},{"location":"portfolio/sfml/feature_importance.html#shapdependence_plot","title":"<code>shap.dependence_plot</code>\u00b6","text":""},{"location":"portfolio/sfml/feature_importance.html#shapforce_plot","title":"<code>shap.force_plot</code>\u00b6","text":""},{"location":"portfolio/sfml/final_movie_rating.html","title":"Final movie rating","text":"In\u00a0[1]: Copied! <pre>import json\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n</pre> import json import pandas as pd import numpy as np from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder from sklearn.linear_model import LinearRegression from sklearn.ensemble import RandomForestRegressor In\u00a0[36]: Copied! <pre>train = pd.read_csv('/kaggle/input/sf-review-stars-prediction/train.csv',index_col=0).drop(columns='Unnamed: 0.1')\ntest = pd.read_csv('/kaggle/input/sf-review-stars-prediction/test.csv',index_col=0).drop(columns='Unnamed: 0.1')\n</pre> train = pd.read_csv('/kaggle/input/sf-review-stars-prediction/train.csv',index_col=0).drop(columns='Unnamed: 0.1') test = pd.read_csv('/kaggle/input/sf-review-stars-prediction/test.csv',index_col=0).drop(columns='Unnamed: 0.1') In\u00a0[37]: Copied! <pre>train.head()\n</pre> train.head() Out[37]: ID asin helpful overall reviewText reviewTime reviewerID reviewerName summary unixReviewTime features 0 0 B000F83SZQ [0, 0] 5.0 I enjoy vintage books and movies so I enjoyed ... 05 5, 2014 A1F6404F1VG29J Avidreader Nice vintage story 1399248000 [-1.2551e-01, 2.7282e-02, -3.9638e-02, -8.9909... 1 1 B000F83SZQ [2, 2] 4.0 This book is a reissue of an old one; the auth... 01 6, 2014 AN0N05A9LIJEQ critters Different... 1388966400 [ 3.6394e-01, -5.9243e-02, 1.0343e-01, -3.1970... 2 2 B000F83SZQ [2, 2] 4.0 This was a fairly interesting read.  It had ol... 04 4, 2014 A795DMNCJILA6 dot Oldie 1396569600 [ 2.8251e-01, -3.5420e-02, -4.2864e-01, 3.1264... 3 3 B000F83SZQ [1, 1] 5.0 I'd never read any of the Amy Brewster mysteri... 02 19, 2014 A1FV0SX13TWVXQ Elaine H. Turley \"Montana Songbird\" I really liked it. 1392768000 [-7.1293e-01, -2.7817e-01, 5.6167e-01, -6.9253... 4 4 B000F83SZQ [0, 1] 4.0 If you like period pieces - clothing, lingo, y... 03 19, 2014 A3SPTOKDG7WBLN Father Dowling Fan Period Mystery 1395187200 [ 3.9603e-01, 5.1992e-01, -8.5361e-01, 7.7056e... <p>\u0420\u0430\u0437\u043d\u0438\u0446\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043c\u0435\u0436\u0434\u0443 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u043e\u0447\u0435\u0432\u0438\u0434\u043d\u043e \u0447\u0442\u043e <code>overall</code> \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0446\u0435\u043b\u0435\u0432\u044b\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u043c</p> In\u00a0[38]: Copied! <pre>set(list(train.columns))-set(list(test.columns))\n</pre> set(list(train.columns))-set(list(test.columns)) Out[38]: <pre>{'overall'}</pre> <p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u0446\u0435\u043b\u0435\u0432\u043e\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0435</p> <ul> <li>\u041e\u0446\u0435\u043d\u043a\u0430 \u043e\u0431\u0437\u043e\u0440\u0430 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043e\u0440\u0434\u0438\u043d\u0430\u043b\u043d\u044b\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u043c (1-5)</li> <li>\u0414\u0430\u043d\u043d\u044b\u0445 \u043e\u0431\u0437\u043e\u0440\u043e\u0432 \u0441 \u043d\u0438\u0437\u043a\u043e\u0439 \u043e\u0446\u0435\u043d\u043a\u043e\u0439 (1-2) \u043d\u0435 \u0442\u0430\u043a \u043c\u043d\u043e\u0433\u043e \u043f\u043e \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044e \u0441 \u043e\u0446\u0435\u043d\u043a\u0430\u043c\u0438 (3-5)</li> </ul> In\u00a0[39]: Copied! <pre>train['overall'].value_counts()\n</pre> train['overall'].value_counts() Out[39]: <pre>5.0    503\n4.0    261\n3.0    140\n2.0     53\n1.0     43\nName: overall, dtype: int64</pre> In\u00a0[40]: Copied! <pre>reviewers_train = list(train['reviewerID'].unique())\nreviewers_test = list(test['reviewerID'].unique())\nreviewers_all = reviewers_train + reviewers_test\nprint(len(reviewers_train),len(reviewers_test),len(reviewers_all),len(set(reviewers_all)))\n</pre> reviewers_train = list(train['reviewerID'].unique()) reviewers_test = list(test['reviewerID'].unique()) reviewers_all = reviewers_train + reviewers_test print(len(reviewers_train),len(reviewers_test),len(reviewers_all),len(set(reviewers_all))) <pre>827 949 1776 1758\n</pre> In\u00a0[41]: Copied! <pre>train['asin'].value_counts()\n# items_all +=list(test['asin'].nunique())\n</pre> train['asin'].value_counts() # items_all +=list(test['asin'].nunique()) Out[41]: <pre>B000JMLBHU    108\nB000R93D4Y     50\nB000WSFBO0     41\nB000NJL7Y6     24\nB000JMLHYC     23\n             ... \nB000JML5JY      5\nB000UH5Z3A      5\nB000UH5Z0I      5\nB000SIZG8A      5\nB000TU16PE      5\nName: asin, Length: 93, dtype: int64</pre> In\u00a0[42]: Copied! <pre>test['asin'].value_counts()\n</pre> test['asin'].value_counts() Out[42]: <pre>B00CBJXM1M    55\nB00CBFZVU6    49\nB00CB7NHC8    43\nB00CBL9IOK    41\nB00CB3SH0O    35\n              ..\nB00CB2CG1Q     5\nB00CB26IBK     5\nB00CB26IA6     5\nB00CB26G7G     5\nB00CB1OPI4     3\nName: asin, Length: 73, dtype: int64</pre> In\u00a0[43]: Copied! <pre>train.drop(columns=['reviewTime'])\ntest.drop(columns=['reviewTime'])\ntrain.head()\n</pre> train.drop(columns=['reviewTime']) test.drop(columns=['reviewTime']) train.head() Out[43]: ID asin helpful overall reviewText reviewTime reviewerID reviewerName summary unixReviewTime features 0 0 B000F83SZQ [0, 0] 5.0 I enjoy vintage books and movies so I enjoyed ... 05 5, 2014 A1F6404F1VG29J Avidreader Nice vintage story 1399248000 [-1.2551e-01, 2.7282e-02, -3.9638e-02, -8.9909... 1 1 B000F83SZQ [2, 2] 4.0 This book is a reissue of an old one; the auth... 01 6, 2014 AN0N05A9LIJEQ critters Different... 1388966400 [ 3.6394e-01, -5.9243e-02, 1.0343e-01, -3.1970... 2 2 B000F83SZQ [2, 2] 4.0 This was a fairly interesting read.  It had ol... 04 4, 2014 A795DMNCJILA6 dot Oldie 1396569600 [ 2.8251e-01, -3.5420e-02, -4.2864e-01, 3.1264... 3 3 B000F83SZQ [1, 1] 5.0 I'd never read any of the Amy Brewster mysteri... 02 19, 2014 A1FV0SX13TWVXQ Elaine H. Turley \"Montana Songbird\" I really liked it. 1392768000 [-7.1293e-01, -2.7817e-01, 5.6167e-01, -6.9253... 4 4 B000F83SZQ [0, 1] 4.0 If you like period pieces - clothing, lingo, y... 03 19, 2014 A3SPTOKDG7WBLN Father Dowling Fan Period Mystery 1395187200 [ 3.9603e-01, 5.1992e-01, -8.5361e-01, 7.7056e... In\u00a0[44]: Copied! <pre>train['day'] = pd.to_datetime(train['unixReviewTime'],unit='s').dt.day\ntrain['month'] = pd.to_datetime(train['unixReviewTime'],unit='s').dt.month\ntrain['dow'] = pd.to_datetime(train['unixReviewTime'],unit='s').dt.dayofweek\n\ntest['day'] = pd.to_datetime(test['unixReviewTime'],unit='s').dt.day\ntest['month'] = pd.to_datetime(test['unixReviewTime'],unit='s').dt.month\ntest['dow'] = pd.to_datetime(test['unixReviewTime'],unit='s').dt.dayofweek\n\ntrain = train.drop(columns=['unixReviewTime'],axis=1)\ntest = test.drop(columns=['unixReviewTime'],axis=1)\n</pre> train['day'] = pd.to_datetime(train['unixReviewTime'],unit='s').dt.day train['month'] = pd.to_datetime(train['unixReviewTime'],unit='s').dt.month train['dow'] = pd.to_datetime(train['unixReviewTime'],unit='s').dt.dayofweek  test['day'] = pd.to_datetime(test['unixReviewTime'],unit='s').dt.day test['month'] = pd.to_datetime(test['unixReviewTime'],unit='s').dt.month test['dow'] = pd.to_datetime(test['unixReviewTime'],unit='s').dt.dayofweek  train = train.drop(columns=['unixReviewTime'],axis=1) test = test.drop(columns=['unixReviewTime'],axis=1) In\u00a0[45]: Copied! <pre>train['helpful']\n</pre> train['helpful'] Out[45]: <pre>0      [0, 0]\n1      [2, 2]\n2      [2, 2]\n3      [1, 1]\n4      [0, 1]\n        ...  \n995    [0, 0]\n996    [0, 0]\n997    [0, 0]\n998    [0, 0]\n999    [0, 0]\nName: helpful, Length: 1000, dtype: object</pre> In\u00a0[46]: Copied! <pre>train['helpful'] = train['helpful'].apply(lambda row: eval(row))\ntrain['helpful'] = train['helpful'].apply(lambda row: row[0] - (row[1] - row[0]))\n</pre> train['helpful'] = train['helpful'].apply(lambda row: eval(row)) train['helpful'] = train['helpful'].apply(lambda row: row[0] - (row[1] - row[0])) In\u00a0[47]: Copied! <pre>print(train['features'][0][:100])\nprint(type(train['features'][0][:100])) \ntype(eval(train['features'][0]))  # \u0441\u043f\u0438\u0441\u043e\u043a \u0434\u0430\u043d\u043d\u044b\u0445 \n</pre> print(train['features'][0][:100]) print(type(train['features'][0][:100]))  type(eval(train['features'][0]))  # \u0441\u043f\u0438\u0441\u043e\u043a \u0434\u0430\u043d\u043d\u044b\u0445  <pre>[-1.2551e-01, 2.7282e-02, -3.9638e-02, -8.9909e-02, 6.7109e-01, -3.5082e-01, -4.4905e-01, 4.4430e-01\n&lt;class 'str'&gt;\n</pre> Out[47]: <pre>list</pre> <p>\u042d\u0442\u043e \u043f\u0440\u0435\u0432\u0440\u0430\u0449\u0430\u0435\u0442 <code>str</code> \u0432 \u0441\u043f\u0438\u0441\u043e\u043a <code>list</code> , \u0434\u0430\u043b\u0435\u0435 \u043f\u0440\u0435\u0432\u0440\u0430\u0449\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 <code>np.array</code></p> In\u00a0[48]: Copied! <pre>train['features'] = train['features'].apply(lambda row:np.array(eval(row)))\ntest['features'] = test['features'].apply(lambda row:np.array(eval(row)))\n</pre> train['features'] = train['features'].apply(lambda row:np.array(eval(row))) test['features'] = test['features'].apply(lambda row:np.array(eval(row))) In\u00a0[49]: Copied! <pre>train['reviewText']\n</pre> train['reviewText'] Out[49]: <pre>0      I enjoy vintage books and movies so I enjoyed ...\n1      This book is a reissue of an old one; the auth...\n2      This was a fairly interesting read.  It had ol...\n3      I'd never read any of the Amy Brewster mysteri...\n4      If you like period pieces - clothing, lingo, y...\n                             ...                        \n995    I enjoyed reading this story.  The characters ...\n996    This was a fun book to read for the 2nd time. ...\n997    Or just a Christmas romance to make you crack-...\n998    I read \"Stop the wedding\" by this author and w...\n999    This was one of the funniest books I have ever...\nName: reviewText, Length: 1000, dtype: object</pre> <ul> <li><code>TfidfVectorizer</code> \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0441 \u0441\u0442\u043e\u043f \u0441\u043b\u043e\u0432\u043c\u0438 <code>stop_words</code></li> <li>\u0422\u0430\u043a \u0436\u0435 \u043c\u044b \u0437\u0430\u043c\u0435\u043d\u0435\u043c \u0432\u0441\u044e \u043f\u0443\u043d\u043a\u0442\u0443\u0430\u0446\u0438\u044e \u043d\u0430 \u043f\u0443\u0441\u0442\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\n\nstop_words = stopwords.words('english')\n\n# stubstitute stop words with ''\nvectoriser = TfidfVectorizer(stop_words=[re.sub(r'[a-zA-Z_0-9\\s]',r'',sw) for sw in stop_words])\n\nX_train = vectoriser.fit_transform(train['reviewText'].apply(lambda row: re.sub(r'[^a-zA-Z_0-9\\s]',r'',row)))\nX_test = vectoriser.transform(test['reviewText'].apply(lambda row: re.sub(r'[^a-zA-Z_0-9\\s]',r'',row)))\n</pre> import re from sklearn.feature_extraction.text import TfidfVectorizer from nltk.corpus import stopwords  stop_words = stopwords.words('english')  # stubstitute stop words with '' vectoriser = TfidfVectorizer(stop_words=[re.sub(r'[a-zA-Z_0-9\\s]',r'',sw) for sw in stop_words])  X_train = vectoriser.fit_transform(train['reviewText'].apply(lambda row: re.sub(r'[^a-zA-Z_0-9\\s]',r'',row))) X_test = vectoriser.transform(test['reviewText'].apply(lambda row: re.sub(r'[^a-zA-Z_0-9\\s]',r'',row))) <p>\u041d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435 \u0443 \u043d\u0430\u0441 sparse \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u0441 \u0441\u043b\u043e\u0432\u0430\u0440\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c 9545 \u0441\u043b\u043e\u0432</p> In\u00a0[51]: Copied! <pre>X_train\n</pre> X_train Out[51]: <pre>&lt;1000x9545 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n\twith 59480 stored elements in Compressed Sparse Row format&gt;</pre> In\u00a0[75]: Copied! <pre>y = train['overall']\ny\n</pre> y = train['overall'] y Out[75]: <pre>0      5.0\n1      4.0\n2      4.0\n3      5.0\n4      4.0\n      ... \n995    5.0\n996    4.0\n997    5.0\n998    1.0\n999    5.0\nName: overall, Length: 1000, dtype: float64</pre> In\u00a0[53]: Copied! <pre>from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\nfrom sklearn.ensemble  import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.metrics import mean_absolute_error as MAE\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nrs = 42\n\nlst_models = []\nclf = RandomForestClassifier(n_estimators=10,max_depth=100,random_state=34567)\nclf2 = ExtraTreesClassifier(random_state=rs)\nclf3 = AdaBoostClassifier(random_state=rs)\n\nber = BernoulliNB(alpha=1)\ncom = ComplementNB(alpha=0.1)\nmn = MultinomialNB(alpha=0.01)\nrc = RidgeClassifier(alpha=0.1,tol=1e-4,solver='auto',random_state=rs)\n\nlst_models.append(('rfc',clf))\nlst_models.append(('etc',clf2))\nlst_models.append(('ada',clf3))\nlst_models.append(('ber',ber))\nlst_models.append(('com',com))\nlst_models.append(('mn',mn))\nlst_models.append(('rc',rc))\n</pre> from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB from sklearn.ensemble  import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier from sklearn.linear_model import RidgeClassifier from sklearn.metrics import mean_absolute_error as MAE from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV from sklearn.metrics import accuracy_score, confusion_matrix  rs = 42  lst_models = [] clf = RandomForestClassifier(n_estimators=10,max_depth=100,random_state=34567) clf2 = ExtraTreesClassifier(random_state=rs) clf3 = AdaBoostClassifier(random_state=rs)  ber = BernoulliNB(alpha=1) com = ComplementNB(alpha=0.1) mn = MultinomialNB(alpha=0.01) rc = RidgeClassifier(alpha=0.1,tol=1e-4,solver='auto',random_state=rs)  lst_models.append(('rfc',clf)) lst_models.append(('etc',clf2)) lst_models.append(('ada',clf3)) lst_models.append(('ber',ber)) lst_models.append(('com',com)) lst_models.append(('mn',mn)) lst_models.append(('rc',rc)) In\u00a0[54]: Copied! <pre>import warnings; warnings.filterwarnings('ignore')\nimport seaborn as sns; \nsns.set_style(\"whitegrid\", {\n    \"ytick.major.size\": 0.1,\n    \"ytick.minor.size\": 0.05,\n    'grid.linestyle': '--'\n })\nimport matplotlib.pyplot as plt\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom copy import deepcopy\nimport spacy\nimport en_core_web_lg\n\ndef hex_to_rgb(h):\n    h = h.lstrip('#')\n    return tuple(int(h[i:i+2], 16)/255 for i in (0, 2, 4))\n\npalette = ['#b4d2b1', '#568f8b', '#1d4a60', '#cd7e59', '#ddb247', '#d15252']\npalette_rgb = [hex_to_rgb(x) for x in palette]\n\n\nclass nlp_evals:\n    \n    def __init__(self,df,corpus,label,\n                 spacy_model='en_core_web_lg',\n                 title='accuracy evaluation'\n                ):\n        \n        self.df = deepcopy(df)\n        self.corpus = corpus\n        self.label = label\n        \n        # spacy embedding not utilised\n        self.spacy_model = spacy_model\n        \n        # generate TF-IDF features\n        self.features = self.get_tfidf()\n        self.seed = 32\n        self.num_folds = 6\n        self.title = title\n    \n    @staticmethod\n    def criterion(y,ypred):\n        return MAE(y,ypred)\n    \n    # TF-IDF vectors\n    def get_tfidf(self):\n        \n        stop_words = stopwords.words('english')\n        vectoriser = TfidfVectorizer(stop_words=[re.sub(r'[a-zA-Z_0-9\\s]',r'',sw) for sw in stop_words])\n        X_train = vectoriser.fit_transform(self.df[self.corpus].apply(lambda row: re.sub(r'[^a-zA-Z_0-9\\s]',r'',row)))\n        print(X_train.shape) \n        return X_train\n        \n    # spacy embeddings\n    def get_embeddings(self):\n        \n        # NLP pipline\n        nlp = spacy.load(self.spacy_model)\n        if(self.spacy_model is 'en_core_web_sm'):\n            embedding_dims = 96\n        elif(self.spacy_model is 'en_core_web_lg'):\n            embedding_dims = 300\n        \n        # average embedding vector for each document\n        all_vectors = np.array([np.array([token.vector for token in nlp(s) ]).mean(axis=0)*np.ones((embedding_dims)) \\\n                                   for s in self.df[self.corpus]])\n        print(all_vectors.shape)\n        print('embeddings loaded!')\n        return all_vectors\n        \n    def tts(self,ratio=0.2):\n        \n        # split out validation dataset for the end\n        Y = self.df[self.label]\n        X = self.features\n\n        X_train, X_test, y_train, y_test = train_test_split(X, Y, \n                                                            test_size=ratio, \n                                                            random_state=42)\n        \n        self.X_train = X_train\n        self.X_test = X_test\n        self.y_train = y_train \n        self.y_test = y_test\n        print('train/test split!')\n        \n    def define_models(self,models):\n        self.models = models\n        print('models set!')\n        \n    def kfold(self):\n        \n        self.results = []\n        self.names = []\n        self.test_results = []\n        self.train_results = []\n        self.cv_results = []\n        \n        lX_train = deepcopy(self.X_train)\n        lX_test = deepcopy(self.X_test)\n        ly_train = deepcopy(self.y_train.to_frame())\n        ly_test = deepcopy(self.y_test.to_frame())\n    \n        self.lst_names = []\n        for name, model in self.models:\n            \n            self.lst_names.append(name)\n            \n            # cross validation on training dataset\n            kfold = KFold(n_splits=self.num_folds, shuffle=True,random_state=self.seed)\n            cv_results = cross_val_score(model, \n                                         self.X_train, self.y_train,\n                                         cv=kfold,\n                                         scoring='neg_mean_absolute_error')\n            self.results.append(cv_results)\n            self.names.append(name)\n            self.cv_results.append(abs(cv_results.mean()))\n\n           # Full Training period\n            res = model.fit(self.X_train, self.y_train)\n            ytrain_res = res.predict(self.X_train)\n            criterion_train = self.criterion(ytrain_res,self.y_train)\n            self.train_results.append(criterion_train)\n\n            # Test results\n            ytest_res = res.predict(self.X_test)\n            criterion_test = self.criterion(ytest_res, self.y_test)\n            self.test_results.append(criterion_test)    \n\n            msg = \"%s: %f (%f) %f %f\" % (name, \n                                         cv_results.mean(), \n                                         cv_results.std(), \n                                         criterion_train, \n                                         criterion_test)\n            \n            ly_train[f'{name}_train'] = ytrain_res\n            ly_test[f'{name}_test'] = ytest_res\n            \n            # print(msg)\n            # print(confusion_matrix(ytest_res, self.y_test))\n            \n        \n        self.ly_train = ly_train\n        self.ly_test = ly_test\n        \n        self.plot_results()\n            \n        print('evaluation finished!')\n        \n    def plot_results(self):\n\n        ldf_res = pd.DataFrame({'cv':self.cv_results,\n                                'train':self.train_results,\n                                'test':self.test_results})\n        \n        plot_df = ldf_res.melt()\n        local_names = deepcopy(self.names)\n        local_names = local_names * 3\n      \n        plot_df['names'] = local_names\n        \n        ptable = pd.pivot_table(plot_df,\n                                values='value',\n                                index='variable',\n                                columns='names')\n\n        fig,ax = plt.subplots(1,2,figsize=(12,3))\n        sns.heatmap(ptable,annot=True,\n                    fmt=\".2f\",\n                    cmap='crest',\n                    linewidth=3,\n                    ax=ax[0])\n        \n        sns.boxplot(data=self.results,orient=\"v\",\n                   width=0.2,ax=ax[1],palette=palette_rgb)\n            \n        ax[1].set_xticklabels(self.lst_names)\n        sns.stripplot(data=self.results,ax=ax[1], orient='v',color=\".3\",linewidth=1)\n        ax[1].set_xticklabels(self.lst_names) \n        \n        sns.despine(left=True, bottom=True,ax=ax[1])\n        plt.title(self.title)\n</pre> import warnings; warnings.filterwarnings('ignore') import seaborn as sns;  sns.set_style(\"whitegrid\", {     \"ytick.major.size\": 0.1,     \"ytick.minor.size\": 0.05,     'grid.linestyle': '--'  }) import matplotlib.pyplot as plt import re from sklearn.feature_extraction.text import TfidfVectorizer from nltk.corpus import stopwords from copy import deepcopy import spacy import en_core_web_lg  def hex_to_rgb(h):     h = h.lstrip('#')     return tuple(int(h[i:i+2], 16)/255 for i in (0, 2, 4))  palette = ['#b4d2b1', '#568f8b', '#1d4a60', '#cd7e59', '#ddb247', '#d15252'] palette_rgb = [hex_to_rgb(x) for x in palette]   class nlp_evals:          def __init__(self,df,corpus,label,                  spacy_model='en_core_web_lg',                  title='accuracy evaluation'                 ):                  self.df = deepcopy(df)         self.corpus = corpus         self.label = label                  # spacy embedding not utilised         self.spacy_model = spacy_model                  # generate TF-IDF features         self.features = self.get_tfidf()         self.seed = 32         self.num_folds = 6         self.title = title          @staticmethod     def criterion(y,ypred):         return MAE(y,ypred)          # TF-IDF vectors     def get_tfidf(self):                  stop_words = stopwords.words('english')         vectoriser = TfidfVectorizer(stop_words=[re.sub(r'[a-zA-Z_0-9\\s]',r'',sw) for sw in stop_words])         X_train = vectoriser.fit_transform(self.df[self.corpus].apply(lambda row: re.sub(r'[^a-zA-Z_0-9\\s]',r'',row)))         print(X_train.shape)          return X_train              # spacy embeddings     def get_embeddings(self):                  # NLP pipline         nlp = spacy.load(self.spacy_model)         if(self.spacy_model is 'en_core_web_sm'):             embedding_dims = 96         elif(self.spacy_model is 'en_core_web_lg'):             embedding_dims = 300                  # average embedding vector for each document         all_vectors = np.array([np.array([token.vector for token in nlp(s) ]).mean(axis=0)*np.ones((embedding_dims)) \\                                    for s in self.df[self.corpus]])         print(all_vectors.shape)         print('embeddings loaded!')         return all_vectors              def tts(self,ratio=0.2):                  # split out validation dataset for the end         Y = self.df[self.label]         X = self.features          X_train, X_test, y_train, y_test = train_test_split(X, Y,                                                              test_size=ratio,                                                              random_state=42)                  self.X_train = X_train         self.X_test = X_test         self.y_train = y_train          self.y_test = y_test         print('train/test split!')              def define_models(self,models):         self.models = models         print('models set!')              def kfold(self):                  self.results = []         self.names = []         self.test_results = []         self.train_results = []         self.cv_results = []                  lX_train = deepcopy(self.X_train)         lX_test = deepcopy(self.X_test)         ly_train = deepcopy(self.y_train.to_frame())         ly_test = deepcopy(self.y_test.to_frame())              self.lst_names = []         for name, model in self.models:                          self.lst_names.append(name)                          # cross validation on training dataset             kfold = KFold(n_splits=self.num_folds, shuffle=True,random_state=self.seed)             cv_results = cross_val_score(model,                                           self.X_train, self.y_train,                                          cv=kfold,                                          scoring='neg_mean_absolute_error')             self.results.append(cv_results)             self.names.append(name)             self.cv_results.append(abs(cv_results.mean()))             # Full Training period             res = model.fit(self.X_train, self.y_train)             ytrain_res = res.predict(self.X_train)             criterion_train = self.criterion(ytrain_res,self.y_train)             self.train_results.append(criterion_train)              # Test results             ytest_res = res.predict(self.X_test)             criterion_test = self.criterion(ytest_res, self.y_test)             self.test_results.append(criterion_test)                  msg = \"%s: %f (%f) %f %f\" % (name,                                           cv_results.mean(),                                           cv_results.std(),                                           criterion_train,                                           criterion_test)                          ly_train[f'{name}_train'] = ytrain_res             ly_test[f'{name}_test'] = ytest_res                          # print(msg)             # print(confusion_matrix(ytest_res, self.y_test))                               self.ly_train = ly_train         self.ly_test = ly_test                  self.plot_results()                      print('evaluation finished!')              def plot_results(self):          ldf_res = pd.DataFrame({'cv':self.cv_results,                                 'train':self.train_results,                                 'test':self.test_results})                  plot_df = ldf_res.melt()         local_names = deepcopy(self.names)         local_names = local_names * 3                plot_df['names'] = local_names                  ptable = pd.pivot_table(plot_df,                                 values='value',                                 index='variable',                                 columns='names')          fig,ax = plt.subplots(1,2,figsize=(12,3))         sns.heatmap(ptable,annot=True,                     fmt=\".2f\",                     cmap='crest',                     linewidth=3,                     ax=ax[0])                  sns.boxplot(data=self.results,orient=\"v\",                    width=0.2,ax=ax[1],palette=palette_rgb)                      ax[1].set_xticklabels(self.lst_names)         sns.stripplot(data=self.results,ax=ax[1], orient='v',color=\".3\",linewidth=1)         ax[1].set_xticklabels(self.lst_names)                   sns.despine(left=True, bottom=True,ax=ax[1])         plt.title(self.title) <p>\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043d\u0430\u0448\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 <code>train</code> \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0435</p> In\u00a0[55]: Copied! <pre># instantiate new evalation class\n\nevals_nlp = nlp_evals(df=train,            # define the corpus dataframe\n                      corpus='reviewText', # define which column contains the corpus\n                      label='overall',  # define the column which contains the label\n                      title='self defined labels') \n\nevals_nlp.tts()\nevals_nlp.define_models(lst_models)\nevals_nlp.kfold()\n</pre> # instantiate new evalation class  evals_nlp = nlp_evals(df=train,            # define the corpus dataframe                       corpus='reviewText', # define which column contains the corpus                       label='overall',  # define the column which contains the label                       title='self defined labels')   evals_nlp.tts() evals_nlp.define_models(lst_models) evals_nlp.kfold() <pre>(1000, 9545)\ntrain/test split!\nmodels set!\nevaluation finished!\n</pre> In\u00a0[56]: Copied! <pre>tr_X = X_train[:-200]; tr_y = y[:-200]\nte_X = X_train[-200:]; te_y = y[-200:]\n\n# RF\nclf.fit(tr_X,tr_y)\ny_pred = clf.predict(te_X)\nMAE(te_y,y_pred)\n</pre> tr_X = X_train[:-200]; tr_y = y[:-200] te_X = X_train[-200:]; te_y = y[-200:]  # RF clf.fit(tr_X,tr_y) y_pred = clf.predict(te_X) MAE(te_y,y_pred) Out[56]: <pre>0.72</pre> In\u00a0[57]: Copied! <pre># ET\nclf2.fit(tr_X,tr_y)\ny_pred2 = clf2.predict(te_X)\nMAE(te_y,y_pred2)\n</pre> # ET clf2.fit(tr_X,tr_y) y_pred2 = clf2.predict(te_X) MAE(te_y,y_pred2) Out[57]: <pre>0.72</pre> In\u00a0[58]: Copied! <pre># AdaBoost\nclf3.fit(tr_X,tr_y)\ny_pred3 = clf3.predict(te_X)\nMAE(te_y,y_pred3)\n</pre> # AdaBoost clf3.fit(tr_X,tr_y) y_pred3 = clf3.predict(te_X) MAE(te_y,y_pred3) Out[58]: <pre>0.83</pre> In\u00a0[59]: Copied! <pre># Bernoulli Naive Bayes\nber.fit(tr_X,tr_y)\ny_pred_ber = ber.predict(te_X)\nMAE(te_y,y_pred_ber)\n</pre> # Bernoulli Naive Bayes ber.fit(tr_X,tr_y) y_pred_ber = ber.predict(te_X) MAE(te_y,y_pred_ber) Out[59]: <pre>0.735</pre> In\u00a0[60]: Copied! <pre># Complement Naive Bayes\ncom.fit(tr_X,tr_y)\ny_pred_com = com.predict(te_X)\nMAE(te_y,y_pred_com)\n</pre> # Complement Naive Bayes com.fit(tr_X,tr_y) y_pred_com = com.predict(te_X) MAE(te_y,y_pred_com) Out[60]: <pre>0.83</pre> In\u00a0[61]: Copied! <pre># Multinomial Naive Bayes\nmn.fit(tr_X,tr_y)\ny_pred_mn = mn.predict(te_X)\nMAE(te_y,y_pred_mn)\n</pre> # Multinomial Naive Bayes mn.fit(tr_X,tr_y) y_pred_mn = mn.predict(te_X) MAE(te_y,y_pred_mn) Out[61]: <pre>0.685</pre> In\u00a0[62]: Copied! <pre># Ridge Regression\nrc.fit(tr_X,tr_y)\ny_pred_rc = rc.predict(te_X)\nMAE(te_y,y_pred_rc)\n</pre> # Ridge Regression rc.fit(tr_X,tr_y) y_pred_rc = rc.predict(te_X) MAE(te_y,y_pred_rc) Out[62]: <pre>0.565</pre> <p>\u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043f\u0440\u043e\u0441\u0442\u044b\u0435 \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u0438 \u043e\u0442\u0432\u0435\u0442\u043e\u0432</p> In\u00a0[63]: Copied! <pre>y_pred_1 = mn.predict(te_X)\ny_pred_2 = rc.predict(te_X)\ny_pred_blend = (y_pred_1 + y_pred_2)/2\nMAE(te_y,y_pred_blend)\n</pre> y_pred_1 = mn.predict(te_X) y_pred_2 = rc.predict(te_X) y_pred_blend = (y_pred_1 + y_pred_2)/2 MAE(te_y,y_pred_blend) Out[63]: <pre>0.59</pre> In\u00a0[64]: Copied! <pre>y_test_rc = rc.predict(X_test)\ny_test_mn = mn.predict(X_test)\ny_pred = (y_test_rc + y_test_mn)/2\n</pre> y_test_rc = rc.predict(X_test) y_test_mn = mn.predict(X_test) y_pred = (y_test_rc + y_test_mn)/2 In\u00a0[65]: Copied! <pre>submission = pd.DataFrame({\n     'ID': test.ID,\n    'overall': y_pred\n })\nsubmission.to_csv('submission_1.csv', index=None)\n</pre> submission = pd.DataFrame({      'ID': test.ID,     'overall': y_pred  }) submission.to_csv('submission_1.csv', index=None) In\u00a0[66]: Copied! <pre># len(json.loads(train_df['features'].iloc[0])) # dimensionality of encoder layers and pooler layer (hidden size)\n# train_features = np.stack(train.features.apply(json.loads))\n# test_features = np.stack(test.features.apply(json.loads))\n# train_features.shape\n\ntrain_features_bert = np.stack(train['features'])      # training data\ntest_features_bert = np.stack(test['features'])        # test data\ntrain_features_bert.shape, test_features_bert.shape\n\ntr_X_bert = train_features_bert[:-200]; tr_y_bert = y[:-200]\nte_X_bert = train_features_bert[-200:]; te_y_bert = y[-200:]\n</pre> # len(json.loads(train_df['features'].iloc[0])) # dimensionality of encoder layers and pooler layer (hidden size) # train_features = np.stack(train.features.apply(json.loads)) # test_features = np.stack(test.features.apply(json.loads)) # train_features.shape  train_features_bert = np.stack(train['features'])      # training data test_features_bert = np.stack(test['features'])        # test data train_features_bert.shape, test_features_bert.shape  tr_X_bert = train_features_bert[:-200]; tr_y_bert = y[:-200] te_X_bert = train_features_bert[-200:]; te_y_bert = y[-200:] In\u00a0[67]: Copied! <pre>clf = RandomForestClassifier(n_estimators=10,max_depth=100,random_state=34567)\nclf2 = ExtraTreesClassifier(random_state=rs)\nclf3 = AdaBoostClassifier(random_state=rs)\n\nber = BernoulliNB(alpha=1)\ncom = ComplementNB(alpha=0.1)\nmn = MultinomialNB(alpha=0.01)\nrc = RidgeClassifier(alpha=0.1,tol=1e-4,solver='auto',random_state=rs)\n</pre> clf = RandomForestClassifier(n_estimators=10,max_depth=100,random_state=34567) clf2 = ExtraTreesClassifier(random_state=rs) clf3 = AdaBoostClassifier(random_state=rs)  ber = BernoulliNB(alpha=1) com = ComplementNB(alpha=0.1) mn = MultinomialNB(alpha=0.01) rc = RidgeClassifier(alpha=0.1,tol=1e-4,solver='auto',random_state=rs) In\u00a0[68]: Copied! <pre># RF\nclf.fit(tr_X_bert,tr_y_bert)\ny_pred = clf.predict(te_X_bert)\nMAE(te_y_bert,y_pred)\n</pre> # RF clf.fit(tr_X_bert,tr_y_bert) y_pred = clf.predict(te_X_bert) MAE(te_y_bert,y_pred) Out[68]: <pre>0.925</pre> In\u00a0[69]: Copied! <pre># Ridge\nrc.fit(tr_X_bert,tr_y_bert)\ny_pred = rc.predict(te_X_bert)\nMAE(te_y_bert,y_pred)\n</pre> # Ridge rc.fit(tr_X_bert,tr_y_bert) y_pred = rc.predict(te_X_bert) MAE(te_y_bert,y_pred) Out[69]: <pre>1.07</pre> In\u00a0[70]: Copied! <pre># horizontally stack data (pandas axis=1)\nX_all = np.hstack((X_train.toarray(),train_features_bert))\nX_all.shape\n</pre> # horizontally stack data (pandas axis=1) X_all = np.hstack((X_train.toarray(),train_features_bert)) X_all.shape Out[70]: <pre>(1000, 10313)</pre> In\u00a0[71]: Copied! <pre>tr_X_bert = X_all[:-200]; tr_y_bert = y[:-200]\nte_X_bert = X_all[-200:]; te_y_bert = y[-200:]\n</pre> tr_X_bert = X_all[:-200]; tr_y_bert = y[:-200] te_X_bert = X_all[-200:]; te_y_bert = y[-200:] In\u00a0[72]: Copied! <pre># RF\nclf.fit(tr_X_bert,tr_y_bert)\ny_pred = clf.predict(te_X_bert)\nMAE(te_y_bert,y_pred)\n</pre> # RF clf.fit(tr_X_bert,tr_y_bert) y_pred = clf.predict(te_X_bert) MAE(te_y_bert,y_pred) Out[72]: <pre>0.74</pre> In\u00a0[73]: Copied! <pre># Ridge\nrc.fit(tr_X_bert,tr_y_bert)\ny_pred = rc.predict(te_X_bert)\nMAE(te_y_bert,y_pred)\n</pre> # Ridge rc.fit(tr_X_bert,tr_y_bert) y_pred = rc.predict(te_X_bert) MAE(te_y_bert,y_pred) Out[73]: <pre>0.93</pre> In\u00a0[74]: Copied! <pre>submission = pd.DataFrame({\n   'ID': test.ID,\n    'overall': y_pred_blend\n})\nsubmission.to_csv('submission.csv', index=None)\n</pre> submission = pd.DataFrame({    'ID': test.ID,     'overall': y_pred_blend }) submission.to_csv('submission.csv', index=None)"},{"location":"portfolio/sfml/final_movie_rating.html#final-hackathon","title":"Final Hackathon\u00b6","text":""},{"location":"portfolio/sfml/final_movie_rating.html#1","title":"1 |  \u0412\u0412\u0415\u0414\u0415\u041d\u0418\u0415\u00b6","text":""},{"location":"portfolio/sfml/final_movie_rating.html","title":"\u0424\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0445\u0430\u043a\u0430\u0442\u043e\u043d\u00b6","text":"<ul> <li>\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0444\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0445\u0430\u043a\u0430\u0442\u043e\u043d\u0430 \u043c\u044b \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u0438\u043b\u0438 \u0434\u043b\u044f \u0432\u0430\u0441 \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044e \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0430 \u043e\u0442\u0437\u044b\u0432\u0430 \u0432 \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d\u0435.</li> <li>\u0421\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u0435 \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043e \u043d\u0430 \u043f\u043b\u043e\u0449\u0430\u0434\u043a\u0435 kaggle.</li> <li>\u0414\u043b\u044f \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043e\u0442\u0437\u044b\u0432\u043e\u0432 \u0441 Amazon Kindle Store</li> </ul>"},{"location":"portfolio/sfml/final_movie_rating.html","title":"\u041f\u043e\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0438 \u0417\u0430\u0434\u0430\u0447\u0438\u00b6","text":"<ul> <li>\u0412 \u044d\u0442\u043e\u043c \u043d\u043e\u0443\u0442\u0435 \u043c\u044b \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043f\u0440\u0438\u043c\u0435\u0440 NLP \u0437\u0430\u0434\u0430\u0447\u044e \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438.</li> <li>\u041d\u0430\u043c \u043f\u0440\u0435\u0434\u0441\u0442\u043e\u0438\u0442 \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043c\u043e\u0436\u0435\u0442 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0442\u043e\u0432\u0430\u0440\u0430, \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0438\u043c\u0435\u044e\u0449\u0438\u0445\u0441\u044f \u0434\u0430\u043d\u043d\u044b\u0445</li> </ul>"},{"location":"portfolio/sfml/final_movie_rating.html#2","title":"2 |  \u0414\u0410\u041d\u041d\u042b\u0415\u00b6","text":""},{"location":"portfolio/sfml/final_movie_rating.html","title":"\u0427\u0442\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u00b6","text":"<p>\u041d\u0430\u0447\u043d\u0435\u043c \u0441 \u0447\u0438\u0435\u043d\u0438\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0445 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438 \u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \u043c\u043e\u0434\u0435\u043b\u0438, \u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f.</p>"},{"location":"portfolio/sfml/final_movie_rating.html","title":"\u041e\u0431\u0437\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u00b6","text":"<p>\u041e\u043f\u0438\u0448\u0438\u043c \u043e\u0441\u0442\u043d\u043e\u0432\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438</p> <ul> <li><code>ID</code> - \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a (\u043d\u0443\u0436\u0435\u043d \u0434\u043b\u044f \u0441\u0430\u0431\u043c\u0438\u0442\u0430)</li> <li><code>asin</code> - ID \u0442\u043e\u0432\u0430\u0440\u0430, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 B000FA64PK</li> <li><code>helpful</code> - \u043f\u043e\u043b\u0435\u0437\u043d\u043e\u0441\u0442\u044c \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0430 \u043f\u043e \u043c\u043d\u0435\u043d\u0438\u044e \u0434\u0440\u0443\u0433\u0438\u0445 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 2/3.</li> <li><code>reviewText</code> - \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043e\u043a \u043e\u0442\u0437\u044b\u0432\u0430.</li> <li><code>reviewTime</code> - \u0432\u0440\u0435\u043c\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u043e\u0442\u0437\u044b\u0432\u0430.</li> <li><code>reviewerID</code> - ID \u0430\u0432\u0442\u043e\u0440\u0430 \u043e\u0442\u0437\u044b\u0432\u0430, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 A3SPTOKDG7WBLN.</li> <li><code>reviewerName</code> - \u0438\u043c\u044f \u0430\u0432\u0442\u043e\u0440\u0430 \u043e\u0442\u0437\u044b\u0432\u0430.</li> <li><code>summary</code> - \u0441\u0436\u0430\u0442\u043e\u0435 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u0435 \u043e\u0442\u0437\u044b\u0432\u0430.</li> <li><code>unixReviewTime</code> - \u0432\u0440\u0435\u043c\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u043e\u0442\u0437\u044b\u0432\u0430 \u0432 \u0432\u0438\u0434\u0435 timestamp.</li> <li><code>overall</code> - \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0442\u043e\u0432\u0430\u0440\u0430 \u2013 \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f.</li> <li><code>features</code> - BERT \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 <code>reviewText</code></li> </ul>"},{"location":"portfolio/sfml/final_movie_rating.html#3","title":"3 |  \u0420\u0410\u0417\u0412\u0415\u0414\u041e\u0412\u0410\u0422\u0415\u041b\u042c\u041d\u042b\u0419 \u0410\u041d\u0410\u041b\u0418\u0417\u00b6","text":""},{"location":"portfolio/sfml/final_movie_rating.html","title":"\u0410\u0432\u0442\u043e\u0440 \u043e\u0431\u0437\u043e\u0440\u0430 \u00b6","text":"<p>\u0414\u0430\u0432\u0430\u0439\u0442\u0435 \u0438\u0441\u043b\u0435\u0434\u0443\u0435\u043c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0430\u0432\u0442\u043e\u0440\u043e\u0432 \u0432 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435</p>"},{"location":"portfolio/sfml/final_movie_rating.html","title":"\u041f\u0440\u0435\u0434\u043c\u0435\u0442 \u043e\u0431\u0437\u043e\u0440\u0430\u00b6","text":"<p>\u0422\u0435\u043f\u0435\u0440\u044c \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u043e\u0431\u043e\u0438\u0445 \u0432\u044b\u0431\u043e\u0440\u043e\u043a \u0434\u043b\u044f \u0444\u0438\u0447\u0438 <code>asin</code> (ID \u0442\u043e\u0432\u0430\u0440\u0430)</p>"},{"location":"portfolio/sfml/final_movie_rating.html","title":"\u0412\u0440\u0435\u043c\u043c\u0435\u043d\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u00b6","text":"<ul> <li>\u0423 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u0434\u0432\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0435 \u0441 \u0432\u0440\u0435\u043c\u0435\u043d\u0435\u043c; <code>ubixReviewTime</code> &amp; <code>reviewTimez</code></li> <li>\u0418\u0437 \u043e\u0434\u043d\u043e\u0433\u043e \u0438\u0437 \u043d\u0438\u0445 \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0447\u0435\u0440\u0435\u0437 \u0430\u0442\u0440\u0438\u0431\u0443\u0442 <code>dt</code>, \u0434\u0440\u0443\u0433\u043e\u0439 \u043d\u0430\u043c \u043d\u0435 \u043d\u0443\u0436\u0435\u043d</li> <li>\u041d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0444\u0438\u0447\u0438 <code>unixReviewTime</code> \u0438 \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u044d\u0442\u0438 \u043d\u043e\u0432\u044b\u0435 \u0444\u0438\u0447\u0438</li> </ul>"},{"location":"portfolio/sfml/final_movie_rating.html","title":"\u041f\u0440\u0438\u0437\u043d\u0430\u043a \u043f\u043e\u043c\u043e\u0449\u0438\u00b6","text":"<ul> <li>\u0412 \u0434\u0430\u043d\u043d\u044b\u0445 \u0443 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043a\u0430\u043a \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0430\u0435\u0442 \u043f\u043e\u043b\u0435\u0437\u043d\u043e\u0441\u0442\u044c \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0437\u043e\u0440\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u044e\u0442 \u0434\u0440\u0443\u0433\u0438\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0438</li> <li>\u041d\u0430\u0439\u0434\u0435\u043c \u0440\u0430\u0437\u043d\u0438\u0446\u0443 \u043c\u0435\u0436\u0434\u0443 \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0438 \u043e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u043e\u0446\u0435\u043d\u043e\u043a</li> <li>\u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043d\u0430\u043c \u043d\u0430\u0434\u043e \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c <code>str</code> \u0444\u043e\u0440\u043c\u0430\u0442 \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 csv \u0432 <code>\u0441\u043f\u0438\u0441\u043e\u043a</code></li> </ul>"},{"location":"portfolio/sfml/final_movie_rating.html#bert","title":"\u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 BERT\u00b6","text":"<ul> <li>\u041d\u0430\u043c \u0434\u0430\u043b\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 hidden state \u043c\u043e\u0434\u0435\u043b\u0438 BERT, \u0441\u043a\u043e\u0440\u0435\u0435 \u0432\u0441\u0435\u0433\u043e \u0441\u0440\u0435\u0434\u043d\u0438\u0439 \u0432\u0435\u043a\u0442\u043e\u0440 \u0432\u0441\u0435\u0445 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432 \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0435 \u0432 <code>ReviewText</code></li> <li>\u0414\u0430\u043d\u043d\u044b\u0435 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u043b\u0438\u0441\u044c \u0432 <code>DataFrame</code>, \u0438 \u0447\u0438\u0442\u0430\u044e\u0442\u0441\u044f \u043a\u0430\u043a <code>str</code>, \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f <code>eval</code> \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0438</li> </ul>"},{"location":"portfolio/sfml/final_movie_rating.html#4","title":"4 |  \u041f\u0420\u0415\u041e\u0411\u0420\u0410\u0411\u041e\u0422\u041a\u0410 \u0422\u0415\u041a\u0421\u0422\u0410\u00b6","text":"<ul> <li>\u041c\u044b \u043c\u043e\u0436\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0444\u0438\u0447\u0435\u0440\u043e\u0432</li> <li>\u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043d\u0430\u043c \u043d\u0430\u0434\u043e \u0431\u0443\u0434\u0435\u0442 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0438 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u0442\u044c \u0438\u0445 \u0432 \u0446\u0438\u0444\u0440\u043e\u0432\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435</li> <li>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u043c\u0435\u0442\u043e\u0434\u043e\u043c TF-IDF TfidfVectorizer (\u041e\u0434\u0438\u043d \u0438\u0437 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u043c\u0435\u0448\u043e\u043a \u0441\u043b\u043e\u0432</li> <li>\u0418\u0437 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430 \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u0447\u0430\u0441\u0442\u043e \u0432\u0441\u0440\u0435\u0447\u0430\u044e\u0449\u0438\u0439\u0441\u044f \u0441\u043b\u043e\u0432\u0430 stopwords</li> </ul>"},{"location":"portfolio/sfml/final_movie_rating.html#5-nlp","title":"5 |  NLP \u041c\u041e\u0414\u0415\u041b\u0418\u00b6","text":""},{"location":"portfolio/sfml/final_movie_rating.html","title":"\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\u00b6","text":"<ul> <li>\u041d\u0430\u043c \u043f\u0440\u0435\u0434\u0441\u0442\u043e\u0438\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043c\u0435\u0442\u0440\u0438\u043a\u0443 <code>MAE</code>, \u043d\u043e \u043c\u044b \u0432\u0441\u0435 \u0440\u0430\u0432\u043d\u043e \u043c\u043e\u0436\u0435\u043c \u0440\u0430\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0442\u044c \u0437\u0430\u0434\u0430\u0447\u0443 \u043a\u0430\u043a \u0437\u0430\u0434\u0430\u0447\u0443 \u043c\u043d\u043e\u0433\u043e\u043a\u043b\u0430\u0441\u0441\u043e\u0432\u043e\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0430, \u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>reviewText</code> \u0438 \u0438\u0437\u0432\u043b\u0435\u0447 \u0438\u0437 \u043d\u0435\u0435 \u0444\u0438\u0447\u0438, \u0447\u0442\u043e \u043c\u044b \u0438 \u0441\u0434\u0435\u043b\u0430\u043b\u0438</li> <li>\u041d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0438\u0437\u0443\u0447\u0438\u0442\u044c \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0445\u043e\u0440\u043e\u0448\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u044e\u0442 \u0440\u0430\u0437\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u044c\u043d\u044b\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044b, \u0432\u0441\u0435 \u0432\u0430\u0440\u0438\u0430\u0442\u043d\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u0443\u0434\u0435\u043c \u043e\u0431\u0443\u0447\u0430\u0442\u044c \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u0432 \u0441\u043f\u0438\u0441\u043e\u043a <code>lst_models</code></li> <li><code>nlp_evals</code> \u0434\u0430\u0435\u0442 \u043d\u0430\u043c \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e <code>\u043f\u0435\u0440\u0435\u043a\u0440\u0435\u0441\u0442\u043e\u0447\u043d\u043e\u0439 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438</code> \u0438 <code>train-test split</code> \u043f\u043e\u0434\u0445\u043e\u0434\u044b, \u0432 \u043a\u043b\u0430\u0441\u0441\u0435 \u0433\u0435\u0440\u0435\u0440\u0438\u0440\u0443\u044e\u0442\u0441\u044f TF-IDF \u0432\u0435\u043a\u0442\u043e\u0440\u0430, \u043a\u0430\u043a \u0438 \u0432 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u043c \u0440\u0430\u0437\u0434\u0435\u043b\u0435</li> <li>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0431\u0443\u0434\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u0438 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u043d\u043e \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0442\u044c \u0431\u0443\u0434\u0435\u043c \u043c\u0435\u0442\u0440\u0438\u043a\u0443 <code>MAE</code></li> <li>\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0435 \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u043d\u0430 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0435 <code>train</code>, \u043e\u0446\u0435\u043d\u0438\u0432 <code>MAE</code> \u043d\u0430 \u043e\u0446\u0435\u043d\u0438\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0438 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 <code>test</code> \u0432 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u043c \u0440\u0430\u0437\u0434\u0435\u043b\u0435</li> </ul>"},{"location":"portfolio/sfml/final_movie_rating.html","title":"\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<p>\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043d\u0430 \u0440\u0430\u0437\u043d\u044b\u0445 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0430\u0445, \u0434\u0430\u0432\u0430\u0439\u0442\u0435 \u043e\u0446\u0435\u043d\u0438\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0430 \u0432\u0441\u0435\u0439 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0438 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435</p>"},{"location":"portfolio/sfml/final_movie_rating.html#6-bert","title":"6 |  BERT \u042d\u041c\u0411\u0415\u0414\u0414\u0418\u041d\u0413 \u041c\u041e\u0414\u0415\u041b\u0418\u00b6","text":"<p>\u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0430\u043b\u044c\u0442\u0435\u0440\u043d\u0430\u0442\u0438\u0432\u043d\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434:</p> <ul> <li>\u0423 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 \u0438\u0437 \u043c\u043e\u0434\u0435\u043b\u0438 <code>BERT</code> \u0434\u043b\u044f \u0442\u0435\u043a\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 <code>reviewText</code></li> <li>\u0422\u0435\u043a\u0441\u0442\u0443 \u0441\u043e\u043f\u043e\u0441\u0442\u043e\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c 50, \u0442\u0435. \u0441\u043c\u044b\u0441\u043b \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u044f \u043e\u0431\u0437\u043e\u0440\u0430 \u0432 \u043a\u043e\u043c\u043f\u0430\u043a\u0442\u043e\u043c \u0432\u0438\u0434\u0435 \u043c\u043e\u0436\u043d\u043e \u0443\u043f\u043e\u043a\u043e\u0432\u0430\u0442\u044c, \u044d\u0442\u043e \u043d\u0430\u043c\u043d\u043e\u0433\u043e \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u0435\u0435 \u0447\u0435\u043c TF-IDF.</li> <li>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0441\u043c\u043e\u0436\u0435\u043c \u043b\u0438 \u043c\u044b \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0442\u0430\u043a\u0438\u043c \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u043c</li> </ul>"},{"location":"portfolio/sfml/final_movie_rating.html","title":"\u041e\u0431\u0443\u0447\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u0438 \u0442\u043e\u043b\u044c\u043a\u043e \u043d\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432\u00b6","text":""},{"location":"portfolio/sfml/final_movie_rating.html#tf-idf","title":"\u0414\u043e\u0431\u0430\u0432\u0438\u043c TF-IDF \u0444\u0438\u0447\u0438 \u043a \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432\u044b\u043c \u0444\u0438\u0447\u0430\u043c\u00b6","text":"<p>\u0421\u0433\u0440\u0443\u043f\u0438\u0440\u0443\u0435\u043c TF-IDF \u0438 BERT \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0432 \u043e\u0434\u043d\u0443 \u043c\u0430\u0442\u0440\u0438\u0446\u0443</p>"},{"location":"portfolio/sfml/mid_treemodels.html","title":"Mid treemodels","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport os; print(os.listdir())\nimport warnings; warnings.filterwarnings('ignore')\n\nfrom sklearn.tree import DecisionTreeClassifier as DTC\nfrom sklearn.ensemble import GradientBoostingClassifier as GBC\nfrom sklearn.ensemble import RandomForestClassifier as RFC\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score, confusion_matrix\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n</pre> import pandas as pd import numpy as np import os; print(os.listdir()) import warnings; warnings.filterwarnings('ignore')  from sklearn.tree import DecisionTreeClassifier as DTC from sklearn.ensemble import GradientBoostingClassifier as GBC from sklearn.ensemble import RandomForestClassifier as RFC from sklearn.preprocessing import MinMaxScaler, StandardScaler from sklearn.model_selection import train_test_split as tts from sklearn.model_selection import GridSearchCV from sklearn.metrics import f1_score, confusion_matrix  from matplotlib import pyplot as plt import seaborn as sns %matplotlib inline <pre>['submission_sample.csv', 'test', 'MTH.ipynb', 'MidTermHack.ipynb', 'train', '.ipynb_checkpoints', 'Untitled.ipynb']\n</pre> In\u00a0[2]: Copied! <pre># \u041e\u0431\u044a\u0435\u0434\u0435\u043d\u0438\u043c \u043d\u0430\u0448\u0438 \u0434\u0430\u043d\u043d\u044b\u0435\ntrain = pd.concat([pd.read_csv(\"./train/train\" + str(i) + \".csv\") for i in range(5)], axis=1)\ntest = pd.concat([pd.read_csv(\"./test/test\" + str(i) + \".csv\") for i in range(5)], axis=1)\ntrain.columns\n</pre> # \u041e\u0431\u044a\u0435\u0434\u0435\u043d\u0438\u043c \u043d\u0430\u0448\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 train = pd.concat([pd.read_csv(\"./train/train\" + str(i) + \".csv\") for i in range(5)], axis=1) test = pd.concat([pd.read_csv(\"./test/test\" + str(i) + \".csv\") for i in range(5)], axis=1) train.columns Out[2]: <pre>Index(['ID', 'Category', 'radius_mean', 'radius_std', 'radius_max',\n       'texture_mean', 'texture_std', 'IT', 'Category', 'area_std', 'area_max',\n       'smoothness_mean', 'smoothness_std', 'smoothness_max',\n       'compactness_mean', 'compactness_std', 'compactness_max',\n       'concavity_mean', 'concavity_std', 'concavity_max', 'ID', 'Category',\n       'concave_points_mean', 'concave_points_std', 'concave_points_max',\n       'symmetry_mean', 'symmetry_std', 'symmetry_max', '1D', 'Category',\n       'texture_max', 'perimeter_mean', 'perimeter_std', 'perimeter_max',\n       'area_mean', 'ID', 'Category', 'fractal_dimension_mean',\n       'fractal_dimension_std', 'fractal_dimension_max'],\n      dtype='object')</pre> In\u00a0[3]: Copied! <pre># \u041f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432 \u0443 \u043d\u0430\u0441 \u043d\u0435\u0442\nprint('missing train:',train.isna().sum().sum())\nprint('missing test:',test.isna().sum().sum())\n</pre> # \u041f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432 \u0443 \u043d\u0430\u0441 \u043d\u0435\u0442 print('missing train:',train.isna().sum().sum()) print('missing test:',test.isna().sum().sum()) <pre>missing train: 0\nmissing test: 0\n</pre> In\u00a0[4]: Copied! <pre>X, y = train.drop(['Category', 'ID', '1D', 'IT'], axis=1), train['Category'].iloc[:, 0].astype(int)\n\nmodel = StandardScaler()\nX_scaled = model.fit_transform(X)\nX.describe()\n</pre> X, y = train.drop(['Category', 'ID', '1D', 'IT'], axis=1), train['Category'].iloc[:, 0].astype(int)  model = StandardScaler() X_scaled = model.fit_transform(X) X.describe() Out[4]: radius_mean radius_std radius_max texture_mean texture_std area_std area_max smoothness_mean smoothness_std smoothness_max ... symmetry_std symmetry_max texture_max perimeter_mean perimeter_std perimeter_max area_mean fractal_dimension_mean fractal_dimension_std fractal_dimension_max count 284.000000 284.000000 284.000000 284.000000 284.000000 284.000000 284.000000 284.000000 284.000000 284.000000 ... 284.000000 284.000000 284.000000 284.000000 284.000000 284.000000 284.000000 284.000000 284.000000 284.000000 mean 14.148996 19.495528 92.026021 658.107394 0.096198 0.419343 1.251892 2.919105 42.442504 0.007101 ... 0.252722 0.269386 0.103987 0.087984 0.049109 0.180905 0.062776 0.114157 0.289535 0.083963 std 3.571511 4.234565 24.513772 357.663036 0.014897 0.309239 0.578635 2.235951 53.575721 0.002999 ... 0.157284 0.204846 0.050364 0.076533 0.038963 0.027561 0.007116 0.066556 0.062742 0.018969 min 6.981000 10.720000 43.790000 143.500000 0.062510 0.114400 0.360200 0.771400 7.254000 0.001713 ... 0.027290 0.000000 0.019380 0.000000 0.000000 0.106000 0.050250 0.000000 0.156500 0.055040 25% 11.667500 16.947500 74.967500 416.950000 0.085140 0.238025 0.849125 1.665500 18.117500 0.005181 ... 0.154075 0.120425 0.066757 0.029555 0.020850 0.160575 0.057495 0.065430 0.251075 0.071843 50% 13.505000 19.075000 87.265000 559.200000 0.095410 0.331250 1.177500 2.296000 24.610000 0.006464 ... 0.211450 0.230600 0.095160 0.065830 0.035125 0.178250 0.061735 0.096315 0.279650 0.079505 75% 16.085000 21.592500 105.925000 799.100000 0.105400 0.508425 1.508500 3.363750 48.442500 0.008279 ... 0.330150 0.374775 0.130000 0.122225 0.070445 0.196400 0.066455 0.161600 0.314800 0.091895 max 28.110000 39.280000 188.500000 2501.000000 0.163400 2.873000 4.885000 21.980000 542.200000 0.021770 ... 1.058000 1.105000 0.283200 0.426400 0.182300 0.304000 0.095750 0.291000 0.555800 0.207500 <p>8 rows \u00d7 30 columns</p> In\u00a0[5]: Copied! <pre>y.value_counts()\n</pre> y.value_counts() Out[5]: <pre>0    178\n1    106\nName: Category, dtype: int64</pre> In\u00a0[6]: Copied! <pre>best_features_ever = ['radius_max', 'perimeter_mean', 'perimeter_std', 'smoothness_mean',\n                      'concave_points_max', 'compactness_std', 'symmetry_mean', \n                      'symmetry_max', 'compactness_mean', 'concavity_std']\n\ncorr = train.loc[:, best_features_ever].corr()\ncorr = corr * abs(np.eye(len(corr))-1)\n# corr = corr.astype('float').round()\ncorr.style\\\n    .format(\"{:.3}\")\\\n    .bar(align='mid', color=['#d65f5f', '#5fba7d'])\n</pre> best_features_ever = ['radius_max', 'perimeter_mean', 'perimeter_std', 'smoothness_mean',                       'concave_points_max', 'compactness_std', 'symmetry_mean',                        'symmetry_max', 'compactness_mean', 'concavity_std']  corr = train.loc[:, best_features_ever].corr() corr = corr * abs(np.eye(len(corr))-1) # corr = corr.astype('float').round() corr.style\\     .format(\"{:.3}\")\\     .bar(align='mid', color=['#d65f5f', '#5fba7d']) Out[6]: radius_max perimeter_mean perimeter_std smoothness_mean concave_points_max compactness_std symmetry_mean symmetry_max compactness_mean concavity_std radius_max 0.0 0.754 0.85 0.71 0.938 0.311 0.126 0.586 0.201 -0.0481 perimeter_mean 0.754 0.0 0.937 0.662 0.717 0.714 0.464 0.877 0.599 0.366 perimeter_std 0.85 0.937 0.0 0.693 0.802 0.518 0.449 0.753 0.444 0.233 smoothness_mean 0.71 0.662 0.693 0.0 0.744 0.399 0.108 0.384 0.305 0.148 concave_points_max 0.938 0.717 0.802 0.744 0.0 0.272 0.195 0.566 0.164 -0.0531 compactness_std 0.311 0.714 0.518 0.399 0.272 0.0 0.243 0.701 0.875 0.669 symmetry_mean 0.126 0.464 0.449 0.108 0.195 0.243 0.0 0.545 0.251 0.253 symmetry_max 0.586 0.877 0.753 0.384 0.566 0.701 0.545 0.0 0.595 0.338 compactness_mean 0.201 0.599 0.444 0.305 0.164 0.875 0.251 0.595 0.0 0.795 concavity_std -0.0481 0.366 0.233 0.148 -0.0531 0.669 0.253 0.338 0.795 0.0 In\u00a0[7]: Copied! <pre>data = pd.concat([train.loc[:, best_features_ever], pd.Series(y, name='y')], axis=1)\ng = sns.PairGrid(data[[data.columns[1],data.columns[2],data.columns[3],\n                       data.columns[4], data.columns[5], 'y']], hue='y',\n                 height=3)\ng = g.map_diag(sns.histplot,edgecolor='k')\ng = g.map_offdiag(plt.scatter, s = 20,linewidths=0.5, edgecolor='k')\n</pre> data = pd.concat([train.loc[:, best_features_ever], pd.Series(y, name='y')], axis=1) g = sns.PairGrid(data[[data.columns[1],data.columns[2],data.columns[3],                        data.columns[4], data.columns[5], 'y']], hue='y',                  height=3) g = g.map_diag(sns.histplot,edgecolor='k') g = g.map_offdiag(plt.scatter, s = 20,linewidths=0.5, edgecolor='k') In\u00a0[9]: Copied! <pre>import warnings; warnings.filterwarnings('ignore')\n\nfig, ax = plt.subplots(2, 5, figsize=(12,4))\naxes = ax.ravel()\nfor i, feature in enumerate(best_features_ever):\n    sns.distplot(data[data[\"y\"] == 1][feature], label=\"1\", hist_kws=dict(edgecolor=None, linewidth=1),ax=axes[i], color='r')\n    sns.distplot(data[data[\"y\"] == 0][feature], label=\"0\", hist_kws=dict(edgecolor=None, linewidth=1),ax=axes[i])\n    axes[i].legend()\nplt.tight_layout()\n</pre> import warnings; warnings.filterwarnings('ignore')  fig, ax = plt.subplots(2, 5, figsize=(12,4)) axes = ax.ravel() for i, feature in enumerate(best_features_ever):     sns.distplot(data[data[\"y\"] == 1][feature], label=\"1\", hist_kws=dict(edgecolor=None, linewidth=1),ax=axes[i], color='r')     sns.distplot(data[data[\"y\"] == 0][feature], label=\"0\", hist_kws=dict(edgecolor=None, linewidth=1),ax=axes[i])     axes[i].legend() plt.tight_layout() In\u00a0[31]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\nf1_score(dt.predict(X_test), y_test)\n</pre> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)  dt = DecisionTreeClassifier() dt.fit(X_train, y_train) f1_score(dt.predict(X_test), y_test) <ul> <li>\u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>GridSearchCV</code> c 10 \u0444\u043e\u043b\u0434\u0430\u043c\u0438, \u0438\u0437 <code>parameter_grid</code> \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u043a\u0430\u043a\u0430\u044f \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u044f \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0434\u0430\u0441\u0442 \u043d\u0430\u043c \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u044e \u043c\u043e\u0434\u0435\u043b\u044c</li> </ul> In\u00a0[\u00a0]: Copied! <pre>def optimize_model(model, param_grid):\n    grid_search = GridSearchCV(model,\n                              param_grid=param_grid,\n                              cv=10)\n    grid_search.fit(X_scaled, y)\n    print(\"Best Score: {}\".format(grid_search.best_score_))\n    print(\"Best params: {}\".format(grid_search.best_params_))\n    return grid_search.best_estimator_\n</pre> def optimize_model(model, param_grid):     grid_search = GridSearchCV(model,                               param_grid=param_grid,                               cv=10)     grid_search.fit(X_scaled, y)     print(\"Best Score: {}\".format(grid_search.best_score_))     print(\"Best params: {}\".format(grid_search.best_params_))     return grid_search.best_estimator_ In\u00a0[10]: Copied! <pre>parameter_grid = {'n_estimators': [20, 25, 30],\n                  'max_depth': np.logspace(0,2,10),\n                  'max_features': list(range(1,11))}\nrf = optimize_model(RandomForestClassifier(), parameter_grid)\n</pre> parameter_grid = {'n_estimators': [20, 25, 30],                   'max_depth': np.logspace(0,2,10),                   'max_features': list(range(1,11))} rf = optimize_model(RandomForestClassifier(), parameter_grid) <pre>Best Score: 0.9614532019704433\nBest params: {'max_depth': 7.742636826811269, 'max_features': 2, 'n_estimators': 30}\n</pre> In\u00a0[11]: Copied! <pre>rf_test = RandomForestClassifier(**{'max_depth': 7.742636826811269, \n                                    'max_features': 10, 'n_estimators': 20, 'warm_start': True})\nscaler = StandardScaler()\nrf_test.fit(scaler.fit_transform(X_train), y_train)\n</pre> rf_test = RandomForestClassifier(**{'max_depth': 7.742636826811269,                                      'max_features': 10, 'n_estimators': 20, 'warm_start': True}) scaler = StandardScaler() rf_test.fit(scaler.fit_transform(X_train), y_train) Out[11]: <pre>RandomForestClassifier(max_depth=7.742636826811269, max_features=10,\n                       n_estimators=20, warm_start=True)</pre> In\u00a0[37]: Copied! <pre>y_pred = (rf_test.predict_proba(scaler.transform(X_test)) &gt; 0.5).astype(int)[:, 1]\nf1_score(y_pred, y_test)\n</pre> y_pred = (rf_test.predict_proba(scaler.transform(X_test)) &gt; 0.5).astype(int)[:, 1] f1_score(y_pred, y_test) Out[37]: <pre>1.0</pre> In\u00a0[38]: Copied! <pre>confusion_matrix(y_pred, y_test)\n</pre> confusion_matrix(y_pred, y_test) Out[38]: <pre>array([[21,  0],\n       [ 0,  8]])</pre> In\u00a0[39]: Copied! <pre>parameter_grid = {'n_estimators': [30, 50],\n                  'max_depth': np.logspace(0,2,10),\n                  'max_features': list(range(1,11)),\n                  'learning_rate': np.logspace(0.001, 1, 10)}\n\ngb = optimize_model(GradientBoostingClassifier(), \n                    parameter_grid)\n</pre> parameter_grid = {'n_estimators': [30, 50],                   'max_depth': np.logspace(0,2,10),                   'max_features': list(range(1,11)),                   'learning_rate': np.logspace(0.001, 1, 10)}  gb = optimize_model(GradientBoostingClassifier(),                      parameter_grid)   <pre>Best Score: 0.9720443349753694\nBest params: {'learning_rate': 1.0023052380778996, 'max_depth': 2.7825594022071245, 'max_features': 9, 'n_estimators': 50}\n</pre> In\u00a0[40]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n</pre> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1) In\u00a0[41]: Copied! <pre>gb_test = GradientBoostingClassifier(**{'learning_rate': 1.0023052380778996, \n                                                     'max_depth': 2.7825594022071245, \n                                                     'max_features': 7, 'n_estimators': 30})\nscaler = StandardScaler()\ngb_test.fit(scaler.fit_transform(X_train), y_train)\n</pre> gb_test = GradientBoostingClassifier(**{'learning_rate': 1.0023052380778996,                                                       'max_depth': 2.7825594022071245,                                                       'max_features': 7, 'n_estimators': 30}) scaler = StandardScaler() gb_test.fit(scaler.fit_transform(X_train), y_train) Out[41]: <pre>GradientBoostingClassifier(learning_rate=1.0023052380778996,\n                           max_depth=2.7825594022071245, max_features=7,\n                           n_estimators=30)</pre> In\u00a0[12]: Copied! <pre>y_pred = (gb_test.predict_proba(scaler.transform(X_test)) &gt; 0.5).astype(int)[:, 1]\nf1_score(y_pred, y_test)\n</pre> y_pred = (gb_test.predict_proba(scaler.transform(X_test)) &gt; 0.5).astype(int)[:, 1] f1_score(y_pred, y_test) Out[12]: <pre>0.9508196721311476</pre> In\u00a0[13]: Copied! <pre>confusion_matrix(y_pred, y_test)\n</pre> confusion_matrix(y_pred, y_test) Out[13]: <pre>array([[78,  5],\n       [ 1, 58]])</pre> In\u00a0[14]: Copied! <pre>from sklearn.feature_selection import SelectKBest, chi2\n</pre> from sklearn.feature_selection import SelectKBest, chi2 In\u00a0[15]: Copied! <pre>bestfeatures = SelectKBest(score_func=chi2, k=10)\nfit = bestfeatures.fit(X, y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score'] \nbest_chi2_feature_names = list(featureScores.nlargest(10,'Score')['Specs'])\n</pre> bestfeatures = SelectKBest(score_func=chi2, k=10) fit = bestfeatures.fit(X, y) dfscores = pd.DataFrame(fit.scores_) dfcolumns = pd.DataFrame(X.columns) featureScores = pd.concat([dfcolumns,dfscores],axis=1) featureScores.columns = ['Specs','Score']  best_chi2_feature_names = list(featureScores.nlargest(10,'Score')['Specs']) In\u00a0[16]: Copied! <pre>best_chi2_feature_names\n</pre> best_chi2_feature_names Out[16]: <pre>['concave_points_max',\n 'texture_mean',\n 'smoothness_std',\n 'concave_points_std',\n 'radius_max',\n 'concavity_max',\n 'smoothness_mean',\n 'radius_mean',\n 'concave_points_mean',\n 'radius_std']</pre> In\u00a0[17]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(X.loc[:, best_chi2_feature_names], y, test_size=0.1)\n</pre> X_train, X_test, y_train, y_test = train_test_split(X.loc[:, best_chi2_feature_names], y, test_size=0.1) In\u00a0[18]: Copied! <pre>gb_test = GradientBoostingClassifier(**{'learning_rate': 1.0023052380778996, \n                                        'max_depth': 2.7825594022071245, \n                                        'max_features': 7, 'n_estimators': 30})\nscaler = StandardScaler()\ngb_test.fit(scaler.fit_transform(X_train), y_train)\n</pre> gb_test = GradientBoostingClassifier(**{'learning_rate': 1.0023052380778996,                                          'max_depth': 2.7825594022071245,                                          'max_features': 7, 'n_estimators': 30}) scaler = StandardScaler() gb_test.fit(scaler.fit_transform(X_train), y_train) Out[18]: <pre>GradientBoostingClassifier(learning_rate=1.0023052380778996,\n                           max_depth=2.7825594022071245, max_features=7,\n                           n_estimators=30)</pre> In\u00a0[19]: Copied! <pre>y_pred = (gb_test.predict_proba(scaler.transform(X_test)) &gt; 0.5).astype(int)[:, 1]\nf1_score(y_pred, y_test)\n</pre> y_pred = (gb_test.predict_proba(scaler.transform(X_test)) &gt; 0.5).astype(int)[:, 1] f1_score(y_pred, y_test) Out[19]: <pre>0.9473684210526316</pre> In\u00a0[20]: Copied! <pre>confusion_matrix(y_pred, y_test)\n</pre> confusion_matrix(y_pred, y_test) Out[20]: <pre>array([[19,  1],\n       [ 0,  9]])</pre> In\u00a0[21]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n</pre> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1) In\u00a0[22]: Copied! <pre>gb_test = GradientBoostingClassifier(**{'learning_rate': 1.0023052380778996, \n                                        'max_depth': 2.7825594022071245, \n                                        'max_features': 7, 'n_estimators': 30})\nscaler = StandardScaler()\ngb_test.fit(scaler.fit_transform(X_train), y_train)\n</pre> gb_test = GradientBoostingClassifier(**{'learning_rate': 1.0023052380778996,                                          'max_depth': 2.7825594022071245,                                          'max_features': 7, 'n_estimators': 30}) scaler = StandardScaler() gb_test.fit(scaler.fit_transform(X_train), y_train) Out[22]: <pre>GradientBoostingClassifier(learning_rate=1.0023052380778996,\n                           max_depth=2.7825594022071245, max_features=7,\n                           n_estimators=30)</pre> In\u00a0[23]: Copied! <pre>feat_importances = pd.Series(gb_test.feature_importances_, index=X.columns)\nbest_tree_feature_names = list(feat_importances.nlargest(10).index)\n</pre> feat_importances = pd.Series(gb_test.feature_importances_, index=X.columns) best_tree_feature_names = list(feat_importances.nlargest(10).index) In\u00a0[24]: Copied! <pre>best_tree_feature_names\n</pre> best_tree_feature_names Out[24]: <pre>['concave_points_std',\n 'perimeter_std',\n 'area_max',\n 'smoothness_std',\n 'symmetry_max',\n 'concavity_max',\n 'radius_std',\n 'fractal_dimension_max',\n 'symmetry_std',\n 'radius_max']</pre> In\u00a0[25]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(X.loc[:, best_tree_feature_names], y, test_size=0.5)\n</pre> X_train, X_test, y_train, y_test = train_test_split(X.loc[:, best_tree_feature_names], y, test_size=0.5) In\u00a0[26]: Copied! <pre>gb_test = GradientBoostingClassifier(**{'learning_rate': 1.0023052380778996, \n                                        'max_depth': 2.7825594022071245, \n                                        'max_features': 7, 'n_estimators': 30})\nscaler = StandardScaler()\ngb_test.fit(scaler.fit_transform(X_train), y_train)\n</pre> gb_test = GradientBoostingClassifier(**{'learning_rate': 1.0023052380778996,                                          'max_depth': 2.7825594022071245,                                          'max_features': 7, 'n_estimators': 30}) scaler = StandardScaler() gb_test.fit(scaler.fit_transform(X_train), y_train) Out[26]: <pre>GradientBoostingClassifier(learning_rate=1.0023052380778996,\n                           max_depth=2.7825594022071245, max_features=7,\n                           n_estimators=30)</pre> In\u00a0[27]: Copied! <pre>y_pred = (gb_test.predict_proba(scaler.transform(X_test)) &gt; 0.5).astype(int)[:, 1]\nf1_score(y_pred, y_test)\n</pre> y_pred = (gb_test.predict_proba(scaler.transform(X_test)) &gt; 0.5).astype(int)[:, 1] f1_score(y_pred, y_test) Out[27]: <pre>0.923076923076923</pre> In\u00a0[28]: Copied! <pre>confusion_matrix(y_pred, y_test)\n</pre> confusion_matrix(y_pred, y_test) Out[28]: <pre>array([[86,  5],\n       [ 3, 48]])</pre>"},{"location":"portfolio/sfml/mid_treemodels.html#mid-term-hackathon","title":"Mid-Term Hackathon\u00b6","text":""},{"location":"portfolio/sfml/mid_treemodels.html","title":"\u0417\u0430\u0434\u0430\u0447\u0430\u00b6","text":"<p>\u041d\u0430\u043c \u043a\u0443\u0436\u043d\u043e \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c \u0442\u0438\u043f \u043e\u043f\u0443\u0445\u043e\u043b\u0438 \u043f\u043e \u0438\u0437\u043c\u0435\u0440\u0435\u043d\u0438\u0438 \u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a \u043e\u043f\u0443\u0445\u043e\u043b\u0438 \u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043d\u0443\u0436\u043d\u043e \u043c\u0435\u0442\u043e\u0434\u044b \u0441\u0432\u044f\u0437\u0430\u043d\u044b \u0441 \u0434\u0435\u0440\u0435\u0432\u044c\u044f\u043c\u0438</p>"},{"location":"portfolio/sfml/mid_treemodels.html","title":"\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0414\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<ul> <li>\u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0432 \u0440\u0430\u0437\u043d\u044b\u0445 \u0444\u0430\u0439\u043b\u0430\u0445 \u0440\u0430\u0437\u043b\u0438\u0447\u0430\u044e\u0442\u0441\u044f, \u0432\u0441\u0435 \u043e\u043d\u0438 \u0438\u043c\u0435\u044e\u0442 \u043e\u0431\u0449\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 <code>ID</code>, <code>Category</code></li> <li>\u041f\u043e\u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432, \u0447\u0442\u043e\u0431\u044b \u0443\u0442\u043e\u0447\u043d\u0438\u0442\u044c \u0435\u0441\u0442\u044c \u043a\u0430\u043a\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0443 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e <code>ID</code></li> <li>\u041e\u0431\u044a\u0435\u0434\u0435\u043d\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e <code>concat</code></li> </ul>"},{"location":"portfolio/sfml/mid_treemodels.html","title":"\u041f\u0440\u043e\u043f\u0443\u0441\u043a\u0438 \u0432 \u0414\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<ul> <li>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c, \u0435\u0441\u0442\u044c \u043b\u0438 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438</li> </ul>"},{"location":"portfolio/sfml/mid_treemodels.html","title":"\u0423\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\u00b6","text":"<ul> <li>\u041f\u043e\u0447\u0438\u0441\u0442\u0438\u043c \u0433\u0440\u044f\u0437\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u0446\u0435\u043b\u0435\u0432\u0443\u044e \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0443\u044e Category (<code>int64</code>)</li> </ul>"},{"location":"portfolio/sfml/mid_treemodels.html","title":"\u0420\u0430\u0441\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u0435\u00b6","text":"<ul> <li>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0441\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043c\u0435\u0442\u043a\u0438</li> </ul>"},{"location":"portfolio/sfml/mid_treemodels.html#eda","title":"EDA\u00b6","text":"<ul> <li>\u0412\u043e\u0437\u044c\u043c\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u043f\u0440\u0435\u0434\u043f\u043e\u043b\u043e\u0433\u0430\u0435\u043c \u0431\u0443\u0434\u0443\u0442 \u043f\u043e\u043b\u0435\u0437\u043d\u044b\u0435</li> <li>\u041f\u043e\u0441\u043b\u0435 \u044d\u0442\u043e\u0433\u043e \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u044e, \u0438\u043d\u0442\u0443\u0438\u0442\u0438\u0432\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>.bar</code> \u0432 dataframe</li> </ul>"},{"location":"portfolio/sfml/mid_treemodels.html#baseline","title":"\u041c\u043e\u0434\u0435\u043b\u044c : Baseline\u00b6","text":"<ul> <li>\u0414\u043b\u044f \u0431\u0430\u0437\u043e\u0432\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0434\u0435\u0440\u0435\u0432\u044c\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u0438, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0431\u0430\u0437\u043e\u0432\u044b\u0435 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438</li> <li>\u0420\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u0443\u044e \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u0441 \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435\u043c 1/10</li> </ul>"},{"location":"portfolio/sfml/mid_treemodels.html#randomforest","title":"\u041c\u043e\u0434\u0435\u043b\u044c : RandomForest\u00b6","text":"<ul> <li>\u0422\u0435\u043f\u0435\u0440\u044c \u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 RSS + Booststrap Agreggating \u0435\u043d\u0441\u0435\u043c\u0431\u043b; \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441</li> <li>\u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>GridSearchCV</code> c 10 \u0444\u043e\u043b\u0434\u0430\u043c\u0438, \u0438\u0437 <code>parameter_grid</code> \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u043a\u0430\u043a\u0430\u044f \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u044f \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0434\u0430\u0441\u0442 \u043d\u0430\u043c \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u044e \u043c\u043e\u0434\u0435\u043b\u044c</li> </ul>"},{"location":"portfolio/sfml/mid_treemodels.html","title":"\u041c\u043e\u0434\u0435\u043b\u044c : \u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0411\u0443\u0441\u0442\u0438\u043d\u0433\u0430\u00b6","text":"<ul> <li>\u0434\u0430\u043b\u0435\u0435 \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433</li> </ul>"},{"location":"portfolio/sfml/mid_treemodels.html","title":"\u041f\u043e\u0434\u0431\u043e\u0440 \u0424\u0438\u0447\u0435\u0439\u00b6","text":""},{"location":"portfolio/sfml/mid_treemodels.html#selectkbest","title":"SelectKBest\u00b6","text":"<p>\u041f\u043e\u0434\u0431\u043e\u0440 \u0441\u0430\u043c\u044b\u0445 \u0432\u0430\u0436\u043d\u044b\u0445 \u0444\u0438\u0447\u0435\u0439 \u043f\u043e \u043c\u0435\u0442\u043e\u0434\u0443 <code>SelectKBest</code> \u0441 \u043c\u0435\u0442\u0440\u0438\u043a\u043e\u0439 <code>chi2</code></p>"},{"location":"portfolio/sfml/mid_treemodels.html#featureimportance_","title":"featureimportance_\u00b6","text":"<p>\u0418\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043f\u043e \u043c\u0435\u0442\u043e\u0434\u0443 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u0430</p>"},{"location":"portfolio/sfml/ml3a_linear_practice1.html","title":"Ml3a linear practice1","text":"In\u00a0[36]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error, f1_score, accuracy_score, roc_curve, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\n</pre> import numpy as np import pandas as pd from sklearn.datasets import load_boston from sklearn.metrics import mean_squared_error, f1_score, accuracy_score, roc_curve, roc_auc_score from sklearn.model_selection import train_test_split from matplotlib import pyplot as plt <ul> <li>\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043f\u0440\u043e \u0446\u0435\u043d\u044b \u0434\u043e\u043c\u043e\u0432 \u0432 \u0411\u043e\u0441\u0442\u043e\u043d\u0435</li> <li>\u041d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u043c\u0435\u0434\u0438\u0430\u043d\u0443 \u0446\u0435\u043d\u044b \u0436\u0438\u043b\u044c\u044f \u0432 506 \u0440\u0430\u0439\u043e\u043d\u0430\u0445</li> </ul> In\u00a0[37]: Copied! <pre>data = load_boston()\ndata['data'].shape\n</pre> data = load_boston() data['data'].shape Out[37]: <pre>(506, 13)</pre> <p>\u041b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u0432\u044b\u0440\u0430\u0436\u0430\u0435\u0442\u0441\u044f \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0439 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u044c\u044e: $$y=X\\theta+\\epsilon,$$ \u0433\u0434\u0435</p> <ul> <li>$X$ \u2014 \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043e\u0431\u044a\u0435\u043a\u0442\u044b-\u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438</li> <li>$y$ \u2014 \u0432\u0435\u043a\u0442\u043e\u0440 \u0446\u0435\u043b\u0435\u0432\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439,</li> </ul> <p>\u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445:</p> <ul> <li>$X$, $\\theta$ \u2014 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438, $\\epsilon$ \u2014 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0448\u0443\u043c.</li> </ul> <p>\u0418\u0437 \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0441\u043b\u0435\u0434\u0443\u0435\u0442 \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0434\u043b\u044f $\\theta$ \u043a\u0430\u043a: $$X^Ty=X^TX\\theta \\rightarrow \\theta=(X^TX)^{-1}X^Ty$$</p> <p>\u0420\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0434\u043b\u044f $\\theta$ \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0439 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0430\u043b\u0433\u0435\u0431\u0440\u044b \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 Numpy:</p> In\u00a0[38]: Copied! <pre>from numpy.linalg import inv\n\n# \u0417\u0410\u0414\u0410\u0427\u0410 \u0420\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e, \u043e\u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043b\u044f\u044e\u0449\u0443\u044e \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u0435 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f theta\ndef linreg_linear(X, y):\n    lsm = inv(np.dot(X.T,X))\n    Xt = np.dot(X.T,y)\n    theta = np.dot(lsm,Xt)\n    return theta\n</pre> from numpy.linalg import inv  # \u0417\u0410\u0414\u0410\u0427\u0410 \u0420\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e, \u043e\u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043b\u044f\u044e\u0449\u0443\u044e \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u0435 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f theta def linreg_linear(X, y):     lsm = inv(np.dot(X.T,X))     Xt = np.dot(X.T,y)     theta = np.dot(lsm,Xt)     return theta In\u00a0[39]: Copied! <pre># \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435\n\nX, y = data['data'], data['target']\n\nX = np.hstack([np.ones(X.shape[0])[:,None], X])\nprint(X.shape)\n</pre> # \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435  X, y = data['data'], data['target']  X = np.hstack([np.ones(X.shape[0])[:,None], X]) print(X.shape) <pre>(506, 14)\n</pre> In\u00a0[40]: Copied! <pre># \u0412\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 theta\ntheta = linreg_linear(X, y)\nprint(theta)\n</pre> # \u0412\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 theta theta = linreg_linear(X, y) print(theta) <pre>[ 3.64594884e+01 -1.08011358e-01  4.64204584e-02  2.05586264e-02\n  2.68673382e+00 -1.77666112e+01  3.80986521e+00  6.92224640e-04\n -1.47556685e+00  3.06049479e-01 -1.23345939e-02 -9.52747232e-01\n  9.31168327e-03 -5.24758378e-01]\n</pre> In\u00a0[41]: Copied! <pre>theta.shape\n</pre> theta.shape Out[41]: <pre>(14,)</pre> In\u00a0[42]: Copied! <pre># \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0434\u043b\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\ny_pred = X.dot(theta)\n</pre> # \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0434\u043b\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 y_pred = X.dot(theta) In\u00a0[43]: Copied! <pre>def print_regression_metrics(y_true, y_pred):\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    print(f'MSE = {mse:.2f}, RMSE = {rmse:.2f}')\n</pre> def print_regression_metrics(y_true, y_pred):     mse = mean_squared_error(y_true, y_pred)     rmse = np.sqrt(mse)     print(f'MSE = {mse:.2f}, RMSE = {rmse:.2f}') <p>\u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0448\u0438\u0431\u043e\u043a MSE \u0438 RMSE \u0434\u043b\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445</p> In\u00a0[44]: Copied! <pre>print_regression_metrics(y, y_pred)\n</pre> print_regression_metrics(y, y_pred) <pre>MSE = 21.89, RMSE = 4.68\n</pre> In\u00a0[45]: Copied! <pre>plt.hist(y);\n</pre> plt.hist(y); In\u00a0[46]: Copied! <pre># \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 train/valid, \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c theta,\n# \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043e\u0448\u0438\u0431\u043a\u0438 MSE \u0438 RMSE\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\ntheta = linreg_linear(X_train, y_train)\ny_pred = X_valid.dot(theta)\ny_train_pred = X_train.dot(theta)\n</pre> # \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 train/valid, \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c theta, # \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043e\u0448\u0438\u0431\u043a\u0438 MSE \u0438 RMSE  X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2) theta = linreg_linear(X_train, y_train) y_pred = X_valid.dot(theta) y_train_pred = X_train.dot(theta) In\u00a0[47]: Copied! <pre>print_regression_metrics(y_valid, y_pred)\nprint_regression_metrics(y_train, y_train_pred)\n</pre> print_regression_metrics(y_valid, y_pred) print_regression_metrics(y_train, y_train_pred) <pre>MSE = 19.20, RMSE = 4.38\nMSE = 22.80, RMSE = 4.77\n</pre> In\u00a0[48]: Copied! <pre>from sklearn.linear_model import LinearRegression\n:\nlr = LinearRegression()\nlr.fit(X,y)\ny_pred = lr.predict(X)\nprint_regression_metrics(y, y_pred)\n</pre> from sklearn.linear_model import LinearRegression : lr = LinearRegression() lr.fit(X,y) y_pred = lr.predict(X) print_regression_metrics(y, y_pred) <pre>MSE = 21.89, RMSE = 4.68\n</pre> <p>\u0414\u043b\u044f \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043e\u0448\u0438\u0431\u043a\u0438 \u0441\u0440\u0435\u0434\u043d\u0435\u0433\u043e \u043a\u0432\u0430\u0434\u0440\u0430\u0442\u0438\u0447\u043d\u043e\u0433\u043e (Mean Squared Error), \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0432\u044b\u043f\u0443\u043a\u043b\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0435\u0439 \u0432 n-\u043c\u0435\u0440\u043d\u043e\u043c \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0435 $\\mathbb{R}^n$ \u0438 \u0432 \u043e\u0431\u0449\u0435\u043c \u0432\u0438\u0434\u0435 \u0432\u044b\u0433\u043b\u044f\u0434\u0438\u0442 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c: $$MSE = \\frac{1}{n} * \\sum_{i=1}^{n}{(y_i - a(x_i))^2}.$$ \u0417\u0434\u0435\u0441\u044c $x_i$ \u2014 \u0432\u0435\u043a\u0442\u043e\u0440-\u043f\u0440\u0438\u0437\u043d\u0430\u043a $i$-\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438, $y_i$ \u2014 \u0438\u0441\u0442\u0438\u043d\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0434\u043b\u044f $i$-\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430, $a(x)$ \u2014 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c, \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0449\u0438\u0439 \u0434\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 $x$ \u0446\u0435\u043b\u0435\u0432\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435, $n$ \u2014 \u043a\u043e\u043b-\u0432\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0435.</p> <p>\u0412 \u0441\u043b\u0443\u0447\u0430\u0435 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 $MSE$ \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043a\u0430\u043a: $$MSE(X, y, \\theta) = \\frac{1}{2n} * \\sum_{i=1}^{n}{(y_i - \\theta^Tx_i)^2} = \\frac{1}{2n} \\lVert{y - X\\theta}\\rVert_{2}^{2}=\\frac{1}{2n} (y - X\\theta)^T(y - X\\theta),$$ \u0433\u0434\u0435 $\\theta$ \u2014 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u043c\u043e\u0434\u0435\u043b\u0438 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438, $X$ \u2014 \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043e\u0431\u044a\u0435\u043a\u0442\u044b-\u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, $y$ - \u0432\u0435\u043a\u0442\u043e\u0440 \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 $X$.</p> <p>\u0412\u043e\u0437\u044c\u043c\u0435\u043c \u043f\u0435\u0440\u0432\u044b\u0439 \u0432\u0430\u0440\u0438\u0430\u043d\u0442 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u0448\u0438\u0431\u043a\u0438 \u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0435\u0435 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u043f\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0443 $\\theta$, \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u0435\u0440\u0435\u0438\u043c\u0435\u043d\u043e\u0432\u0430\u0432 $MSE$ \u0432 $L$: $$L=\\frac{1}{2n} * \\sum_{i=1}^{n}{(y_i - \\theta^Tx_i)^2}$$ $$\\nabla L = \\frac{1}{n}\\sum_{i=1}^{n}{(\\theta^Tx_i - y_i) \\cdot x_i} = \\frac{1}{n}X^T(X\\theta - y)$$</p> <p>\u0418\u0441\u0445\u043e\u0434\u044f \u0438\u0437 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0433\u043e \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0430, \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430:</p> In\u00a0[49]: Copied! <pre>from tqdm.notebook import tqdm\n\n# \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0430 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 MSE\n\ndef calc_mse_gradient(X, y, theta):\n    n = X.shape[0]\n    grad = (1.0/n) * X.T.dot(X.dot(theta) - y)\n    return grad\n\n# \u0428\u0430\u0433 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430  \n\ndef gradient_step(theta, theta_grad, alpha):\n    return theta - alpha * theta_grad\n\n# \u041f\u0440\u043e\u0446\u0435\u0434\u0443\u0440\u0430 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438\n\ndef optimize(X, y, grad_func, start_theta, alpha, n_iters):\n    \n    # \u043d\u0430\u0447\u0430\u043b\u044c\u043d\u043e\u0435 theta\n    theta = start_theta.copy()\n    \n    for i in tqdm(range(n_iters)):\n        theta_grad = grad_func(X, y, theta)\n        theta = gradient_step(theta, theta_grad, alpha)\n    \n    return theta\n</pre> from tqdm.notebook import tqdm  # \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0430 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 MSE  def calc_mse_gradient(X, y, theta):     n = X.shape[0]     grad = (1.0/n) * X.T.dot(X.dot(theta) - y)     return grad  # \u0428\u0430\u0433 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430    def gradient_step(theta, theta_grad, alpha):     return theta - alpha * theta_grad  # \u041f\u0440\u043e\u0446\u0435\u0434\u0443\u0440\u0430 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438  def optimize(X, y, grad_func, start_theta, alpha, n_iters):          # \u043d\u0430\u0447\u0430\u043b\u044c\u043d\u043e\u0435 theta     theta = start_theta.copy()          for i in tqdm(range(n_iters)):         theta_grad = grad_func(X, y, theta)         theta = gradient_step(theta, theta_grad, alpha)          return theta  In\u00a0[50]: Copied! <pre># \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0442\u0430\u0431\u043b\u0438\u0446\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u043c\u0430\u0442\u0440\u0438\u0446\u044b X \u0438 y\nX, y = data['data'], data['target']\n\n# \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438)\nX = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])\nm = X.shape[1]\n</pre> # \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0442\u0430\u0431\u043b\u0438\u0446\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u043c\u0430\u0442\u0440\u0438\u0446\u044b X \u0438 y X, y = data['data'], data['target']  # \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438) X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X]) m = X.shape[1] In\u00a0[51]: Copied! <pre># \u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 theta \u043d\u0430 \u0432\u0441\u0435\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\ntheta = optimize(X, y, calc_mse_gradient, np.ones(m), 0.001, 100)\n</pre> # \u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 theta \u043d\u0430 \u0432\u0441\u0435\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 theta = optimize(X, y, calc_mse_gradient, np.ones(m), 0.001, 100) <pre>  0%|          | 0/100 [00:00&lt;?, ?it/s]</pre> In\u00a0[52]: Copied! <pre>theta\n</pre> theta Out[52]: <pre>array([7.41647399e+246, 3.32349992e+247, 7.39564172e+247, 8.96295209e+247,\n       5.07578059e+245, 4.22030567e+246, 4.63094053e+247, 5.29083888e+248,\n       2.65643383e+247, 8.19991211e+247, 3.27135991e+249, 1.38363846e+248,\n       2.64323053e+249, 9.88835598e+247])</pre> In\u00a0[53]: Copied! <pre># \u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u043e \u043a\u0430\u0436\u0434\u043e\u043c\u0443 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0443 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445\nX.max(axis=0)\n</pre> # \u041f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u043e \u043a\u0430\u0436\u0434\u043e\u043c\u0443 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0443 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445 X.max(axis=0) Out[53]: <pre>array([  1.    ,  88.9762, 100.    ,  27.74  ,   1.    ,   0.871 ,\n         8.78  , 100.    ,  12.1265,  24.    , 711.    ,  22.    ,\n       396.9   ,  37.97  ])</pre> In\u00a0[54]: Copied! <pre>print(data['feature_names'][np.argmax(X.std(axis=0)) + 1])\nprint(np.max(X.std(axis=0)))\n</pre> print(data['feature_names'][np.argmax(X.std(axis=0)) + 1]) print(np.max(X.std(axis=0))) <pre>B\n168.3704950393814\n</pre> In\u00a0[55]: Copied! <pre># \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u043d\u044b\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0439 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438\nX, y = data['data'], data['target']\nX = (X - X.mean(axis=0)) / X.std(axis=0)\n</pre> # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u043d\u044b\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0439 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 X, y = data['data'], data['target'] X = (X - X.mean(axis=0)) / X.std(axis=0) In\u00a0[56]: Copied! <pre># \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438)\nX = np.hstack([np.ones(X.shape[0])[:,None], X])\nX.max(axis=0)\n</pre> # \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438) X = np.hstack([np.ones(X.shape[0])[:,None], X]) X.max(axis=0) Out[56]: <pre>array([1.        , 9.9339306 , 3.80423444, 2.42256516, 3.66839786,\n       2.73234648, 3.55504427, 1.11749449, 3.96051769, 1.66124525,\n       1.79819419, 1.63882832, 0.44105193, 3.54877081])</pre> In\u00a0[57]: Copied! <pre># \u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c theta \u043d\u0430 \u043d\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\ntheta = optimize(X, y, calc_mse_gradient, np.ones(m), 0.01, 5000)\n</pre> # \u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c theta \u043d\u0430 \u043d\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 theta = optimize(X, y, calc_mse_gradient, np.ones(m), 0.01, 5000) <pre>  0%|          | 0/5000 [00:00&lt;?, ?it/s]</pre> In\u00a0[58]: Copied! <pre>theta\n</pre> theta Out[58]: <pre>array([ 2.25328063e+01, -9.21740195e-01,  1.07033639e+00,  1.06388396e-01,\n        6.86667316e-01, -2.05006416e+00,  2.68062168e+00,  1.40667969e-02,\n       -3.10608483e+00,  2.57511475e+00, -1.97802851e+00, -2.05725099e+00,\n        8.48690321e-01, -3.74025884e+00])</pre> In\u00a0[59]: Copied! <pre># \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043f\u0440\u0438 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u0445\ny_pred = X.dot(theta)\n</pre> # \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043f\u0440\u0438 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u0445 y_pred = X.dot(theta) In\u00a0[60]: Copied! <pre># \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0448\u0438\u0431\u043e\u043a MSE \u0438 RMSE \u0434\u043b\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\nprint_regression_metrics(y, y_pred)\n</pre> # \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0448\u0438\u0431\u043e\u043a MSE \u0438 RMSE \u0434\u043b\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 print_regression_metrics(y, y_pred) <pre>MSE = 21.90, RMSE = 4.68\n</pre> In\u00a0[61]: Copied! <pre># \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 train/valid, \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c theta,\n# \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043e\u0448\u0438\u0431\u043a\u0438 MSE \u0438 RMSE\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\ntheta = optimize(X_train, y_train, calc_mse_gradient, np.ones(m), 0.01, 5000)\ny_pred = X_valid.dot(theta)\n\nprint_regression_metrics(y_valid, y_pred)\n</pre> # \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 train/valid, \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c theta, # \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043e\u0448\u0438\u0431\u043a\u0438 MSE \u0438 RMSE  X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2) theta = optimize(X_train, y_train, calc_mse_gradient, np.ones(m), 0.01, 5000) y_pred = X_valid.dot(theta)  print_regression_metrics(y_valid, y_pred) <pre>  0%|          | 0/5000 [00:00&lt;?, ?it/s]</pre> <pre>MSE = 20.05, RMSE = 4.48\n</pre> In\u00a0[62]: Copied! <pre>from sklearn.linear_model import LinearRegression as LR\n\nX, y = data['data'], data['target']\nX = np.hstack([np.ones(X.shape[0])[:,None], X])\n\nmodel = LR()\nmodel.fit(X,y)\ny_pred = model.predict(X)\n\nprint_regression_metrics(y, y_pred)\n</pre> from sklearn.linear_model import LinearRegression as LR  X, y = data['data'], data['target'] X = np.hstack([np.ones(X.shape[0])[:,None], X])  model = LR() model.fit(X,y) y_pred = model.predict(X)  print_regression_metrics(y, y_pred) <pre>MSE = 21.89, RMSE = 4.68\n</pre> In\u00a0[63]: Copied! <pre>df = pd.DataFrame(data['data'],columns=data['feature_names'])\ndf.head()\n</pre> df = pd.DataFrame(data['data'],columns=data['feature_names']) df.head() Out[63]: CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 In\u00a0[64]: Copied! <pre>df.describe()\n</pre> df.describe() Out[64]: CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT count 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 mean 3.613524 11.363636 11.136779 0.069170 0.554695 6.284634 68.574901 3.795043 9.549407 408.237154 18.455534 356.674032 12.653063 std 8.601545 23.322453 6.860353 0.253994 0.115878 0.702617 28.148861 2.105710 8.707259 168.537116 2.164946 91.294864 7.141062 min 0.006320 0.000000 0.460000 0.000000 0.385000 3.561000 2.900000 1.129600 1.000000 187.000000 12.600000 0.320000 1.730000 25% 0.082045 0.000000 5.190000 0.000000 0.449000 5.885500 45.025000 2.100175 4.000000 279.000000 17.400000 375.377500 6.950000 50% 0.256510 0.000000 9.690000 0.000000 0.538000 6.208500 77.500000 3.207450 5.000000 330.000000 19.050000 391.440000 11.360000 75% 3.677083 12.500000 18.100000 0.000000 0.624000 6.623500 94.075000 5.188425 24.000000 666.000000 20.200000 396.225000 16.955000 max 88.976200 100.000000 27.740000 1.000000 0.871000 8.780000 100.000000 12.126500 24.000000 711.000000 22.000000 396.900000 37.970000 In\u00a0[65]: Copied! <pre>X, y = data['data'], data['target']\n\nprint(data.feature_names)\nprint(X.std(axis=0))\n</pre> X, y = data['data'], data['target']  print(data.feature_names) print(X.std(axis=0)) <pre>['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n 'B' 'LSTAT']\n[8.59304135e+00 2.32993957e+01 6.85357058e+00 2.53742935e-01\n 1.15763115e-01 7.01922514e-01 2.81210326e+01 2.10362836e+00\n 8.69865112e+00 1.68370495e+02 2.16280519e+00 9.12046075e+01\n 7.13400164e+00]\n</pre> In\u00a0[66]: Copied! <pre>X, y = data['data'], data['target']\n\nmodel = LR()\nmodel.fit(X,y)\ny_pred = model.predict(X)\n\nprint_regression_metrics(y, y_pred)\n</pre> X, y = data['data'], data['target']  model = LR() model.fit(X,y) y_pred = model.predict(X)  print_regression_metrics(y, y_pred) <pre>MSE = 21.89, RMSE = 4.68\n</pre> In\u00a0[67]: Copied! <pre>df = pd.DataFrame(data['data'],columns=data['feature_names'])\ndfy = pd.DataFrame(data['target'],columns=['target'])\ndf_all = pd.concat([df,dfy],axis=1)\n\nldf = df_all[~(df_all['B'] &lt; 50)]\nldf.head()\n</pre> df = pd.DataFrame(data['data'],columns=data['feature_names']) dfy = pd.DataFrame(data['target'],columns=['target']) df_all = pd.concat([df,dfy],axis=1)  ldf = df_all[~(df_all['B'] &lt; 50)] ldf.head() Out[67]: CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT target 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 In\u00a0[68]: Copied! <pre>y = ldf['target']\nX = ldf.drop(['target'],axis=1)\n\nX = np.hstack([np.ones(X.shape[0])[:,None], X])\n\nmodel = LR()\nmodel.fit(X,y)\ny_pred = model.predict(X)\n\nprint_regression_metrics(y, y_pred)\n</pre> y = ldf['target'] X = ldf.drop(['target'],axis=1)  X = np.hstack([np.ones(X.shape[0])[:,None], X])  model = LR() model.fit(X,y) y_pred = model.predict(X)  print_regression_metrics(y, y_pred) <pre>MSE = 21.79, RMSE = 4.67\n</pre> In\u00a0[70]: Copied! <pre># \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0442\u0430\u0431\u043b\u0438\u0446\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u043c\u0430\u0442\u0440\u0438\u0446\u044b X \u0438 y\nX, y = data['data'], data['target']\nX = (X - X.mean(axis=0)) / X.std(axis=0)\n\n# \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438)\nX = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])\n</pre> # \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0442\u0430\u0431\u043b\u0438\u0446\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u043c\u0430\u0442\u0440\u0438\u0446\u044b X \u0438 y X, y = data['data'], data['target'] X = (X - X.mean(axis=0)) / X.std(axis=0)  # \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438) X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X]) In\u00a0[73]: Copied! <pre>def linreg_linear(X, y):\n    lsm = inv(np.dot(X.T,X))\n    Xt = np.dot(X.T,y)\n    theta = np.dot(lsm,Xt)\n    return theta\n\ntheta = linreg_linear(X,y)\ny_pred = X.dot(theta)\n\nprint_regression_metrics(y, y_pred)\n</pre> def linreg_linear(X, y):     lsm = inv(np.dot(X.T,X))     Xt = np.dot(X.T,y)     theta = np.dot(lsm,Xt)     return theta  theta = linreg_linear(X,y) y_pred = X.dot(theta)  print_regression_metrics(y, y_pred) <pre>MSE = 21.89, RMSE = 4.68\n</pre>"},{"location":"portfolio/sfml/ml3a_linear_practice1.html","title":"\u041b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430\u00b6","text":""},{"location":"portfolio/sfml/ml3a_linear_practice1.html","title":"\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0421\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0438 \u0414\u043e\u043c\u0430\u00b6","text":""},{"location":"portfolio/sfml/ml3a_linear_practice1.html","title":"\u0414\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 \u0411\u043e\u0441\u0442\u043e\u043d\u0430\u00b6","text":"<p>\u0412 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0445 \u0437\u0430\u0434\u0430\u043d\u0438\u044f\u0445 \u0431\u0443\u0434\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0434\u0430\u0442\u0430\u0441\u0435\u0442 boston \u0438\u0437 <code>sklearn.datasets</code></p>"},{"location":"portfolio/sfml/ml3a_linear_practice1.html","title":"\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u0445 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0439\u00b6","text":""},{"location":"portfolio/sfml/ml3a_linear_practice1.html","title":"\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u043e\u0434\u043d\u0443 \u0432\u044b\u0431\u043e\u0440\u043a\u0443\u00b6","text":""},{"location":"portfolio/sfml/ml3a_linear_practice1.html","title":"\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043e\u0431\u043e\u0431\u0449\u0430\u044e\u0449\u0438\u044e \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<ul> <li>\u041c\u044b \u043e\u0431\u0443\u0447\u0438\u043b\u0438 \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u043d\u0435\u043a\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435, \u0438 \u043d\u0430 \u043d\u0438\u0445 \u0436\u0435 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u043b\u0438 \u043c\u0435\u0442\u0440\u0438\u043a\u0443 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430</li> <li>\u041c\u044b \u043d\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043b\u0438 \u043e\u0431\u043e\u0431\u0449\u0430\u044e\u0449\u0438\u044e \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438</li> </ul>"},{"location":"portfolio/sfml/ml3a_linear_practice1.html","title":"\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 (\u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0421\u043f\u0443\u0441\u043a)\u00b6","text":""},{"location":"portfolio/sfml/ml3a_linear_practice1.html","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/sfml/ml3a_linear_practice1.html#361","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3.6.1\u00b6","text":"<p>\u0420\u0435\u0430\u043b\u0438\u0437\u0443\u0439\u0442\u0435 \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u0443\u044e \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e. \u041a\u0430\u043a\u043e\u0439 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0441\u044f RMSE?</p> <p>4.68 (\u0443\u0436\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u043e)</p>"},{"location":"portfolio/sfml/ml3a_linear_practice1.html#362","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3.6.2\u00b6","text":"<p>\u041f\u043e\u0441\u0442\u0440\u043e\u0439\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u044c \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 sklearn. \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e, \u043e\u0431\u0443\u0447\u0438\u0442\u0435 \u043d\u0430 \u0432\u0441\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0439\u0442\u0435 RMSE.</p>"},{"location":"portfolio/sfml/ml3a_linear_practice1.html#363","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3.6.3\u00b6","text":"<p>\u0423 \u043a\u0430\u043a\u043e\u0433\u043e \u0438\u0437 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043d\u0430\u0438\u0431\u043e\u043b\u044c\u0448\u0435\u0435 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0435 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0435? \u0427\u0435\u043c\u0443 \u043e\u043d\u043e \u0440\u0430\u0432\u043d\u043e?</p>"},{"location":"portfolio/sfml/ml3a_linear_practice1.html#364","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3.6.4\u00b6","text":"<p>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e \u0431\u0435\u0437 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u0430 \u0435\u0434\u0438\u043d\u0438\u0446. \u041a\u0430\u043a\u043e\u0439 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0441\u044f RMSE?</p>"},{"location":"portfolio/sfml/ml3a_linear_practice1.html#365","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3.6.5\u00b6","text":"<p>\u041e\u0447\u0438\u0441\u0442\u0438\u0442\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043e\u0442 \u0441\u0442\u0440\u043e\u043a, \u0433\u0434\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430  \u043c\u0435\u043d\u044c\u0448\u0435 . \u041a\u0430\u043a\u043e\u0439 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0441\u044f RMSE?</p>"},{"location":"portfolio/sfml/ml3a_linear_practice1.html#366","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3.6.6\u00b6","text":"<p>\u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0439\u0442\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0438 \u043e\u0431\u0443\u0447\u0438\u0442\u0435 \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u043c \u043c\u0435\u0442\u043e\u0434\u043e\u043c. \u041a\u0430\u043a\u043e\u0439 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0441\u044f RMSE?</p>"},{"location":"portfolio/sfml/ml3a_linear_practice2.html","title":"Ml3a linear practice2","text":"In\u00a0[24]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error, f1_score, accuracy_score, roc_curve, roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\nfrom matplotlib import pyplot as plt\n</pre> import numpy as np import pandas as pd from sklearn.datasets import load_boston from sklearn.metrics import mean_squared_error, f1_score, accuracy_score, roc_curve, roc_auc_score from sklearn.model_selection import train_test_split  from matplotlib import pyplot as plt In\u00a0[25]: Copied! <pre>def print_regression_metrics(y_true, y_pred):\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    print(f'MSE = {mse:.2f}, RMSE = {rmse:.2f}')\n    \ndef prepare_boston_data():\n    data = load_boston()\n    X, y = data['data'], data['target']\n    # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u043d\u044b\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0439 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438\n    X = (X - X.mean(axis=0)) / X.std(axis=0)\n    # \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438)\n    X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])\n    \n    return X, y\n</pre> def print_regression_metrics(y_true, y_pred):     mse = mean_squared_error(y_true, y_pred)     rmse = np.sqrt(mse)     print(f'MSE = {mse:.2f}, RMSE = {rmse:.2f}')      def prepare_boston_data():     data = load_boston()     X, y = data['data'], data['target']     # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u043d\u044b\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0439 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438     X = (X - X.mean(axis=0)) / X.std(axis=0)     # \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438)     X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])          return X, y <p>\u041f\u0440\u0435\u0436\u0434\u0435 \u0447\u0435\u043c \u043d\u0430\u0447\u0430\u0442\u044c, \u043e\u0431\u0435\u0440\u043d\u0435\u043c \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u043d\u0443\u044e \u043d\u0430\u043c\u0438</p> <ul> <li>\u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e \u043c\u0435\u0442\u043e\u0434\u043e\u043c \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u044b \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0439 \u0432 \u043a\u043b\u0430\u0441\u0441:</li> </ul> In\u00a0[26]: Copied! <pre>class LinRegAlgebra():\n    def __init__(self):\n        self.theta = None\n    \n    def fit(self, X, y):\n        self.theta = np.linalg.inv(X.T.dot(X)).dot(X.transpose()).dot(y)\n    \n    def predict(self, X):\n        return X.dot(self.theta)\n</pre> class LinRegAlgebra():     def __init__(self):         self.theta = None          def fit(self, X, y):         self.theta = np.linalg.inv(X.T.dot(X)).dot(X.transpose()).dot(y)          def predict(self, X):         return X.dot(self.theta) <ul> <li>\u041f\u0440\u043e\u0432\u0435\u0434\u0435\u043c \u0437\u0430\u043c\u0435\u0440\u044b \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u0438 \u0440\u0430\u0431\u043e\u0442\u044b \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u043d\u0430 \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u0445 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u044f\u0445 \u0438 \u043d\u0430 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u043c \u0441\u043f\u0443\u0441\u043a\u0435.</li> <li>\u041f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043d\u0430\u0439\u0434\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0434\u043b\u044f \u043c\u0435\u0442\u043e\u0434\u0430, \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e \u043d\u0430 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u043c \u0441\u043f\u0443\u0441\u043a\u0435, \u0442\u0430\u043a, \u0447\u0442\u043e\u0431\u044b \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043c\u0435\u0442\u0440\u0438\u043a \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e \u0441\u043e\u0432\u043f\u0430\u0434\u0430\u043b\u043e \u0441\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0432 \u0441\u043b\u0443\u0447\u0430\u0435 \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430.</li> </ul> In\u00a0[27]: Copied! <pre>X, y = prepare_boston_data()\n</pre> X, y = prepare_boston_data() In\u00a0[28]: Copied! <pre>linreg_alg = LinRegAlgebra()\nlinreg_alg.fit(X, y)\ny_pred = linreg_alg.predict(X)\n\n# \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0448\u0438\u0431\u043e\u043a MSE \u0438 RMSE \u0434\u043b\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\nprint_regression_metrics(y, y_pred)\n</pre> linreg_alg = LinRegAlgebra() linreg_alg.fit(X, y) y_pred = linreg_alg.predict(X)  # \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0448\u0438\u0431\u043e\u043a MSE \u0438 RMSE \u0434\u043b\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 print_regression_metrics(y, y_pred) <pre>MSE = 21.89, RMSE = 4.68\n</pre> In\u00a0[29]: Copied! <pre>class RegOptimizer():\n    def __init__(self, alpha, n_iters):\n        self.theta = None\n        self._alpha = alpha\n        self._n_iters = n_iters\n    \n    def gradient_step(self, theta, theta_grad):\n        return theta - self._alpha * theta_grad\n    \n    def grad_func(self, X, y, theta):\n        raise NotImplementedError()\n\n    def optimize(self, X, y, start_theta, n_iters):\n        theta = start_theta.copy()\n\n        for i in range(n_iters):\n            theta_grad = self.grad_func(X, y, theta)\n            theta = self.gradient_step(theta, theta_grad)\n\n        return theta\n    \n    def fit(self, X, y):\n        m = X.shape[1]\n        start_theta = np.ones(m)\n        self.theta = self.optimize(X, y, start_theta, self._n_iters)\n        \n    def predict(self, X):\n        raise NotImplementedError()\n</pre> class RegOptimizer():     def __init__(self, alpha, n_iters):         self.theta = None         self._alpha = alpha         self._n_iters = n_iters          def gradient_step(self, theta, theta_grad):         return theta - self._alpha * theta_grad          def grad_func(self, X, y, theta):         raise NotImplementedError()      def optimize(self, X, y, start_theta, n_iters):         theta = start_theta.copy()          for i in range(n_iters):             theta_grad = self.grad_func(X, y, theta)             theta = self.gradient_step(theta, theta_grad)          return theta          def fit(self, X, y):         m = X.shape[1]         start_theta = np.ones(m)         self.theta = self.optimize(X, y, start_theta, self._n_iters)              def predict(self, X):         raise NotImplementedError() In\u00a0[30]: Copied! <pre>class LinReg(RegOptimizer):\n\n    def grad_func(self, X, y, theta):\n        n = X.shape[0]\n        grad = 1. / n * X.T.dot(X.dot(theta) - y)\n        return grad\n    \n    def predict(self, X):\n        if self.theta is None:\n            raise Exception('You should train the model first')\n        \n        y_pred = X.dot(self.theta)\n        \n        return y_pred\n</pre> class LinReg(RegOptimizer):      def grad_func(self, X, y, theta):         n = X.shape[0]         grad = 1. / n * X.T.dot(X.dot(theta) - y)         return grad          def predict(self, X):         if self.theta is None:             raise Exception('You should train the model first')                  y_pred = X.dot(self.theta)                  return y_pred In\u00a0[31]: Copied! <pre>linreg_crit = LinReg(0.2,1000)\nlinreg_crit.fit(X, y)\ny_pred = linreg_crit.predict(X)\n\n# \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0448\u0438\u0431\u043e\u043a MSE \u0438 RMSE \u0434\u043b\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\nprint_regression_metrics(y, y_pred)\n</pre> linreg_crit = LinReg(0.2,1000) linreg_crit.fit(X, y) y_pred = linreg_crit.predict(X)  # \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0448\u0438\u0431\u043e\u043a MSE \u0438 RMSE \u0434\u043b\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 print_regression_metrics(y, y_pred) <pre>MSE = 21.89, RMSE = 4.68\n</pre> <p>\u0422\u0435\u043f\u0435\u0440\u044c \u0438\u0437\u043c\u0435\u0440\u0438\u043c \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f</p> In\u00a0[32]: Copied! <pre>%timeit linreg_alg.fit(X, y)\n</pre> %timeit linreg_alg.fit(X, y) <pre>165 \u00b5s \u00b1 1.29 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n</pre> In\u00a0[33]: Copied! <pre>%timeit linreg_crit.fit(X, y)\n</pre> %timeit linreg_crit.fit(X, y) <pre>15.4 ms \u00b1 79.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</pre> In\u00a0[34]: Copied! <pre>linreg_crit.fit(X, y)\n</pre> linreg_crit.fit(X, y) In\u00a0[35]: Copied! <pre># Boston Data. Attribute Information (in order):\n#     - CRIM     per capita crime rate by town\n#     - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n#     - INDUS    proportion of non-retail business acres per town\n#     - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n#     - NOX      nitric oxides concentration (parts per 10 million)\n#     - RM       average number of rooms per dwelling\n#     - AGE      proportion of owner-occupied units built prior to 1940\n#     - DIS      weighted distances to five Boston employment centres\n#     - RAD      index of accessibility to radial highways\n#     - TAX      full-value property-tax rate per `$10000`\n#     - PTRATIO  pupil-teacher ratio by town\n#     - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n#     - LSTAT    % lower status of the population\n#     - MEDV     Median value of owner-occupied homes in $1000's\n</pre> # Boston Data. Attribute Information (in order): #     - CRIM     per capita crime rate by town #     - ZN       proportion of residential land zoned for lots over 25,000 sq.ft. #     - INDUS    proportion of non-retail business acres per town #     - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) #     - NOX      nitric oxides concentration (parts per 10 million) #     - RM       average number of rooms per dwelling #     - AGE      proportion of owner-occupied units built prior to 1940 #     - DIS      weighted distances to five Boston employment centres #     - RAD      index of accessibility to radial highways #     - TAX      full-value property-tax rate per `$10000` #     - PTRATIO  pupil-teacher ratio by town #     - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town #     - LSTAT    % lower status of the population #     - MEDV     Median value of owner-occupied homes in $1000's In\u00a0[36]: Copied! <pre>def prepare_boston_data_new():\n    data = load_boston()\n    X, y = data['data'], data['target']\n    \n    X = np.hstack([X, np.sqrt(X[:, 5:6]), X[:, 6:7] ** 3])\n    \n    # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u043d\u044b\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0439 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438\n    X = (X - X.mean(axis=0)) / X.std(axis=0)\n    # \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438)\n    X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])\n    \n    return X, y\n</pre> def prepare_boston_data_new():     data = load_boston()     X, y = data['data'], data['target']          X = np.hstack([X, np.sqrt(X[:, 5:6]), X[:, 6:7] ** 3])          # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u043d\u044b\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0439 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438     X = (X - X.mean(axis=0)) / X.std(axis=0)     # \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438)     X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])          return X, y In\u00a0[37]: Copied! <pre>def train_validate(X, y):\n    # \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 train/valid\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=1)\n\n    # \u0421\u043e\u0437\u0434\u0430\u0442\u044c \u0438 \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e\n    linreg_alg = LinRegAlgebra()\n    linreg_alg.fit(X_train, y_train)\n\n    # \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043f\u043e \u0432\u0430\u043b\u0438\u0434\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\n    y_pred = linreg_alg.predict(X_valid)\n\n    # \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0448\u0438\u0431\u043e\u043a MSE \u0438 RMSE \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\n    print_regression_metrics(y_valid, y_pred)\n</pre> def train_validate(X, y):     # \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 train/valid     X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=1)      # \u0421\u043e\u0437\u0434\u0430\u0442\u044c \u0438 \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e     linreg_alg = LinRegAlgebra()     linreg_alg.fit(X_train, y_train)      # \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043f\u043e \u0432\u0430\u043b\u0438\u0434\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435     y_pred = linreg_alg.predict(X_valid)      # \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0448\u0438\u0431\u043e\u043a MSE \u0438 RMSE \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445     print_regression_metrics(y_valid, y_pred) In\u00a0[38]: Copied! <pre># \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u0431\u0435\u0437 \u043c\u043e\u0434\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\nX, y = prepare_boston_data()\n# \u041f\u0440\u043e\u0432\u0435\u0441\u0442\u0438 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\ntrain_validate(X, y)\n</pre> # \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u0431\u0435\u0437 \u043c\u043e\u0434\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 X, y = prepare_boston_data() # \u041f\u0440\u043e\u0432\u0435\u0441\u0442\u0438 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442 train_validate(X, y) <pre>MSE = 23.38, RMSE = 4.84\n</pre> In\u00a0[39]: Copied! <pre># \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u0431\u0435\u0437 \u043c\u043e\u0434\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\nX, y = prepare_boston_data_new()\n# \u041f\u0440\u043e\u0432\u0435\u0441\u0442\u0438 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\ntrain_validate(X, y)\n</pre> # \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u0431\u0435\u0437 \u043c\u043e\u0434\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 X, y = prepare_boston_data_new() # \u041f\u0440\u043e\u0432\u0435\u0441\u0442\u0438 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442 train_validate(X, y) <pre>MSE = 14.28, RMSE = 3.78\n</pre> <p>\u041a\u0430\u043a \u0432\u0438\u0434\u043d\u043e \u0438\u0437 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432, \u043c\u044b \u0434\u043e\u0431\u0438\u043b\u0438\u0441\u044c \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439 \u043d\u0430 40%, \u0432\u0441\u0435\u0433\u043e \u043b\u0438\u0448\u044c \u0434\u043e\u0431\u0430\u0432\u0438\u0432 \u043f\u0430\u0440\u0443 \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0432 \u0438\u043c\u0435\u044e\u0449\u0438\u043c\u0441\u044f. \u041c\u043e\u0436\u0435\u0442\u0435 \u043f\u043e\u0438\u0433\u0440\u0430\u0442\u044c \u0441 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c\u0438 \u0438 \u0435\u0449\u0435 \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442.</p> In\u00a0[40]: Copied! <pre># \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0433\u0434\u0430\u0440\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430\nclass RegOptimizer():\n    \n    def __init__(self, alpha, n_iters, limiter=None):\n        self.theta = None\n        self._alpha = alpha\n        self._n_iters = n_iters\n        self._limiter = limiter\n    \n    def gradient_step(self, theta, theta_grad):\n        return theta - self._alpha * theta_grad\n    \n    def grad_func(self, X, y, theta):\n        raise NotImplementedError()\n\n    def optimize(self, X, y, start_theta, n_iters):\n\n        theta = start_theta.copy()\n        for i in range(n_iters):\n            theta_grad = self.grad_func(X, y, theta)\n            theta = self.gradient_step(theta, theta_grad)\n            if(self._limiter != None):\n                if(max(theta_grad)&lt;self._limiter):\n                    print(f'max theta_grad reached: {i}')\n                    break\n        \n        return theta\n    \n    def fit(self, X, y):\n        m = X.shape[1]\n        start_theta = np.ones(m)\n        self.theta = self.optimize(X, y, start_theta, self._n_iters)\n        \n    def predict(self, X):\n        raise NotImplementedError()\n\n# \u041b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0441\u043f\u0443\u0441\u043a\nclass LinReg(RegOptimizer):\n    \n    def grad_func(self, X, y, theta):\n        n = X.shape[0]\n        grad = 1. / n * X.transpose().dot(X.dot(theta) - y)\n        return grad\n    \n    def predict(self, X):\n        if self.theta is None:\n            raise Exception('You should train the model first')\n        \n        y_pred = X.dot(self.theta)\n        \n        return y_pred\n</pre> # \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0433\u0434\u0430\u0440\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430 class RegOptimizer():          def __init__(self, alpha, n_iters, limiter=None):         self.theta = None         self._alpha = alpha         self._n_iters = n_iters         self._limiter = limiter          def gradient_step(self, theta, theta_grad):         return theta - self._alpha * theta_grad          def grad_func(self, X, y, theta):         raise NotImplementedError()      def optimize(self, X, y, start_theta, n_iters):          theta = start_theta.copy()         for i in range(n_iters):             theta_grad = self.grad_func(X, y, theta)             theta = self.gradient_step(theta, theta_grad)             if(self._limiter != None):                 if(max(theta_grad) In\u00a0[41]: Copied! <pre># \u0414\u043b\u044f \u0437\u0430\u0434\u0430\u043d\u0438\u044f 3.7.1 \ndef train_validate_limiter(X, y, limiter=0.01):\n    \n    # \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 train/valid\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                          test_size=0.2, \n                                                          shuffle=True, \n                                                          random_state=1)\n\n    # \u0421\u043e\u0437\u0434\u0430\u0442\u044c \u0438 \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e\n    linreg_alg = LinReg(0.2,1000,limiter)\n    linreg_alg.fit(X_train, y_train)\n\n    # \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043f\u043e \u0432\u0430\u043b\u0438\u0434\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\n    y_pred = linreg_alg.predict(X_valid)\n\n    # \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0448\u0438\u0431\u043e\u043a MSE \u0438 RMSE \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\n    print_regression_metrics(y_valid, y_pred)\n\nX1,y1 = prepare_boston_data()\ntrain_validate_limiter(X1,y1,0.01)\n</pre> # \u0414\u043b\u044f \u0437\u0430\u0434\u0430\u043d\u0438\u044f 3.7.1  def train_validate_limiter(X, y, limiter=0.01):          # \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 train/valid     X_train, X_valid, y_train, y_valid = train_test_split(X, y,                                                            test_size=0.2,                                                            shuffle=True,                                                            random_state=1)      # \u0421\u043e\u0437\u0434\u0430\u0442\u044c \u0438 \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e     linreg_alg = LinReg(0.2,1000,limiter)     linreg_alg.fit(X_train, y_train)      # \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043f\u043e \u0432\u0430\u043b\u0438\u0434\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435     y_pred = linreg_alg.predict(X_valid)      # \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0448\u0438\u0431\u043e\u043a MSE \u0438 RMSE \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445     print_regression_metrics(y_valid, y_pred)  X1,y1 = prepare_boston_data() train_validate_limiter(X1,y1,0.01) <pre>max theta_grad reached: 217\nMSE = 23.40, RMSE = 4.84\n</pre> In\u00a0[42]: Copied! <pre>data = load_boston()\ndata.feature_names\ndf = pd.DataFrame(data.data,columns=data.feature_names)\ndf.head()\n</pre> data = load_boston() data.feature_names df = pd.DataFrame(data.data,columns=data.feature_names) df.head() Out[42]: CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 In\u00a0[43]: Copied! <pre>list(df.columns).index('DIS')\n</pre> list(df.columns).index('DIS') Out[43]: <pre>7</pre> In\u00a0[44]: Copied! <pre># \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \n\ndef prepare_boston_data_new():\n    \n    data = load_boston()\n    X, y = data['data'], data['target']\n    \n    # \u0414\u043e\u0431\u0430\u0432\u044f\u0435\u043c \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\n    X = np.hstack([X, \n                   np.sqrt(X[:, 5:6]), \n                   X[:, 6:7] ** 3])\n    \n    # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u043d\u044b\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \n    # \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0439 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438\n    model = StandardScaler()\n    X = model.fit_transform(X)\n    \n    # \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 \n    # (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438)\n    X = np.hstack([np.ones(X.shape[0])[:, None], X])\n    \n    return X, y\n\n# \u0420\u0430\u0437\u0431\u0442\u0432\u0430\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 train,test\n# \u0422\u0440\u0435\u043d\u0438\u0440\u0443\u0435\u043c \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e (\u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u0445 \u0443\u043c\u043d\u0430\u0436\u0435\u043d\u0438\u0438)\n\ndef train_validate_algebra(X, y):\n    \n    # \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 train/valid\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                          test_size=0.2, \n                                                          shuffle=True, \n                                                          random_state=1)\n\n    # \u0421\u043e\u0437\u0434\u0430\u0442\u044c \u0438 \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e\n    linreg_alg = LinRegAlgebra()\n    linreg_alg.fit(X_train, y_train)\n\n    # \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043f\u043e \u0432\u0430\u043b\u0438\u0434\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\n    y_pred = linreg_alg.predict(X_valid)\n\n    # \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0448\u0438\u0431\u043e\u043a MSE \u0438 RMSE \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\n    print_regression_metrics(y_valid, y_pred)\n    \n# \u0420\u0430\u0437\u0431\u0442\u0432\u0430\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 train,test\n# \u0422\u0440\u0435\u043d\u0438\u0440\u0443\u0435\u043c \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e (\u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0441\u043f\u0443\u0441\u043a)\n    \ndef train_validate(X, y):\n    \n    # \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 train/valid\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2,shuffle=True,random_state=1)\n\n    # \u0421\u043e\u0437\u0434\u0430\u0442\u044c \u0438 \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e\n    linreg_alg = LinReg(0.2,1000)\n    linreg_alg.fit(X_train, y_train)\n\n    # \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043f\u043e \u0432\u0430\u043b\u0438\u0434\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\n    y_pred = linreg_alg.predict(X_valid)\n\n# \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0448\u0438\u0431\u043e\u043a MSE \u0438 RMSE \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\n    print_regression_metrics(y_valid, y_pred)\n</pre> # \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445   def prepare_boston_data_new():          data = load_boston()     X, y = data['data'], data['target']          # \u0414\u043e\u0431\u0430\u0432\u044f\u0435\u043c \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438     X = np.hstack([X,                     np.sqrt(X[:, 5:6]),                     X[:, 6:7] ** 3])          # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u043d\u044b\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e      # \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0439 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438     model = StandardScaler()     X = model.fit_transform(X)          # \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446      # (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438)     X = np.hstack([np.ones(X.shape[0])[:, None], X])          return X, y  # \u0420\u0430\u0437\u0431\u0442\u0432\u0430\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 train,test # \u0422\u0440\u0435\u043d\u0438\u0440\u0443\u0435\u043c \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e (\u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u0445 \u0443\u043c\u043d\u0430\u0436\u0435\u043d\u0438\u0438)  def train_validate_algebra(X, y):          # \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 train/valid     X_train, X_valid, y_train, y_valid = train_test_split(X, y,                                                            test_size=0.2,                                                            shuffle=True,                                                            random_state=1)      # \u0421\u043e\u0437\u0434\u0430\u0442\u044c \u0438 \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e     linreg_alg = LinRegAlgebra()     linreg_alg.fit(X_train, y_train)      # \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043f\u043e \u0432\u0430\u043b\u0438\u0434\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435     y_pred = linreg_alg.predict(X_valid)      # \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0448\u0438\u0431\u043e\u043a MSE \u0438 RMSE \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445     print_regression_metrics(y_valid, y_pred)      # \u0420\u0430\u0437\u0431\u0442\u0432\u0430\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 train,test # \u0422\u0440\u0435\u043d\u0438\u0440\u0443\u0435\u043c \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e (\u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0441\u043f\u0443\u0441\u043a)      def train_validate(X, y):          # \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 train/valid     X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2,shuffle=True,random_state=1)      # \u0421\u043e\u0437\u0434\u0430\u0442\u044c \u0438 \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e     linreg_alg = LinReg(0.2,1000)     linreg_alg.fit(X_train, y_train)      # \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043f\u043e \u0432\u0430\u043b\u0438\u0434\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435     y_pred = linreg_alg.predict(X_valid)  # \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0448\u0438\u0431\u043e\u043a MSE \u0438 RMSE \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445     print_regression_metrics(y_valid, y_pred) In\u00a0[45]: Copied! <pre>from sklearn.preprocessing import StandardScaler\n\n# \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \ndef prepare_boston_data_new_mod():\n    \n    data = load_boston()\n    X, y = data['data'], data['target']\n    \n    # \u0414\u043e\u0431\u0430\u0432\u044f\u0435\u043c \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\n    X = np.hstack([X, \n                   np.sqrt(X[:, 5:6]), \n                   X[:, 6:7] ** 3,\n                   X[:, 7:8] ** 2])\n                   \n# \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u043d\u044b\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \n    # \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0439 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438\n    model = StandardScaler()\n    X = model.fit_transform(X)\n    \n    # \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 \n    # (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438)\n    X = np.hstack([np.ones(X.shape[0])[:, None], X])\n    \n    return X, y\n\nX,y = prepare_boston_data_new_mod()\ntrain_validate_algebra(X,y)\n\n# 3.69\n</pre> from sklearn.preprocessing import StandardScaler  # \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445  def prepare_boston_data_new_mod():          data = load_boston()     X, y = data['data'], data['target']          # \u0414\u043e\u0431\u0430\u0432\u044f\u0435\u043c \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438     X = np.hstack([X,                     np.sqrt(X[:, 5:6]),                     X[:, 6:7] ** 3,                    X[:, 7:8] ** 2])                     # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u043d\u044b\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e      # \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0439 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438     model = StandardScaler()     X = model.fit_transform(X)          # \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446      # (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438)     X = np.hstack([np.ones(X.shape[0])[:, None], X])          return X, y  X,y = prepare_boston_data_new_mod() train_validate_algebra(X,y)  # 3.69 <pre>MSE = 13.59, RMSE = 3.69\n</pre> In\u00a0[46]: Copied! <pre># \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \ndef prepare_boston_data_new_mod():\n    \n    data = load_boston()\n    X, y = data['data'], data['target']\n    \n    # \u0414\u043e\u0431\u0430\u0432\u044f\u0435\u043c \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\n    X = np.hstack([X, \n                   np.sqrt(X[:, 5:6]), \n                   X[:, 6:7] ** 3])\n                   \n    # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u043d\u044b\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \n    # \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0439 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438\n#     model = StandardScaler()\n#     X = model.fit_transform(X)\n    \n    # \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 \n    # (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438)\n    X = np.hstack([np.ones(X.shape[0])[:, None], X])\n    \n    return X, y\n\nX,y = prepare_boston_data_new_mod()\ntrain_validate_algebra(X,y)\n</pre> # \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445  def prepare_boston_data_new_mod():          data = load_boston()     X, y = data['data'], data['target']          # \u0414\u043e\u0431\u0430\u0432\u044f\u0435\u043c \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438     X = np.hstack([X,                     np.sqrt(X[:, 5:6]),                     X[:, 6:7] ** 3])                         # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u043d\u044b\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e      # \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0439 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 #     model = StandardScaler() #     X = model.fit_transform(X)          # \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446      # (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438)     X = np.hstack([np.ones(X.shape[0])[:, None], X])          return X, y  X,y = prepare_boston_data_new_mod() train_validate_algebra(X,y)  <pre>MSE = 14.28, RMSE = 3.78\n</pre>"},{"location":"portfolio/sfml/ml3a_linear_practice2.html#2","title":"\u041b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430 2\u00b6","text":""},{"location":"portfolio/sfml/ml3a_linear_practice2.html","title":"\u0421\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u0432\u00b6","text":"<ul> <li>\u0421\u0440\u0430\u0432\u043d\u0438\u043c \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u0435 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 \u0441 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u043c \u0441\u043f\u0443\u0441\u043a\u0435</li> <li>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u043a \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u044c</li> </ul>"},{"location":"portfolio/sfml/ml3a_linear_practice2.html","title":"\u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0443\u043b\u0438\u00b6","text":""},{"location":"portfolio/sfml/ml3a_linear_practice2.html","title":"\u041a\u043e\u0433\u0434\u0430 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u0435 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438\u00b6","text":"<p>\u0432\u043c\u0435\u0441\u0442\u043e \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0432\u043f\u0443\u0441\u043a\u0430</p>"},{"location":"portfolio/sfml/ml3a_linear_practice2.html","title":"\u041a\u043e\u0433\u0434\u0430 \u043a\u0430\u043a\u043e\u0439 \u043c\u0435\u0442\u043e\u0434 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u00b6","text":"<p>\u0420\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043d\u0430 \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u0445 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u044f\u0445 \u043e\u043f\u0435\u0440\u0435\u0436\u0430\u0435\u0442 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e \u043d\u0430 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u043c \u0441\u043f\u0443\u0441\u043a\u0435 \u0432 \u0441\u043e\u0442\u043d\u0438 \u0440\u0430\u0437</p> <ul> <li>\u041d\u043e \u0432\u0441\u0435\u0433\u0434\u0430 \u043b\u0438 \u044d\u0442\u043e \u0442\u0430\u043a \u0438 \u043a\u0430\u043a\u0438\u0435 \u043f\u043e\u0434\u0432\u043e\u0434\u043d\u044b\u0435 \u043a\u0430\u043c\u043d\u0438 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c?</li> <li>\u041d\u0438\u0436\u0435 \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d \u043d\u0430\u0431\u043e\u0440 \u0441\u043b\u0443\u0447\u0430\u0435\u0432, \u043f\u0440\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0432\u0435\u0440\u0441\u0438\u044f \u0441 \u0433\u0440\u0430\u0434\u0435\u043d\u0442\u043d\u044b\u043c \u0441\u043f\u0443\u0441\u043a\u043e\u043c \u043f\u0440\u0435\u0434\u043f\u043e\u0447\u0442\u0438\u0442\u0435\u043b\u044c\u043d\u0435\u0435:</li> </ul> <ol> <li>\u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0441\u043f\u0443\u0441\u043a \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0431\u044b\u0441\u0442\u0440\u0435\u0435 \u0432 \u0441\u043b\u0443\u0447\u0430\u0435 \u043c\u0430\u0442\u0440\u0438\u0446 \u0441 \u0431\u043e\u043b\u044c\u0448\u0438\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432. \u041e\u0441\u043d\u043e\u0432\u043d\u0430\u044f \u043f\u043e \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u044f \u2014 \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043e\u0431\u0440\u0430\u0442\u043d\u043e\u0439 \u043c\u0430\u0442\u0440\u0438\u0446\u044b $(X^T X)^{-1}$.</li> <li>\u041d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043e\u0431\u0440\u0430\u0442\u043d\u043e\u0439 \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u043c\u043e\u0436\u0435\u0442 \u0442\u0430\u043a\u0436\u0435 \u043f\u043e\u0442\u0440\u0435\u0431\u043e\u0432\u0430\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u0435 \u043e\u043f\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u043e\u0439 \u043f\u0430\u043c\u044f\u0442\u0438</li> <li>\u041c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u0435 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 \u043c\u043e\u0433\u0443\u0442 \u0442\u0430\u043a\u0436\u0435 \u043f\u0440\u043e\u0438\u0433\u0440\u044b\u0432\u0430\u0442\u044c \u0438 \u0432 \u0441\u043b\u0443\u0447\u0430\u0435 \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u043e\u0431\u044a\u0435\u043c\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445, \u043d\u043e \u043f\u0440\u0438 \u043f\u043b\u043e\u0445\u043e\u0439 \u043f\u0430\u0440\u0430\u043b\u043b\u0435\u043b\u044c\u043d\u043e\u0439 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0438\u043b\u0438 \u043d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u044b\u0445 \u0440\u0435\u0441\u0443\u0440\u0441\u0430\u0445.</li> <li>\u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0441\u043f\u0443\u0441\u043a \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0443\u0441\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u0441\u0442\u0432\u043e\u0432\u0430\u043d \u0434\u043e \u0442\u0430\u043a \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u043c\u043e\u0433\u043e \u0441\u0442\u043e\u0445\u0430\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430, (\u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u043e\u0434\u0433\u0440\u0443\u0436\u0430\u044e\u0442\u0441\u044f \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u0438\u043c\u0438 \u043d\u0430\u0431\u043e\u0440\u0430\u043c\u0438), \u0447\u0442\u043e \u0443\u043c\u0435\u043d\u044c\u0448\u0430\u0435\u0442 \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u043e \u043f\u0430\u043c\u044f\u0442\u0438.</li> <li>\u0412 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0441\u043b\u0443\u0447\u0430\u044f\u0445 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0432 \u0441\u043b\u0443\u0447\u0430\u0435 \u043b\u0438\u043d\u0435\u0439\u043d\u043e-\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u044b\u0445 \u0441\u0442\u0440\u043e\u043a) \u0430\u043b\u0433\u0435\u0431\u0440\u0430\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0441\u043f\u043e\u0441\u043e\u0431 \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u043d\u0435 \u0431\u0443\u0434\u0435\u0442 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441\u043e\u0432\u0441\u0435\u043c \u0432 \u0432\u0438\u0434\u0443 \u043d\u0435\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u043d\u0430\u0439\u0442\u0438 \u043e\u0431\u0440\u0430\u0442\u043d\u0443\u044e \u043c\u0430\u0442\u0440\u0438\u0446\u0443.</li> </ol>"},{"location":"portfolio/sfml/ml3a_linear_practice2.html","title":"\u041f\u0440\u0435\u0432\u0440\u0430\u0449\u0435\u043d\u0438\u0435 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0432 \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e\u00b6","text":"<ul> <li>\u041d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0442\u0441\u044f \u043d\u0430\u043c\u043d\u043e\u0433\u043e \u0447\u0430\u0449\u0435 \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445</li> <li>\u041d\u0430 \u0441\u0430\u043c\u043e\u043c \u0434\u0435\u043b\u0435 \u043f\u0440\u043e\u0441\u0442\u0435\u0439\u0448\u0430\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u0430 \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0438\u0432\u0430\u0442\u044c \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438</li> <li>\u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u0438\u0437 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435\u043c \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u0444\u0443\u043d\u043a\u0446\u0438\u0439</li> <li>\u0412\u043e\u0437\u044c\u043c\u0435\u043c \u0443\u0436\u0435 \u0437\u043d\u0430\u043a\u043e\u043c\u044b\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0441 \u0446\u0435\u043d\u0430\u043c\u0438 \u043d\u0430 \u043a\u0432\u0430\u0440\u0442\u0438\u0440\u044b \u0432 \u0411\u043e\u0441\u0442\u043e\u043d\u0435 \u0438 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u0441\u0442\u0430\u043d\u0435\u043c \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0442\u044c \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043a \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c:</li> </ul>"},{"location":"portfolio/sfml/ml3a_linear_practice2.html","title":"\u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\u00b6","text":"<p>\u041c\u044b \u0434\u043e\u0431\u0430\u0432\u0438\u043b\u0438 \u0434\u0432\u0430 \u043d\u043e\u0432\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430:</p> <ol> <li>\u0412\u0437\u044f\u043b\u0438 \u043a\u043e\u0440\u0435\u043d\u044c \u0438\u0437 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 RM (\u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0447\u0438\u0441\u043b\u043e \u043a\u043e\u043c\u043d\u0430\u0442 \u043d\u0430 \u0441\u043e\u0436\u0438\u0442\u0435\u043b\u044f)</li> <li>\u0412\u043e\u0437\u0432\u0435\u043b\u0438 \u0432 \u043a\u0443\u0431 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 AGE</li> </ol> <p>\u042d\u0442\u043e \u0442\u043e\u043b\u044c\u043a\u043e \u0434\u0432\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u0430. \u0412\u0441\u0435\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0445 \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0438 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u043d\u044b\u0445 \u043a \u043d\u0438\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u0439 \u043d\u0435\u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e.</p>"},{"location":"portfolio/sfml/ml3a_linear_practice2.html","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/sfml/ml3a_linear_practice2.html#371","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3.7.1\u00b6","text":"<ul> <li>\u0421\u0434\u0435\u043b\u0430\u0439\u0442\u0435 \u0434\u043b\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430 \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0443 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430, \u0441\u043b\u0438 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0438\u0437 \u0430\u0431\u0441\u043e\u043b\u044e\u0442\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0430 \u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u0441\u044f \u043c\u0435\u043d\u044c\u0448\u0435 0.01.</li> <li>\u0421\u0440\u0430\u0432\u043d\u0438\u0442\u0435 \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u043c \u0441\u043f\u0443\u0441\u043a\u043e\u043c \u0438 \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u043c\u0438 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u044f\u043c\u0438</li> <li>\u0414\u043b\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u0435 alpha = 0.2.</li> <li>\u041d\u0430 \u043a\u0430\u043a\u043e\u0439 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438 \u043e\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0441\u043f\u0443\u0441\u043a?</li> </ul>"},{"location":"portfolio/sfml/ml3a_linear_practice2.html#372","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3.7.2\u00b6","text":"<ul> <li>\u0414\u043e\u0431\u0430\u0432\u044c\u0442\u0435 \u043a \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043a\u0432\u0430\u0434\u0440\u0430\u0442 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 <code>DIS</code> \u0438 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0438\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u044c</li> <li>\u041a\u0430\u043a\u043e\u0439 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0441\u044f RMSE?</li> <li>\u041f\u043e\u0434\u0441\u043a\u0430\u0437\u043a\u0430: \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435 \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u043d\u0443\u044e \u043d\u0430\u043c\u0438 \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e \u043c\u0435\u0442\u043e\u0434\u043e\u043c \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u0445 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0439.</li> </ul>"},{"location":"portfolio/sfml/ml3a_linear_practice2.html#373","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3.7.3\u00b6","text":"<p>\u0423\u0431\u0435\u0440\u0438\u0442\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e \u0438 \u043e\u0441\u0442\u0430\u0432\u044c\u0442\u0435 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 <code>RM</code> \u0438 <code>AGE</code>, \u041a\u0430\u043a\u043e\u0439 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0441\u044f RMSE?</p>"},{"location":"portfolio/sfml/ml3a_linear_prep.html","title":"Ml3a linear prep","text":"In\u00a0[2]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline In\u00a0[3]: Copied! <pre>#\u041f\u043e\u0434\u0433\u0440\u0443\u0437\u0438\u043c \u0444\u0430\u0439\u043b \u0438 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0435\u0433\u043e \u043f\u043a\u0440\u0432\u044b\u0435 \u0441\u0442\u0440\u043e\u043a\u0438\ndata = pd.read_csv('data_flats2.csv.xls',sep=';')\ndata.head()\n</pre> #\u041f\u043e\u0434\u0433\u0440\u0443\u0437\u0438\u043c \u0444\u0430\u0439\u043b \u0438 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0435\u0433\u043e \u043f\u043a\u0440\u0432\u044b\u0435 \u0441\u0442\u0440\u043e\u043a\u0438 data = pd.read_csv('data_flats2.csv.xls',sep=';') data.head() Out[3]: id full_sq life_sq floor preschool_education_centers_raion school_education_centers_raion school_education_centers_top_20_raion university_top_20_raion sport_objects_raion additional_education_raion ... shopping_centers_raion metro_km_walk kindergarten_km school_km park_km green_zone_km mkad_km bulvar_ring_km kremlin_km price_doc 0 1 43 27.0 4.0 5 5 0 0 7 3 ... 16 1.131260 0.145700 0.177975 2.158587 0.600973 1.422391 13.675657 15.156211 5850000 1 2 34 19.0 3.0 5 8 0 0 6 1 ... 3 0.635053 0.147754 0.273345 0.550690 0.065321 9.503405 8.132640 8.698054 6000000 2 3 43 29.0 2.0 4 7 0 0 5 1 ... 0 1.445960 0.049102 0.158072 0.374848 0.453172 5.604800 8.054252 9.067885 5700000 3 4 89 50.0 9.0 9 10 0 0 17 6 ... 11 0.963802 0.179441 0.236455 0.078090 0.106125 2.677824 18.309433 19.487005 13100000 4 5 77 77.0 4.0 7 9 0 2 25 2 ... 10 0.688859 0.247901 0.376838 0.258289 0.236214 11.616653 0.787593 2.578671 16331452 <p>5 rows \u00d7 21 columns</p> <ul> <li>\u0422\u0430\u043a\u0436\u0435 \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u0437\u0430\u043f\u0440\u0430\u0448\u0438\u0432\u0430\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0441 \u0438\u0445 \u0442\u0438\u043f\u0430\u043c\u0438</li> <li>\u0437\u0430\u043e\u0434\u043d\u043e \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0443 \u043d\u0430\u0441 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445</li> </ul> In\u00a0[4]: Copied! <pre>data.info()\n</pre> data.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 30469 entries, 0 to 30468\nData columns (total 21 columns):\n #   Column                                 Non-Null Count  Dtype  \n---  ------                                 --------------  -----  \n 0   id                                     30469 non-null  int64  \n 1   full_sq                                30469 non-null  int64  \n 2   life_sq                                24086 non-null  float64\n 3   floor                                  30302 non-null  float64\n 4   preschool_education_centers_raion      30469 non-null  int64  \n 5   school_education_centers_raion         30469 non-null  int64  \n 6   school_education_centers_top_20_raion  30469 non-null  int64  \n 7   university_top_20_raion                30469 non-null  int64  \n 8   sport_objects_raion                    30469 non-null  int64  \n 9   additional_education_raion             30469 non-null  int64  \n 10  culture_objects_top_25_raion           30469 non-null  int64  \n 11  shopping_centers_raion                 30469 non-null  int64  \n 12  metro_km_walk                          30444 non-null  float64\n 13  kindergarten_km                        30469 non-null  float64\n 14  school_km                              30469 non-null  float64\n 15  park_km                                30469 non-null  float64\n 16  green_zone_km                          30469 non-null  float64\n 17  mkad_km                                30469 non-null  float64\n 18  bulvar_ring_km                         30469 non-null  float64\n 19  kremlin_km                             30469 non-null  float64\n 20  price_doc                              30469 non-null  int64  \ndtypes: float64(10), int64(11)\nmemory usage: 4.9 MB\n</pre> <ul> <li>\u0423 \u043d\u0430\u0441 20 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u043e\u0434\u0438\u043d \u0438\u0437 \u043d\u0438\u0445 <code>id</code> (\u043d\u0435 \u0438\u043c\u0435\u0435\u0442 \u0441\u043c\u044b\u0441\u043b\u043e\u0432\u043e\u0439 \u043d\u0430\u0433\u0440\u0443\u0437\u043a\u0438)</li> <li>\u041e\u0434\u0438\u043d \u0438\u0437 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u043d\u0430\u0448\u0430 \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f <code>price_doc</code> (\u0446\u0435\u043d\u0430 \u043a\u0432\u0430\u0440\u0442\u0438\u0440\u044b \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u0431\u0443\u0434\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c)</li> <li>\u0421\u0440\u0430\u0437\u0443 \u043c\u043e\u0436\u043d\u043e \u0435\u0449\u0435 \u043e\u0442\u043c\u0435\u0442\u0438\u0442\u044c \u043d\u0430\u043b\u0438\u0447\u0438\u0435 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432</li> </ul> In\u00a0[6]: Copied! <pre># data.isnull().sum()\nimport seaborn as sns\n\nfix,ax = plt.subplots(figsize=(10,6))\nsns_heatmap = sns.heatmap(data.isnull(),\n                          yticklabels=False,\n                          cbar=False,cmap='viridis')\n</pre> # data.isnull().sum() import seaborn as sns  fix,ax = plt.subplots(figsize=(10,6)) sns_heatmap = sns.heatmap(data.isnull(),                           yticklabels=False,                           cbar=False,cmap='viridis') <ul> <li>\u0424\u0438\u043e\u043b\u0435\u0442\u043e\u0432\u044b\u043c \u043f\u043e\u043a\u0430\u0437\u0430\u043d\u044b \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u043d\u044b\u0435 \u044f\u0447\u0435\u0439\u043a\u0438,\u0436\u0435\u043b\u0442\u044b\u0435 \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f</li> <li>\u0423 \u043d\u0430\u0441 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438 \u043f\u043e \u0441\u0443\u0442\u0438 \u0442\u043e\u043b\u044c\u043a\u043e \u0432 \u043e\u0434\u043d\u043e\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0435; <code>life_sq</code> (\u043f\u043b\u043e\u0449\u0430\u0434\u044c \u0436\u0438\u043b\u043e\u0439 \u043f\u043b\u043e\u0449\u0430\u0434\u0438),</li> <li>\u041d\u0435 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0435\u0435 \u0432 \u043c\u043e\u0434\u0435\u043b\u0438 \u0442\u0430\u043a \u043a\u0430\u043a \u0434\u043e\u043b\u044f \u043d\u0435 \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0430\u044f</li> </ul> In\u00a0[7]: Copied! <pre>data.price_doc.hist(bins=50)\nplt.show()\n</pre> data.price_doc.hist(bins=50) plt.show() <ul> <li>\u0423 \u043d\u0430\u0448\u0435\u0433\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0435\u0441\u0442\u044c \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u043c\u0430; \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0441\u0438\u043b\u044c\u043d\u044b\u0439 \u043f\u0435\u0440\u0435\u043f\u0430\u0434.</li> <li>\u041c\u043d\u043e\u0433\u043e \u043a\u0432\u0430\u0440\u0442\u0438\u0440 \u0432 \u0441\u0440\u0435\u0434\u043d\u0435\u043c \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0435, \u043d\u043e \u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u043c\u043d\u043e\u0433\u043e \u0434\u043e\u0440\u043e\u0433\u0438\u0445 \u043a\u0432\u0430\u0440\u0442\u0438\u0440</li> <li>\u041f\u0440\u0438\u043c\u0435\u043d\u0438\u043c \u0447\u0430\u0441\u0442\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0435\u043c\u0443\u044e \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443; \u043b\u043e\u0433\u0430\u0440\u0438\u0444\u043c\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u0435\u043e\u0431\u0440\u043e\u0436\u0435\u043d\u0438\u0435</li> <li>\u042d\u0442\u043e \u0434\u0435\u043b\u0430\u0435\u0438\u0441\u044f \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u0443\u043c\u0435\u043d\u044c\u0448\u0438\u0442\u044c \u043f\u0435\u0440\u0435\u043f\u0430\u0434 \u0438 \u0441\u0433\u043b\u0430\u0434\u0438\u0442\u044c \u0445\u0432\u043e\u0441\u0442\u043e\u0432\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435</li> </ul> In\u00a0[8]: Copied! <pre>data['price_doc'] = data['price_doc'].apply(lambda x: np.log(x+1))\ndata['price_doc'].hist(bins=50)\nplt.show()\n</pre> data['price_doc'] = data['price_doc'].apply(lambda x: np.log(x+1)) data['price_doc'].hist(bins=50) plt.show() In\u00a0[9]: Copied! <pre>sns.set(font_scale=1)\nplt.subplots(figsize=(15,15))\nsns.heatmap(data.corr(),square=True,annot=True,\n            fmt=\".2f\",lw=0.1,cmap='RdBu')\n</pre> sns.set(font_scale=1) plt.subplots(figsize=(15,15)) sns.heatmap(data.corr(),square=True,annot=True,             fmt=\".2f\",lw=0.1,cmap='RdBu') Out[9]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x110391780&gt;</pre> <p>\u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0443 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0441\u0438\u043b\u044c\u043d\u0430\u044f \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u044c:</p> <ul> <li>\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0448\u043a\u043e\u043b \u0438 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0434\u0435\u0442\u0441\u043a\u0438\u0445 \u0441\u0430\u0434\u043e\u0432 <code>preschool_education_centres_raion</code> <code>school_education_centres_raion</code> (\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c <code>school_education_centres_raion</code>)</li> <li>\u0420\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0435 \u0434\u043e \u0448\u043a\u043e\u043b \u0438 \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u044f \u0434\u043e \u0434\u0435\u0442\u0441\u043a\u0438\u0445 \u0441\u0430\u0434\u043e\u0432 <code>kindergarten_km</code> <code>school_km</code> (\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c <code>school_km</code>)</li> <li>\u0420\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u044f \u0434\u043e \u043f\u0430\u0440\u043a\u0430 \u0438 \u0434\u043e \u043c\u0435\u0442\u0440\u043e <code>park_km</code> <code>metro_km_walk</code> (\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c <code>metro_km_walk</code>)</li> <li>\u0420\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u044f \u0434\u043e \u043a\u0440\u0435\u043c\u043b\u044f \u0438 \u0434\u043e \u0431\u0443\u043b\u044c\u0432\u0430\u0440\u043d\u043e\u0433\u043e \u043a\u043e\u0433\u044c\u0446\u0430 <code>bulvar_ring_km</code> <code>kremlin_km</code> (\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c <code>bulvar_ring_km</code>)</li> </ul> In\u00a0[2]: Copied! <pre># \u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0443\u0431\u0438\u0440\u0430\u0442\u044c\ndrop_columns = ['preschool_education_centers_raion',\n                'kindergarten_km','park_km','kremlin_km',\n                'life_sq','id']\n\ndata.drop(drop_columns,axis=1,inplace=True)\ndata = data.dropna()\n</pre> # \u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0443\u0431\u0438\u0440\u0430\u0442\u044c drop_columns = ['preschool_education_centers_raion',                 'kindergarten_km','park_km','kremlin_km',                 'life_sq','id']  data.drop(drop_columns,axis=1,inplace=True) data = data.dropna() In\u00a0[3]: Copied! <pre>from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler,StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error as MSE\n\ndef evaluate(target):\n    \n    y = data['price_doc'].copy()\n    X = data.drop(['price_doc'],axis=1)\n\n# \u0440\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u043c \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u0430\u0447\u043d\u0443\u044e \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443\n    X_train, X_test, y_train, y_test = train_test_split(X,y,\n                                                        random_state=77,\n                                                        test_size=0.2)\n\n    # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0438 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\n    scaler = RobustScaler()\n    X_train_sca = scaler.fit_transform(X_train)\n    X_test_sca = scaler.transform(X_test)\n\n    # \u0422\u0440\u0435\u043d\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\n    model = LinearRegression()\n    model.fit(X_train_sca,y_train)\n    y_pred_train = model.predict(X_train_sca)\n    y_pred_test = model.predict(X_test_sca)\n\n    # \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c MSE (\u043d\u0435\u0437\u0430\u0431\u044b\u0432\u0430\u0435\u043c \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u0442\u044c \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e)\n    error_mse = np.round(MSE(np.exp(y_test) - 1, \n                             np.exp(y_pred_test) - 1))\n    print(error_mse)\n    \nevaluate('price_doc')\n</pre> from sklearn.model_selection import train_test_split from sklearn.preprocessing import RobustScaler,StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error as MSE  def evaluate(target):          y = data['price_doc'].copy()     X = data.drop(['price_doc'],axis=1)  # \u0440\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u043c \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u0430\u0447\u043d\u0443\u044e \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443     X_train, X_test, y_train, y_test = train_test_split(X,y,                                                         random_state=77,                                                         test_size=0.2)      # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0438 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e     scaler = RobustScaler()     X_train_sca = scaler.fit_transform(X_train)     X_test_sca = scaler.transform(X_test)      # \u0422\u0440\u0435\u043d\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c     model = LinearRegression()     model.fit(X_train_sca,y_train)     y_pred_train = model.predict(X_train_sca)     y_pred_test = model.predict(X_test_sca)      # \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c MSE (\u043d\u0435\u0437\u0430\u0431\u044b\u0432\u0430\u0435\u043c \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u0442\u044c \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e)     error_mse = np.round(MSE(np.exp(y_test) - 1,                               np.exp(y_pred_test) - 1))     print(error_mse)      evaluate('price_doc') <pre>16818684371715.0\n</pre>"},{"location":"portfolio/sfml/ml3a_linear_prep.html","title":"\u041b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430\u00b6","text":""},{"location":"portfolio/sfml/ml3a_linear_prep.html","title":"\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0421\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0438 \u041a\u0432\u0430\u0440\u0442\u0438\u0440\u044b\u00b6","text":""},{"location":"portfolio/sfml/ml3a_linear_prep.html","title":"\u0414\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 \u041c\u043e\u0441\u043a\u0432\u044b\u00b6","text":"<ul> <li>\u0412 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u043c \u043c\u043e\u0434\u0443\u043b\u0435 \u043c\u044b \u0441\u0434\u0435\u043b\u0430\u043b\u0438 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e \u043a\u0432\u0430\u0440\u0442\u0438\u0440\u0430\u0445, \u0432\u044b\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0445 \u043d\u0430 \u043f\u0440\u043e\u0434\u0430\u0436\u0443</li> <li>\u0414\u0432\u0435 \u0432\u0430\u0436\u043d\u044b\u0435 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0434\u043b\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439; <code>\u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u0430\u0446\u0438\u044f</code> \u0438 \u0443\u0431\u043e\u0440\u043a\u0430 <code>\u043c\u0443\u043b\u044c\u0442\u0438\u043a\u043e\u043b\u043b\u0438\u0430\u043b\u044c\u043d\u044b\u0445</code> \u0444\u0438\u0447\u0435\u0439</li> <li>\u0414\u0430\u043d\u043d\u044b\u0435 \u0433\u043e\u0442\u043e\u0432\u044b \u0434\u043b\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u043e\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438</li> <li>\u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u0446\u0435\u043d\u0443 \u043d\u0430 \u043a\u0432\u0430\u0440\u0442\u0438\u0440\u044b \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043a\u0432\u0430\u0440\u0442\u0438\u0440\u044b</li> </ul>"},{"location":"portfolio/sfml/ml3a_linear_prep.html","title":"\u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438\u00b6","text":""},{"location":"portfolio/sfml/ml3a_linear_prep.html","title":"\u0427\u0438\u0442\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\u00b6","text":"<ul> <li>\u041f\u043e\u0434\u0433\u0440\u0443\u0437\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0438 \u043f\u043e\u0441\u0442\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043f\u0435\u0440\u0432\u044b\u0435 \u0441\u0442\u0440\u043e\u043a\u0438</li> </ul>"},{"location":"portfolio/sfml/ml3a_linear_prep.html","title":"\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432\u00b6","text":"<ul> <li>\u041e\u0434\u0438\u043d \u0438\u0437 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u043d\u0430\u0448\u0430 \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f <code>price_doc</code> (\u0446\u0435\u043d\u0430 \u043a\u0432\u0430\u0440\u0442\u0438\u0440\u044b), \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c</li> <li>\u0422\u043e\u0447\u043d\u043e \u0432\u0438\u0434\u0435\u043c \u0447\u0442\u043e \u0435\u0441\u0442\u044c \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438, \u0442\u0430\u043a \u043a\u0430\u043a \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u0438 \u0440\u0430\u0437\u043d\u043e\u0435 \u0443 \u0440\u0430\u0437\u043d\u044b\u0445 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445</li> </ul>"},{"location":"portfolio/sfml/ml3a_linear_prep.html","title":"\u041e\u0446\u0435\u043d\u0438\u043c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439\u00b6","text":"<ul> <li>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u0430\u0448\u0435\u0439 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439, \u0447\u0442\u043e \u0431\u044b \u043f\u043e\u043d\u044f\u0442\u044c \u043a\u0430\u043a\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c</li> </ul>"},{"location":"portfolio/sfml/ml3a_linear_prep.html","title":"\u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0438 \u0434\u043b\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\u00b6","text":"<ul> <li>\u0414\u0430\u043b\u044c\u0448\u0435 \u0431\u0443\u0434\u0435\u043c \u043e\u0442\u0431\u0438\u0440\u0430\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438</li> <li>\u041d\u0443\u0436\u043d\u043e \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0435\u0441\u0442\u044c \u043b\u0438 \u043c\u0443\u043b\u044c\u0442\u0438\u043a\u043e\u043b\u043b\u0438\u043d\u0435\u0430\u0440\u043d\u043e\u0441\u0442\u0438 (\u0441\u0438\u043b\u044c\u043d\u043e\u0439 \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u0438 \u043c\u0435\u0436\u0434\u0443 \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u044b\u043c\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c\u0438) \u0432 \u0434\u0430\u043d\u043d\u044b\u0445</li> <li>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0438 \u0434\u043b\u044f \u043d\u0430\u0448\u0438\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432</li> </ul>"},{"location":"portfolio/sfml/ml3a_linear_prep.html","title":"\u0423\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\u00b6","text":"<ul> <li>\u041f\u043e \u0438\u0442\u043e\u0433\u0430\u043c \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u0440\u043e\u0441\u0443\u0441\u043a\u043e\u0432 \u0438 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u043a\u043e\u0440\u0440\u0435\u043b\u0438\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438</li> <li>\u0422\u0430\u043a\u0436\u0435 \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u0441\u0442\u0440\u043e\u043a\u0438/\u0440\u044f\u0434\u0430 \u0433\u0434\u0435 \u0435\u0441\u0442\u044c \u0445\u043e\u0442\u044c \u043e\u0434\u0438\u043d \u043f\u0440\u043e\u043f\u0443\u0441\u043a</li> </ul>"},{"location":"portfolio/sfml/ml3a_linear_prep.html","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/sfml/ml3a_linear_prep.html#351","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3.5.1\u00b6","text":"<p>\u0423\u0434\u0430\u043b\u0438\u0442\u0435 \u0441\u0442\u0440\u043e\u043a\u0438, \u0433\u0434\u0435 \u0435\u0441\u0442\u044c \u0445\u043e\u0442\u044f \u0431\u044b \u043e\u0434\u0438\u043d \u043f\u0440\u043e\u043f\u0443\u0441\u043a. \u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e, \u0432\u0437\u044f\u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u0440\u0430\u0437\u043c\u0435\u0440\u0430 0.2, random_state=77.\u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0439\u0442\u0435 \u043e\u0442\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0438 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e(!) \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u0438 RobustScaler() (\u0444\u0443\u043d\u043a\u0446\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u043e\u0434\u043e\u0431\u043d\u0430 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u043c \u0432\u0430\u043c \u043c\u0435\u0442\u043e\u0434\u0430\u043c \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438, \u043e\u0434\u043d\u0430\u043a\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u043c\u0435\u0434\u0438\u0430\u043d\u044b \u0438 \u043a\u0432\u0430\u043d\u0442\u0438\u043b\u0438 \u0434\u043b\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043d\u0435 \u0447\u0443\u0432\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u0430 \u043a \u0432\u044b\u0431\u0440\u043e\u0441\u0430\u043c \u0438 \u043c\u043e\u0436\u0435\u0442 \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u0442\u044c \u043a \u043b\u0443\u0447\u0448\u0438\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c). \u0412\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435 MSE , \u043e\u0442\u0432\u0435\u0442 \u0437\u0430\u043f\u0438\u0448\u0438\u0442\u0435 \u0441 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c\u044e \u0434\u043e \u0441\u043e\u0442\u044b\u0445. \u041d\u0435 \u0437\u0430\u0431\u0443\u0434\u044c\u0442\u0435, \u0447\u0442\u043e id \u043d\u0435 \u043d\u0435\u0441\u0435\u0442 \u043d\u0438\u043a\u0430\u043a\u043e\u0439 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043d\u0435 \u043d\u0430\u0434\u043e \u0431\u0440\u0430\u0442\u044c \u0435\u0433\u043e \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043f\u0440\u0435\u0434\u0438\u043a\u0442\u043e\u0440\u0430! \u0422\u0430\u043a\u0436\u0435 \u043d\u0435 \u0437\u0430\u0431\u0443\u0434\u044c\u0442\u0435, \u0447\u0442\u043e \u043c\u044b \u043b\u043e\u0433\u0430\u0440\u0438\u0444\u043c\u0438\u0440\u043e\u0432\u0430\u043b\u0438 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u0443\u044e \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0443\u044e, \u0430 \u0437\u043d\u0430\u0447\u0438\u0442 \u043f\u0440\u0438 \u043f\u043e\u0434\u0441\u0447\u0435\u0442\u0435 MSE \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043d\u0430\u0439\u0442\u0438 \u044d\u043a\u0441\u043f\u043e\u043d\u0435\u043d\u0446\u0438\u0430\u043b\u044c\u043d\u0443\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043e\u0442 ! \u041e\u043a\u0440\u0443\u0433\u043b\u0438\u0442\u0435 \u043e\u0442\u0432\u0435\u0442 \u0434\u043e \u0446\u0435\u043b\u044b\u0445.</p>"},{"location":"portfolio/sfml/ml3b_2_metrics.html","title":"Ml3b 2 metrics","text":"In\u00a0[1]: Copied! <pre>from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n</pre> from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression In\u00a0[2]: Copied! <pre>from sklearn.datasets import load_breast_cancer # \u043f\u043e\u0434\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\nbreast_cancer = load_breast_cancer()\nbreast_cancer.keys()\n</pre> from sklearn.datasets import load_breast_cancer # \u043f\u043e\u0434\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 breast_cancer = load_breast_cancer() breast_cancer.keys() Out[2]: <pre>dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])</pre> In\u00a0[3]: Copied! <pre>Y = breast_cancer.target   # \u041d\u0430\u0448\u0430 \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f, 0 \u2014 \u0435\u0441\u043b\u0438 \u0440\u0430\u043a\u0430 \u043d\u0435\u0442, 1 \u2014 \u0435\u0441\u043b\u0438 \u0435\u0441\u0442\u044c \nX = breast_cancer.data     # X - \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u043f\u043e \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u0440\u0430\u043a \n</pre> Y = breast_cancer.target   # \u041d\u0430\u0448\u0430 \u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f, 0 \u2014 \u0435\u0441\u043b\u0438 \u0440\u0430\u043a\u0430 \u043d\u0435\u0442, 1 \u2014 \u0435\u0441\u043b\u0438 \u0435\u0441\u0442\u044c  X = breast_cancer.data     # X - \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u043f\u043e \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u0440\u0430\u043a  In\u00a0[4]: Copied! <pre>X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.3)\nmodel = LogisticRegression(max_iter=5000)\nmodel.fit(X_train, Y_train)\n</pre> X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.3) model = LogisticRegression(max_iter=5000) model.fit(X_train, Y_train) Out[4]: <pre>LogisticRegression(max_iter=5000)</pre> In\u00a0[5]: Copied! <pre>from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nY_predicted = model.predict(X_val)\nprint('\u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u043e\u0441\u0442\u044c',accuracy_score(Y_val,Y_predicted))\nprint('\u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c',precision_score(Y_val,Y_predicted))\nprint('\u043f\u043e\u043b\u043d\u043e\u0442\u0430',recall_score(Y_val,Y_predicted))\nprint('f1 \u043c\u0435\u0440\u0430',f1_score(Y_val,Y_predicted))\n</pre> from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  Y_predicted = model.predict(X_val) print('\u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u043e\u0441\u0442\u044c',accuracy_score(Y_val,Y_predicted)) print('\u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c',precision_score(Y_val,Y_predicted)) print('\u043f\u043e\u043b\u043d\u043e\u0442\u0430',recall_score(Y_val,Y_predicted)) print('f1 \u043c\u0435\u0440\u0430',f1_score(Y_val,Y_predicted)) <pre>\u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u043e\u0441\u0442\u044c 0.9415204678362573\n\u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c 0.9646017699115044\n\u043f\u043e\u043b\u043d\u043e\u0442\u0430 0.9478260869565217\nf1 \u043c\u0435\u0440\u0430 0.956140350877193\n</pre> In\u00a0[6]: Copied! <pre># 15 political, 20 economic (TP+TN+FP+FN = 35)\n# 15 political (6 political 9 econimic)\n# 20 economic (6 political, 15 economic)\n\nTP = 6; FP = 9\nTN = 14; FN = 6\n\nacc = (TP + TN)/(TP+TN+FP+FN)\nprint(acc)\n</pre> # 15 political, 20 economic (TP+TN+FP+FN = 35) # 15 political (6 political 9 econimic) # 20 economic (6 political, 15 economic)  TP = 6; FP = 9 TN = 14; FN = 6  acc = (TP + TN)/(TP+TN+FP+FN) print(acc) <pre>0.5714285714285714\n</pre> In\u00a0[7]: Copied! <pre>from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\niris = load_iris()\niris.keys()\n</pre> from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  iris = load_iris() iris.keys() Out[7]: <pre>dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])</pre> In\u00a0[8]: Copied! <pre>Y = iris.target   \nX = iris.data\n</pre> Y = iris.target    X = iris.data In\u00a0[9]: Copied! <pre>X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.3, random_state=50)\nmodel = LogisticRegression(max_iter=5000)\nmodel.fit(X_train, Y_train)\n</pre> X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.3, random_state=50) model = LogisticRegression(max_iter=5000) model.fit(X_train, Y_train) Out[9]: <pre>LogisticRegression(max_iter=5000)</pre> In\u00a0[10]: Copied! <pre>from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nY_predicted = model.predict(X_val)\nprint('\u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u043e\u0441\u0442\u044c',round(accuracy_score(Y_val,Y_predicted),3))\n</pre> from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  Y_predicted = model.predict(X_val) print('\u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u043e\u0441\u0442\u044c',round(accuracy_score(Y_val,Y_predicted),3)) <pre>\u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u043e\u0441\u0442\u044c 0.978\n</pre>"},{"location":"portfolio/sfml/ml3b_2_metrics.html","title":"\u041c\u0435\u0442\u0440\u0438\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438\u00b6","text":""},{"location":"portfolio/sfml/ml3b_2_metrics.html","title":"\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0421\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0438 \u041a\u0432\u0430\u0440\u0442\u0438\u0440\u044b\u00b6","text":"<p>\u0422\u0435\u043f\u0435\u0440\u044c \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044e \u0438 \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c \u0440\u0430\u0437\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438. \u0414\u043b\u044f \u043d\u0430\u0447\u0430\u043b\u0430 \u043f\u043e\u0434\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438</p>"},{"location":"portfolio/sfml/ml3b_2_metrics.html","title":"\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\u00b6","text":""},{"location":"portfolio/sfml/ml3b_2_metrics.html","title":"\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u044b\u0435 \u0438 \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u044b\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435\u00b6","text":"<p>\u0417\u0430\u0434\u0430\u0434\u0438\u043c \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u044b\u0435 (Y) \u0438 \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u044b\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 (X)</p>"},{"location":"portfolio/sfml/ml3b_2_metrics.html#traintest","title":"\u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u0438 \u043d\u0430 train/test\u00b6","text":"<p>\u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0443\u044e \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0438 \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043d\u0430\u0448\u0443 \u043c\u043e\u0434\u0435\u043b\u044c:</p>"},{"location":"portfolio/sfml/ml3b_2_metrics.html","title":"\u041e\u0446\u0435\u043d\u043a\u0430 \u041c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<p>\u0418 \u043e\u0446\u0435\u043d\u0438\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0438</p>"},{"location":"portfolio/sfml/ml3b_2_metrics.html","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/sfml/ml3b_2_metrics.html#3b21","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3B.2.1\u00b6","text":"<p>\u0412\u044b \u0441\u043e\u0437\u0434\u0430\u043b\u0438 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0440\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u0442 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0438 \u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043d\u043e\u0432\u043e\u0441\u0442\u0438 \u043d\u0430 \u0434\u0432\u0430 \u0440\u0430\u0437\u043d\u044b\u0445 Telegram-\u043a\u0430\u043d\u0430\u043b\u0430, \u0438 \u0445\u043e\u0442\u0438\u0442\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c \u0435\u0433\u043e \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e.</p> <ul> <li>\u0417\u0430 \u0434\u0435\u043d\u044c \u0432\u044b\u0448\u043b\u043e 15 \u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043d\u043e\u0432\u043e\u0441\u0442\u0435\u0439 \u0438 20 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0445.</li> <li>\u0412\u0430\u0448 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0438\u0437 15 \u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043d\u043e\u0432\u043e\u0441\u0442\u0435\u0439 \u043e\u0442\u043c\u0435\u0442\u0438\u043b 9 \u043a\u0430\u043a \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0435, \u0430 \u0438\u0437 20 \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u2014 6 \u043a\u0430\u043a \u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435.</li> </ul>"},{"location":"portfolio/sfml/ml3b_2_metrics.html#3b22","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3B.2.2\u00b6","text":"<ul> <li>\u0417\u0430\u0433\u0440\u0443\u0437\u0438\u0442\u0435 \u0432\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u044b\u0439 \u0432 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443 sklearn \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043f\u0440\u043e \u0438\u0440\u0438\u0441\u044b \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u0438 load_iris.</li> <li>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u044c \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 (random_state=50, \u0440\u0430\u0437\u043c\u0435\u0440 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 0.3) \u0438 \u0443\u043a\u0430\u0436\u0438\u0442\u0435 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438</li> </ul>"},{"location":"portfolio/sfml/ml3b_3_practice.html","title":"Ml3b 3 practice","text":"In\u00a0[2]: Copied! <pre>import os\nos.listdir()\n</pre> import os os.listdir() Out[2]: <pre>['ml3b_3.ipynb', 'train_mobile.csv', 'ml3b_2.ipynb']</pre> In\u00a0[6]: Copied! <pre>import pandas as pd\n\ndf = pd.read_csv('train_mobile.csv',delimiter=';')\ndf\n</pre> import pandas as pd  df = pd.read_csv('train_mobile.csv',delimiter=';') df Out[6]: battery_power blue clock_speed dual_sim fc four_g int_memory m_dep mobile_wt n_cores ... px_height px_width ram sc_h sc_w talk_time three_g touch_screen wifi price_range 0 842 0 2.2 0 1 0 7 0.6 188 2 ... 20 756 2549 9 7 19 0 0 1 0 1 1021 1 0.5 1 0 1 53 0.7 136 3 ... 905 1988 2631 17 3 7 1 1 0 1 2 563 1 0.5 1 2 1 41 0.9 145 5 ... 1263 1716 2603 11 2 9 1 1 0 1 3 615 1 2.5 0 0 0 10 0.8 131 6 ... 1216 1786 2769 16 8 11 1 0 0 1 4 1821 1 1.2 0 13 1 44 0.6 141 2 ... 1208 1212 1411 8 2 15 1 1 0 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 1995 794 1 0.5 1 0 1 2 0.8 106 6 ... 1222 1890 668 13 4 19 1 1 0 0 1996 1965 1 2.6 1 0 0 39 0.2 187 4 ... 915 1965 2032 11 10 16 1 1 1 1 1997 1911 0 0.9 1 1 1 36 0.7 108 8 ... 868 1632 3057 9 1 5 1 1 0 1 1998 1512 0 0.9 0 4 1 46 0.1 145 5 ... 336 670 869 18 10 19 1 1 1 0 1999 510 1 2.0 1 5 1 45 0.9 168 6 ... 483 754 3919 19 4 2 1 1 1 1 <p>2000 rows \u00d7 21 columns</p> <p>\u041a\u043e\u0440\u0440\u0435\u043b\u0438\u0440\u0443\u044e\u0449\u0438\u0439\u0441\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438:</p> <ul> <li><code>ram</code> (0.822),</li> <li><code>battery_power</code> (0.149),</li> <li><code>px_width</code> (0.117),</li> <li><code>px_height</code> (0.098)</li> <li><code>touch_screen</code> (0.04)</li> </ul> In\u00a0[14]: Copied! <pre>most_corr = list(df.corr()['price_range'].abs().sort_values(ascending=False)[:6].index[1:])\nmost_corr\n</pre> most_corr = list(df.corr()['price_range'].abs().sort_values(ascending=False)[:6].index[1:]) most_corr Out[14]: <pre>['ram', 'battery_power', 'px_width', 'px_height', 'touch_screen']</pre> In\u00a0[4]: Copied! <pre>from sklearn.model_selection import train_test_split as tts\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_score as precision\n\ny = df['price_range'].copy()\nX = df.drop(['price_range'],axis=1)\nX = X[most_corr]\nX\n</pre> from sklearn.model_selection import train_test_split as tts from sklearn.linear_model import LogisticRegression from sklearn.metrics import precision_score as precision  y = df['price_range'].copy() X = df.drop(['price_range'],axis=1) X = X[most_corr] X Out[4]: ram battery_power px_width px_height touch_screen 0 2549 842 756 20 0 1 2631 1021 1988 905 1 2 2603 563 1716 1263 1 3 2769 615 1786 1216 0 4 1411 1821 1212 1208 1 ... ... ... ... ... ... 1995 668 794 1890 1222 1 1996 2032 1965 1965 915 1 1997 3057 1911 1632 868 1 1998 869 1512 670 336 1 1999 3919 510 754 483 1 <p>2000 rows \u00d7 5 columns</p> In\u00a0[9]: Copied! <pre>X_train,X_test,y_train,y_test = tts(X,y,test_size=0.2,random_state=31)\n\nmodel = LogisticRegression(random_state=31)\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\n\nprint(f'precision: {round(precision(y_test,y_pred),4)}')\n</pre> X_train,X_test,y_train,y_test = tts(X,y,test_size=0.2,random_state=31)  model = LogisticRegression(random_state=31) model.fit(X_train,y_train) y_pred = model.predict(X_test)  print(f'precision: {round(precision(y_test,y_pred),4)}') <pre>precision: 0.9859\n</pre>"},{"location":"portfolio/sfml/ml3b_3_practice.html","title":"\u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430\u00b6","text":""},{"location":"portfolio/sfml/ml3b_3_practice.html","title":"\u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0438 \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u0430\u00b6","text":"<p>\u0423 \u0432\u0430\u0441 \u0435\u0441\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 \u043c\u043e\u0431\u0438\u043b\u044c\u043d\u044b\u0445 \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u043e\u0432. \u041f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f price_range \u043e\u0442\u0432\u0435\u0447\u0430\u0435\u0442 \u0437\u0430 \u0442\u043e, \u043a \u043a\u0430\u043a\u043e\u0439 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0438 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0441\u044f \u0442\u0435\u043b\u0435\u0444\u043e\u043d: 1 \u2014 \u0434\u043e\u0440\u043e\u0433\u0438\u0435, 0 \u2014 \u0434\u0435\u0448\u0435\u0432\u044b\u0435</p> <p>\u0412\u0430\u0448\u0430 \u0437\u0430\u0434\u0430\u0447\u0430 \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0432 \u0442\u043e\u043c, \u0447\u0442\u043e\u0431\u044b \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0442\u043e\u0447\u043d\u043e \u043d\u0430\u0443\u0447\u0438\u0442\u044c\u0441\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u044b \u043f\u043e \u044d\u0442\u0438\u043c \u0434\u0432\u0443\u043c \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f\u043c \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u0438\u0438 \u0434\u0440\u0443\u0433\u0438\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432.</p>"},{"location":"portfolio/sfml/ml3b_3_practice.html","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/sfml/ml3b_3_practice.html#3b31","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3B.3.1\u00b6","text":"<p>\u0412\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u043f\u044f\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u0443 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043d\u0430\u0438\u0431\u043e\u043b\u044c\u0448\u0430\u044f \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u044c \u0441 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 (\u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0438). \u041e\u0442\u043c\u0435\u0442\u044c\u0442\u0435 \u043e\u0442\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438:</p>"},{"location":"portfolio/sfml/ml3b_3_practice.html#3b32","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3B.3.2\u00b6","text":"<p>\u0422\u0435\u043f\u0435\u0440\u044c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c.  \u0414\u043b\u044f \u043d\u0430\u0447\u0430\u043b\u0430 \u0440\u0430\u0437\u0431\u0435\u0439\u0442\u0435 \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0438 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0443\u044e, \u0440\u0430\u0437\u043c\u0435\u0440 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0437\u0430\u0434\u0430\u0439\u0442\u0435 0.2. \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 random_state=31. \u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0432\u043e\u0437\u044c\u043c\u0438\u0442\u0435 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e. \u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043f\u0440\u0435\u0434\u0438\u043a\u0442\u043e\u0440\u043e\u0432 \u0432\u043e\u0437\u044c\u043c\u0438\u0442\u0435 \u043f\u044f\u0442\u044c \u0440\u0430\u043d\u0435\u0435 \u043e\u0442\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432.</p> <p>\u0420\u0430\u0441\u0441\u0447\u0438\u0442\u0430\u0439\u0442\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0443, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u043e\u043a\u0430\u0436\u0435\u0442</p> <ul> <li>\u043a\u0430\u043a\u0430\u044f \u0434\u043e\u043b\u044f \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u043e\u0432, \u043e\u0431\u043e\u0437\u043d\u0430\u0447\u0435\u043d\u043d\u044b\u0445 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u043e\u043c \u043a\u0430\u043a \u0434\u043e\u0440\u043e\u0433\u0438\u0435, \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0441\u044f \u043a \u044d\u0442\u043e\u0439 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0438.</li> <li>\u042d\u0442\u043e precision  (\u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c)</li> </ul>"},{"location":"portfolio/sfml/ml3b_3_practice.html#3b33","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3B.3.3\u00b6","text":"<p>\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435</p>"},{"location":"portfolio/sfml/ml3b_6_practice.html","title":"Ml3b 6 practice","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error, f1_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\n</pre> import numpy as np import pandas as pd from sklearn.datasets import load_boston from sklearn.metrics import mean_squared_error, f1_score, accuracy_score from sklearn.model_selection import train_test_split from matplotlib import pyplot as plt <p>\u0421\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0439 <code>\u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u0448\u0438\u0431\u043a\u0438</code> \u0440\u0430\u0432\u0435\u043d:</p> <p>$$\\nabla L=\\frac{1}{n}\\sum_{i=1}^{n}{(h_{\\theta}(x_i)-y_i)x_i}$$</p> <p>\u0420\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u0431\u0443\u0434\u0435\u0442 \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u0430 \u043d\u0430 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u0448\u0438\u0431\u043a\u0438 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u043c \u0441\u043f\u0443\u0441\u043a\u043e\u043c.</p> In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error, f1_score, accuracy_score, roc_curve, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\n</pre> import numpy as np import pandas as pd from sklearn.datasets import load_boston from sklearn.metrics import mean_squared_error, f1_score, accuracy_score, roc_curve, roc_auc_score from sklearn.model_selection import train_test_split from matplotlib import pyplot as plt In\u00a0[3]: Copied! <pre>adult = pd.read_csv('adult.data',\n                    names=['age', 'workclass', \n                           'fnlwgt', 'education',\n                           'education-num', \n                           'marital-status', \n                           'occupation',\n                           'relationship', \n                           'race', 'sex', \n                           'capital-gain',\n                           'capital-loss',\n                           'hours-per-week',\n                           'native-country', \n                           'salary'])\n\nadult.head()\n</pre> adult = pd.read_csv('adult.data',                     names=['age', 'workclass',                             'fnlwgt', 'education',                            'education-num',                             'marital-status',                             'occupation',                            'relationship',                             'race', 'sex',                             'capital-gain',                            'capital-loss',                            'hours-per-week',                            'native-country',                             'salary'])  adult.head() Out[3]: age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary 0 39 State-gov 77516 Bachelors 13 Never-married Adm-clerical Not-in-family White Male 2174 0 40 United-States &lt;=50K 1 50 Self-emp-not-inc 83311 Bachelors 13 Married-civ-spouse Exec-managerial Husband White Male 0 0 13 United-States &lt;=50K 2 38 Private 215646 HS-grad 9 Divorced Handlers-cleaners Not-in-family White Male 0 0 40 United-States &lt;=50K 3 53 Private 234721 11th 7 Married-civ-spouse Handlers-cleaners Husband Black Male 0 0 40 United-States &lt;=50K 4 28 Private 338409 Bachelors 13 Married-civ-spouse Prof-specialty Wife Black Female 0 0 40 Cuba &lt;=50K In\u00a0[4]: Copied! <pre># \u0418\u0437\u0431\u0430\u0432\u0438\u0442\u044c\u0441\u044f \u043e\u0442 \u043b\u0438\u0448\u043d\u0438\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\nadult.drop(['native-country'], axis=1, inplace=True)\n\n# \u0421\u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0432 \u0431\u0438\u043d\u0430\u0440\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\nadult['salary'] = (adult['salary'] != ' &lt;=50K').astype('int32')\n\n# \u0421\u0434\u0435\u043b\u0430\u0442\u044c one-hot encoding \u0434\u043b\u044f \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\nadult = pd.get_dummies(adult, columns=['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex'])\nadult.head()\n</pre> # \u0418\u0437\u0431\u0430\u0432\u0438\u0442\u044c\u0441\u044f \u043e\u0442 \u043b\u0438\u0448\u043d\u0438\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 adult.drop(['native-country'], axis=1, inplace=True)  # \u0421\u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0432 \u0431\u0438\u043d\u0430\u0440\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f adult['salary'] = (adult['salary'] != ' &lt;=50K').astype('int32')  # \u0421\u0434\u0435\u043b\u0430\u0442\u044c one-hot encoding \u0434\u043b\u044f \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 adult = pd.get_dummies(adult, columns=['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex']) adult.head() Out[4]: age fnlwgt education-num capital-gain capital-loss hours-per-week salary workclass_ ? workclass_ Federal-gov workclass_ Local-gov ... relationship_ Own-child relationship_ Unmarried relationship_ Wife race_ Amer-Indian-Eskimo race_ Asian-Pac-Islander race_ Black race_ Other race_ White sex_ Female sex_ Male 0 39 77516 13 2174 0 40 0 0 0 0 ... 0 0 0 0 0 0 0 1 0 1 1 50 83311 13 0 0 13 0 0 0 0 ... 0 0 0 0 0 0 0 1 0 1 2 38 215646 9 0 0 40 0 0 0 0 ... 0 0 0 0 0 0 0 1 0 1 3 53 234721 7 0 0 40 0 0 0 0 ... 0 0 0 0 0 1 0 0 0 1 4 28 338409 13 0 0 40 0 0 0 0 ... 0 0 1 0 0 1 0 0 1 0 <p>5 rows \u00d7 67 columns</p> <ul> <li>\u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435, \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043c \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043a 0, \u0430 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0435 \u043e\u0442\u043a\u043b\u043e\u043d\u0438\u0435\u043d\u0438\u0435 \u043a 1</li> </ul> In\u00a0[5]: Copied! <pre># \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u043d\u0443\u0436\u0434\u0430\u044e\u0449\u0438\u0435\u0441\u044f \u0432 \u044d\u0442\u043e\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\na_features = adult[['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']].values\nnorm_features = (a_features - a_features.mean(axis=0)) / a_features.std(axis=0)\nadult.loc[:, ['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']] = norm_features\nadult.head()\n</pre> # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u043d\u0443\u0436\u0434\u0430\u044e\u0449\u0438\u0435\u0441\u044f \u0432 \u044d\u0442\u043e\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 a_features = adult[['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']].values norm_features = (a_features - a_features.mean(axis=0)) / a_features.std(axis=0) adult.loc[:, ['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']] = norm_features adult.head() Out[5]: age fnlwgt education-num capital-gain capital-loss hours-per-week salary workclass_ ? workclass_ Federal-gov workclass_ Local-gov ... relationship_ Own-child relationship_ Unmarried relationship_ Wife race_ Amer-Indian-Eskimo race_ Asian-Pac-Islander race_ Black race_ Other race_ White sex_ Female sex_ Male 0 0.030671 -1.063611 1.134739 0.148453 -0.21666 -0.035429 0 0 0 0 ... 0 0 0 0 0 0 0 1 0 1 1 0.837109 -1.008707 1.134739 -0.145920 -0.21666 -2.222153 0 0 0 0 ... 0 0 0 0 0 0 0 1 0 1 2 -0.042642 0.245079 -0.420060 -0.145920 -0.21666 -0.035429 0 0 0 0 ... 0 0 0 0 0 0 0 1 0 1 3 1.057047 0.425801 -1.197459 -0.145920 -0.21666 -0.035429 0 0 0 0 ... 0 0 0 0 0 1 0 0 0 1 4 -0.775768 1.408176 1.134739 -0.145920 -0.21666 -0.035429 0 0 0 0 ... 0 0 1 0 0 1 0 0 1 0 <p>5 rows \u00d7 67 columns</p> In\u00a0[6]: Copied! <pre># \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0442\u0430\u0431\u043b\u0438\u0446\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u043c\u0430\u0442\u0440\u0438\u0446\u044b X \u0438 y\nX = adult[list(set(adult.columns) - set(['salary']))].values\ny = adult['salary'].values\n\n# \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438)\nX = np.hstack([np.ones(X.shape[0])[:,None], X])\nm = X.shape[1]\nprint(m,'features')\nprint(X.shape[0],'instances')\n</pre> # \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0442\u0430\u0431\u043b\u0438\u0446\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u043c\u0430\u0442\u0440\u0438\u0446\u044b X \u0438 y X = adult[list(set(adult.columns) - set(['salary']))].values y = adult['salary'].values  # \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438) X = np.hstack([np.ones(X.shape[0])[:,None], X]) m = X.shape[1] print(m,'features') print(X.shape[0],'instances') <pre>67 features\n32561 instances\n</pre> In\u00a0[7]: Copied! <pre>from tqdm.notebook import tqdm\n\n# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0421\u0438\u0433\u043c\u043e\u0439\u0434\u0430\ndef sigmoid(X, theta):\n    return 1. / (1. + np.exp(-X.dot(theta)))\n\n# \u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0431\u0438\u043d\u0430\u0440\u043d\u043e\u0439 \u043a\u0440\u043e\u0441\u0441 \u044d\u043d\u0442\u0440\u043e\u043f\u0438\u0438\n# sigmoid(X,theta) - \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438\n# X.theta - \u0440\u0435\u0433\u0440\u0435\u0441\u0438\u0438\n\ndef bin_crossentropy_grad(X, y, theta):\n    n = X.shape[0]\n    grad = 1. / n * X.T.dot(sigmoid(X, theta) - y  )\n    return grad\n\n# \u0428\u0430\u0433 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u043a\u0430\ndef gradient_step(theta, theta_grad, alpha):\n    return theta - alpha * theta_grad\n\n# \u0413\u043b\u0430\u0432\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f \ndef optimize(X, y, grad_func, start_theta, alpha, n_iters):\n    \n    theta = start_theta.copy()\n    \n    for i in tqdm(range(n_iters)):\n        theta_grad = grad_func(X, y, theta)\n        theta = gradient_step(theta, theta_grad, alpha)\n    \n    return theta\n</pre> from tqdm.notebook import tqdm  # \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0421\u0438\u0433\u043c\u043e\u0439\u0434\u0430 def sigmoid(X, theta):     return 1. / (1. + np.exp(-X.dot(theta)))  # \u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0431\u0438\u043d\u0430\u0440\u043d\u043e\u0439 \u043a\u0440\u043e\u0441\u0441 \u044d\u043d\u0442\u0440\u043e\u043f\u0438\u0438 # sigmoid(X,theta) - \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 # X.theta - \u0440\u0435\u0433\u0440\u0435\u0441\u0438\u0438  def bin_crossentropy_grad(X, y, theta):     n = X.shape[0]     grad = 1. / n * X.T.dot(sigmoid(X, theta) - y  )     return grad  # \u0428\u0430\u0433 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u043a\u0430 def gradient_step(theta, theta_grad, alpha):     return theta - alpha * theta_grad  # \u0413\u043b\u0430\u0432\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f  def optimize(X, y, grad_func, start_theta, alpha, n_iters):          theta = start_theta.copy()          for i in tqdm(range(n_iters)):         theta_grad = grad_func(X, y, theta)         theta = gradient_step(theta, theta_grad, alpha)          return theta In\u00a0[8]: Copied! <pre>theta_0 = np.ones(m)\ntheta_0[:10]\n</pre> theta_0 = np.ones(m) theta_0[:10] Out[8]: <pre>array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])</pre> In\u00a0[9]: Copied! <pre># \u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 theta \u043d\u0430 \u0432\u0441\u0435\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\ntheta = optimize(X=X, y=y,\n                 grad_func=bin_crossentropy_grad, # gradient evaluation function\n                 start_theta=theta_0,  # staring theta\n                 alpha=1.0,   # learning rate\n                 n_iters=300)  # number of iterations\n\ntheta[:10]   \n</pre> # \u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 theta \u043d\u0430 \u0432\u0441\u0435\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 theta = optimize(X=X, y=y,                  grad_func=bin_crossentropy_grad, # gradient evaluation function                  start_theta=theta_0,  # staring theta                  alpha=1.0,   # learning rate                  n_iters=300)  # number of iterations  theta[:10]    <pre>  0%|          | 0/300 [00:00&lt;?, ?it/s]</pre> Out[9]: <pre>array([-3.18220152,  2.2171966 ,  0.90971055,  0.42479979, -0.42823004,\n       -0.63788846,  0.02282751,  0.63788356,  0.23652037,  0.768918  ])</pre> In\u00a0[10]: Copied! <pre>def print_logisitc_metrics(y_true, y_pred):\n    acc = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    print(f'acc = {acc:.2f} F1-score = {f1:.2f}')\n\n# \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0438\n# \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 accuracy \u0438 F1-score\ny_pred = sigmoid(X, theta) &gt; 0.5\nprint(y_pred[:10])\nprint_logisitc_metrics(y, y_pred)\n</pre> def print_logisitc_metrics(y_true, y_pred):     acc = accuracy_score(y_true, y_pred)     f1 = f1_score(y_true, y_pred)     print(f'acc = {acc:.2f} F1-score = {f1:.2f}')  # \u0421\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0438 # \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 accuracy \u0438 F1-score y_pred = sigmoid(X, theta) &gt; 0.5 print(y_pred[:10]) print_logisitc_metrics(y, y_pred)  <pre>[False False False False  True  True False False  True  True]\nacc = 0.85 F1-score = 0.65\n</pre> In\u00a0[11]: Copied! <pre># \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 train/valid, \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c theta,\n# \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043e\u0448\u0438\u0431\u043a\u0443 F1-score\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n\ntheta = optimize(X_train, y_train, bin_crossentropy_grad, np.ones(m), 1., 300)\ny_pred = sigmoid(X_valid, theta) &gt; 0.5\n\nprint_logisitc_metrics(y_valid, y_pred)\n</pre> # \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 train/valid, \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c theta, # \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043e\u0448\u0438\u0431\u043a\u0443 F1-score  X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)  theta = optimize(X_train, y_train, bin_crossentropy_grad, np.ones(m), 1., 300) y_pred = sigmoid(X_valid, theta) &gt; 0.5  print_logisitc_metrics(y_valid, y_pred) <pre>  0%|          | 0/300 [00:00&lt;?, ?it/s]</pre> <pre>acc = 0.85 F1-score = 0.65\n</pre> In\u00a0[12]: Copied! <pre>from sklearn.metrics import roc_curve, roc_auc_score\n\n# \u041e\u0442\u0440\u0438\u0441\u043e\u0432\u0430\u0442\u044c ROC \u043a\u0440\u0438\u0432\u0443\u044e\ndef calc_and_plot_roc(y_true, y_pred_proba):\n    # \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f ROC \u043a\u0440\u0438\u0432\u043e\u0439 \u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043f\u043b\u043e\u0449\u0430\u0434\u0438 \u043f\u043e\u0434 \u043a\u0440\u0438\u0432\u043e\u0439 AUC\n    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n    roc_auc = roc_auc_score(y_true, y_pred_proba)\n    \n    plt.figure(figsize=(4, 4))\n    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n    plt.title('Receiver Operating Characteristic', fontsize=10)\n    plt.xlabel('False positive rate (FPR)', fontsize=10)\n    plt.ylabel('True positive rate (TPR)', fontsize=10)\n    plt.legend(fontsize=10)\n</pre> from sklearn.metrics import roc_curve, roc_auc_score  # \u041e\u0442\u0440\u0438\u0441\u043e\u0432\u0430\u0442\u044c ROC \u043a\u0440\u0438\u0432\u0443\u044e def calc_and_plot_roc(y_true, y_pred_proba):     # \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f ROC \u043a\u0440\u0438\u0432\u043e\u0439 \u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043f\u043b\u043e\u0449\u0430\u0434\u0438 \u043f\u043e\u0434 \u043a\u0440\u0438\u0432\u043e\u0439 AUC     fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)     roc_auc = roc_auc_score(y_true, y_pred_proba)          plt.figure(figsize=(4, 4))     plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')     plt.title('Receiver Operating Characteristic', fontsize=10)     plt.xlabel('False positive rate (FPR)', fontsize=10)     plt.ylabel('True positive rate (TPR)', fontsize=10)     plt.legend(fontsize=10) In\u00a0[13]: Copied! <pre># \u0412\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043a\u043b\u0430\u0441\u0441\u0443 1 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u0438\u0437 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\ny_pred_proba = sigmoid(X_valid, theta)\nprint(y_pred_proba)\ncalc_and_plot_roc(y_valid, y_pred_proba)\n</pre> # \u0412\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043a\u043b\u0430\u0441\u0441\u0443 1 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u0438\u0437 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 y_pred_proba = sigmoid(X_valid, theta) print(y_pred_proba) calc_and_plot_roc(y_valid, y_pred_proba) <pre>[1.12714531e-02 7.33161358e-02 5.34950011e-04 ... 3.15595076e-01\n 8.02795296e-01 2.24602462e-01]\n</pre> In\u00a0[14]: Copied! <pre>class regOpt():\n    \n    # initialisation\n    \n    def __init__(self, alpha, n_iters):\n        self.theta = None\n        self._alpha = alpha\n        self._n_iters = n_iters\n    \n    # gradient step\n    \n    def gradient_step(self, theta, theta_grad):\n        return theta - self._alpha * theta_grad\n    \n    # gradient function\n    \n    def grad_func(self, X, y, theta):\n        raise NotImplementedError()\n\n    # optimisation process of theta\n        \n    def optimize(self, X, y, start_theta, n_iters):\n        \n        # theta_0\n        theta = start_theta.copy()\n\n        for _ in range(n_iters):\n            theta_grad = self.grad_func(X, y, theta)\n            theta = self.gradient_step(theta, theta_grad)\n\n        return theta\n    \n    def fit(self, X, y):\n        m = X.shape[1]\n        start_theta = np.ones(m)\n        self.theta = self.optimize(X, y, start_theta, self._n_iters)\n        \n    def predict(self, X):\n        raise NotImplementedError()\n</pre> class regOpt():          # initialisation          def __init__(self, alpha, n_iters):         self.theta = None         self._alpha = alpha         self._n_iters = n_iters          # gradient step          def gradient_step(self, theta, theta_grad):         return theta - self._alpha * theta_grad          # gradient function          def grad_func(self, X, y, theta):         raise NotImplementedError()      # optimisation process of theta              def optimize(self, X, y, start_theta, n_iters):                  # theta_0         theta = start_theta.copy()          for _ in range(n_iters):             theta_grad = self.grad_func(X, y, theta)             theta = self.gradient_step(theta, theta_grad)          return theta          def fit(self, X, y):         m = X.shape[1]         start_theta = np.ones(m)         self.theta = self.optimize(X, y, start_theta, self._n_iters)              def predict(self, X):         raise NotImplementedError() <p>\u0414\u043b\u044f \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0413\u0421 \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0434\u0432\u0435 \u0444\u0443\u043d\u043a\u0438\u0438 \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>regOptimiser</code></p> <ul> <li>\u0412\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0430 <code>grad_func</code></li> <li>\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 <code>predict</code></li> </ul> In\u00a0[15]: Copied! <pre># linReg \u0431\u0443\u0434\u0435\u0442 \u0438\u043c\u0435\u0442\u044c \u0434\u043e\u0441\u0442\u0443\u043f \u043a \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0438\u0437 regOptimiser\n\nclass LinReg(regOpt):\n    \n    # \u0434\u043b\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0438\u0438 \u0443 \u043d\u0430\u0441 \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c\n    \n    def grad_func(self, X, y, theta):\n        n = X.shape[0]\n        grad = 1.0 / n * X.T.dot(X.dot(theta) - y)\n        return grad\n\n    def predict(self, X):\n        if(self.theta is None):\n            raise Exception('You should train the model first')\n        \n        y_pred = X.dot(self.theta)\n        \n        return y_pred\n</pre> # linReg \u0431\u0443\u0434\u0435\u0442 \u0438\u043c\u0435\u0442\u044c \u0434\u043e\u0441\u0442\u0443\u043f \u043a \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0438\u0437 regOptimiser  class LinReg(regOpt):          # \u0434\u043b\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0438\u0438 \u0443 \u043d\u0430\u0441 \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c          def grad_func(self, X, y, theta):         n = X.shape[0]         grad = 1.0 / n * X.T.dot(X.dot(theta) - y)         return grad      def predict(self, X):         if(self.theta is None):             raise Exception('You should train the model first')                  y_pred = X.dot(self.theta)                  return y_pred <ul> <li>\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c, \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u043b\u0438 \u043a\u043b\u0430\u0441\u0441 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430 \u043a\u0430\u043a \u043d\u0443\u0436\u043d\u043e \u0434\u043b\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438</li> </ul> In\u00a0[16]: Copied! <pre>def print_regression_metrics(y_true, y_pred):\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    print(f'MSE = {mse:.2f}, RMSE = {rmse:.2f}')\n    \ndef prepare_boston_data():\n    data = load_boston()\n    X, y = data['data'], data['target']\n    # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u043d\u044b\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0439 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438\n    X = (X - X.mean(axis=0)) / X.std(axis=0)\n    # \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438)\n    X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])\n    \n    return X, y\n\nlinreg = LinReg(0.01, 500)\nX, y = prepare_boston_data()\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n\nlinreg.fit(X_train, y_train)\ny_pred = linreg.predict(X_valid)\nprint_regression_metrics(y_valid, y_pred)\n</pre> def print_regression_metrics(y_true, y_pred):     mse = mean_squared_error(y_true, y_pred)     rmse = np.sqrt(mse)     print(f'MSE = {mse:.2f}, RMSE = {rmse:.2f}')      def prepare_boston_data():     data = load_boston()     X, y = data['data'], data['target']     # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u043d\u044b\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0439 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438     X = (X - X.mean(axis=0)) / X.std(axis=0)     # \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438)     X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])          return X, y  linreg = LinReg(0.01, 500) X, y = prepare_boston_data() X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)  linreg.fit(X_train, y_train) y_pred = linreg.predict(X_valid) print_regression_metrics(y_valid, y_pred) <pre>MSE = 21.89, RMSE = 4.68\n</pre> In\u00a0[17]: Copied! <pre>class LogReg(regOpt):\n    \n    def sigmoid(self, X, theta):\n        return 1. / (1. + np.exp(-X.dot(theta)))\n    \n    def grad_func(self, X, y, theta):\n        n = X.shape[0]\n        grad = 1. / n * X.T.dot(self.sigmoid(X, theta) - y)\n\n        return grad\n    \n    def predict_proba(self, X):\n        return self.sigmoid(X, self.theta)\n    \n    def predict(self, X):\n        if self.theta is None:\n            raise Exception('You should train the model first')\n        \n        y_pred = self.predict_proba(X) &gt; 0.5\n        \n        return y_pred\n</pre> class LogReg(regOpt):          def sigmoid(self, X, theta):         return 1. / (1. + np.exp(-X.dot(theta)))          def grad_func(self, X, y, theta):         n = X.shape[0]         grad = 1. / n * X.T.dot(self.sigmoid(X, theta) - y)          return grad          def predict_proba(self, X):         return self.sigmoid(X, self.theta)          def predict(self, X):         if self.theta is None:             raise Exception('You should train the model first')                  y_pred = self.predict_proba(X) &gt; 0.5                  return y_pred In\u00a0[18]: Copied! <pre>def prepare_adult_data():\n    \n    adult = pd.read_csv('adult.data',\n                        names=['age', 'workclass', 'fnlwgt', 'education',\n                               'education-num', 'marital-status', 'occupation',\n                               'relationship', 'race', 'sex', 'capital-gain',\n                               'capital-loss', 'hours-per-week', 'native-country', 'salary'])\n    \n    # \u0418\u0437\u0431\u0430\u0432\u0438\u0442\u044c\u0441\u044f \u043e\u0442 \u043b\u0438\u0448\u043d\u0438\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    adult.drop(['native-country'], axis=1, inplace=True)\n    # \u0421\u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0432 \u0431\u0438\u043d\u0430\u0440\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\n    adult['salary'] = (adult['salary'] != ' &lt;=50K').astype('int32')\n    # \u0421\u0434\u0435\u043b\u0430\u0442\u044c one-hot encoding \u0434\u043b\u044f \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    adult = pd.get_dummies(adult, columns=['workclass', 'education', 'marital-status', \n                                           'occupation', 'relationship', 'race', 'sex'])\n    \n    # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u043d\u0443\u0436\u0434\u0430\u044e\u0449\u0438\u0435\u0441\u044f \u0432 \u044d\u0442\u043e\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\n    a_features = adult[['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']].values\n    norm_features = (a_features - a_features.mean(axis=0)) / a_features.std(axis=0)\n    adult.loc[:, ['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']] = norm_features\n    \n    # \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0442\u0430\u0431\u043b\u0438\u0446\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u043c\u0430\u0442\u0440\u0438\u0446\u044b X \u0438 y\n    X = adult[list(set(adult.columns) - set(['salary']))].values\n    y = adult['salary'].values\n\n    # \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438)\n    X = np.hstack([np.ones(X.shape[0])[:,None], X])\n    \n    return X, y\n</pre> def prepare_adult_data():          adult = pd.read_csv('adult.data',                         names=['age', 'workclass', 'fnlwgt', 'education',                                'education-num', 'marital-status', 'occupation',                                'relationship', 'race', 'sex', 'capital-gain',                                'capital-loss', 'hours-per-week', 'native-country', 'salary'])          # \u0418\u0437\u0431\u0430\u0432\u0438\u0442\u044c\u0441\u044f \u043e\u0442 \u043b\u0438\u0448\u043d\u0438\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432     adult.drop(['native-country'], axis=1, inplace=True)     # \u0421\u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0432 \u0431\u0438\u043d\u0430\u0440\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f     adult['salary'] = (adult['salary'] != ' &lt;=50K').astype('int32')     # \u0421\u0434\u0435\u043b\u0430\u0442\u044c one-hot encoding \u0434\u043b\u044f \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432     adult = pd.get_dummies(adult, columns=['workclass', 'education', 'marital-status',                                             'occupation', 'relationship', 'race', 'sex'])          # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u043d\u0443\u0436\u0434\u0430\u044e\u0449\u0438\u0435\u0441\u044f \u0432 \u044d\u0442\u043e\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438     a_features = adult[['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']].values     norm_features = (a_features - a_features.mean(axis=0)) / a_features.std(axis=0)     adult.loc[:, ['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']] = norm_features          # \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0442\u0430\u0431\u043b\u0438\u0446\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u043c\u0430\u0442\u0440\u0438\u0446\u044b X \u0438 y     X = adult[list(set(adult.columns) - set(['salary']))].values     y = adult['salary'].values      # \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438)     X = np.hstack([np.ones(X.shape[0])[:,None], X])          return X, y <ul> <li>\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c, \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u043b\u0438 \u043a\u043b\u0430\u0441\u0441 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430 \u043a\u0430\u043a \u043d\u0443\u0436\u043d\u043e \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0438\u0438</li> </ul> In\u00a0[19]: Copied! <pre>logreg = LogReg(1., 300)\nX, y = prepare_adult_data()\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n\n# \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 train/valid, \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c theta,\n# \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043e\u0448\u0438\u0431\u043a\u0443 F1-score\n\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_valid)\nprint_logisitc_metrics(y_valid, y_pred)\n</pre> logreg = LogReg(1., 300) X, y = prepare_adult_data() X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)  # \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 train/valid, \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c theta, # \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043e\u0448\u0438\u0431\u043a\u0443 F1-score  logreg.fit(X_train, y_train) y_pred = logreg.predict(X_valid) print_logisitc_metrics(y_valid, y_pred) <pre>acc = 0.85 F1-score = 0.66\n</pre> In\u00a0[20]: Copied! <pre>y_pred_proba = logreg.predict_proba(X_valid)\ncalc_and_plot_roc(y_valid, y_pred_proba)\n</pre> y_pred_proba = logreg.predict_proba(X_valid) calc_and_plot_roc(y_valid, y_pred_proba) <p>\u041f\u043e\u0441\u043b\u0435 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438 <code>\u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043e\u0448\u0438\u0431\u043a\u0438</code> \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u0431\u0443\u0434\u0435\u0442 \u0432\u044b\u0433\u043b\u044f\u0434\u0435\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c:</p> <p>$$L=\\frac{1}{2n} * \\sum_{i=1}^{n}{(y_i - \\theta^Tx_i)^2} + \\frac{\\lambda}{2m}\\sum_{j}^{m}{\\theta_j^2}$$</p> <p>\u0410 \u0435\u0435 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u043f\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0443 $\\theta$:</p> <p>$$\\nabla L = \\frac{1}{n}\\sum_{i=1}^{n}{(\\theta^Tx_i - y_i) \\cdot x_i} + \\frac{\\lambda}{m}\\sum_{j=1}^{m}{\\theta_j} = \\frac{1}{n}X^T(X\\theta - y) + \\frac{\\lambda}{m}\\sum_{j=1}^{m}{\\theta_j}$$</p> In\u00a0[21]: Copied! <pre>class rLinReg(LinReg):\n    \n    def __init__(self, alpha, lambd, n_iters):\n        super(rLinReg, self).__init__(alpha, n_iters)\n        self._lambd = lambd \n    \n    def grad_func(self, X, y, theta):\n        n = X.shape[0]\n        grad = 1. / n * X.T.dot(X.dot(theta) - y) # \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0439 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c\n        grad_term = self._lambd * np.mean(theta)  # \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0442\u0435\u0440\u043c\u0438\u043d\n        return grad + grad_term\n</pre> class rLinReg(LinReg):          def __init__(self, alpha, lambd, n_iters):         super(rLinReg, self).__init__(alpha, n_iters)         self._lambd = lambd           def grad_func(self, X, y, theta):         n = X.shape[0]         grad = 1. / n * X.T.dot(X.dot(theta) - y) # \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0439 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c         grad_term = self._lambd * np.mean(theta)  # \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0442\u0435\u0440\u043c\u0438\u043d         return grad + grad_term In\u00a0[22]: Copied! <pre>linreg = rLinReg(alpha=0.01, lambd=0.05, n_iters=500)\nX, y = prepare_boston_data()\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n</pre> linreg = rLinReg(alpha=0.01, lambd=0.05, n_iters=500) X, y = prepare_boston_data() X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2) In\u00a0[23]: Copied! <pre>linreg.fit(X_train, y_train)\ny_pred = linreg.predict(X_valid)\nprint_regression_metrics(y_valid, y_pred)\n</pre> linreg.fit(X_train, y_train) y_pred = linreg.predict(X_valid) print_regression_metrics(y_valid, y_pred) <pre>MSE = 26.19, RMSE = 5.12\n</pre> <p>\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043e\u0448\u0438\u0431\u043a\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u0432 \u0441\u043b\u0443\u0447\u0430\u0435 \u0431\u0438\u043d\u0430\u0440\u043d\u043e\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0441 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0442\u043e\u0440\u043e\u043c \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c:</p> <p>$$L=-\\frac{1}{n}(y_i \\log h_{\\theta}(x_i) + (1-y_i) \\log(1-h_{\\theta}(x_i)))+\\frac{\\lambda}{2m}\\sum_{j}^{m}{\\theta_j^2},$$</p> <p>\u0433\u0434\u0435 $x_i$ \u2014 \u0432\u0435\u043a\u0442\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 $i$-\u0433\u043e \u043f\u0440\u0438\u043c\u0435\u0440\u0430 \u0438\u0437 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438, $y_i$ \u2014 \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043a\u043b\u0430\u0441\u0441 \u0434\u043b\u044f \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0433\u043e \u043f\u0440\u0438\u043c\u0435\u0440\u0430 (0 \u0438\u043b\u0438 1), $n$ \u2014 \u0447\u0438\u0441\u043b\u043e \u043f\u0440\u0438\u043c\u0435\u0440\u043e\u0432 \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435, $m$ \u2014 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, $\\lambda$ \u2014 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438, $h_{\\theta}(x)$ \u2014 sigmoid \u0444\u0443\u043d\u043a\u0446\u0438\u044f, \u0440\u0430\u0432\u043d\u0430\u044f:</p> <p>$$h_{\\theta}(x)=\\frac{1}{1+\\exp^{-\\theta x}},$$</p> <p>\u0433\u0434\u0435 $\\theta$ \u2014 \u0432\u0435\u043a\u0442\u043e\u0440 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438, $x$ - \u0432\u0435\u043a\u0442\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u0438\u0437 \u0432\u044b\u0431\u043e\u0440\u043a\u0438.</p> <p>\u0421\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0439 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u0448\u0438\u0431\u043a\u0438 \u0440\u0430\u0432\u0435\u043d:</p> <p>$$\\nabla L=\\frac{1}{n}\\sum_{i=1}^{n}{(h_{\\theta}(x_i)-y_i)x_i}+\\frac{\\lambda}{m}\\sum_{j}^{m}{\\theta_j}$$</p> In\u00a0[24]: Copied! <pre># \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0438\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u0433\u0438\u0431\u043a\u0438\n\nclass rLogReg(LogReg):\n    \n    def __init__(self, alpha, lambd, n_iters):\n        super(rLogReg, self).__init__(alpha, n_iters)\n        self._lambd = lambd\n    \n    # \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0442\u0435\u0440\u043c\u0438\u043d\n    def grad_func(self, X, y, theta):\n        n = X.shape[0]\n        grad = 1.0 / n * X.T.dot(self.sigmoid(X, theta) - y) \n        grad_term = self._lambd * np.mean(theta)\n        return grad + grad_term\n</pre> # \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0438\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u0433\u0438\u0431\u043a\u0438  class rLogReg(LogReg):          def __init__(self, alpha, lambd, n_iters):         super(rLogReg, self).__init__(alpha, n_iters)         self._lambd = lambd          # \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0442\u0435\u0440\u043c\u0438\u043d     def grad_func(self, X, y, theta):         n = X.shape[0]         grad = 1.0 / n * X.T.dot(self.sigmoid(X, theta) - y)          grad_term = self._lambd * np.mean(theta)         return grad + grad_term In\u00a0[25]: Copied! <pre>logreg = rLogReg(alpha=1.0, lambd=1.0, n_iters=300)\nX, y = prepare_adult_data()\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n</pre> logreg = rLogReg(alpha=1.0, lambd=1.0, n_iters=300) X, y = prepare_adult_data() X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2) In\u00a0[26]: Copied! <pre># \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 train/valid, \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c theta,\n# \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043e\u0448\u0438\u0431\u043a\u0443 F1-score\n\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_valid)\n\nprint_logisitc_metrics(y_valid, y_pred)\n</pre> # \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u043d\u0430 train/valid, \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c theta, # \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043e\u0448\u0438\u0431\u043a\u0443 F1-score  logreg.fit(X_train, y_train) y_pred = logreg.predict(X_valid)  print_logisitc_metrics(y_valid, y_pred) <pre>acc = 0.85 F1-score = 0.66\n</pre> In\u00a0[27]: Copied! <pre>y_pred_proba = logreg.predict_proba(X_valid)\ncalc_and_plot_roc(y_valid, y_pred_proba)\n</pre> y_pred_proba = logreg.predict_proba(X_valid) calc_and_plot_roc(y_valid, y_pred_proba) In\u00a0[28]: Copied! <pre>from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\n\nX,y = prepare_adult_data()\n\nmodel = LogisticRegression(max_iter=4000)\nmodel.fit(X,y)\ny_pred = model.predict(X)\nf1_score(y,y_pred)\n</pre> from sklearn.linear_model import LogisticRegression from sklearn.metrics import f1_score  X,y = prepare_adult_data()  model = LogisticRegression(max_iter=4000) model.fit(X,y) y_pred = model.predict(X) f1_score(y,y_pred) Out[28]: <pre>0.6619411888553584</pre> In\u00a0[29]: Copied! <pre>from sklearn.metrics import confusion_matrix\n\nprint(confusion_matrix(y,y_pred))\n</pre> from sklearn.metrics import confusion_matrix  print(confusion_matrix(y,y_pred)) <pre>[[23028  1692]\n [ 3125  4716]]\n</pre> In\u00a0[30]: Copied! <pre>print(y_pred)\npy_pred = model.predict_proba(X)\nprint(py_pred)\n</pre> print(y_pred) py_pred = model.predict_proba(X) print(py_pred) <pre>[0 0 0 ... 0 0 1]\n[[0.87154159 0.12845841]\n [0.59943446 0.40056554]\n [0.97193906 0.02806094]\n ...\n [0.96305526 0.03694474]\n [0.99609306 0.00390694]\n [0.00498354 0.99501646]]\n</pre> In\u00a0[31]: Copied! <pre>calc_and_plot_roc(y,py_pred[:,1])\n</pre> calc_and_plot_roc(y,py_pred[:,1]) In\u00a0[32]: Copied! <pre>model = LogisticRegression(penalty='none',max_iter=3000)\nmodel.fit(X,y)\ny_pred = model.predict(X)\nf1_score(y,y_pred)\n</pre> model = LogisticRegression(penalty='none',max_iter=3000) model.fit(X,y) y_pred = model.predict(X) f1_score(y,y_pred) Out[32]: <pre>0.6615233553093001</pre> In\u00a0[33]: Copied! <pre>f1_vals = []; store_c = []\nfor l2_value in np.arange(0.01,1.0,0.01):\n    model = LogisticRegression(penalty='l2',C=l2_value,max_iter=500)\n    model.fit(X,y)\n    y_pred = model.predict(X)\n    f1_vals.append(f1_score(y,y_pred))\n    store_c.append(l2_value)\n    \nprint(store_c[f1_vals.index(max(f1_vals))],\n      f1_vals[f1_vals.index(max(f1_vals))])  \n</pre> f1_vals = []; store_c = [] for l2_value in np.arange(0.01,1.0,0.01):     model = LogisticRegression(penalty='l2',C=l2_value,max_iter=500)     model.fit(X,y)     y_pred = model.predict(X)     f1_vals.append(f1_score(y,y_pred))     store_c.append(l2_value)      print(store_c[f1_vals.index(max(f1_vals))],       f1_vals[f1_vals.index(max(f1_vals))])   <pre>0.9600000000000001 0.6620815495824268\n</pre> In\u00a0[34]: Copied! <pre>adult = pd.read_csv('adult.data',\n                    names=['age', 'workclass', \n                           'fnlwgt', 'education',\n                           'education-num', \n                           'marital-status', \n                           'occupation',\n                           'relationship', \n                           'race', 'sex', \n                           'capital-gain',\n                           'capital-loss',\n                           'hours-per-week',\n                           'native-country', \n                           'salary'])\n\n# \u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u043c\u043e\u0441\u0442\u044c \n\nvals = adult['native-country'].value_counts()\nlst_index = list(vals[vals&lt;100].index) # index of subsets which are in category other\nadult['native-country'] = adult['native-country'].replace(lst_index,'other')\n\n# \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438 \u0441\u043e\u0437\u0434\u0430\u0442\u044c OHE\n\nohe_native_country = pd.get_dummies(adult['native-country'],drop_first=True)\nupd_adult = adult.drop(['native-country'],axis=1)\nadult = pd.concat([upd_adult,ohe_native_country],axis=1)\n\n# \u0432\u0441\u0435 \u043a\u0430\u043a \u0438 \u0440\u0430\u043d\u044c\u0448\u0435\n\nadult['salary'] = (adult['salary'] != ' &lt;=50K').astype('int32')\nadult['salary'].value_counts()\nadult = pd.get_dummies(adult, columns=['workclass', 'education', 'marital-status', \n                                        'occupation', 'relationship', 'race', 'sex'])\n\n# \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u043d\u0443\u0436\u0434\u0430\u044e\u0449\u0438\u0435\u0441\u044f \u0432 \u044d\u0442\u043e\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\na_features = adult[['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']].values\nnorm_features = (a_features - a_features.mean(axis=0)) / a_features.std(axis=0)\nadult.loc[:, ['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']] = norm_features\n\n# \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0442\u0430\u0431\u043b\u0438\u0446\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u043c\u0430\u0442\u0440\u0438\u0446\u044b X \u0438 y\nX = adult[list(set(adult.columns) - set(['salary']))].values\ny = adult['salary'].values\n\n# \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438)\nX = np.hstack([np.ones(X.shape[0])[:,None], X])\n\nmodel = LogisticRegression(max_iter=3000)\nmodel.fit(X,y)\ny_pred = model.predict(X)\nf1_score(y,y_pred)\n</pre> adult = pd.read_csv('adult.data',                     names=['age', 'workclass',                             'fnlwgt', 'education',                            'education-num',                             'marital-status',                             'occupation',                            'relationship',                             'race', 'sex',                             'capital-gain',                            'capital-loss',                            'hours-per-week',                            'native-country',                             'salary'])  # \u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u043c\u043e\u0441\u0442\u044c   vals = adult['native-country'].value_counts() lst_index = list(vals[vals&lt;100].index) # index of subsets which are in category other adult['native-country'] = adult['native-country'].replace(lst_index,'other')  # \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438 \u0441\u043e\u0437\u0434\u0430\u0442\u044c OHE  ohe_native_country = pd.get_dummies(adult['native-country'],drop_first=True) upd_adult = adult.drop(['native-country'],axis=1) adult = pd.concat([upd_adult,ohe_native_country],axis=1)  # \u0432\u0441\u0435 \u043a\u0430\u043a \u0438 \u0440\u0430\u043d\u044c\u0448\u0435  adult['salary'] = (adult['salary'] != ' &lt;=50K').astype('int32') adult['salary'].value_counts() adult = pd.get_dummies(adult, columns=['workclass', 'education', 'marital-status',                                          'occupation', 'relationship', 'race', 'sex'])  # \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u043d\u0443\u0436\u0434\u0430\u044e\u0449\u0438\u0435\u0441\u044f \u0432 \u044d\u0442\u043e\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 a_features = adult[['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']].values norm_features = (a_features - a_features.mean(axis=0)) / a_features.std(axis=0) adult.loc[:, ['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']] = norm_features  # \u0420\u0430\u0437\u0431\u0438\u0442\u044c \u0442\u0430\u0431\u043b\u0438\u0446\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u043c\u0430\u0442\u0440\u0438\u0446\u044b X \u0438 y X = adult[list(set(adult.columns) - set(['salary']))].values y = adult['salary'].values  # \u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0435\u0434\u0438\u043d\u0438\u0446 (bias \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438) X = np.hstack([np.ones(X.shape[0])[:,None], X])  model = LogisticRegression(max_iter=3000) model.fit(X,y) y_pred = model.predict(X) f1_score(y,y_pred) Out[34]: <pre>0.6631121088817175</pre>"},{"location":"portfolio/sfml/ml3b_6_practice.html","title":"\u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430\u00b6","text":""},{"location":"portfolio/sfml/ml3b_6_practice.html","title":"\u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0438 \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u0430\u00b6","text":"<p>\u0420\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e</p> <ul> <li>\u0412 \u0441\u043b\u0443\u0447\u0430\u0435 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0435\u0439, \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0441\u043f\u0443\u0441\u043a</li> <li>\u041f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u043d\u0435\u0442 \u044f\u0432\u043d\u043e\u0433\u043e \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u043e\u0433\u043e \u0441\u043f\u043e\u0441\u043e\u0431\u0430 \u043d\u0430\u0439\u0442\u0438 \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0435 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u044b</li> </ul> <p>\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0430\u043b\u044c\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432\u043e\u0437\u044c\u043c\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043e \u0434\u043e\u0445\u043e\u0434\u0430\u0445 \u0433\u0440\u0430\u0436\u0434\u0430\u043d \u0432 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0441\u0442\u0440\u0430\u043d\u0430\u0445 Adult Income \u0438 \u0441\u0434\u0435\u043b\u0430\u0435\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u0443\u044e \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443</p>"},{"location":"portfolio/sfml/ml3b_6_practice.html","title":"\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u041f\u043e\u0442\u0435\u0440\u044c\u00b6","text":"<p>\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043e\u0448\u0438\u0431\u043a\u0438 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u0432 \u0441\u043b\u0443\u0447\u0430\u0435 \u0431\u0438\u043d\u0430\u0440\u043d\u043e\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f <code>\u0431\u0438\u043d\u0430\u0440\u043d\u043e\u0439 \u043a\u0440\u043e\u0441\u0441-\u044d\u043d\u0442\u0440\u043e\u043f\u0438\u0435\u0439</code> (log loss) \u0438 \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c:</p> <p>$$L=-\\frac{1}{n}(y_i \\log h_{\\theta}(x_i) + (1-y_i) \\log(1-h_{\\theta}(x_i))),$$</p> <p>\u0433\u0434\u0435 $x_i$ \u2014 \u0432\u0435\u043a\u0442\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 $i$-\u0433\u043e \u043f\u0440\u0438\u043c\u0435\u0440\u0430 \u0438\u0437 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438, $y_i$ \u2014 \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043a\u043b\u0430\u0441\u0441 \u0434\u043b\u044f \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0433\u043e \u043f\u0440\u0438\u043c\u0435\u0440\u0430 (0 \u0438\u043b\u0438 1), $n$ \u2014 \u0447\u0438\u0441\u043b\u043e \u043f\u0440\u0438\u043c\u0435\u0440\u043e\u0432 \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435, $h_{\\theta}(x)$ \u2014 sigmoid \u0444\u0443\u043d\u043a\u0446\u0438\u044f, \u0440\u0430\u0432\u043d\u0430\u044f:</p> <p>$$h_{\\theta}(x)=\\frac{1}{1+\\exp^{-\\theta x}},$$</p> <p>\u0433\u0434\u0435 $\\theta$ \u2014 \u0432\u0435\u043a\u0442\u043e\u0440 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438, $x$ - \u0432\u0435\u043a\u0442\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u0438\u0437 \u0432\u044b\u0431\u043e\u0440\u043a\u0438.</p>"},{"location":"portfolio/sfml/ml3b_6_practice.html","title":"\u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0443\u043b\u0438\u00b6","text":""},{"location":"portfolio/sfml/ml3b_6_practice.html","title":"\u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0414\u0430\u043d\u043d\u044b\u0435\u00b6","text":"<p>\u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0438 \u0434\u043e\u0431\u0430\u0432\u0438\u043c \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u0444\u0438\u0447\u0435\u0440\u043e\u0432</p>"},{"location":"portfolio/sfml/ml3b_6_practice.html","title":"\u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0414\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<ul> <li>\u0423\u0431\u0438\u0440\u0430\u0435\u043c \u043b\u0438\u0448\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438</li> <li>\u041f\u0440\u0435\u0432\u0440\u0430\u0442\u0438\u043c \u0446\u0435\u043b\u0435\u0432\u043e\u0443\u044e \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0443\u044e \u0432 \u0431\u0438\u043d\u0430\u0440\u043d\u0443\u044e</li> <li>\u041f\u0440\u0435\u0432\u0440\u0430\u0442\u0438\u043c \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0432 \u0446\u0438\u0444\u0440\u043e\u0432\u044b\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f OHE</li> </ul>"},{"location":"portfolio/sfml/ml3b_6_practice.html","title":"\u0420\u0435\u0430\u043b\u0438\u0437\u0438\u0446\u0438\u044f \u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0421\u043f\u0443\u0441\u043a\u0430\u00b6","text":""},{"location":"portfolio/sfml/ml3b_6_practice.html","title":"\u0432\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438\u00b6","text":"<ul> <li>\u0420\u0438\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e <code>sigmoid</code></li> <li>\u0424\u0443\u043d\u043a\u0446\u0438\u044e \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u044e\u0449\u0443\u044e \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 <code>bin_crossentropy_grad</code></li> <li>\u041e\u0434\u0438\u043d \u0448\u0430\u0433 \u0432 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u043c \u0441\u043f\u0443\u0441\u043a\u0435 <code>gradient_step</code></li> <li>\u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u0446\u0438\u043a\u043b \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 <code>optimize</code></li> </ul>"},{"location":"portfolio/sfml/ml3b_6_practice.html#1-theta","title":"(1) \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c theta\u00b6","text":"<p>\u0414\u043b\u044f \u0432\u0441\u0435\u0445 \u0440\u044f\u0434\u043e\u0432 \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c theta0, \u044d\u0442\u043e \u043d\u0430\u0447\u0430\u043b\u044c\u043d\u043e\u0435 \u0443\u0441\u043b\u043e\u0432\u0438\u0435 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0438</p>"},{"location":"portfolio/sfml/ml3b_6_practice.html#2-theta","title":"(2) \u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u0435\u043c theta\u00b6","text":"<ul> <li><code>optimize</code> \u0442\u0440\u0435\u0431\u0443\u0435\u0442 \u0432\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 (X,y), \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u043b\u043e\u0441\u0441 \u0444\u0443\u043d\u043a\u0446\u0438\u0438, \u043d\u0430\u0447\u0430\u043b\u044c\u043d\u043e\u0435 \u0443\u0441\u043b\u043e\u0432\u0438\u0435 \u0434\u043b\u044f theta, \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u0433\u0440\u0430\u0434\u0438\u0435\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430 \u0438 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438</li> <li>\u041d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b</li> </ul>"},{"location":"portfolio/sfml/ml3b_6_practice.html#3","title":"(3) \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430\u00b6","text":"<ul> <li>\u0414\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0441\u0438\u0433\u043c\u043e\u0439\u0434\u0430, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043d\u0430 \u0432\u0445\u043e\u0434 \u0442\u0440\u0435\u0431\u0443\u0435\u0442 \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0444\u0438\u0447\u0435\u0439 \u0438 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0438 \u043d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435 \u0434\u0430\u0435\u0442 \u043d\u0430\u043c \u0432\u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 (\u0430\u043d\u0430\u043b\u043e\u0433 <code>predict_proba</code>)</li> <li>\u0427\u0442\u043e\u0431\u044b \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u0443\u044e \u043c\u0435\u0442\u043a\u0443 \u043d\u0430\u043c \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c \u043f\u043e\u0440\u043e\u0433 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 0.5)</li> </ul>"},{"location":"portfolio/sfml/ml3b_6_practice.html","title":"\u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043d\u0430 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438\u00b6","text":"<ul> <li>\u041c\u044b \u0443\u0436\u0435 \u0437\u043d\u0430\u0435\u043c, \u0447\u0442\u043e \u044d\u0442\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u043d\u0435\u043b\u044c\u0437\u044f \u0434\u043e\u0432\u0435\u0440\u044f\u0442\u044c, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0438 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u0435\u043c</li> <li>\u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0442\u043e\u0442 \u0436\u0435. \u041e\u0442\u0440\u0438\u0441\u0443\u0435\u043c ROC-\u043a\u0440\u0438\u0432\u0443\u044e, \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0435\u0451 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043f\u043b\u043e\u0449\u0430\u0434\u0438 \u043f\u043e\u0434 \u043a\u0440\u0438\u0432\u043e\u0439 AUC</li> </ul>"},{"location":"portfolio/sfml/ml3b_6_practice.html#roc-auc","title":"ROC-AUC\u00b6","text":"<p>\u041c\u043e\u0436\u0435\u043c \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0441\u044f \u0431\u043e\u043b\u0435\u0435 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u044b\u043c\u0438 \u043c\u0435\u0442\u0440\u0438\u043a\u0430\u043c\u0438, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 ROC-AUC, \u0443 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u043d\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435\u0438 \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u043c\u0435\u0442\u043a\u0438</p>"},{"location":"portfolio/sfml/ml3b_6_practice.html","title":"\u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438\u00b6","text":"<ul> <li>\u0414\u043b\u044f \u0431\u043e\u0440\u044c\u0431\u044b \u0441 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\u043c \u0434\u043e\u0431\u0430\u0432\u0438\u043c \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044e, \u043d\u0430\u043c \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0442\u0435\u0440\u043c\u0438\u043d \u0432 \u043b\u043e\u0441\u0441 \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u044d\u0442\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438</li> <li>\u041e\u0431\u0435\u0440\u043d\u0451\u043c \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e \u0432 \u043a\u043b\u0430\u0441\u0441</li> <li>\u041f\u043e\u0442\u043e\u043c \u0434\u043e\u0431\u0430\u0432\u0438\u043c \u0432\u0430\u0440\u0438\u0430\u043d\u0442 \u0441 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438</li> </ul>"},{"location":"portfolio/sfml/ml3b_6_practice.html#1","title":"(1) \u041e\u0431\u043e\u0440\u0430\u0447\u0438\u0432\u0430\u043d\u0438\u0435 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438\u00b6","text":"<ul> <li>\u0414\u043e\u0431\u0430\u0432\u0438\u043c \u0442\u043e \u0447\u0442\u043e \u043c\u044b \u0434\u043e\u0431\u0430\u0432\u0438\u043b\u0438 \u0440\u0430\u043d\u0435\u0435 \u0432 \u043a\u043b\u0430\u0441\u0441\u043e\u0432\u0443\u044e \u0444\u043e\u0440\u043c\u0443</li> <li><code>regOptimiser</code> \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043e\u0431\u0449\u0438\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u043a\u0430\u043a \u0438 \u0432 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u0442\u0430\u043a \u0438 \u0432 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0413\u0421</li> </ul>"},{"location":"portfolio/sfml/ml3b_6_practice.html#2","title":"(2) \u041e\u0431\u043e\u0440\u0430\u0447\u0438\u0432\u0430\u043d\u0438\u0435 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438\u00b6","text":"<ul> <li>\u0422\u0435\u043f\u0435\u0440\u044c \u0442\u0430\u043a \u0436\u0435 \u0438 \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438, \u043d\u0443\u0436\u0435\u043d <code>grad_func</code> \u0438 <code>predict</code></li> </ul>"},{"location":"portfolio/sfml/ml3b_6_practice.html#3","title":"(3) \u041e\u0431\u043e\u0431\u0449\u0430\u044e\u0449\u0430\u044f \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c, \u0447\u0442\u043e \u0438 \u043a\u0443\u0434\u0430 \u043d\u0443\u0436\u043d\u043e \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c\u00b6","text":"<p>\u0412 \u0441\u043b\u0443\u0447\u0430\u044f\u0445 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0438 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u0431\u0443\u0434\u0435\u043c \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0442\u044c \u043a <code>\u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u0448\u0438\u0431\u043a\u0438</code> \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0443\u044e\u0449\u0443\u044e \u0447\u0430\u0441\u0442\u044c \u043a\u0430\u043a: $$\\frac{\\lambda}{2m}\\sum_{j}^{m}{\\theta_j^2},$$ \u0433\u0434\u0435 $\\theta$ \u2014 \u0432\u0435\u043a\u0442\u043e\u0440 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0431\u0435\u0437 \u0444\u0438\u043a\u0442\u0438\u0432\u043d\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 (intercept/bias term), $m$ \u2014 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0444\u0438\u043a\u0442\u0438\u0432\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, $\\lambda$ \u2014 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438.</p>"},{"location":"portfolio/sfml/ml3b_6_practice.html#a","title":"(a) \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0442\u043e\u0440\u0430 \u0432 \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e\u00b6","text":""},{"location":"portfolio/sfml/ml3b_6_practice.html#b","title":"(b) \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0442\u043e\u0440\u0430 \u0432 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e\u00b6","text":""},{"location":"portfolio/sfml/ml3b_6_practice.html","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/sfml/ml3b_6_practice.html#361","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3.6.1\u00b6","text":"<p>\u041f\u043e\u0441\u0442\u0440\u043e\u0439\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u044c \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 sklearn. \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e, \u043e\u0431\u0443\u0447\u0438\u0442\u0435 \u043d\u0430 \u0432\u0441\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0439\u0442\u0435 F1 score.</p>"},{"location":"portfolio/sfml/ml3b_6_practice.html#362","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3.6.2\u00b6","text":"<p>\u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0439\u0442\u0435 confusion matrix \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430 \u0438\u0437 \u0437\u0430\u0434\u0430\u0447\u0438 3.6.1. \u0414\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u043c\u043e\u0436\u043d\u043e \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u043c\u0435\u0442\u043e\u0434\u043e\u043c sklearn.metrics.confusion_matrix(y_true, y_pred), \u043b\u0438\u0431\u043e \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043a\u0430\u0436\u0434\u044b\u0439 \u044d\u043b\u0435\u043c\u0435\u043d\u0442 \u0432\u0440\u0443\u0447\u043d\u0443\u044e.</p>"},{"location":"portfolio/sfml/ml3b_6_practice.html#363","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3.6.3\u00b6","text":"<p>\u041f\u043e\u0441\u0442\u0440\u043e\u0439\u0442\u0435 ROC-\u043a\u0440\u0438\u0432\u0443\u044e \u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0439\u0442\u0435  \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430 \u0438\u0437 \u0437\u0430\u0434\u0430\u0447\u0438 3.6.1.</p>"},{"location":"portfolio/sfml/ml3b_6_practice.html#364","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3.6.4\u00b6","text":"<p>\u041f\u043e\u0441\u0442\u0440\u043e\u0439\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u044c \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 sklearn \u0431\u0435\u0437 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438. \u0427\u0435\u043c\u0443 \u0440\u0430\u0432\u0435\u043d f1 score?</p>"},{"location":"portfolio/sfml/ml3b_6_practice.html#365","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3.6.5\u00b6","text":"<p>\u041f\u0435\u0440\u0435\u0431\u0435\u0440\u0438\u0442\u0435 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u044b l2 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u043e\u0442 0.01 \u0434\u043e 1 \u0441 \u0448\u0430\u0433\u043e\u043c 0.01 \u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u0435, \u043d\u0430 \u043a\u0430\u043a\u043e\u043c \u0438\u0437 \u043d\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u044c \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u0438\u0437 sklearn \u0434\u0430\u0451\u0442 \u043d\u0430\u0438\u0431\u043e\u043b\u044c\u0448\u0438\u0439 fscore.</p>"},{"location":"portfolio/sfml/ml3b_6_practice.html#366","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 3.6.6\u00b6","text":"<p>\u0417\u0430\u043c\u0435\u043d\u0438\u0442\u0435 \u0432 \u0441\u0442\u043e\u043b\u0431\u0446\u0435 <code>native-country</code> \u0441\u0442\u0440\u0430\u043d\u044b, \u0443 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043c\u0435\u043d\u044c\u0448\u0435 \u0441\u0442\u0430 \u0437\u0430\u043f\u0438\u0441\u0435\u0439, \u043d\u0430 <code>other</code>, \u043f\u043e\u043c\u0435\u043d\u044f\u0439\u0442\u0435 \u044d\u0442\u043e\u0442 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u043d\u0430 dummy-\u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435, \u043e\u0431\u0443\u0447\u0438\u0442\u0435 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u043d\u0430 \u0432\u0441\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0439\u0442\u0435 f score</p>"},{"location":"portfolio/sfml/ml4_11_%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D1%81%D0%B8%D0%BB%D1%83%D1%8D%D1%82%D0%B0.html","title":"ml4 11 \u041a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u0441\u0438\u043b\u0443\u044d\u0442\u0430","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nfrom sklearn import datasets\nimport numpy as np\nfrom itertools import cycle, islice\n\nn_samples = 1500\ndataset = datasets.make_blobs(n_samples=n_samples, centers=2, center_box=(-7.0, 7.5),\n                              cluster_std=[1.4, 1.7],\n                              random_state=42)\nX_2, _ = datasets.make_blobs(n_samples=n_samples, random_state=170, centers=[[-4, -3]], cluster_std=[1.9])\ntransformation = [[1.2, -0.8], [-0.4, 1.7]]\nX_2 = np.dot(X_2, transformation)\nX, y = np.concatenate((dataset[0], X_2)), np.concatenate((dataset[1], np.array([2] * len(X_2))))\n</pre> import matplotlib.pyplot as plt from sklearn import datasets import numpy as np from itertools import cycle, islice  n_samples = 1500 dataset = datasets.make_blobs(n_samples=n_samples, centers=2, center_box=(-7.0, 7.5),                               cluster_std=[1.4, 1.7],                               random_state=42) X_2, _ = datasets.make_blobs(n_samples=n_samples, random_state=170, centers=[[-4, -3]], cluster_std=[1.9]) transformation = [[1.2, -0.8], [-0.4, 1.7]] X_2 = np.dot(X_2, transformation) X, y = np.concatenate((dataset[0], X_2)), np.concatenate((dataset[1], np.array([2] * len(X_2)))) In\u00a0[2]: Copied! <pre># \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\ndef plot_scatter():\n    plt.rcParams['figure.figsize'] = 3, 3\n    colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n                                         '#f781bf', '#a65628', '#984ea3',\n                                         '#999999', '#e41a1c', '#dede00']),\n                                  int(max(y_pred) + 1))))\n    plt.scatter(X[:, 0], X[:, 1], s=10,ec='k',alpha=0.25,color=colors[y_pred])\n</pre> # \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 def plot_scatter():     plt.rcParams['figure.figsize'] = 3, 3     colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',                                          '#f781bf', '#a65628', '#984ea3',                                          '#999999', '#e41a1c', '#dede00']),                                   int(max(y_pred) + 1))))     plt.scatter(X[:, 0], X[:, 1], s=10,ec='k',alpha=0.25,color=colors[y_pred]) In\u00a0[3]: Copied! <pre>from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u043c\u0435\u0442\u043e\u0434\u0430 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438\nkmeans = KMeans(n_clusters=3, random_state=42)\nkmeans.fit(X)\ny_pred = kmeans.labels_\ny_pred\n</pre> from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score  # \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u043c\u0435\u0442\u043e\u0434\u0430 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 kmeans = KMeans(n_clusters=3, random_state=42) kmeans.fit(X) y_pred = kmeans.labels_ y_pred Out[3]: <pre>array([2, 2, 1, ..., 0, 0, 0], dtype=int32)</pre> In\u00a0[4]: Copied! <pre># \u0442\u0435\u043f\u0435\u0440\u044c \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u0441\u0438\u043b\u0443\u044d\u0442\u0430\nsilhouette_score(X=X, labels=y_pred, metric='euclidean')\n</pre> # \u0442\u0435\u043f\u0435\u0440\u044c \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u0441\u0438\u043b\u0443\u044d\u0442\u0430 silhouette_score(X=X, labels=y_pred, metric='euclidean') Out[4]: <pre>0.5131660482634046</pre> In\u00a0[5]: Copied! <pre>plot_scatter()\n</pre> plot_scatter() In\u00a0[6]: Copied! <pre>from sklearn.mixture import GaussianMixture as GM\nfrom sklearn.metrics import silhouette_score\n\ngm = GM(n_components=3,random_state=42)\ny_pred = gm.fit_predict(X)\ny_pred\n</pre> from sklearn.mixture import GaussianMixture as GM from sklearn.metrics import silhouette_score  gm = GM(n_components=3,random_state=42) y_pred = gm.fit_predict(X) y_pred Out[6]: <pre>array([1, 1, 1, ..., 0, 2, 0])</pre> In\u00a0[7]: Copied! <pre>silhouette_score(X,y_pred,metric='euclidean')\n</pre> silhouette_score(X,y_pred,metric='euclidean') Out[7]: <pre>0.3988405457243407</pre> In\u00a0[8]: Copied! <pre>plot_scatter()\n</pre> plot_scatter() In\u00a0[9]: Copied! <pre>import warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import DBSCAN\n\n# K-Means\nkmeans = KMeans(n_clusters=3,\n                random_state=42)\nkmeans.fit(X)\ny_pred = kmeans.labels_\nsil = silhouette_score(X=X, labels=y_pred, metric='euclidean')\nprint('kmeans',sil)\n\n# Gaussian Mixture\nem_gm = GaussianMixture(n_components=3, \n                        random_state=42)\nem_gm.fit(X)       \ny_pred = em_gm.predict(X)\nsil = silhouette_score(X=X, labels=y_pred, metric='euclidean')\nprint('gm',sil)\n\n# Agglomerative Cluster\nac = AgglomerativeClustering(n_clusters=3)\nac.fit(X)\ny_pred = ac.labels_.astype(np.int)\nsil = silhouette_score(X=X, labels=y_pred, metric='euclidean')\nprint('agglomerative',sil)\n\n# DBSCAN\ndbscan = DBSCAN(eps=0.9, min_samples=35)\ndbscan.fit(X)\ny_pred = dbscan.labels_.astype(np.int)\nsil = silhouette_score(X=X, labels=y_pred, metric='euclidean')\nprint('dbscan',sil)\n</pre> import warnings warnings.filterwarnings('ignore')  from sklearn.cluster import KMeans from sklearn.mixture import GaussianMixture from sklearn.cluster import AgglomerativeClustering from sklearn.cluster import DBSCAN  # K-Means kmeans = KMeans(n_clusters=3,                 random_state=42) kmeans.fit(X) y_pred = kmeans.labels_ sil = silhouette_score(X=X, labels=y_pred, metric='euclidean') print('kmeans',sil)  # Gaussian Mixture em_gm = GaussianMixture(n_components=3,                          random_state=42) em_gm.fit(X)        y_pred = em_gm.predict(X) sil = silhouette_score(X=X, labels=y_pred, metric='euclidean') print('gm',sil)  # Agglomerative Cluster ac = AgglomerativeClustering(n_clusters=3) ac.fit(X) y_pred = ac.labels_.astype(np.int) sil = silhouette_score(X=X, labels=y_pred, metric='euclidean') print('agglomerative',sil)  # DBSCAN dbscan = DBSCAN(eps=0.9, min_samples=35) dbscan.fit(X) y_pred = dbscan.labels_.astype(np.int) sil = silhouette_score(X=X, labels=y_pred, metric='euclidean') print('dbscan',sil) <pre>kmeans 0.5131660482634046\ngm 0.3988405457243407\nagglomerative 0.4811992210663849\ndbscan 0.4454335539277996\n</pre> In\u00a0[16]: Copied! <pre>from sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import KMeans\n\nlst_clusters = [i for i in range(2,11)]\n\nmax_sil = [-777,-777,-777]\nopt_cluster = [-777,-777,-777]\nfor n_cluster in lst_clusters:\n    \n    print('\\nclusters',n_cluster)\n\n    # K-Means\n    kmeans = KMeans(n_clusters=n_cluster,\n                    random_state=42)\n    kmeans.fit(X)\n    y_pred = kmeans.labels_\n    sil = silhouette_score(X=X, labels=y_pred, metric='euclidean')\n    if(sil &gt; max_sil[0]):\n        print('kmeans',n_cluster, round(sil,4))\n        max_sil[0] = sil\n        opt_cluster[0] = n_cluster\n\n    # Gaussian Mixture\n    em_gm = GaussianMixture(n_components=n_cluster, \n                            random_state=42)\n    em_gm.fit(X)       \n    y_pred = em_gm.predict(X)\n    sil = silhouette_score(X=X, labels=y_pred, metric='euclidean')\n    if(sil &gt; max_sil[1]):\n        print('gm',n_cluster, round(sil,4))\n        max_sil[1] = sil\n        opt_cluster[1] = n_cluster\n\n    # Agglomerative Cluster\n    ac = AgglomerativeClustering(n_clusters=n_cluster)\n    ac.fit(X)\n    y_pred = ac.labels_.astype(np.int)\n    sil = silhouette_score(X=X, labels=y_pred, metric='euclidean')\n    if(sil &gt; max_sil[2]):\n        print('agglomerative',n_cluster, round(sil,4))\n        max_sil[2] = sil\n        opt_cluster[2] = n_cluster\n\nprint('optimum number of clusters:',opt_cluster)\n</pre> from sklearn.cluster import AgglomerativeClustering from sklearn.cluster import KMeans  lst_clusters = [i for i in range(2,11)]  max_sil = [-777,-777,-777] opt_cluster = [-777,-777,-777] for n_cluster in lst_clusters:          print('\\nclusters',n_cluster)      # K-Means     kmeans = KMeans(n_clusters=n_cluster,                     random_state=42)     kmeans.fit(X)     y_pred = kmeans.labels_     sil = silhouette_score(X=X, labels=y_pred, metric='euclidean')     if(sil &gt; max_sil[0]):         print('kmeans',n_cluster, round(sil,4))         max_sil[0] = sil         opt_cluster[0] = n_cluster      # Gaussian Mixture     em_gm = GaussianMixture(n_components=n_cluster,                              random_state=42)     em_gm.fit(X)            y_pred = em_gm.predict(X)     sil = silhouette_score(X=X, labels=y_pred, metric='euclidean')     if(sil &gt; max_sil[1]):         print('gm',n_cluster, round(sil,4))         max_sil[1] = sil         opt_cluster[1] = n_cluster      # Agglomerative Cluster     ac = AgglomerativeClustering(n_clusters=n_cluster)     ac.fit(X)     y_pred = ac.labels_.astype(np.int)     sil = silhouette_score(X=X, labels=y_pred, metric='euclidean')     if(sil &gt; max_sil[2]):         print('agglomerative',n_cluster, round(sil,4))         max_sil[2] = sil         opt_cluster[2] = n_cluster  print('optimum number of clusters:',opt_cluster) <pre>\nclusters 2\nkmeans 2 0.4553\ngm 2 0.4551\nagglomerative 2 0.4056\n\nclusters 3\nkmeans 3 0.5132\nagglomerative 3 0.4812\n\nclusters 4\ngm 4 0.5107\nagglomerative 4 0.4847\n\nclusters 5\n\nclusters 6\n\nclusters 7\n\nclusters 8\n\nclusters 9\n\nclusters 10\noptimum number of clusters: [3, 4, 4]\n</pre>"},{"location":"portfolio/sfml/ml4_11_%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D1%81%D0%B8%D0%BB%D1%83%D1%8D%D1%82%D0%B0.html","title":"\u041a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u0441\u0438\u043b\u0443\u044d\u0442\u0430. \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430\u00b6","text":""},{"location":"portfolio/sfml/ml4_11_%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D1%81%D0%B8%D0%BB%D1%83%D1%8D%D1%82%D0%B0.html","title":"\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\u00b6","text":"<p>\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0443\u0436\u0435 \u0437\u043d\u0430\u043a\u043e\u043c\u044b\u0439 \u043d\u0430\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 <code>make_blobs</code></p>"},{"location":"portfolio/sfml/ml4_11_%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D1%81%D0%B8%D0%BB%D1%83%D1%8D%D1%82%D0%B0.html","title":"\u041f\u0440\u0438\u043c\u0435\u0440 \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438\u00b6","text":"<p><code>\u041a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u0441\u0438\u043b\u0443\u044d\u0442\u0430</code> \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 <code>sklearn</code></p>"},{"location":"portfolio/sfml/ml4_11_%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D1%81%D0%B8%D0%BB%D1%83%D1%8D%D1%82%D0%B0.html","title":"\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u041c\u0435\u0442\u043e\u0434\u0430\u00b6","text":"<p>\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 <code>silhouette_score</code> \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f:</p> <ul> <li><code>X</code> \u2014 \u043c\u0430\u0441\u0441\u0438\u0432 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0438\u043b\u0438 \u043c\u0430\u0441\u0441\u0438\u0432 \u043f\u043e\u043f\u0430\u0440\u043d\u044b\u0445 \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0439 \u043c\u0435\u0436\u0434\u0443 \u043e\u0431\u044a\u0435\u043a\u0442\u0430\u043c\u0438;</li> <li><code>Y</code> \u2014 \u043c\u0430\u0441\u0441\u0438\u0432 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0445 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u0434\u043b\u044f \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0438;</li> <li><code>metric</code> \u2014 \u043c\u0435\u0442\u0440\u0438\u043a\u0430, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0430\u044f \u0434\u043b\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u044f \u043c\u0435\u0436\u0434\u0443 \u043e\u0431\u044a\u0435\u043a\u0442\u0430\u043c\u0438, \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c euclidean (\u0415\u0432\u043a\u043b\u0438\u0434\u043e\u0432\u043e \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0435)</li> </ul>"},{"location":"portfolio/sfml/ml4_11_%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D1%81%D0%B8%D0%BB%D1%83%D1%8D%D1%82%D0%B0.html","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/sfml/ml4_11_%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D1%81%D0%B8%D0%BB%D1%83%D1%8D%D1%82%D0%B0.html#4111","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.11.1\u00b6","text":"<p>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u044c <code>GaussianMixture</code> \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 <code>n_components=3</code> \u0438 <code>random_state=42</code> \u043d\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430. \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0439\u0442\u0435 <code>\u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u0441\u0438\u043b\u0443\u044d\u0442\u0430</code> \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0435\u0439\u0441\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438</p>"},{"location":"portfolio/sfml/ml4_11_%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D1%81%D0%B8%D0%BB%D1%83%D1%8D%D1%82%D0%B0.html#4112","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.11.2\u00b6","text":"<p>\u0421\u0440\u0430\u0432\u043d\u0438\u0442\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0447\u0435\u0442\u044b\u0440\u0451\u0445 \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0435\u043d\u043d\u044b\u0445 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u043d\u0430 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 <code>\u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u0430 \u0441\u0438\u043b\u0443\u044d\u0442\u0430</code>, \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0439\u0442\u0435 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u044b \u0441\u043e \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438:</p> <ul> <li><code>K-means</code> \u2014 n_clusters=3, random_state=42</li> <li><code>EM-\u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c</code> (GaussianMixture) \u2014 n_components=3, random_state=42</li> <li><code>\u0410\u0433\u043b\u043e\u043c\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u0430\u044f</code> \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u2013 n_clusters=3</li> <li><code>DBSCAN</code> \u2013 eps=0.9, min_samples=35</li> </ul> <p>\u0423\u043a\u0430\u0436\u0438\u0442\u0435 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u0430 \u0441\u0438\u043b\u0443\u044d\u0442\u0430, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0435 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439</p>"},{"location":"portfolio/sfml/ml4_11_%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D1%81%D0%B8%D0%BB%D1%83%D1%8D%D1%82%D0%B0.html#4113","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.11.3\u00b6","text":"<p>\u041f\u043e\u0434\u0431\u0435\u0440\u0438\u0442\u0435 \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e <code>\u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u0430 \u0441\u0438\u043b\u0443\u044d\u0442\u0430</code>. \u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043d\u0430\u0439\u0434\u0438\u0442\u0435 \u0442\u0430\u043a\u043e\u0435 \u0447\u0438\u0441\u043b\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432, \u043f\u0440\u0438 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u0430 \u0431\u0443\u0434\u0435\u0442 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u043c.</p> <p>\u0412 \u0442\u0440\u0451\u0445 \u0438\u0437 \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0435\u043d\u043d\u044b\u0445 \u043d\u0430\u043c\u0438 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0437\u0430\u0434\u0430\u0442\u044c \u0447\u0438\u0441\u043b\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u043f\u0440\u0438 \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438: <code>K-means</code>, <code>EM-\u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c</code> \u0438 <code>\u0430\u0433\u043b\u043e\u043c\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f</code>.</p> <p>\u041d\u0430\u0439\u0434\u0438\u0442\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u0430 \u0441\u0438\u043b\u0443\u044d\u0442\u0430 \u0434\u043b\u044f \u0434\u0430\u043d\u043d\u044b\u0445 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u043f\u0440\u0438 \u0447\u0438\u0441\u043b\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u043e\u0442 2 \u0434\u043e 10 \u0432\u043a\u043b\u044e\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e. \u0414\u043b\u044f K-means \u0438 EM-\u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 random_state=42.</p> <p>\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0442\u0432\u0435\u0442\u0430 \u0447\u0435\u0440\u0435\u0437 \u043f\u0440\u043e\u0431\u0435\u043b \u0432\u0432\u0435\u0434\u0438\u0442\u0435 \u0447\u0438\u0441\u043b\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432, \u043f\u0440\u0438 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u0430 \u0441\u0438\u043b\u0443\u044d\u0442\u0430 \u0434\u043b\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0431\u044b\u043b\u043e \u043d\u0430\u0438\u0431\u043e\u043b\u044c\u0448\u0438\u043c \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0437 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432. \u0412\u0432\u043e\u0434\u0438\u0442\u0435 \u0432 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u043c \u043f\u043e\u0440\u044f\u0434\u043a\u0435: K-means, EM-\u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c, \u0430\u0433\u043b\u043e\u043c\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f.</p>"},{"location":"portfolio/sfml/ml4_12_%D0%9E%D0%B4%D0%BD%D0%BE%D1%80%D0%BE%D0%B4%D0%BD%D0%BE%D1%81%D1%82%D1%8C.html","title":"ml4 12 \u041e\u0434\u043d\u043e\u0440\u043e\u0434\u043d\u043e\u0441\u0442\u044c","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nfrom sklearn import datasets\nimport numpy as np\nfrom itertools import cycle, islice\n\nn_samples = 1500\ndataset = datasets.make_blobs(n_samples=n_samples, centers=2, center_box=(-7.0, 7.5),\n                              cluster_std=[1.4, 1.7],\n                              random_state=42)\nX_2, _ = datasets.make_blobs(n_samples=n_samples, random_state=170, centers=[[-4, -3]], cluster_std=[1.9])\ntransformation = [[1.2, -0.8], [-0.4, 1.7]]\nX_2 = np.dot(X_2, transformation)\nX, y = np.concatenate((dataset[0], X_2)), np.concatenate((dataset[1], np.array([2] * len(X_2))))\n</pre> import matplotlib.pyplot as plt from sklearn import datasets import numpy as np from itertools import cycle, islice  n_samples = 1500 dataset = datasets.make_blobs(n_samples=n_samples, centers=2, center_box=(-7.0, 7.5),                               cluster_std=[1.4, 1.7],                               random_state=42) X_2, _ = datasets.make_blobs(n_samples=n_samples, random_state=170, centers=[[-4, -3]], cluster_std=[1.9]) transformation = [[1.2, -0.8], [-0.4, 1.7]] X_2 = np.dot(X_2, transformation) X, y = np.concatenate((dataset[0], X_2)), np.concatenate((dataset[1], np.array([2] * len(X_2)))) In\u00a0[2]: Copied! <pre># \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\ndef plot_scatter():\n    plt.rcParams['figure.figsize'] = 3, 3\n    colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n                                         '#f781bf', '#a65628', '#984ea3',\n                                         '#999999', '#e41a1c', '#dede00']),\n                                  int(max(y_pred) + 1))))\n    plt.scatter(X[:, 0], X[:, 1], s=10,ec='k',alpha=0.25,color=colors[y_pred])\n</pre> # \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 def plot_scatter():     plt.rcParams['figure.figsize'] = 3, 3     colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',                                          '#f781bf', '#a65628', '#984ea3',                                          '#999999', '#e41a1c', '#dede00']),                                   int(max(y_pred) + 1))))     plt.scatter(X[:, 0], X[:, 1], s=10,ec='k',alpha=0.25,color=colors[y_pred]) In\u00a0[3]: Copied! <pre>from sklearn.cluster import KMeans\nfrom sklearn.metrics.cluster import homogeneity_score\nfrom sklearn.preprocessing import StandardScaler\n\n# \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u043c\u0435\u0442\u043e\u0434\u0430 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438\nkmeans = KMeans(n_clusters=3, random_state=42)\nX = StandardScaler().fit_transform(X)\nkmeans.fit(X)\ny_pred = kmeans.labels_ \n\n# \u0442\u0435\u043f\u0435\u0440\u044c \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043e\u0434\u043d\u043e\u0440\u043e\u0434\u043d\u043e\u0441\u0442\u044c\nhomogeneity_score(labels_true=y, labels_pred=y_pred)\n</pre> from sklearn.cluster import KMeans from sklearn.metrics.cluster import homogeneity_score from sklearn.preprocessing import StandardScaler  # \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u043c\u0435\u0442\u043e\u0434\u0430 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 kmeans = KMeans(n_clusters=3, random_state=42) X = StandardScaler().fit_transform(X) kmeans.fit(X) y_pred = kmeans.labels_   # \u0442\u0435\u043f\u0435\u0440\u044c \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043e\u0434\u043d\u043e\u0440\u043e\u0434\u043d\u043e\u0441\u0442\u044c homogeneity_score(labels_true=y, labels_pred=y_pred) Out[3]: <pre>0.804474693112785</pre> In\u00a0[4]: Copied! <pre>plot_scatter()\n</pre> plot_scatter() In\u00a0[10]: Copied! <pre>import warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics.cluster import homogeneity_score\n\n# K-Means\nkmeans = KMeans(n_clusters=3,\n                random_state=42)\nkmeans.fit(X)\ny_pred = kmeans.labels_\nhomosc = homogeneity_score(labels_true=y, labels_pred=y_pred)\nprint('kmeans',round(homosc,4))\n\n# Gaussian Mixture\nem_gm = GaussianMixture(n_components=3, \n                        random_state=42)\nem_gm.fit(X)       \ny_pred = em_gm.predict(X)\nhomosc = homogeneity_score(labels_true=y, labels_pred=y_pred)\nprint('gmm',round(homosc,4))\n\n# Agglomerative Cluster\nac = AgglomerativeClustering(n_clusters=3)\nac.fit(X)\ny_pred = ac.labels_.astype(np.int)\nhomosc = homogeneity_score(labels_true=y, labels_pred=y_pred)\nprint('agglomerative',round(homosc,4))\n\n# DBSCAN\ndbscan = DBSCAN(eps=0.9, min_samples=35)\ndbscan.fit(X)\ny_pred = dbscan.labels_.astype(np.int)\nhomosc = homogeneity_score(labels_true=y, labels_pred=y_pred)\nprint('dbscan',round(homosc,4))\n</pre> import warnings warnings.filterwarnings('ignore')  from sklearn.cluster import KMeans from sklearn.mixture import GaussianMixture from sklearn.cluster import AgglomerativeClustering from sklearn.cluster import DBSCAN from sklearn.preprocessing import StandardScaler from sklearn.metrics.cluster import homogeneity_score  # K-Means kmeans = KMeans(n_clusters=3,                 random_state=42) kmeans.fit(X) y_pred = kmeans.labels_ homosc = homogeneity_score(labels_true=y, labels_pred=y_pred) print('kmeans',round(homosc,4))  # Gaussian Mixture em_gm = GaussianMixture(n_components=3,                          random_state=42) em_gm.fit(X)        y_pred = em_gm.predict(X) homosc = homogeneity_score(labels_true=y, labels_pred=y_pred) print('gmm',round(homosc,4))  # Agglomerative Cluster ac = AgglomerativeClustering(n_clusters=3) ac.fit(X) y_pred = ac.labels_.astype(np.int) homosc = homogeneity_score(labels_true=y, labels_pred=y_pred) print('agglomerative',round(homosc,4))  # DBSCAN dbscan = DBSCAN(eps=0.9, min_samples=35) dbscan.fit(X) y_pred = dbscan.labels_.astype(np.int) homosc = homogeneity_score(labels_true=y, labels_pred=y_pred) print('dbscan',round(homosc,4)) <pre>kmeans 0.8045\ngmm 0.934\nagglomerative 0.91\ndbscan 0.0004\n</pre>"},{"location":"portfolio/sfml/ml4_12_%D0%9E%D0%B4%D0%BD%D0%BE%D1%80%D0%BE%D0%B4%D0%BD%D0%BE%D1%81%D1%82%D1%8C.html","title":"\u041e\u0434\u043d\u043e\u0440\u043e\u0434\u043d\u043e\u0441\u0442\u044c \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430\u00b6","text":""},{"location":"portfolio/sfml/ml4_12_%D0%9E%D0%B4%D0%BD%D0%BE%D1%80%D0%BE%D0%B4%D0%BD%D0%BE%D1%81%D1%82%D1%8C.html","title":"\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\u00b6","text":"<p>\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0443\u0436\u0435 \u0437\u043d\u0430\u043a\u043e\u043c\u044b\u0439 \u043d\u0430\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 <code>make_blobs</code></p>"},{"location":"portfolio/sfml/ml4_12_%D0%9E%D0%B4%D0%BD%D0%BE%D1%80%D0%BE%D0%B4%D0%BD%D0%BE%D1%81%D1%82%D1%8C.html","title":"\u041f\u0440\u0438\u043c\u0435\u0440 \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438\u00b6","text":"<p><code>\u041a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u0441\u0438\u043b\u0443\u044d\u0442\u0430</code> \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 <code>sklearn</code></p>"},{"location":"portfolio/sfml/ml4_12_%D0%9E%D0%B4%D0%BD%D0%BE%D1%80%D0%BE%D0%B4%D0%BD%D0%BE%D1%81%D1%82%D1%8C.html","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/sfml/ml4_12_%D0%9E%D0%B4%D0%BD%D0%BE%D1%80%D0%BE%D0%B4%D0%BD%D0%BE%D1%81%D1%82%D1%8C.html#4121","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.12.1\u00b6","text":"<p>\u0421\u0440\u0430\u0432\u043d\u0438\u0442\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 <code>k-means</code>, <code>GaussianMixture</code>, <code>AgglomerativeClustering</code> \u0438 <code>DBSCAN</code> \u043d\u0430 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u043e\u0434\u043d\u043e\u0440\u043e\u0434\u043d\u043e\u0441\u0442\u0438, \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0439\u0442\u0435 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u044b \u0441\u043e \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438:</p> <ul> <li><code>kmeans</code> \u2014 n_clusters=3, random_state=42</li> <li><code>GaussianMixture</code> \u2014 n_components=3, random_state=42</li> <li><code>AgglomerativeClustering</code> \u2014 n_clusters=3</li> <li><code>DBSCAN</code> \u2014 eps=0.9, min_samples=35</li> </ul> <p>\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0442\u0432\u0435\u0442\u0430 \u0443\u043a\u0430\u0436\u0438\u0442\u0435 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0434\u043d\u043e\u0440\u043e\u0434\u043d\u043e\u0441\u0442\u0438, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0435 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439</p>"},{"location":"portfolio/sfml/ml4_13_%D0%9F%D0%BE%D0%BB%D0%BD%D0%BE%D1%82%D0%B0.html","title":"ml4 13 \u041f\u043e\u043b\u043d\u043e\u0442\u0430","text":"In\u00a0[2]: Copied! <pre>import matplotlib.pyplot as plt\nfrom sklearn import datasets\nimport numpy as np\nfrom itertools import cycle, islice\n\nn_samples = 1500\ndataset = datasets.make_blobs(n_samples=n_samples, centers=2, center_box=(-7.0, 7.5),\n                              cluster_std=[1.4, 1.7],\n                              random_state=42)\nX_2, _ = datasets.make_blobs(n_samples=n_samples, random_state=170, centers=[[-4, -3]], cluster_std=[1.9])\ntransformation = [[1.2, -0.8], [-0.4, 1.7]]\nX_2 = np.dot(X_2, transformation)\nX, y = np.concatenate((dataset[0], X_2)), np.concatenate((dataset[1], np.array([2] * len(X_2))))\n</pre> import matplotlib.pyplot as plt from sklearn import datasets import numpy as np from itertools import cycle, islice  n_samples = 1500 dataset = datasets.make_blobs(n_samples=n_samples, centers=2, center_box=(-7.0, 7.5),                               cluster_std=[1.4, 1.7],                               random_state=42) X_2, _ = datasets.make_blobs(n_samples=n_samples, random_state=170, centers=[[-4, -3]], cluster_std=[1.9]) transformation = [[1.2, -0.8], [-0.4, 1.7]] X_2 = np.dot(X_2, transformation) X, y = np.concatenate((dataset[0], X_2)), np.concatenate((dataset[1], np.array([2] * len(X_2)))) In\u00a0[3]: Copied! <pre># \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\ndef plot_scatter():\n    plt.rcParams['figure.figsize'] = 3, 3\n    colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n                                         '#f781bf', '#a65628', '#984ea3',\n                                         '#999999', '#e41a1c', '#dede00']),\n                                  int(max(y_pred) + 1))))\n    plt.scatter(X[:, 0], X[:, 1], s=10,ec='k',alpha=0.25,color=colors[y_pred])\n</pre> # \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 def plot_scatter():     plt.rcParams['figure.figsize'] = 3, 3     colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',                                          '#f781bf', '#a65628', '#984ea3',                                          '#999999', '#e41a1c', '#dede00']),                                   int(max(y_pred) + 1))))     plt.scatter(X[:, 0], X[:, 1], s=10,ec='k',alpha=0.25,color=colors[y_pred]) In\u00a0[4]: Copied! <pre>from sklearn.cluster import KMeans\nfrom sklearn.metrics.cluster import completeness_score\n\n# \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u043c\u0435\u0442\u043e\u0434\u0430 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438\nkmeans = KMeans(n_clusters=3, random_state=42)\nkmeans.fit(X)\ny_pred = kmeans.labels_\n\n# \u0442\u0435\u043f\u0435\u0440\u044c \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043e\u0434\u043d\u043e\u0440\u043e\u0434\u043d\u043e\u0441\u0442\u044c\ncompleteness_score(labels_true=y, labels_pred=y_pred)\n</pre> from sklearn.cluster import KMeans from sklearn.metrics.cluster import completeness_score  # \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u043c\u0435\u0442\u043e\u0434\u0430 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 kmeans = KMeans(n_clusters=3, random_state=42) kmeans.fit(X) y_pred = kmeans.labels_  # \u0442\u0435\u043f\u0435\u0440\u044c \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043e\u0434\u043d\u043e\u0440\u043e\u0434\u043d\u043e\u0441\u0442\u044c completeness_score(labels_true=y, labels_pred=y_pred) Out[4]: <pre>0.7859676398774584</pre> In\u00a0[5]: Copied! <pre>plot_scatter()\n</pre> plot_scatter() In\u00a0[6]: Copied! <pre>from sklearn.preprocessing import StandardScaler\nfrom sklearn.mixture import GaussianMixture\n\nmodel = StandardScaler()\nX = model.fit_transform(X)\n\n# Gaussian Mixture\nem_gm = GaussianMixture(n_components=3, \n                        random_state=42)\nem_gm.fit(X)       \ny_pred = em_gm.predict(X)\ncompleteness = completeness_score(labels_true=y,\n                                labels_pred=y_pred)\nround(completeness,3)\n</pre> from sklearn.preprocessing import StandardScaler from sklearn.mixture import GaussianMixture  model = StandardScaler() X = model.fit_transform(X)  # Gaussian Mixture em_gm = GaussianMixture(n_components=3,                          random_state=42) em_gm.fit(X)        y_pred = em_gm.predict(X) completeness = completeness_score(labels_true=y,                                 labels_pred=y_pred) round(completeness,3) Out[6]: <pre>0.933</pre> In\u00a0[7]: Copied! <pre>import warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\n\nmodel = StandardScaler()\nX = model.fit_transform(X)\n\n# K-Means\nkmeans = KMeans(n_clusters=3,\n                random_state=42)\nkmeans.fit(X)\ny_pred = kmeans.labels_\ncompleteness = completeness_score(labels_true=y,\n                                  labels_pred=y_pred)\nprint('kmeans',completeness)\n\n# Gaussian Mixture\nem_gm = GaussianMixture(n_components=3, \n                        random_state=42)\nem_gm.fit(X)       \ny_pred = em_gm.predict(X)\ncompleteness = completeness_score(labels_true=y,\n                                  labels_pred=y_pred)\nprint('gmm',completeness)\n\n# Agglomerative Cluster\nac = AgglomerativeClustering(n_clusters=3)\nac.fit(X)\ny_pred = ac.labels_.astype(np.int)\ncompleteness = completeness_score(labels_true=y,\n                                  labels_pred=y_pred)\nprint('agglomerative',completeness)\n\n# DBSCAN\ndbscan = DBSCAN(eps=0.9, min_samples=35)\ndbscan.fit(X)\ny_pred = dbscan.labels_.astype(np.int)\ncompleteness = completeness_score(labels_true=y,\n                                  labels_pred=y_pred)\nprint('dbscan',completeness)\n</pre> import warnings warnings.filterwarnings('ignore')  from sklearn.cluster import KMeans from sklearn.mixture import GaussianMixture from sklearn.cluster import AgglomerativeClustering from sklearn.cluster import DBSCAN from sklearn.preprocessing import StandardScaler  model = StandardScaler() X = model.fit_transform(X)  # K-Means kmeans = KMeans(n_clusters=3,                 random_state=42) kmeans.fit(X) y_pred = kmeans.labels_ completeness = completeness_score(labels_true=y,                                   labels_pred=y_pred) print('kmeans',completeness)  # Gaussian Mixture em_gm = GaussianMixture(n_components=3,                          random_state=42) em_gm.fit(X)        y_pred = em_gm.predict(X) completeness = completeness_score(labels_true=y,                                   labels_pred=y_pred) print('gmm',completeness)  # Agglomerative Cluster ac = AgglomerativeClustering(n_clusters=3) ac.fit(X) y_pred = ac.labels_.astype(np.int) completeness = completeness_score(labels_true=y,                                   labels_pred=y_pred) print('agglomerative',completeness)  # DBSCAN dbscan = DBSCAN(eps=0.9, min_samples=35) dbscan.fit(X) y_pred = dbscan.labels_.astype(np.int) completeness = completeness_score(labels_true=y,                                   labels_pred=y_pred) print('dbscan',completeness) <pre>kmeans 0.7828224102025245\ngmm 0.9325740421656737\nagglomerative 0.9058386997451113\ndbscan 0.08342237034907717\n</pre>"},{"location":"portfolio/sfml/ml4_13_%D0%9F%D0%BE%D0%BB%D0%BD%D0%BE%D1%82%D0%B0.html","title":"\u041f\u043e\u043b\u043d\u043e\u0442\u0430 \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430\u00b6","text":""},{"location":"portfolio/sfml/ml4_13_%D0%9F%D0%BE%D0%BB%D0%BD%D0%BE%D1%82%D0%B0.html","title":"\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\u00b6","text":"<p>\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0443\u0436\u0435 \u0437\u043d\u0430\u043a\u043e\u043c\u044b\u0439 \u043d\u0430\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 <code>make_blobs</code></p>"},{"location":"portfolio/sfml/ml4_13_%D0%9F%D0%BE%D0%BB%D0%BD%D0%BE%D1%82%D0%B0.html","title":"\u041f\u0440\u0438\u043c\u0435\u0440 \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438\u00b6","text":"<p><code>\u041a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u0441\u0438\u043b\u0443\u044d\u0442\u0430</code> \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 <code>sklearn</code></p>"},{"location":"portfolio/sfml/ml4_13_%D0%9F%D0%BE%D0%BB%D0%BD%D0%BE%D1%82%D0%B0.html","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/sfml/ml4_13_%D0%9F%D0%BE%D0%BB%D0%BD%D0%BE%D1%82%D0%B0.html#4131","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.13.1\u00b6","text":"<ul> <li>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u044c <code>GaussianMixture</code> \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 <code>n_components=3</code> \u0438 <code>random_state=42</code> \u043d\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430.</li> <li>\u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0439\u0442\u0435 <code>\u043f\u043e\u043b\u043d\u043e\u0442\u0443</code> \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0435\u0439\u0441\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438.</li> </ul>"},{"location":"portfolio/sfml/ml4_13_%D0%9F%D0%BE%D0%BB%D0%BD%D0%BE%D1%82%D0%B0.html#4132","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.13.2\u00b6","text":"<p>\u0421\u0440\u0430\u0432\u043d\u0438\u0442\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 <code>K-means</code>, <code>GaussianMixture</code>, <code>AgglomerativeClustering</code> \u0438 <code>DBSCAN</code> \u043d\u0430 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 <code>\u043f\u043e\u043b\u043d\u043e\u0442\u044b</code>, \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0439\u0442\u0435 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u044b \u0441\u043e \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438:</p> <ul> <li><code>K-means</code> \u2013 n_clusters=3, random_state=42</li> <li><code>GaussianMixture</code> \u2013 n_components=3, random_state=42</li> <li><code>AgglomerativeClustering</code> \u2013 n_clusters=3</li> <li><code>DBSCAN</code> \u2013 eps=0.9, min_samples=35</li> </ul>"},{"location":"portfolio/sfml/ml4_14_V-%D0%9C%D0%B5%D1%80%D0%B0.html","title":"ml4 14 V \u041c\u0435\u0440\u0430","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nfrom sklearn import datasets\nimport numpy as np\nfrom itertools import cycle, islice\n\nn_samples = 1500\ndataset = datasets.make_blobs(n_samples=n_samples, centers=2, center_box=(-7.0, 7.5),\n                              cluster_std=[1.4, 1.7],\n                              random_state=42)\nX_2, _ = datasets.make_blobs(n_samples=n_samples, random_state=170, centers=[[-4, -3]], cluster_std=[1.9])\ntransformation = [[1.2, -0.8], [-0.4, 1.7]]\nX_2 = np.dot(X_2, transformation)\nX, y = np.concatenate((dataset[0], X_2)), np.concatenate((dataset[1], np.array([2] * len(X_2))))\n</pre> import matplotlib.pyplot as plt from sklearn import datasets import numpy as np from itertools import cycle, islice  n_samples = 1500 dataset = datasets.make_blobs(n_samples=n_samples, centers=2, center_box=(-7.0, 7.5),                               cluster_std=[1.4, 1.7],                               random_state=42) X_2, _ = datasets.make_blobs(n_samples=n_samples, random_state=170, centers=[[-4, -3]], cluster_std=[1.9]) transformation = [[1.2, -0.8], [-0.4, 1.7]] X_2 = np.dot(X_2, transformation) X, y = np.concatenate((dataset[0], X_2)), np.concatenate((dataset[1], np.array([2] * len(X_2)))) In\u00a0[2]: Copied! <pre># \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\ndef plot_scatter():\n    plt.rcParams['figure.figsize'] = 3, 3\n    colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n                                         '#f781bf', '#a65628', '#984ea3',\n                                         '#999999', '#e41a1c', '#dede00']),\n                                  int(max(y_pred) + 1))))\n    plt.scatter(X[:, 0], X[:, 1], s=10,ec='k',alpha=0.25,color=colors[y_pred])\n</pre> # \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 def plot_scatter():     plt.rcParams['figure.figsize'] = 3, 3     colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',                                          '#f781bf', '#a65628', '#984ea3',                                          '#999999', '#e41a1c', '#dede00']),                                   int(max(y_pred) + 1))))     plt.scatter(X[:, 0], X[:, 1], s=10,ec='k',alpha=0.25,color=colors[y_pred]) In\u00a0[3]: Copied! <pre>from sklearn.cluster import KMeans\nfrom sklearn.metrics.cluster import v_measure_score\n\n# \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u043c\u0435\u0442\u043e\u0434\u0430 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438\nkmeans = KMeans(n_clusters=3, random_state=42)\nkmeans.fit(X)\ny_pred = kmeans.labels_\n\n# \u0442\u0435\u043f\u0435\u0440\u044c \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043e\u0434\u043d\u043e\u0440\u043e\u0434\u043d\u043e\u0441\u0442\u044c\nv_measure_score(labels_true=y, labels_pred=y_pred)\n</pre> from sklearn.cluster import KMeans from sklearn.metrics.cluster import v_measure_score  # \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u043c\u0435\u0442\u043e\u0434\u0430 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 kmeans = KMeans(n_clusters=3, random_state=42) kmeans.fit(X) y_pred = kmeans.labels_  # \u0442\u0435\u043f\u0435\u0440\u044c \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043e\u0434\u043d\u043e\u0440\u043e\u0434\u043d\u043e\u0441\u0442\u044c v_measure_score(labels_true=y, labels_pred=y_pred) Out[3]: <pre>0.7972774344500205</pre> In\u00a0[4]: Copied! <pre>plot_scatter()\n</pre> plot_scatter() In\u00a0[5]: Copied! <pre>from sklearn.preprocessing import StandardScaler\nfrom sklearn.mixture import GaussianMixture\n\n# model = StandardScaler()\n# X = model.fit_transform(X)\n\n# Gaussian Mixture\nem_gm = GaussianMixture(n_components=3, \n                        random_state=42)\nem_gm.fit(X)       \ny_pred = em_gm.predict(X)\n\n# \u0442\u0435\u043f\u0435\u0440\u044c \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043e\u0434\u043d\u043e\u0440\u043e\u0434\u043d\u043e\u0441\u0442\u044c\nv_measure_score(labels_true=y, labels_pred=y_pred)\n</pre> from sklearn.preprocessing import StandardScaler from sklearn.mixture import GaussianMixture  # model = StandardScaler() # X = model.fit_transform(X)  # Gaussian Mixture em_gm = GaussianMixture(n_components=3,                          random_state=42) em_gm.fit(X)        y_pred = em_gm.predict(X)  # \u0442\u0435\u043f\u0435\u0440\u044c \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043e\u0434\u043d\u043e\u0440\u043e\u0434\u043d\u043e\u0441\u0442\u044c v_measure_score(labels_true=y, labels_pred=y_pred) Out[5]: <pre>0.619672859003694</pre> In\u00a0[6]: Copied! <pre>import warnings;warnings.filterwarnings('ignore')\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import DBSCAN\n\n# K-Means\nkmeans = KMeans(n_clusters=3,\n                random_state=42)\nkmeans.fit(X)\ny_pred = kmeans.labels_\nv_measure = v_measure_score(labels_true=y, labels_pred=y_pred)\nprint('kmeans',v_measure)\n\n# Gaussian Mixture\nem_gm = GaussianMixture(n_components=3, \n                        random_state=42)\nem_gm.fit(X)       \ny_pred = em_gm.predict(X)\nv_measure = v_measure_score(labels_true=y, labels_pred=y_pred)  \nprint('gmm',v_measure)\n\n# Agglomerative Cluster\nac = AgglomerativeClustering(n_clusters=3)\nac.fit(X)\ny_pred = ac.labels_.astype(np.int)\nv_measure = v_measure_score(labels_true=y, labels_pred=y_pred)  \nprint('agglomerative',v_measure)\n\n# DBSCAN\ndbscan = DBSCAN(eps=0.9, min_samples=35)\ndbscan.fit(X)\ny_pred = dbscan.labels_.astype(np.int)\nv_measure = v_measure_score(labels_true=y, labels_pred=y_pred)\nprint('dbscan',v_measure)\n</pre>   import warnings;warnings.filterwarnings('ignore')  from sklearn.cluster import KMeans from sklearn.mixture import GaussianMixture from sklearn.cluster import AgglomerativeClustering from sklearn.cluster import DBSCAN  # K-Means kmeans = KMeans(n_clusters=3,                 random_state=42) kmeans.fit(X) y_pred = kmeans.labels_ v_measure = v_measure_score(labels_true=y, labels_pred=y_pred) print('kmeans',v_measure)  # Gaussian Mixture em_gm = GaussianMixture(n_components=3,                          random_state=42) em_gm.fit(X)        y_pred = em_gm.predict(X) v_measure = v_measure_score(labels_true=y, labels_pred=y_pred)   print('gmm',v_measure)  # Agglomerative Cluster ac = AgglomerativeClustering(n_clusters=3) ac.fit(X) y_pred = ac.labels_.astype(np.int) v_measure = v_measure_score(labels_true=y, labels_pred=y_pred)   print('agglomerative',v_measure)  # DBSCAN dbscan = DBSCAN(eps=0.9, min_samples=35) dbscan.fit(X) y_pred = dbscan.labels_.astype(np.int) v_measure = v_measure_score(labels_true=y, labels_pred=y_pred) print('dbscan',v_measure) <pre>kmeans 0.7972774344500205\ngmm 0.619672859003694\nagglomerative 0.703747024360433\ndbscan 0.7732549110297919\n</pre> In\u00a0[7]: Copied! <pre>import warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.cluster import KMeans\n\nprint('initialisation test')\n\n# K-Means hyperparameters #1\nkmeans = KMeans(n_clusters=3,\n                n_init=1,\n                init='k-means++',\n                random_state=42)\nkmeans.fit(X)\ny_pred = kmeans.labels_\nv_measure = v_measure_score(labels_true=y, labels_pred=y_pred)\nprint('kmeans++',round(v_measure,3))\n\n# K-Means hyperparameters #1\nkmeans = KMeans(n_clusters=3,\n                n_init=1,\n                init='random',\n                random_state=42)\nkmeans.fit(X)\ny_pred = kmeans.labels_\nv_measure = v_measure_score(labels_true=y, labels_pred=y_pred)\nprint('random',round(v_measure,3))\n</pre> import warnings warnings.filterwarnings('ignore')  from sklearn.cluster import KMeans  print('initialisation test')  # K-Means hyperparameters #1 kmeans = KMeans(n_clusters=3,                 n_init=1,                 init='k-means++',                 random_state=42) kmeans.fit(X) y_pred = kmeans.labels_ v_measure = v_measure_score(labels_true=y, labels_pred=y_pred) print('kmeans++',round(v_measure,3))  # K-Means hyperparameters #1 kmeans = KMeans(n_clusters=3,                 n_init=1,                 init='random',                 random_state=42) kmeans.fit(X) y_pred = kmeans.labels_ v_measure = v_measure_score(labels_true=y, labels_pred=y_pred) print('random',round(v_measure,3)) <pre>initialisation test\nkmeans++ 0.599\nrandom 0.788\n</pre> In\u00a0[8]: Copied! <pre>from sklearn.cluster import MiniBatchKMeans\n\n# K-Means hyperparameters #1\nkmeans_mini_batch = MiniBatchKMeans(n_clusters=3, random_state=42)\nkmeans_mini_batch.fit(X)\nkmeans_mini_batch_pred = kmeans_mini_batch.labels_\nv_measure = v_measure_score(labels_true=y, labels_pred=kmeans_mini_batch_pred)\nprint('MiniBatchKMeans',v_measure)\n\n# K-Means hyperparameters #2\nkmeans = KMeans(n_clusters=3,\n                n_init=1,\n                random_state=42)\nkmeans.fit(X)\ny_pred = kmeans.labels_\nv_measure = v_measure_score(labels_true=y, labels_pred=y_pred)\nprint('KMeans',v_measure)\n</pre> from sklearn.cluster import MiniBatchKMeans  # K-Means hyperparameters #1 kmeans_mini_batch = MiniBatchKMeans(n_clusters=3, random_state=42) kmeans_mini_batch.fit(X) kmeans_mini_batch_pred = kmeans_mini_batch.labels_ v_measure = v_measure_score(labels_true=y, labels_pred=kmeans_mini_batch_pred) print('MiniBatchKMeans',v_measure)  # K-Means hyperparameters #2 kmeans = KMeans(n_clusters=3,                 n_init=1,                 random_state=42) kmeans.fit(X) y_pred = kmeans.labels_ v_measure = v_measure_score(labels_true=y, labels_pred=y_pred) print('KMeans',v_measure) <pre>MiniBatchKMeans 0.7810452632465894\nKMeans 0.5990620007488798\n</pre> In\u00a0[9]: Copied! <pre>linkage_id = ['ward','complete','average','single']\n\nfor linkage in linkage_id:\n    ac = AgglomerativeClustering(n_clusters=3,\n                                 linkage=linkage)\n    ac.fit(X)\n    y_pred = ac.labels_.astype(np.int)\n    v_measure = v_measure_score(labels_true=y,\n                                labels_pred=y_pred)\n    print('agglo',linkage,round(v_measure,3))\n</pre> linkage_id = ['ward','complete','average','single']  for linkage in linkage_id:     ac = AgglomerativeClustering(n_clusters=3,                                  linkage=linkage)     ac.fit(X)     y_pred = ac.labels_.astype(np.int)     v_measure = v_measure_score(labels_true=y,                                 labels_pred=y_pred)     print('agglo',linkage,round(v_measure,3)) <pre>agglo ward 0.704\nagglo complete 0.411\nagglo average 0.539\nagglo single 0.001\n</pre> In\u00a0[10]: Copied! <pre>from sklearn.neighbors import kneighbors_graph\n\nconnectivity = kneighbors_graph(X, n_neighbors=6, \n                                include_self= False)\nconnectivity = 0.5 * (connectivity + connectivity.T)\n\n# Standard\nac = AgglomerativeClustering(n_clusters=3)\nac.fit(X)\ny_pred = ac.labels_.astype(np.int)\nv_measure = v_measure_score(labels_true=y,\n                            labels_pred=y_pred)\nprint('standard:',round(v_measure,3))\n\n# with connectivity\nac = AgglomerativeClustering(n_clusters=3,\n                             connectivity=connectivity)\nac.fit(X)\ny_pred = ac.labels_.astype(np.int)\nv_measure = v_measure_score(labels_true=y,\n                            labels_pred=y_pred)\nprint('connectivity:',round(v_measure,3))\n</pre> from sklearn.neighbors import kneighbors_graph  connectivity = kneighbors_graph(X, n_neighbors=6,                                  include_self= False) connectivity = 0.5 * (connectivity + connectivity.T)  # Standard ac = AgglomerativeClustering(n_clusters=3) ac.fit(X) y_pred = ac.labels_.astype(np.int) v_measure = v_measure_score(labels_true=y,                             labels_pred=y_pred) print('standard:',round(v_measure,3))  # with connectivity ac = AgglomerativeClustering(n_clusters=3,                              connectivity=connectivity) ac.fit(X) y_pred = ac.labels_.astype(np.int) v_measure = v_measure_score(labels_true=y,                             labels_pred=y_pred) print('connectivity:',round(v_measure,3))  <pre>standard: 0.704\nconnectivity: 0.903\n</pre> In\u00a0[11]: Copied! <pre>from sklearn.cluster import DBSCAN\n\n# DBSCAN (eps=0.9)\ndbscan = DBSCAN(eps=0.9, min_samples=35)\ndbscan.fit(X)\ny_pred = dbscan.labels_.astype(np.int)\nv_measure = v_measure_score(labels_true=y, labels_pred=y_pred)\nprint('dbscan (eps 0.9)',round(v_measure,3))\n\n# DBSCAN (esp=0.8)\ndbscan = DBSCAN(eps=0.8, min_samples=35)\ndbscan.fit(X)\ny_pred = dbscan.labels_.astype(np.int)\nv_measure = v_measure_score(labels_true=y, labels_pred=y_pred)\nprint('dbscan (eps 0.8)',round(v_measure,3))\n</pre> from sklearn.cluster import DBSCAN  # DBSCAN (eps=0.9) dbscan = DBSCAN(eps=0.9, min_samples=35) dbscan.fit(X) y_pred = dbscan.labels_.astype(np.int) v_measure = v_measure_score(labels_true=y, labels_pred=y_pred) print('dbscan (eps 0.9)',round(v_measure,3))  # DBSCAN (esp=0.8) dbscan = DBSCAN(eps=0.8, min_samples=35) dbscan.fit(X) y_pred = dbscan.labels_.astype(np.int) v_measure = v_measure_score(labels_true=y, labels_pred=y_pred) print('dbscan (eps 0.8)',round(v_measure,3)) <pre>dbscan (eps 0.9) 0.773\ndbscan (eps 0.8) 0.706\n</pre> In\u00a0[12]: Copied! <pre>from numpy.random import random_sample\n\ndbscan = DBSCAN(eps=0.9, min_samples=35)\ndbscan.fit(X)\ny_pred = dbscan.labels_.astype(np.int)\nv_measure = v_measure_score(labels_true=y, labels_pred=y_pred)\nprint('dbscan',round(v_measure,3))\n\nbase = y_pred[dbscan.core_sample_indices_] # \u0431\u0435\u0437 \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432\nbase_y = y[dbscan.core_sample_indices_] # \u0431\u0435\u0437 \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432\n\nv_measure = v_measure_score(labels_true=base_y, labels_pred=base)\nprint('dbscan-core',round(v_measure,3))\n</pre> from numpy.random import random_sample  dbscan = DBSCAN(eps=0.9, min_samples=35) dbscan.fit(X) y_pred = dbscan.labels_.astype(np.int) v_measure = v_measure_score(labels_true=y, labels_pred=y_pred) print('dbscan',round(v_measure,3))  base = y_pred[dbscan.core_sample_indices_] # \u0431\u0435\u0437 \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432 base_y = y[dbscan.core_sample_indices_] # \u0431\u0435\u0437 \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432  v_measure = v_measure_score(labels_true=base_y, labels_pred=base) print('dbscan-core',round(v_measure,3)) <pre>dbscan 0.773\ndbscan-core 0.991\n</pre> In\u00a0[13]: Copied! <pre>from sklearn.preprocessing import StandardScaler\n\nmodel = StandardScaler()\nX_sca = model.fit_transform(X)\n\n# Standard\nac = AgglomerativeClustering(n_clusters=3)\nac.fit(X)\ny_pred = ac.labels_.astype(np.int)\nv_measure = v_measure_score(labels_true=y,\n                            labels_pred=y_pred)\nprint('\u0431\u0435\u0437 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438:',round(v_measure,3))\n\n# Standard\nac = AgglomerativeClustering(n_clusters=3)\nac.fit(X_sca)\ny_pred = ac.labels_.astype(np.int)\nv_measure = v_measure_score(labels_true=y,\n                            labels_pred=y_pred)\nprint('\u0441 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438:',round(v_measure,3)) \n</pre> from sklearn.preprocessing import StandardScaler  model = StandardScaler() X_sca = model.fit_transform(X)  # Standard ac = AgglomerativeClustering(n_clusters=3) ac.fit(X) y_pred = ac.labels_.astype(np.int) v_measure = v_measure_score(labels_true=y,                             labels_pred=y_pred) print('\u0431\u0435\u0437 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438:',round(v_measure,3))  # Standard ac = AgglomerativeClustering(n_clusters=3) ac.fit(X_sca) y_pred = ac.labels_.astype(np.int) v_measure = v_measure_score(labels_true=y,                             labels_pred=y_pred) print('\u0441 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438:',round(v_measure,3))   <pre>\u0431\u0435\u0437 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438: 0.704\n\u0441 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438: 0.908\n</pre> In\u00a0[3]: Copied! <pre>from sklearn.preprocessing import MinMaxScaler\n\n# Standard\nac = AgglomerativeClustering(n_clusters=3)\nac.fit(X)\ny_pred = ac.labels_.astype(np.int)\nv_measure = v_measure_score(labels_true=y,\n                            labels_pred=y_pred)\nprint('\u0431\u0435\u0437 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438:',round(v_measure,3)) \n\n# Standard\nac = AgglomerativeClustering(n_clusters=3)\nac.fit(X_sca)\ny_pred = ac.labels_.astype(np.int)\nv_measure = v_measure_score(labels_true=y,\n                            labels_pred=y_pred)\nprint('c \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438:',round(v_measure,3)) \n</pre> from sklearn.preprocessing import MinMaxScaler  # Standard ac = AgglomerativeClustering(n_clusters=3) ac.fit(X) y_pred = ac.labels_.astype(np.int) v_measure = v_measure_score(labels_true=y,                             labels_pred=y_pred) print('\u0431\u0435\u0437 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438:',round(v_measure,3))   # Standard ac = AgglomerativeClustering(n_clusters=3) ac.fit(X_sca) y_pred = ac.labels_.astype(np.int) v_measure = v_measure_score(labels_true=y,                             labels_pred=y_pred) print('c \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438:',round(v_measure,3))  <pre>\u0431\u0435\u0437 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438: 0.704\nc \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438: 0.893\n</pre>"},{"location":"portfolio/sfml/ml4_14_V-%D0%9C%D0%B5%D1%80%D0%B0.html#v-","title":"V-\u043c\u0435\u0440\u0430. \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430\u00b6","text":""},{"location":"portfolio/sfml/ml4_14_V-%D0%9C%D0%B5%D1%80%D0%B0.html","title":"\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\u00b6","text":"<p>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0443\u0436\u0435 \u0437\u043d\u0430\u043a\u043e\u043c\u044b\u0439 \u043d\u0430\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 <code>make_blobs</code></p>"},{"location":"portfolio/sfml/ml4_14_V-%D0%9C%D0%B5%D1%80%D0%B0.html","title":"\u041f\u0440\u0438\u043c\u0435\u0440 \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438\u00b6","text":"<p><code>\u041a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u0441\u0438\u043b\u0443\u044d\u0442\u0430</code> \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 <code>sklearn</code></p>"},{"location":"portfolio/sfml/ml4_14_V-%D0%9C%D0%B5%D1%80%D0%B0.html","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/sfml/ml4_14_V-%D0%9C%D0%B5%D1%80%D0%B0.html#4141","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.14.1\u00b6","text":"<ul> <li>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u044c <code>GaussianMixture</code> \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 <code>n_components=3</code> \u0438 <code>random_state=42</code> \u043d\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430</li> <li>\u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0439\u0442\u0435 <code>v-\u043c\u0435\u0440\u0443</code> \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0435\u0439\u0441\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438</li> </ul>"},{"location":"portfolio/sfml/ml4_14_V-%D0%9C%D0%B5%D1%80%D0%B0.html#4142","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.14.2\u00b6","text":"<p>\u0421\u0440\u0430\u0432\u043d\u0438\u0442\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 <code>K-means</code>, <code>GaussianMixture</code>, <code>AgglomerativeClustering</code> \u0438 <code>DBSCAN</code> \u043d\u0430 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 <code>\u043f\u043e\u043b\u043d\u043e\u0442\u044b</code>, \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0439\u0442\u0435 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u044b \u0441\u043e \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438:</p> <ul> <li><code>K-means</code> \u2013 n_clusters=3, random_state=42</li> <li><code>GaussianMixture</code> \u2013 n_components=3, random_state=42</li> <li><code>AgglomerativeClustering</code> \u2013 n_clusters=3</li> <li><code>DBSCAN</code> \u2013 eps=0.9, min_samples=35</li> </ul> <p>\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0442\u0432\u0435\u0442\u0430 \u0443\u043a\u0430\u0436\u0438\u0442\u0435 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 <code>v-\u043c\u0435\u0440\u044b</code>, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0435 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439.</p>"},{"location":"portfolio/sfml/ml4_14_V-%D0%9C%D0%B5%D1%80%D0%B0.html#4143","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.14.3\u00b6","text":"<p>\u0421\u0440\u0430\u0432\u043d\u0438\u043c \u043c\u043e\u0434\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 K-means \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0439 \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0446\u0435\u043d\u0442\u0440\u043e\u0438\u0434 \u0438 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 K-means++ \u0434\u043b\u044f \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0446\u0435\u043d\u0442\u0440\u043e\u0438\u0434.</p> <p>\u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0438\u043c \u043d\u0430 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 2 \u043c\u043e\u0434\u0435\u043b\u0438 K-means \u0441\u043e \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438:</p> <ul> <li><code>n_clusters</code>=3, <code>init</code>='k-means++', <code>n_init</code>=1, <code>random_state</code>=42</li> <li><code>n_clusters</code>=3, <code>init</code>='random', <code>n_init</code>=1, <code>random_state</code>=42</li> </ul> <p>\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0442\u0432\u0435\u0442\u0430 \u0443\u043a\u0430\u0436\u0438\u0442\u0435 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 <code>v-\u043c\u0435\u0440\u044b</code>, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0435 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439.</p>"},{"location":"portfolio/sfml/ml4_14_V-%D0%9C%D0%B5%D1%80%D0%B0.html#4144","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.14.4\u00b6","text":"<ul> <li>\u0422\u0435\u043f\u0435\u0440\u044c \u0441\u0440\u0430\u0432\u043d\u0438\u043c <code>K-means</code> \u0441 \u0435\u0449\u0451 \u043e\u0434\u043d\u043e\u0439 \u043c\u043e\u0434\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0435\u0439 \u2013 <code>K-means mini batch</code>.</li> <li>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0435\u0439 K-means mini batch \u0432 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0435 sklearn \u043c\u043e\u0436\u043d\u043e \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c:</li> </ul> <pre>fromsklearn.clusterimport MiniBatchKMeans\n\nkmeans_mini_batch = MiniBatchKMeans(n_clusters=\n3, random_state=\n42)\nkmeans_mini_batch.fit(X)\nkmeans_mini_batch_pred = kmeans_mini_batch.labels_\n</pre> <ul> <li><p>\u041c\u0435\u0445\u0430\u043d\u0438\u0437\u043c \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0432\u0435\u0440\u0441\u0438\u0438 K-means mini batch \u0441\u0445\u043e\u0436 \u0441 \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0439 \u0432\u0435\u0440\u0441\u0438\u0435\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430.</p> </li> <li><p>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u043d\u0430 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u0434\u0432\u0435 \u043c\u043e\u0434\u0435\u043b\u0438:</p> </li> <li><p>K-means \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 n_clusters=3, n_init=1, random_state=42</p> </li> <li><p>MiniBatchKMeans \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 n_clusters=3, n_init=1, random_state=42</p> </li> <li><p>\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0442\u0432\u0435\u0442\u0430 \u0432\u0432\u0435\u0434\u0438\u0442\u0435 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 <code>v-\u043c\u0435\u0440\u044b</code>, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0435 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439.</p> </li> <li><p>\u0412 \u0437\u0430\u0434\u0430\u043d\u0438\u0438 \u043c\u043e\u0436\u0435\u0442 \u043f\u043e\u043d\u0430\u0434\u043e\u0431\u0438\u0442\u044c\u0441\u044f, \u0430 \u043c\u043e\u0436\u0435\u0442 \u043d\u0435 \u043f\u043e\u043d\u0430\u0434\u043e\u0431\u0438\u0442\u044c\u0441\u044f \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0438 \u044d\u0442\u043e \u043d\u0443\u0436\u043d\u043e \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u0437\u0430\u0434\u0430\u043d\u0438\u044f.</p> </li> </ul>"},{"location":"portfolio/sfml/ml4_14_V-%D0%9C%D0%B5%D1%80%D0%B0.html#4145","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.14.5\u00b6","text":"<ul> <li>\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0430\u0433\u043b\u043e\u043c\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u0443\u044e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044e</li> <li>\u0421\u0440\u0430\u0432\u043d\u0438\u043c, \u043a\u0430\u043a \u0432\u043b\u0438\u044f\u0435\u0442 \u043d\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0440\u0430\u0437\u043d\u044b\u0439 \u0442\u0438\u043f \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u044f \u043c\u0435\u0436\u0434\u0443 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0430\u043c\u0438</li> <li>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u043d\u0430 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u0447\u0435\u0442\u044b\u0440\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 AgglomerativeClustering (<code>n_clusters=3</code>, \u043c\u0435\u043d\u044f\u044f \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 <code>linkage</code>)</li> </ul>"},{"location":"portfolio/sfml/ml4_14_V-%D0%9C%D0%B5%D1%80%D0%B0.html#4146","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.14.6\u00b6","text":"<p>\u0421\u0440\u0430\u0432\u043d\u0438\u043c, \u043a\u0430\u043a \u0432\u043b\u0438\u044f\u0435\u0442 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0440\u0430\u0441\u0447\u0451\u0442 <code>\u043c\u0430\u0442\u0440\u0438\u0446\u044b \u0441\u043c\u0435\u0436\u043d\u043e\u0441\u0442\u0438</code> \u043d\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e <code>\u0430\u0433\u043b\u043e\u043c\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u043e\u0439 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438</code></p> <p>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u043d\u0430 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u0434\u0432\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 AgglomerativeClustering:</p> <ul> <li>\u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u043c <code>n_clusters=3</code></li> <li>\u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u043c <code>n_clusters=3</code> \u0438 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u043d\u043d\u043e\u0439 \u043c\u0430\u0442\u0440\u0438\u0446\u0435\u0439 \u0441\u043c\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430</li> </ul> <p>\u041f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0441\u043c\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043c\u043e\u0436\u043d\u043e \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043a\u043e\u0434\u0430:</p> <pre>fromsklearn.neighborsimport kneighbors_graph\n\nconnectivity = kneighbors_graph(X, n_neighbors=\n6, include_self=\nFalse)\nconnectivity = 0.5 * (connectivity + connectivity.T)\n</pre> <ul> <li>\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0442\u0432\u0435\u0442\u0430 \u0432\u0432\u0435\u0434\u0438\u0442\u0435 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 <code>v-\u043c\u0435\u0440\u044b</code>, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0435 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439</li> <li>\u0412 \u0437\u0430\u0434\u0430\u043d\u0438\u0438 \u043c\u043e\u0436\u0435\u0442 \u043f\u043e\u043d\u0430\u0434\u043e\u0431\u0438\u0442\u044c\u0441\u044f, \u0430 \u043c\u043e\u0436\u0435\u0442 \u043d\u0435 \u043f\u043e\u043d\u0430\u0434\u043e\u0431\u0438\u0442\u044c\u0441\u044f \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0438 \u044d\u0442\u043e \u043d\u0443\u0436\u043d\u043e \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u0437\u0430\u0434\u0430\u043d\u0438\u044f</li> </ul>"},{"location":"portfolio/sfml/ml4_14_V-%D0%9C%D0%B5%D1%80%D0%B0.html#4147","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.14.7\u00b6","text":"<p>\u0410\u043b\u0433\u043e\u0440\u0438\u0442\u043c <code>DBSCAN</code> \u043e\u0447\u0435\u043d\u044c \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u043d \u043a \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c: \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0435 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0432 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0435 <code>eps</code> \u0438\u043b\u0438 <code>max_samples</code> \u043c\u043e\u0436\u0435\u0442 \u0438\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438</p> <p>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u043d\u0430 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u0434\u0432\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 <code>DBSCAN</code>:</p> <ul> <li>\u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 <code>eps=0.9</code>, <code>min_samples=35</code></li> <li>\u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 <code>eps=0.8</code>, <code>min_samples=35</code></li> </ul> <p>\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0442\u0432\u0435\u0442\u0430 \u0443\u043a\u0430\u0436\u0438\u0442\u0435 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 <code>v-\u043c\u0435\u0440\u044b</code>, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0435 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439.</p> <p>\u0412 \u0437\u0430\u0434\u0430\u043d\u0438\u0438 \u043c\u043e\u0436\u0435\u0442 \u043f\u043e\u043d\u0430\u0434\u043e\u0431\u0438\u0442\u044c\u0441\u044f, \u0430 \u043c\u043e\u0436\u0435\u0442 \u043d\u0435 \u043f\u043e\u043d\u0430\u0434\u043e\u0431\u0438\u0442\u044c\u0441\u044f \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0438 \u044d\u0442\u043e \u043d\u0443\u0436\u043d\u043e \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u0437\u0430\u0434\u0430\u043d\u0438\u044f.</p>"},{"location":"portfolio/sfml/ml4_14_V-%D0%9C%D0%B5%D1%80%D0%B0.html#4148","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.14.8\u00b6","text":"<ul> <li><p>\u041e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u044c\u044e \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 <code>DBSCAN</code> \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0442\u043e, \u0447\u0442\u043e \u043f\u043e\u043c\u0438\u043c\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u044d\u0442\u043e\u0442 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442 \u0432\u044b\u0431\u0440\u043e\u0441\u044b \u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 (-1)</p> </li> <li><p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0431\u0435\u0437 \u0443\u0447\u0451\u0442\u0430 \u0442\u0430\u043a\u0438\u0445 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432</p> </li> <li><p>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u043d\u0430 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u044c <code>DBSCAN</code> \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 <code>eps=0.9</code>, <code>min_samples=35</code></p> </li> <li><p>\u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0439\u0442\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 v-\u043c\u0435\u0440\u044b \u0442\u043e\u043b\u044c\u043a\u043e \u0434\u043b\u044f \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0445 \u0438 \u0433\u0440\u0430\u043d\u0438\u0447\u043d\u044b\u0445 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0438,</p> </li> <li><p>\u0442\u043e \u0435\u0441\u0442\u044c \u0434\u043b\u044f \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432, \u0447\u0442\u043e \u043d\u0435 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0432\u044b\u0431\u0440\u043e\u0441\u0430\u043c\u0438. \u041e\u0442\u0432\u0435\u0442 \u043e\u043a\u0440\u0443\u0433\u043b\u0438\u0442\u0435 \u0434\u043e \u0441\u043e\u0442\u044b\u0445 \u0438 \u0437\u0430\u043f\u0438\u0448\u0438\u0442\u0435 \u0441 \u0442\u043e\u0447\u043a\u043e\u0439.</p> </li> </ul>"},{"location":"portfolio/sfml/ml4_14_V-%D0%9C%D0%B5%D1%80%D0%B0.html#4149","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.14.9\u00b6","text":"<p>\u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430:</p> <ul> <li>\u0412 \u043a\u0443\u0440\u0441\u0435 \u043c\u044b \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0435\u043b\u0438 \u0434\u0432\u0435 \u043c\u0435\u0442\u043e\u0434\u0430 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445:</li> <li><code>MinMax</code> \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f (MinMaxScaler) \u2014 \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u043a \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0443 \u043c\u0435\u0436\u0434\u0443 0 \u0438 1.</li> <li><code>\u0421\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u0430\u044f \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f</code> (StandardScaler) \u2014 \u0434\u0430\u043d\u043d\u044b\u0435 \u0438\u043c\u0435\u044e\u0442 \u0441\u0440\u0435\u0434\u043d\u0435\u0435 0 \u0438 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0435 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0435 1.</li> </ul> <p>\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c, \u0432\u043b\u0438\u044f\u0435\u0442 \u043b\u0438 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438.</p> <ul> <li>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u0434\u0432\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 <code>AgglomerativeClustering</code> \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 <code>n_clusters=3</code></li> <li>\u043d\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430, \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0438\u0440\u0443\u0439\u0442\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 <code>\u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0439 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438</code></li> </ul>"},{"location":"portfolio/sfml/ml4_14_V-%D0%9C%D0%B5%D1%80%D0%B0.html#41410","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.14.10\u00b6","text":"<p>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u0434\u0432\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 <code>AgglomerativeClustering</code> \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 <code>n_clusters=3</code></p> <ul> <li>\u043d\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430,</li> <li>\u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0438\u0440\u0443\u0439\u0442\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 <code>MinMax</code> \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438</li> </ul> <p>\u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0439\u0442\u0435 <code>v-\u043c\u0435\u0440\u0443</code> \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0445\u0441\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432, \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0442\u0432\u0435\u0442\u0430 \u0432\u0432\u0435\u0434\u0438\u0442\u0435 \u043d\u0430\u0438\u0431\u043e\u043b\u044c\u0448\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435.</p>"},{"location":"portfolio/sfml/ml4_15_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_%D0%A2%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%B2.html","title":"ml4 15 \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u0422\u0435\u043a\u0441\u0442\u043e\u0432","text":"In\u00a0[46]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt  from sklearn import metrics from sklearn.cluster import KMeans from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import CountVectorizer In\u00a0[47]: Copied! <pre># \u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c 4 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0438 \u043d\u043e\u0432\u043e\u0441\u0442\u0435\u0439 \u0434\u043b\u044f \u043b\u0435\u0433\u043a\u043e\u0432\u0435\u0441\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u043c\u0435\u0440\u0430\ncategories = [\n    'rec.sport.hockey', # \u0445\u043e\u043a\u043a\u0435\u0439\n    'talk.politics.mideast', # \u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043d\u043e\u0432\u043e\u0441\u0442\u0438 \u043e \u0411\u043b\u0438\u0436\u043d\u0435\u043c \u0412\u043e\u0441\u0442\u043e\u043a\u0435\n    'comp.graphics', # \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043d\u0430\u044f \u0433\u0440\u0430\u0444\u0438\u043a\u0430\n    'sci.crypt' # \u043a\u0440\u0438\u043f\u0442\u043e\u0433\u0440\u0430\u0444\u0438\u044f\n]\n\n# \u0421\u043a\u0430\u0447\u0438\u0432\u0430\u0435\u043c \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445\ndataset = fetch_20newsgroups(subset='all', categories=categories,\n                             shuffle=True, random_state=42)\n\nprint(\"%d \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432\" % len(dataset.data))\nprint(\"%d \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0438\" % len(dataset.target_names))\n\n# \u0417\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0439 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u043d\u043e\u0432\u043e\u0441\u0442\u0438\nlabels = dataset.target\nlabels[:10]\n</pre> # \u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c 4 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0438 \u043d\u043e\u0432\u043e\u0441\u0442\u0435\u0439 \u0434\u043b\u044f \u043b\u0435\u0433\u043a\u043e\u0432\u0435\u0441\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u043c\u0435\u0440\u0430 categories = [     'rec.sport.hockey', # \u0445\u043e\u043a\u043a\u0435\u0439     'talk.politics.mideast', # \u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043d\u043e\u0432\u043e\u0441\u0442\u0438 \u043e \u0411\u043b\u0438\u0436\u043d\u0435\u043c \u0412\u043e\u0441\u0442\u043e\u043a\u0435     'comp.graphics', # \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043d\u0430\u044f \u0433\u0440\u0430\u0444\u0438\u043a\u0430     'sci.crypt' # \u043a\u0440\u0438\u043f\u0442\u043e\u0433\u0440\u0430\u0444\u0438\u044f ]  # \u0421\u043a\u0430\u0447\u0438\u0432\u0430\u0435\u043c \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 dataset = fetch_20newsgroups(subset='all', categories=categories,                              shuffle=True, random_state=42)  print(\"%d \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432\" % len(dataset.data)) print(\"%d \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0438\" % len(dataset.target_names))  # \u0417\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0439 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u043d\u043e\u0432\u043e\u0441\u0442\u0438 labels = dataset.target labels[:10] <pre>3903 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432\n4 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0438\n</pre> Out[47]: <pre>array([1, 1, 2, 2, 2, 3, 3, 2, 0, 2])</pre> In\u00a0[48]: Copied! <pre># \u0421\u043e\u0437\u0434\u0430\u0451\u043c \u043e\u0431\u044a\u0435\u043a\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435\nanalyzer = CountVectorizer(stop_words='english').build_analyzer()\n\n# \u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445\ndocs = []\nfor document in dataset.data:\n    docs.append(analyzer(document.replace('_', '')))\n\n# \u041f\u0435\u0440\u0432\u044b\u0435 10 \u0441\u043b\u043e\u0432 \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\ndocs[0][:10]\n</pre> # \u0421\u043e\u0437\u0434\u0430\u0451\u043c \u043e\u0431\u044a\u0435\u043a\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 analyzer = CountVectorizer(stop_words='english').build_analyzer()  # \u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 docs = [] for document in dataset.data:     docs.append(analyzer(document.replace('_', '')))  # \u041f\u0435\u0440\u0432\u044b\u0435 10 \u0441\u043b\u043e\u0432 \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430 docs[0][:10] Out[48]: <pre>['c5ff',\n 'jupiter',\n 'sun',\n 'csd',\n 'unb',\n 'ca',\n 'cook',\n 'charlie',\n 'subject',\n 'nhl']</pre> In\u00a0[49]: Copied! <pre>from gensim.models import Word2Vec\n\n# \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0432\u0435\u043a\u0442\u043e\u0440\u0430\u0439\u0437\u0435\u0440\u0430 \u043d\u0430 \u043d\u0430\u0448\u0435\u043c \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\n# \u041d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435 \u043c\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u0432\u0435\u043a\u0442\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430\nmodel = Word2Vec(docs, min_count=20, vector_size=50)\n\n# \u041d\u0430\u0438\u0432\u043d\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u043a \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044e \u0435\u0434\u0438\u043d\u043e\u0433\u043e \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0430 \u0434\u043b\u044f \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430 \u2013 \u0441\u0440\u0435\u0434\u043d\u0438\u0439 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433 \u043f\u043e \u0441\u043b\u043e\u0432\u0430\u043c\ndef doc_vectorizer(doc, model):\n    doc_vector = []\n    num_words = 0\n    for word in doc:\n        try:\n            if num_words == 0:\n                doc_vector = model.wv[word]\n            else:\n                doc_vector = np.add(doc_vector, model.wv[word])\n            num_words += 1\n        except:\n            # pass if word is not found\n            pass\n     \n    return np.asarray(doc_vector) / num_words\n\nX = []\nfor doc in docs:\n    X.append(doc_vectorizer(doc, model))\n    \nprint(f'Sentences: {len(X)}')\nprint(f'Each sentence has {X[0].shape} dimensions')\n</pre> from gensim.models import Word2Vec  # \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0432\u0435\u043a\u0442\u043e\u0440\u0430\u0439\u0437\u0435\u0440\u0430 \u043d\u0430 \u043d\u0430\u0448\u0435\u043c \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 # \u041d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435 \u043c\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u0432\u0435\u043a\u0442\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430 model = Word2Vec(docs, min_count=20, vector_size=50)  # \u041d\u0430\u0438\u0432\u043d\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u043a \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044e \u0435\u0434\u0438\u043d\u043e\u0433\u043e \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0430 \u0434\u043b\u044f \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430 \u2013 \u0441\u0440\u0435\u0434\u043d\u0438\u0439 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433 \u043f\u043e \u0441\u043b\u043e\u0432\u0430\u043c def doc_vectorizer(doc, model):     doc_vector = []     num_words = 0     for word in doc:         try:             if num_words == 0:                 doc_vector = model.wv[word]             else:                 doc_vector = np.add(doc_vector, model.wv[word])             num_words += 1         except:             # pass if word is not found             pass           return np.asarray(doc_vector) / num_words  X = [] for doc in docs:     X.append(doc_vectorizer(doc, model))      print(f'Sentences: {len(X)}') print(f'Each sentence has {X[0].shape} dimensions') <pre>Sentences: 3903\nEach sentence has (50,) dimensions\n</pre> In\u00a0[50]: Copied! <pre>X[0]\n</pre> X[0] Out[50]: <pre>array([-1.58635736e+00,  1.63898841e-01,  8.08457077e-01,  1.00274825e+00,\n        2.41569087e-01, -1.22120626e-01,  1.00032461e+00,  7.58375943e-01,\n       -8.77375901e-01, -1.50272891e-01,  9.70111862e-02, -8.17403615e-01,\n       -4.17878687e-01,  2.04743981e-01,  2.33034179e-01, -4.36471403e-02,\n       -2.71623224e-01,  4.84225806e-03, -1.14730798e-01, -4.46125358e-01,\n       -1.83330595e-01, -1.27363995e-01,  1.00556612e+00, -7.95672178e-01,\n        2.74445832e-01,  3.66903514e-01, -8.54147792e-01, -1.98808476e-01,\n       -2.93522835e-01, -7.79545486e-01, -8.95587206e-02,  1.49794325e-01,\n        4.46273059e-01,  5.50577581e-01, -1.22709799e+00, -8.10710043e-02,\n       -2.80248135e-01, -1.90826192e-01,  6.40392661e-01, -8.74473095e-01,\n        7.35989451e-01, -3.69897068e-01, -5.24353564e-01,  1.53910264e-03,\n        3.04219544e-01,  3.05280328e-01, -4.45715249e-01, -8.36181343e-01,\n       -1.15008622e-01, -7.36768246e-02], dtype=float32)</pre> In\u00a0[51]: Copied! <pre>import warnings; warnings.filterwarnings('ignore')\n\n# t-SNE \u2013 \u043c\u0435\u0442\u043e\u0434 \u043f\u043e\u043d\u0438\u0436\u0435\u043d\u0438\u044f \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438\nfrom sklearn.manifold import TSNE\n\n# \u0421\u043e\u0437\u0434\u0430\u0451\u043c \u043e\u0431\u044a\u0435\u043a\u0442 \u0434\u043b\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f t-SNE\ntsne = TSNE(n_components=2, random_state=0)\n\n# \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u043d\u0430\u0448\u0438 \u0434\u0430\u043d\u043d\u044b\u0435, \u043f\u043e\u043d\u0438\u0437\u0438\u0432 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0441 50 \u0434\u043e 2\nX = tsne.fit_transform(X)\n</pre> import warnings; warnings.filterwarnings('ignore')  # t-SNE \u2013 \u043c\u0435\u0442\u043e\u0434 \u043f\u043e\u043d\u0438\u0436\u0435\u043d\u0438\u044f \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 from sklearn.manifold import TSNE  # \u0421\u043e\u0437\u0434\u0430\u0451\u043c \u043e\u0431\u044a\u0435\u043a\u0442 \u0434\u043b\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f t-SNE tsne = TSNE(n_components=2, random_state=0)  # \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u043d\u0430\u0448\u0438 \u0434\u0430\u043d\u043d\u044b\u0435, \u043f\u043e\u043d\u0438\u0437\u0438\u0432 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0441 50 \u0434\u043e 2 X = tsne.fit_transform(X) In\u00a0[52]: Copied! <pre># \u0421\u043e\u0437\u0434\u0430\u0451\u043c KMeans \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0442\u043e\u0440 \nkmeans = KMeans(n_clusters=4)\n\n# \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0442\u043e\u0440 \u043d\u0430 \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043b\u0435\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\nkmeans.fit(X)\n\n# \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u044b\ny_pred = kmeans.labels_.astype(np.int)\n\n# \u041a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u044b \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0446\u0435\u043d\u0442\u0440\u043e\u0438\u0434\u043e\u0432\nprint (\"\u041a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u044b \u0446\u0435\u043d\u0442\u0440\u043e\u0438\u0434\u043e\u0432:\\n\", kmeans.cluster_centers_)\n</pre> # \u0421\u043e\u0437\u0434\u0430\u0451\u043c KMeans \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0442\u043e\u0440  kmeans = KMeans(n_clusters=4)  # \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0442\u043e\u0440 \u043d\u0430 \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043b\u0435\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 kmeans.fit(X)  # \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u044b y_pred = kmeans.labels_.astype(np.int)  # \u041a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u044b \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0446\u0435\u043d\u0442\u0440\u043e\u0438\u0434\u043e\u0432 print (\"\u041a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u044b \u0446\u0435\u043d\u0442\u0440\u043e\u0438\u0434\u043e\u0432:\\n\", kmeans.cluster_centers_) <pre>\u041a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u044b \u0446\u0435\u043d\u0442\u0440\u043e\u0438\u0434\u043e\u0432:\n [[ -6.8084016   46.407467  ]\n [ -5.7084146  -37.110226  ]\n [-39.812744    -0.33722958]\n [ 53.020237     1.1617556 ]]\n</pre> In\u00a0[59]: Copied! <pre>print (\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, y_pred, metric='euclidean'))\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, y_pred))\nprint(\"completeness: %0.3f\" % metrics.completeness_score(labels, y_pred))\nprint(\"V-meaure: %0.3f\" % metrics.v_measure_score(labels, y_pred))\n</pre> print (\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, y_pred, metric='euclidean')) print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, y_pred)) print(\"completeness: %0.3f\" % metrics.completeness_score(labels, y_pred)) print(\"V-meaure: %0.3f\" % metrics.v_measure_score(labels, y_pred)) <pre>Silhouette Coefficient: 0.510\nHomogeneity: 0.768\ncompleteness: 0.771\nV-meaure: 0.770\n</pre> In\u00a0[75]: Copied! <pre># \u0413\u0440\u0430\u0444\u0438\u043a\u0438 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432\n\nplt.rcParams['figure.figsize'] = 6,6\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, s=20,ec='k',alpha=.5)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', s=200, marker='+')\nplt.title('clusterisation result')\nplt.show()\n</pre> # \u0413\u0440\u0430\u0444\u0438\u043a\u0438 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432  plt.rcParams['figure.figsize'] = 6,6 plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=20,ec='k',alpha=.5) plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', s=200, marker='+') plt.title('clusterisation result') plt.show() In\u00a0[76]: Copied! <pre># \u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u0433\u0440\u0430\u0444\u0438\u043a \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0445 \u043a\u043b\u0430\u0441\u0441\u043e\u0432\ncolors = (\"red\", \"green\", \"blue\", \"yellow\")\n\nfor i in range(4):\n    plt.scatter(X[labels==i][:, 0], X[labels==i][:, 1], \\\n                s=20, alpha=.5, ec='k', c=colors[i], label=dataset.target_names[i])\n    plt.legend(loc=2)\n\nplt.title('true class labels')\nplt.show()\n</pre> # \u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u0433\u0440\u0430\u0444\u0438\u043a \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0445 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 colors = (\"red\", \"green\", \"blue\", \"yellow\")  for i in range(4):     plt.scatter(X[labels==i][:, 0], X[labels==i][:, 1], \\                 s=20, alpha=.5, ec='k', c=colors[i], label=dataset.target_names[i])     plt.legend(loc=2)  plt.title('true class labels') plt.show()"},{"location":"portfolio/sfml/ml4_15_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_%D0%A2%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%B2.html","title":"\u041a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u041d\u043e\u0432\u043e\u0441\u0442\u0435\u0439\u00b6","text":""},{"location":"portfolio/sfml/ml4_15_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_%D0%A2%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%B2.html","title":"\u041d\u0430\u0448\u0430 \u0417\u0430\u0434\u0430\u0447\u0430\u00b6","text":"<p>\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043f\u0440\u0438\u043c\u0435\u0440 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432 \u043d\u0430 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u043c \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445, \u043d\u043e\u0432\u043e\u0441\u0442\u044f\u0445, \u0432\u044b\u0431\u0435\u0440\u0435\u043c \u0432 \u043d\u0430\u0431\u043e\u0440\u0435 4 \u0438\u0437 20 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0439</p>"},{"location":"portfolio/sfml/ml4_15_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_%D0%A2%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%B2.html","title":"\u0421\u043a\u0430\u0447\u0438\u0432\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\u00b6","text":"<p>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f <code>fetch_20newsgroups</code>, \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e 4 \u043a\u043b\u0430\u0441\u0441\u0430\u043c\u0438</p>"},{"location":"portfolio/sfml/ml4_15_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_%D0%A2%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%B2.html","title":"\u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f\u00b6","text":"<p>\u0422\u0435\u043a\u0441\u0442\u044b \u043d\u043e\u0432\u043e\u0441\u0442\u0435\u0439 \u043d\u0443\u0436\u043d\u043e \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u0442\u044c \u0432 \u0447\u0438\u0441\u043b\u0430 \u0438 \u043f\u043e\u0447\u0438\u0441\u0442\u0438\u0442\u044c \u043e\u0442 \u043f\u043e\u0441\u0442\u043e\u0440\u043e\u043d\u043d\u0438\u0445 \u0441\u0438\u043c\u0432\u043e\u043b\u043e\u0432. \u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u043e\u043c \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 sklearn, \u043e\u043d \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u0442 <code>\u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435</code> \u0434\u0430\u043d\u043d\u044b\u0435</p> <p>\u041f\u0440\u0438\u043c\u0435\u0447\u0430\u043d\u0438\u0435: \u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u2014 \u044d\u0442\u043e \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0442\u0435\u043a\u0441\u0442\u0430 \u043d\u0430 \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u044b. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u043f\u043e \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f\u043c \u2014 \u044d\u0442\u043e \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043f\u0438\u0441\u044c\u043c\u0435\u043d\u043d\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 \u043d\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f-\u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u044b. \u0410 \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u043f\u043e \u0441\u043b\u043e\u0432\u0430\u043c \u2014 \u044d\u0442\u043e \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \u043d\u0430 \u0441\u043b\u043e\u0432\u0430-\u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u044b.</p>"},{"location":"portfolio/sfml/ml4_15_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_%D0%A2%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%B2.html","title":"\u0412\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u0421\u043b\u043e\u0432\u00b6","text":"<ul> <li>\u041a\u0430\u0436\u0434\u044b\u0439 \u0442\u0435\u043a\u0441\u0442 \u043f\u0440\u0435\u0432\u0440\u0430\u0442\u0438\u043b\u0441\u044f \u0432 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u043d\u0430\u0431\u043e\u0440 \u0441\u043b\u043e\u0432.</li> <li>\u041f\u0435\u0440\u0435\u0439\u0434\u0451\u043c \u043a \u0432\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432: \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u043c\u043e\u0434\u0435\u043b\u044c\u044e Word2Vec, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430 \u0441\u0442\u0440\u043e\u0438\u0442 \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u0439 \u0432\u0435\u043a\u0442\u043e\u0440, \u0432 \u043d\u0430\u0448\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u043a\u0430\u0436\u0434\u043e\u0435 \u0441\u043b\u043e\u0432\u043e \u0431\u0443\u0434\u0435\u0442 \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u0442\u044c\u0441\u044f 50 \u0447\u0438\u0441\u043b\u0430\u043c\u0438.</li> <li>\u041f\u0440\u0438 \u044d\u0442\u043e\u043c \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u0431\u0443\u0434\u0435\u043c \u0434\u0435\u043b\u0430\u0442\u044c \u0434\u043b\u044f \u0442\u0435\u0445 \u0441\u043b\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0442\u0441\u044f \u0431\u043e\u043b\u044c\u0448\u0435 20 \u0440\u0430\u0437 \u0432\u043e \u0432\u0441\u0435\u0445 \u0442\u0435\u043a\u0441\u0442\u0430\u0445 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432.</li> <li>\u0422\u0430\u043a\u0436\u0435 \u0441\u0434\u0435\u043b\u0430\u0435\u043c \u0441\u0440\u0435\u0434\u043d\u0438\u0439 \u0432\u0435\u043a\u0442\u043e\u0440 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0441\u043b\u043e\u0432, \u0442\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u0432\u0435\u043a\u0442\u043e\u0440 \u0434\u043b\u044f \u0442\u0435\u043a\u0441\u0442\u0430 \u0432 \u0446\u0435\u043b\u043e\u043c.</li> </ul>"},{"location":"portfolio/sfml/ml4_15_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_%D0%A2%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%B2.html","title":"\u0423\u043c\u0435\u043d\u044c\u0448\u0435\u043d\u0438\u044f \u0420\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438\u00b6","text":"<p>\u041d\u0430\u0448\u0430 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u0431\u0443\u0434\u0435\u0442 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043b\u0443\u0447\u0448\u0435, \u0435\u0441\u043b\u0438 \u0441\u043d\u0438\u0437\u0438\u0442\u044c \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c, \u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u043c\u0435\u0442\u043e\u0434\u043e\u043c <code>TSNE</code></p> <p>\u041f\u0440\u0438\u043c\u0435\u0447\u0430\u043d\u0438\u0435: <code>TSNE</code> \u2014 \u044d\u0442\u043e \u0442\u0435\u0445\u043d\u0438\u043a\u0430 \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0433\u043e \u0441\u043d\u0438\u0436\u0435\u043d\u0438\u044f \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \u0438 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u043c\u043d\u043e\u0433\u043e\u043c\u0435\u0440\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u043f\u043e\u0447\u0438\u0442\u0430\u0442\u044c \u043e \u043d\u0435\u0439 \u043c\u043e\u0436\u043d\u043e, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0437\u0434\u0435\u0441\u044c.</p> <p>\u041f\u0440\u0438 \u043f\u043e\u043d\u0438\u0436\u0435\u043d\u0438\u0438 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \u043c\u044b \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u0431\u043b\u0438\u0437\u043e\u0441\u0442\u044c \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432, \u0442\u043e \u0435\u0441\u0442\u044c \u0435\u0441\u043b\u0438 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u044b \u0431\u044b\u043b\u0438 \u0431\u043b\u0438\u0437\u043a\u0438 \u043f\u0440\u0438 <code>vector_size=50</code>, \u0442\u043e \u043e\u043d\u0438 \u043e\u0441\u0442\u0430\u043d\u0443\u0442\u0441\u044f \u0431\u043b\u0438\u0437\u043a\u0438 \u0438 \u043f\u0440\u0438 <code>vector_size=2</code></p>"},{"location":"portfolio/sfml/ml4_15_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_%D0%A2%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%B2.html","title":"\u041a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f\u00b6","text":"<p>\u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0442\u043e\u0440 <code>KMeans</code> \u0438 \u043e\u0431\u0443\u0447\u0438\u043c \u043d\u0430 \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043b\u0435\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445. \u041c\u044b \u043c\u043e\u0436\u0435\u043c \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0435\u0441\u044f \u0446\u0435\u043d\u0442\u0440\u043e\u0438\u0434\u044b \u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u044b.</p>"},{"location":"portfolio/sfml/ml4_15_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_%D0%A2%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%B2.html","title":"\u041e\u0446\u0435\u043d\u043a\u0430 \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432\u00b6","text":"<p>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0447\u0442\u043e \u043c\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438 \u0438 \u0441\u0440\u0430\u0432\u043d\u0438\u043c \u0441 \u0438\u0441\u0442\u0438\u043d\u044b\u043c\u0438 \u043c\u0435\u0442\u0430\u043a\u0430\u043c\u0438</p> <p><code>\u041a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u0441\u0438\u043b\u0443\u044d\u0442\u0430</code>, <code>\u043e\u0434\u043d\u043e\u0440\u043e\u0434\u043d\u043e\u0441\u0442\u044c</code>, <code>\u043f\u043e\u043b\u043d\u043e\u0442\u0430</code> \u0438 <code>V-\u043c\u0435\u0440\u0430</code> \u0433\u043e\u0432\u043e\u0440\u044f\u0442 \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u0434\u043e\u0432\u043e\u043b\u044c\u043d\u043e \u0446\u0435\u043b\u044c\u043d\u044b\u0435, \u0432\u044b\u0441\u043e\u043a\u0438\u0435 \u0441\u043e\u0432\u043f\u0430\u0434\u0435\u043d\u0438\u044f \u043c\u0435\u0436\u0434\u0443 \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u043c\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438</p>"},{"location":"portfolio/sfml/ml4_5_kMeans.html","title":"ml4 5 kMeans","text":"In\u00a0[1]: Copied! <pre>from sklearn.preprocessing import StandardScaler\nfrom itertools import cycle, islice\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n# import seaborn as sns; sns.set(style='whitegrid')\n\nnp.random.seed(0)\n\n# \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435\nn_samples = 1500\n\n# \u0412\u043f\u0438\u0441\u0430\u043d\u043d\u044b\u0435 \u043a\u0440\u0443\u0433\u0438\nnoisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n                                      noise=.05)\n# \u041e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430, \u0442\u0430\u043a \u043a\u0430\u043a \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u043d\u0430\u043c \u043d\u0435 \u043d\u0443\u0436\u043d\u044b \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0441\u044b \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432\nX, y = noisy_circles\nnoisy_circles = X\n\n# \u041f\u043e\u043b\u0443\u043a\u0440\u0443\u0433\u0438\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\nX, y = noisy_moons\nnoisy_moons = X\n\n# \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u0432 \u0444\u043e\u0440\u043c\u0435 \u043a\u0440\u0443\u0433\u0430\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\nX, y = blobs\nblobs = X\n\n# \u041e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043d\u043e\u0439 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b\nno_structure = np.random.rand(n_samples, 2)\n\n# \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u043b\u0435\u043d\u0442\u043e\u0432\u0438\u0434\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u044b    \nrandom_state = 170\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = X_aniso\n\n# \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u0432 \u0444\u043e\u0440\u043c\u0435 \u043a\u0440\u0443\u0433\u043e\u0432 \u0441 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u043e\u0439 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0435\u0439\nvaried = datasets.make_blobs(n_samples=n_samples,\n                             cluster_std=[1.0, 2.5, 0.5],\n                             random_state=random_state)\nX, y = varied\nvaried = X\n</pre> from sklearn.preprocessing import StandardScaler from itertools import cycle, islice  import numpy as np import seaborn as sns import matplotlib.pyplot as plt from sklearn import datasets # import seaborn as sns; sns.set(style='whitegrid')  np.random.seed(0)  # \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 n_samples = 1500  # \u0412\u043f\u0438\u0441\u0430\u043d\u043d\u044b\u0435 \u043a\u0440\u0443\u0433\u0438 noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,                                       noise=.05) # \u041e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430, \u0442\u0430\u043a \u043a\u0430\u043a \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u043d\u0430\u043c \u043d\u0435 \u043d\u0443\u0436\u043d\u044b \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0441\u044b \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 X, y = noisy_circles noisy_circles = X  # \u041f\u043e\u043b\u0443\u043a\u0440\u0443\u0433\u0438 noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05) X, y = noisy_moons noisy_moons = X  # \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u0432 \u0444\u043e\u0440\u043c\u0435 \u043a\u0440\u0443\u0433\u0430 blobs = datasets.make_blobs(n_samples=n_samples, random_state=8) X, y = blobs blobs = X  # \u041e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043d\u043e\u0439 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b no_structure = np.random.rand(n_samples, 2)  # \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u043b\u0435\u043d\u0442\u043e\u0432\u0438\u0434\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u044b     random_state = 170 X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state) transformation = [[0.6, -0.6], [-0.4, 0.8]] X_aniso = np.dot(X, transformation) aniso = X_aniso  # \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u0432 \u0444\u043e\u0440\u043c\u0435 \u043a\u0440\u0443\u0433\u043e\u0432 \u0441 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u043e\u0439 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0435\u0439 varied = datasets.make_blobs(n_samples=n_samples,                              cluster_std=[1.0, 2.5, 0.5],                              random_state=random_state) X, y = varied varied = X In\u00a0[8]: Copied! <pre>from sklearn.cluster import KMeans\n\n# k_means = KMeans(n_clusters=8,   \n#                  init='k-means++', # 'k-means++', 'random', numpy.array # \u0441\u043f\u043e\u0441\u043e\u0431 \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438\n#                  max_iter=300   # \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0432\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438 \n#                 )\n</pre> from sklearn.cluster import KMeans  # k_means = KMeans(n_clusters=8,    #                  init='k-means++', # 'k-means++', 'random', numpy.array # \u0441\u043f\u043e\u0441\u043e\u0431 \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 #                  max_iter=300   # \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0432\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438  #                 ) <p>\u0414\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 K-means \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0432\u044b\u0437\u0432\u0430\u0442\u044c \u043c\u0435\u0442\u043e\u0434 fit, \u043f\u0435\u0440\u0435\u0434\u0430\u0432 \u0432\u0445\u043e\u0434\u043d\u044b\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430:</p> <pre>kmeans_fit(X)\n</pre> <p>\u041e\u0431\u0443\u0447\u0438\u0432, \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u0446\u0435\u043d\u0442\u0440\u043e\u0438\u0434\u044b \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432:</p> <pre>k_means.cluster_centers_\n</pre> <p>\u0418 \u0443\u0437\u043d\u0430\u0442\u044c, \u0432 \u043a\u0430\u043a\u043e\u0439 \u043a\u043b\u0430\u0441\u0442\u0435\u0440 \u043f\u043e\u043f\u0430\u043b \u043a\u0430\u0436\u0434\u044b\u0439 \u0438\u0437 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432:</p> <pre>k_means.labels_\n</pre> In\u00a0[4]: Copied! <pre># \u0441\u043e\u0437\u0434\u0430\u0451\u043c \u043c\u0430\u0441\u0441\u0438\u0432 \u043f\u0430\u0440 \u2013 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u0434\u043b\u044f \u043d\u0435\u0433\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430\ndatasets_params_list = [\n    (blobs, {'n_clusters': 3}),\n    (varied, {'n_clusters': 3}),\n    (aniso, {'n_clusters': 3}),\n    (noisy_circles, {'n_clusters': 2}),\n    (noisy_moons, {'n_clusters': 2}),\n    (no_structure, {'n_clusters': 3})]\n\nfor i, (X, k_means_params) in enumerate(datasets_params_list, start=1):\n    X = StandardScaler().fit_transform(X)\n    k_means = KMeans(n_clusters=k_means_params['n_clusters'])\n    \n    k_means.fit(X)\n    y_pred = k_means.labels_.astype(np.int)\n\n    plt.subplot(f'23{i}')\n    plt.xticks([]); plt.yticks([])\n    colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n                                         '#f781bf', '#a65628', '#984ea3',\n                                         '#999999', '#e41a1c', '#dede00']),\n                                  int(max(y_pred) + 1))))\n    plt.scatter(X[:, 0], X[:, 1], s=5,color=colors[y_pred])\n</pre> # \u0441\u043e\u0437\u0434\u0430\u0451\u043c \u043c\u0430\u0441\u0441\u0438\u0432 \u043f\u0430\u0440 \u2013 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u0434\u043b\u044f \u043d\u0435\u0433\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 datasets_params_list = [     (blobs, {'n_clusters': 3}),     (varied, {'n_clusters': 3}),     (aniso, {'n_clusters': 3}),     (noisy_circles, {'n_clusters': 2}),     (noisy_moons, {'n_clusters': 2}),     (no_structure, {'n_clusters': 3})]  for i, (X, k_means_params) in enumerate(datasets_params_list, start=1):     X = StandardScaler().fit_transform(X)     k_means = KMeans(n_clusters=k_means_params['n_clusters'])          k_means.fit(X)     y_pred = k_means.labels_.astype(np.int)      plt.subplot(f'23{i}')     plt.xticks([]); plt.yticks([])     colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',                                          '#f781bf', '#a65628', '#984ea3',                                          '#999999', '#e41a1c', '#dede00']),                                   int(max(y_pred) + 1))))     plt.scatter(X[:, 0], X[:, 1], s=5,color=colors[y_pred]) In\u00a0[9]: Copied! <pre>models = KMeans(init='random')\n</pre> models = KMeans(init='random') In\u00a0[3]: Copied! <pre># import numpy as np\n# a = [0.4, 0.6, 1.7]\n# print(np.round(a).astype(np.int))\n</pre> # import numpy as np # a = [0.4, 0.6, 1.7] # print(np.round(a).astype(np.int)) <pre>[0 1 2]\n</pre> In\u00a0[11]: Copied! <pre>model = KMeans(n_clusters=3,\n                init='random',\n                random_state=42)\n</pre> model = KMeans(n_clusters=3,                 init='random',                 random_state=42) In\u00a0[31]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\nn_samples = 1500\ndataset = datasets.make_blobs(n_samples=n_samples, centers=2, center_box=(-7.0, 7.5),\n                              cluster_std=[1.4, 1.7],\n                              random_state=42)\nX_2, _ = datasets.make_blobs(n_samples=n_samples, random_state=170, centers=[[-4, -3]], cluster_std=[1.9])\ntransformation = [[1.2, -0.8], [-0.4, 1.7]]\nX_2 = np.dot(X_2, transformation)\nX, y = np.concatenate((dataset[0], X_2)), np.concatenate((dataset[1], np.array([2] * len(X_2))))\n\n# \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\nplt.rcParams['figure.figsize'] = 5, 5\nplt.scatter(X[:, 0], X[:, 1], s=5,c=y, alpha=0.5)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn import datasets  n_samples = 1500 dataset = datasets.make_blobs(n_samples=n_samples, centers=2, center_box=(-7.0, 7.5),                               cluster_std=[1.4, 1.7],                               random_state=42) X_2, _ = datasets.make_blobs(n_samples=n_samples, random_state=170, centers=[[-4, -3]], cluster_std=[1.9]) transformation = [[1.2, -0.8], [-0.4, 1.7]] X_2 = np.dot(X_2, transformation) X, y = np.concatenate((dataset[0], X_2)), np.concatenate((dataset[1], np.array([2] * len(X_2))))  # \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 plt.rcParams['figure.figsize'] = 5, 5 plt.scatter(X[:, 0], X[:, 1], s=5,c=y, alpha=0.5) plt.show() In\u00a0[26]: Copied! <pre>model.fit(X)\nlabels = model.labels_  # \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u044c \u043a\u043b\u0430\u0441\u0441\u043e\u0432\ncentroids = model.cluster_centers_\ncentroids\n</pre> model.fit(X) labels = model.labels_  # \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u044c \u043a\u043b\u0430\u0441\u0441\u043e\u0432 centroids = model.cluster_centers_ centroids Out[26]: <pre>array([[-2.35624901,  6.26585521],\n       [ 3.63560063,  1.74586847],\n       [-3.20312056, -2.87616171]])</pre> In\u00a0[19]: Copied! <pre>np.round(centroids).astype(int)\n</pre> np.round(centroids).astype(int) Out[19]: <pre>array([[-2,  6],\n       [ 4,  2],\n       [-3, -3]])</pre> In\u00a0[22]: Copied! <pre>from collections import Counter\n\nCounter(labels)\n</pre> from collections import Counter  Counter(labels) Out[22]: <pre>Counter({1: 741, 0: 933, 2: 1326})</pre> In\u00a0[32]: Copied! <pre>plt.rcParams['figure.figsize'] = 5, 5\nplt.scatter(X[:, 0], X[:, 1], s=5,c=labels, alpha=0.5)\nplt.show()\n</pre> plt.rcParams['figure.figsize'] = 5, 5 plt.scatter(X[:, 0], X[:, 1], s=5,c=labels, alpha=0.5) plt.show()"},{"location":"portfolio/sfml/ml4_5_kMeans.html#k-means","title":"K-means \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430\u00b6","text":"<ul> <li>\u0412 \u0445\u043e\u0434\u0435 \u044d\u0442\u043e\u0433\u043e \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 \u0432\u0441\u0435 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u044b \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0430 \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u044e\u0442\u0441\u044f \u043d\u0430 \u0437\u0430\u0440\u0430\u043d\u0435\u0435 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u043e\u0435 (\u0437\u0430\u0434\u0430\u043d\u043d\u043e\u0435) \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432.</li> <li>\u0421\u0443\u0442\u044c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0432 \u0442\u043e\u043c, \u0447\u0442\u043e \u043e\u043d \u043f\u044b\u0442\u0430\u0435\u0442\u0441\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0435 \u0432\u0441\u0435\u0445 \u0442\u043e\u0447\u0435\u043a \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0430 \u043e\u0442 \u0446\u0435\u043d\u0442\u0440\u0430 \u044d\u0442\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0430.</li> <li>\u0422\u043e \u0435\u0441\u0442\u044c \u043e\u043d \u0444\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u0442 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u0442\u0430\u043a, \u0447\u0442\u043e\u0431\u044b \u0432\u043d\u0443\u0442\u0440\u0438 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u0443\u043c\u043c\u0430 \u043a\u0432\u0430\u0434\u0440\u0430\u0442\u043e\u0432 \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0439 \u043e\u0442 \u0442\u043e\u0447\u0435\u043a \u0434\u043e \u0446\u0435\u043d\u0442\u0440\u0430 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0430 \u0431\u044b\u043b\u0430 \u043a\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u043c\u0435\u043d\u044c\u0448\u0435.</li> </ul>"},{"location":"portfolio/sfml/ml4_5_kMeans.html#k-means","title":"\u0421\u0445\u0435\u043c\u0430 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 k-means\u00b6","text":"<ul> <li>\u0412\u044b\u0431\u0440\u0430\u0442\u044c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u043d\u0430\u043c \u043a\u0430\u0436\u0435\u0442\u0441\u044f \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u043c.</li> <li>\u0412\u044b\u0431\u0440\u0430\u0442\u044c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c \u0432 \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u043e \u043d\u0430\u0448\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0446\u0435\u043d\u0442\u0440\u043e\u0438\u0434\u044b.</li> <li>\u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u0442\u043e\u0447\u043a\u0438 \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c, \u043a \u043a\u0430\u043a\u043e\u043c\u0443 \u0446\u0435\u043d\u0442\u0440\u043e\u0438\u0434\u0443 \u043e\u043d\u0430 \u0431\u043b\u0438\u0436\u0435.</li> <li>\u041f\u0435\u0440\u0435\u043c\u0435\u0441\u0442\u0438\u0442\u044c \u043a\u0430\u0436\u0434\u044b\u0439 \u0446\u0435\u043d\u0442\u0440\u043e\u0438\u0434 \u0432 \u0446\u0435\u043d\u0442\u0440 \u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043c\u044b \u043e\u0442\u043d\u0435\u0441\u043b\u0438 \u043a \u044d\u0442\u043e\u043c\u0443 \u0446\u0435\u043d\u0442\u0440\u043e\u0438\u0434\u0443. \u041a\u0430\u0436\u0434\u044b\u0439 \u0446\u0435\u043d\u0442\u0440\u043e\u0438\u0434 \u043d\u0430 \u043a\u0430\u0436\u0434\u043e\u0439 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438 \u2014 \u0432\u0435\u043a\u0442\u043e\u0440, \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u044b \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u0441\u043e\u0431\u043e\u0439 \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u043d\u044b\u0435 \u043f\u043e \u0432\u0441\u0435\u043c \u0437\u0430\u043f\u0438\u0441\u044f\u043c \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0430.</li> <li>\u041f\u043e\u0432\u0442\u043e\u0440\u044f\u0442\u044c \u0448\u0430\u0433\u0438 3-4 \u0444\u0438\u043a\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0435 \u0447\u0438\u0441\u043b\u043e \u0440\u0430\u0437 \u0438\u043b\u0438 \u0434\u043e \u0442\u0435\u0445 \u043f\u043e\u0440, \u043f\u043e\u043a\u0430 \u0446\u0435\u043d\u0442\u0440\u043e\u0438\u0434\u044b \u043d\u0435 \u0441\u043e\u0439\u0434\u0443\u0442\u0441\u044f.</li> </ul>"},{"location":"portfolio/sfml/ml4_5_kMeans.html","title":"\u041d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043a\u0438:\u00b6","text":"<ul> <li>\u0447\u0438\u0441\u043b\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u043d\u0430\u0434\u043e \u0437\u043d\u0430\u0442\u044c \u0437\u0430\u0440\u0430\u043d\u0435\u0435;</li> <li>\u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u043e\u0447\u0435\u043d\u044c \u0447\u0443\u0432\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u0435\u043d \u043a \u043f\u0435\u0440\u0432\u0438\u0447\u043d\u043e\u043c\u0443 \u0432\u044b\u0431\u043e\u0440\u0443 \u0446\u0435\u043d\u0442\u0440\u043e\u0438\u0434\u043e\u0432;</li> <li>\u043d\u0435 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u0443\u0435\u0442 \u0434\u043e\u0441\u0442\u0438\u0436\u0435\u043d\u0438\u0435 \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u043c\u0438\u043d\u0438\u043c\u0443\u043c\u0430 \u0441\u0443\u043c\u043c\u044b \u043a\u0432\u0430\u0434\u0440\u0430\u0442\u043e\u0432 \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0439, \u0447\u0430\u0441\u0442\u043e \u00ab\u0437\u0430\u0441\u0442\u0440\u0435\u0432\u0430\u0435\u0442\u00bb \u0432 \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u043e\u043c \u043c\u0438\u043d\u0438\u043c\u0443\u043c\u0435.</li> </ul> <p>\u0423 \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 \u0435\u0441\u0442\u044c \u0442\u0430\u043a\u0436\u0435 \u0432\u0430\u0440\u0438\u0430\u0446\u0438\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u044e\u0442\u0441\u044f \u0432 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0441\u043f\u0435\u0446\u0438\u0444\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0441\u043b\u0443\u0447\u0430\u044f\u0445. \u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0438\u0445 \u043d\u0438\u0436\u0435.</p>"},{"location":"portfolio/sfml/ml4_5_kMeans.html#mini-batch-k-means","title":"Mini-Batch K-means\u00b6","text":"<ul> <li>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0432 \u0441\u043b\u0443\u0447\u0430\u0435, \u0435\u0441\u043b\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e\u0447\u0435\u043d\u044c \u043c\u043d\u043e\u0433\u043e. \u0418\u0437-\u0437\u0430 \u043e\u0431\u044a\u0435\u043c\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \u0446\u0435\u043d\u0442\u0440\u043e\u0432 \u043f\u043e \u0432\u0441\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442 \u0434\u043e\u043b\u0433\u043e.</li> <li>\u0420\u0435\u0448\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b: \u043d\u0430 \u043a\u0430\u0436\u0434\u043e\u043c \u0448\u0430\u0433\u0435 k-means \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u043e\u0439 \u0434\u0430\u043d\u043d\u044b\u0445.</li> <li>\u0412 \u043e\u0431\u0449\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u0443\u043f\u0440\u043e\u0449\u0451\u043d\u043d\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0434\u043e\u043b\u0436\u0435\u043d \u0441\u0445\u043e\u0434\u0438\u0442\u044c\u0441\u044f \u043a \u0442\u043e\u043c\u0443 \u0436\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0443, \u0447\u0442\u043e \u0438 \u043d\u0430 \u043f\u043e\u043b\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435.</li> <li>\u041e\u0434\u043d\u0430\u043a\u043e \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0442, \u0447\u0442\u043e \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u043c\u043e\u0433\u0443\u0442 \u0443\u0445\u0443\u0434\u0448\u0430\u0442\u044c\u0441\u044f \u043f\u043e \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044e \u0441 \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0438\u043c k-means.</li> </ul>"},{"location":"portfolio/sfml/ml4_5_kMeans.html#k-means","title":"K-means++\u00b6","text":"<ul> <li>\u0415\u0449\u0451 \u043e\u0434\u043d\u0443 \u0432\u0430\u0440\u0438\u0430\u0446\u0438\u044e \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 k-means \u043c\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0432 \u0442\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435, \u0435\u0441\u043b\u0438 \u0443 \u043d\u0430\u0441 \u043e\u0447\u0435\u043d\u044c \u043c\u043d\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432.</li> <li>\u041a\u0430\u043a \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u043e, \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0438 \u0432\u0440\u0435\u043c\u044f \u0440\u0430\u0431\u043e\u0442\u044b k-means \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u0438\u0437\u043d\u0430\u0447\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0432\u044b\u0431\u043e\u0440\u0430 \u0446\u0435\u043d\u0442\u0440\u043e\u0432. \u0427\u0442\u043e\u0431\u044b \u043c\u0438\u043d\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0437\u0430\u0442\u0440\u0430\u0442\u044b, \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c:</li> <li>\u041f\u0435\u0440\u0432\u044b\u0439 \u0446\u0435\u043d\u0442\u0440 \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e \u0438\u0437 \u0440\u0430\u0432\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0433\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043d\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0435.</li> <li>\u041a\u0430\u0436\u0434\u044b\u0439 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0446\u0435\u043d\u0442\u0440 \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e \u0438\u0437 \u043e\u0441\u0442\u0430\u0432\u0448\u0438\u0445\u0441\u044f \u0442\u043e\u0447\u0435\u043a \u0442\u0430\u043a, \u0447\u0442\u043e\u0431\u044b \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u0432\u044b\u0431\u0440\u0430\u0442\u044c \u043a\u0430\u0436\u0434\u0443\u044e \u0442\u043e\u0447\u043a\u0443 \u0431\u044b\u043b\u0430 \u043f\u0440\u043e\u043f\u043e\u0440\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u0430 \u043a\u0432\u0430\u0434\u0440\u0430\u0442\u0443 \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u044f \u043e\u0442 \u043d\u0435\u0451 \u0434\u043e \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0435\u0433\u043e \u0446\u0435\u043d\u0442\u0440\u0430.</li> </ul>"},{"location":"portfolio/sfml/ml4_5_kMeans.html","title":"\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\u00b6","text":""},{"location":"portfolio/sfml/ml4_5_kMeans.html","title":"\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u041c\u043e\u0434\u0435\u043b\u044c\u00b6","text":"<p>\u0413\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438</p> <ul> <li><code>n_clusters</code> \u0447\u0438\u0441\u043b\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043c\u044b \u043e\u0436\u0438\u0434\u0430\u0435\u043c</li> <li><code>init</code> \u0441\u043f\u043e\u0441\u043e\u0431 \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438</li> <li><code>max_iter</code> \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0432\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438</li> </ul>"},{"location":"portfolio/sfml/ml4_5_kMeans.html","title":"\u041f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u041c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<p>\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c 6 \u0440\u0430\u0437\u043d\u044b\u0445 \u0442\u0438\u043f\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445, \u0438 \u043a\u0430\u043a <code>k-means</code> \u0441\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0441 \u043d\u0438\u043c\u0438</p>"},{"location":"portfolio/sfml/ml4_5_kMeans.html","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/sfml/ml4_5_kMeans.html#451","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.5.1\u00b6","text":"<p>\u041f\u0440\u0438 \u043a\u0430\u043a\u043e\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430 init \u043f\u0440\u0438 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0438 \u043e\u0431\u044a\u0435\u043a\u0442\u0430 K-means \u0446\u0435\u043d\u0442\u0440\u043e\u0438\u0434\u044b \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0431\u0443\u0434\u0443\u0442 \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c \u0432 \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430?</p> <p>\u0414\u043b\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0437\u0430\u0434\u0430\u043d\u0438\u044f \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043e\u0437\u043d\u0430\u043a\u043e\u043c\u0438\u0442\u0441\u044f \u0441 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0435\u0439 \u043a \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 K-means \u0432 \u043f\u0430\u043a\u0435\u0442\u0435 scikit-learn.</p>"},{"location":"portfolio/sfml/ml4_5_kMeans.html#452","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.5.2\u00b6","text":"<p>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u044c K-means \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 <code>n_clusters=3</code> \u0438 <code>random_state=42</code> \u043d\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430.</p> <p>\u041a\u0430\u043a\u0438\u0435 \u0446\u0435\u043d\u0442\u0440\u043e\u0438\u0434\u044b \u0431\u0443\u0434\u0443\u0442 \u0443 \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0445\u0441\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432? \u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u043e\u0442\u0432\u0435\u0442 \u0432 \u0432\u0438\u0434\u0435 \u043c\u0430\u0441\u0441\u0438\u0432\u0430. \u041a\u0430\u0436\u0434\u043e\u0435 \u0447\u0438\u0441\u043b\u043e \u0432 \u043e\u0442\u0432\u0435\u0442\u0435 \u043e\u043a\u0440\u0443\u0433\u043b\u0438\u0442\u0435 \u0434\u043e \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0435\u0433\u043e \u0446\u0435\u043b\u043e\u0433\u043e. \u0414\u043b\u044f \u043e\u043a\u0440\u0443\u0433\u043b\u0435\u043d\u0438\u044f \u043c\u043e\u0436\u043d\u043e \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0435\u0439 numpy.round</p>"},{"location":"portfolio/sfml/ml4_5_kMeans.html#453","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.5.3\u00b6","text":"<ul> <li>\u041f\u043e\u0434\u0441\u0447\u0438\u0442\u0430\u0439\u0442\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u0438\u0437 \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0445\u0441\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432.</li> <li>\u0417\u0430\u043f\u0438\u0448\u0438\u0442\u0435 \u0432 \u0444\u043e\u0440\u043c\u0443 \u043d\u0438\u0436\u0435 \u0442\u0440\u0438 \u0447\u0438\u0441\u043b\u0430 \u0447\u0435\u0440\u0435\u0437 \u043f\u0440\u043e\u0431\u0435\u043b(\u0431\u0435\u0437 \u0437\u0430\u043f\u044f\u0442\u044b\u0445!): \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0435 0, \u0432 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0435 1 \u0438 \u0432 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0435 2.</li> <li>\u0417\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0439\u0442\u0435 \u0441\u0442\u0440\u043e\u0433\u043e \u0432 \u0442\u0430\u043a\u043e\u043c \u043f\u043e\u0440\u044f\u0434\u043a\u0435.</li> </ul> <p>\u0414\u043b\u044f \u043f\u043e\u0434\u0441\u0447\u0451\u0442\u0430 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u0441\u043f\u0438\u0441\u043a\u0435 \u043c\u043e\u0436\u043d\u043e \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0435\u0439 <code>numpy.unique</code> \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u043c <code>return_counts=True</code></p>"},{"location":"portfolio/sfml/ml4_6_EM_%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC.html","title":"ml4 6 EM \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c","text":"In\u00a0[2]: Copied! <pre>from sklearn.preprocessing import StandardScaler\nfrom itertools import cycle, islice\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n# import seaborn as sns; sns.set(style='whitegrid')\n\nnp.random.seed(0)\n\n# \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435\nn_samples = 1500\n\n# \u0412\u043f\u0438\u0441\u0430\u043d\u043d\u044b\u0435 \u043a\u0440\u0443\u0433\u0438\nnoisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n                                      noise=.05)\n# \u041e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430, \u0442\u0430\u043a \u043a\u0430\u043a \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u043d\u0430\u043c \u043d\u0435 \u043d\u0443\u0436\u043d\u044b \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0441\u044b \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432\nX, y = noisy_circles\nnoisy_circles = X\n\n# \u041f\u043e\u043b\u0443\u043a\u0440\u0443\u0433\u0438\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\nX, y = noisy_moons\nnoisy_moons = X\n\n# \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u0432 \u0444\u043e\u0440\u043c\u0435 \u043a\u0440\u0443\u0433\u0430\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\nX, y = blobs\nblobs = X\n\n# \u041e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043d\u043e\u0439 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b\nno_structure = np.random.rand(n_samples, 2)\n\n# \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u043b\u0435\u043d\u0442\u043e\u0432\u0438\u0434\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u044b    \nrandom_state = 170\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = X_aniso\n\n# \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u0432 \u0444\u043e\u0440\u043c\u0435 \u043a\u0440\u0443\u0433\u043e\u0432 \u0441 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u043e\u0439 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0435\u0439\nvaried = datasets.make_blobs(n_samples=n_samples,\n                             cluster_std=[1.0, 2.5, 0.5],\n                             random_state=random_state)\nX, y = varied\nvaried = X\n</pre> from sklearn.preprocessing import StandardScaler from itertools import cycle, islice  import numpy as np import seaborn as sns import matplotlib.pyplot as plt from sklearn import datasets # import seaborn as sns; sns.set(style='whitegrid')  np.random.seed(0)  # \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 n_samples = 1500  # \u0412\u043f\u0438\u0441\u0430\u043d\u043d\u044b\u0435 \u043a\u0440\u0443\u0433\u0438 noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,                                       noise=.05) # \u041e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430, \u0442\u0430\u043a \u043a\u0430\u043a \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u043d\u0430\u043c \u043d\u0435 \u043d\u0443\u0436\u043d\u044b \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0441\u044b \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 X, y = noisy_circles noisy_circles = X  # \u041f\u043e\u043b\u0443\u043a\u0440\u0443\u0433\u0438 noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05) X, y = noisy_moons noisy_moons = X  # \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u0432 \u0444\u043e\u0440\u043c\u0435 \u043a\u0440\u0443\u0433\u0430 blobs = datasets.make_blobs(n_samples=n_samples, random_state=8) X, y = blobs blobs = X  # \u041e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043d\u043e\u0439 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b no_structure = np.random.rand(n_samples, 2)  # \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u043b\u0435\u043d\u0442\u043e\u0432\u0438\u0434\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u044b     random_state = 170 X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state) transformation = [[0.6, -0.6], [-0.4, 0.8]] X_aniso = np.dot(X, transformation) aniso = X_aniso  # \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u0432 \u0444\u043e\u0440\u043c\u0435 \u043a\u0440\u0443\u0433\u043e\u0432 \u0441 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u043e\u0439 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0435\u0439 varied = datasets.make_blobs(n_samples=n_samples,                              cluster_std=[1.0, 2.5, 0.5],                              random_state=random_state) X, y = varied varied = X In\u00a0[4]: Copied! <pre>from sklearn.mixture import GaussianMixture\n\n# em_gm = GaussianMixture(n_components=1, \n#                         max_iter=100,\n#                         init_params='kmeans' # 'kmeans\u2019, \u2018random\u2019\n#                        )\n</pre> from sklearn.mixture import GaussianMixture  # em_gm = GaussianMixture(n_components=1,  #                         max_iter=100, #                         init_params='kmeans' # 'kmeans\u2019, \u2018random\u2019 #                        ) <p>\u041f\u0440\u0438\u043c\u0435\u0440 \u0440\u0430\u0431\u043e\u0442\u044b EM-\u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430:</p> <ul> <li>\u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0447\u0438\u0441\u043b\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u043d\u0430\u043c \u043a\u0430\u0436\u0435\u0442\u0441\u044f \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u043c, \u0432 \u043d\u0430\u0448\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u0438\u0445 \u0434\u0432\u0430;</li> <li>\u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439;</li> <li>\u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u0442\u043e\u0447\u043a\u0438 \u043d\u0430\u0431\u043e\u0440\u0430 \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043a\u0430\u0436\u0434\u043e\u043c\u0443 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044e;</li> <li>\u041f\u0435\u0440\u0435\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f;</li> <li>\u041f\u043e\u0432\u0442\u043e\u0440\u044f\u0435\u043c \u0448\u0430\u0433\u0438: \u0441\u043d\u043e\u0432\u0430 \u043f\u0440\u0438\u0441\u0432\u0430\u0438\u0432\u0430\u0435\u043c \u0442\u043e\u0447\u043a\u0443 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044e \u0438 \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b.</li> </ul> <p>\u0414\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 <code>GaussianMixture</code> \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0432\u044b\u0437\u0432\u0430\u0442\u044c \u043c\u0435\u0442\u043e\u0434 <code>fit</code>, \u043f\u0435\u0440\u0435\u0434\u0430\u0432 \u0432\u0445\u043e\u0434\u043d\u044b\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430:</p> <pre>gm.fit(X)\n</pre> <p>\u041f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u043e\u0436\u043d\u043e \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u043e\u0434\u0430 predict \u043f\u043e\u0441\u043b\u0435 \u043c\u0435\u0442\u043e\u0434\u0430 fit:</p> <pre>y_pred = gm.predict(X)\n</pre> <p>\u041c\u043e\u0436\u043d\u043e \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u0438\u0442\u044c \u044d\u0442\u0438 \u0434\u0432\u0430 \u0448\u0430\u0433\u0430 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u043c\u0435\u0442\u043e\u0434\u0430 <code>fit_predict</code>:</p> <pre>y_pred = gm.fit_predict(X)\n</pre> <p>\u041e\u0431\u0443\u0447\u0438\u0432, \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0441\u0440\u0435\u0434\u043d\u0438\u0435:</p> <pre>means = gm.means_\n</pre> In\u00a0[5]: Copied! <pre># \u0441\u043e\u0437\u0434\u0430\u0451\u043c \u043c\u0430\u0441\u0441\u0438\u0432 \u043f\u0430\u0440 \u2013 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u0434\u043b\u044f \u043d\u0435\u0433\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430\ndatasets_params_list = [\n    (blobs, {'n_clusters': 3}),\n    (varied, {'n_clusters': 3}),\n    (aniso, {'n_clusters': 3}),\n    (noisy_circles, {'n_clusters': 2}),\n    (noisy_moons, {'n_clusters': 2}),\n    (no_structure, {'n_clusters': 3})]\n\nfor i, (X, k_means_params) in enumerate(datasets_params_list, start=1):\n    X = StandardScaler().fit_transform(X)\n    gm = GaussianMixture(n_components=k_means_params['n_clusters'])\n    \n    gm.fit(X)\n    y_pred = gm.predict(X).astype(np.int)\n\n    plt.subplot(f'23{i}')\n    plt.xticks([]); plt.yticks([])\n    colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n                                         '#f781bf', '#a65628', '#984ea3',\n                                         '#999999', '#e41a1c', '#dede00']),\n                                  int(max(y_pred) + 1))))\n    plt.scatter(X[:, 0], X[:, 1], s=5,color=colors[y_pred])\n</pre> # \u0441\u043e\u0437\u0434\u0430\u0451\u043c \u043c\u0430\u0441\u0441\u0438\u0432 \u043f\u0430\u0440 \u2013 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u0434\u043b\u044f \u043d\u0435\u0433\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 datasets_params_list = [     (blobs, {'n_clusters': 3}),     (varied, {'n_clusters': 3}),     (aniso, {'n_clusters': 3}),     (noisy_circles, {'n_clusters': 2}),     (noisy_moons, {'n_clusters': 2}),     (no_structure, {'n_clusters': 3})]  for i, (X, k_means_params) in enumerate(datasets_params_list, start=1):     X = StandardScaler().fit_transform(X)     gm = GaussianMixture(n_components=k_means_params['n_clusters'])          gm.fit(X)     y_pred = gm.predict(X).astype(np.int)      plt.subplot(f'23{i}')     plt.xticks([]); plt.yticks([])     colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',                                          '#f781bf', '#a65628', '#984ea3',                                          '#999999', '#e41a1c', '#dede00']),                                   int(max(y_pred) + 1))))     plt.scatter(X[:, 0], X[:, 1], s=5,color=colors[y_pred]) In\u00a0[6]: Copied! <pre>models = GaussianMixture(n_components=3)\n</pre> models = GaussianMixture(n_components=3) In\u00a0[3]: Copied! <pre># import numpy as np\n# a = [0.4, 0.6, 1.7]\n# print(np.round(a).astype(np.int))\n</pre> # import numpy as np # a = [0.4, 0.6, 1.7] # print(np.round(a).astype(np.int)) <pre>[0 1 2]\n</pre> In\u00a0[9]: Copied! <pre>model = GaussianMixture(n_components=3,\n                        random_state=42)\n</pre> model = GaussianMixture(n_components=3,                         random_state=42) In\u00a0[10]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\nn_samples = 1500\ndataset = datasets.make_blobs(n_samples=n_samples, centers=2, center_box=(-7.0, 7.5),\n                              cluster_std=[1.4, 1.7],\n                              random_state=42)\nX_2, _ = datasets.make_blobs(n_samples=n_samples, random_state=170, centers=[[-4, -3]], cluster_std=[1.9])\ntransformation = [[1.2, -0.8], [-0.4, 1.7]]\nX_2 = np.dot(X_2, transformation)\nX, y = np.concatenate((dataset[0], X_2)), np.concatenate((dataset[1], np.array([2] * len(X_2))))\n\n# \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\nplt.rcParams['figure.figsize'] = 5, 5\nplt.scatter(X[:, 0], X[:, 1], s=5,c=y, alpha=0.5)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn import datasets  n_samples = 1500 dataset = datasets.make_blobs(n_samples=n_samples, centers=2, center_box=(-7.0, 7.5),                               cluster_std=[1.4, 1.7],                               random_state=42) X_2, _ = datasets.make_blobs(n_samples=n_samples, random_state=170, centers=[[-4, -3]], cluster_std=[1.9]) transformation = [[1.2, -0.8], [-0.4, 1.7]] X_2 = np.dot(X_2, transformation) X, y = np.concatenate((dataset[0], X_2)), np.concatenate((dataset[1], np.array([2] * len(X_2))))  # \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 plt.rcParams['figure.figsize'] = 5, 5 plt.scatter(X[:, 0], X[:, 1], s=5,c=y, alpha=0.5) plt.show() In\u00a0[13]: Copied! <pre>model.fit(X)\nlabels = model.predict(X)\ncentroids = model.means_\ncentroids\n</pre> model.fit(X) labels = model.predict(X) centroids = model.means_ centroids Out[13]: <pre>array([[-2.21861264, -4.15574239],\n       [ 1.01124148,  4.31664695],\n       [-4.94579669,  0.04257022]])</pre> In\u00a0[14]: Copied! <pre>np.round(centroids).astype(int)\n</pre> np.round(centroids).astype(int) Out[14]: <pre>array([[-2, -4],\n       [ 1,  4],\n       [-5,  0]])</pre> In\u00a0[16]: Copied! <pre>from collections import Counter\n\nCounter(labels)\n</pre> from collections import Counter  Counter(labels) Out[16]: <pre>Counter({1: 1510, 0: 788, 2: 702})</pre> In\u00a0[17]: Copied! <pre>plt.rcParams['figure.figsize'] = 5, 5\nplt.scatter(X[:, 0], X[:, 1], s=5,c=labels, alpha=0.5)\nplt.show()\n</pre> plt.rcParams['figure.figsize'] = 5, 5 plt.scatter(X[:, 0], X[:, 1], s=5,c=labels, alpha=0.5) plt.show()"},{"location":"portfolio/sfml/ml4_6_EM_%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC.html#em","title":"EM \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430\u00b6","text":""},{"location":"portfolio/sfml/ml4_6_EM_%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC.html","title":"\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\u00b6","text":""},{"location":"portfolio/sfml/ml4_6_EM_%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC.html","title":"\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u041c\u043e\u0434\u0435\u043b\u044c\u00b6","text":"<p>\u0413\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438</p> <ul> <li><code>n_components</code> \u0447\u0438\u0441\u043b\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043c\u044b \u043e\u0436\u0438\u0434\u0430\u0435\u043c</li> <li><code>init_params</code> \u0441\u043f\u043e\u0441\u043e\u0431 \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438</li> <li><code>max_iter</code> \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0432\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438</li> </ul>"},{"location":"portfolio/sfml/ml4_6_EM_%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC.html","title":"\u041f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u041c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<p>\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c 6 \u0440\u0430\u0437\u043d\u044b\u0445 \u0442\u0438\u043f\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445, \u0438 \u043a\u0430\u043a <code>GaussianMixture</code> \u0441\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0441 \u043d\u0438\u043c\u0438</p>"},{"location":"portfolio/sfml/ml4_6_EM_%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC.html","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/sfml/ml4_6_EM_%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC.html#461","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.6.1\u00b6","text":"<p>\u0412 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0435 K-means \u0437\u0430 \u0447\u0438\u0441\u043b\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u043e\u0442\u0432\u0435\u0447\u0430\u043b \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 n_clusters. \u041a\u0430\u043a\u043e\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u0437\u0430\u0434\u0430\u0451\u0442 \u0447\u0438\u0441\u043b\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u0432 EM-\u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0435?</p>"},{"location":"portfolio/sfml/ml4_6_EM_%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC.html#462","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.6.2\u00b6","text":"<p>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u044c <code>GaussianMixture</code> \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 <code>n_components=3</code> \u0438 <code>random_state=42</code> \u043d\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430.</p> <p>\u041a\u0430\u043a\u0438\u0435 \u0446\u0435\u043d\u0442\u0440\u043e\u0438\u0434\u044b \u0431\u0443\u0434\u0443\u0442 \u0443 \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0445\u0441\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432? \u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u043e\u0442\u0432\u0435\u0442 \u0432 \u0432\u0438\u0434\u0435 \u043c\u0430\u0441\u0441\u0438\u0432\u0430. \u041a\u0430\u0436\u0434\u043e\u0435 \u0447\u0438\u0441\u043b\u043e \u0432 \u043e\u0442\u0432\u0435\u0442\u0435 \u043e\u043a\u0440\u0443\u0433\u043b\u0438\u0442\u0435 \u0434\u043e \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0435\u0433\u043e \u0446\u0435\u043b\u043e\u0433\u043e. \u0414\u043b\u044f \u043e\u043a\u0440\u0443\u0433\u043b\u0435\u043d\u0438\u044f \u043c\u043e\u0436\u043d\u043e \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0435\u0439 numpy.round</p>"},{"location":"portfolio/sfml/ml4_6_EM_%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC.html#463","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.6.3\u00b6","text":"<ul> <li>\u041f\u043e\u0434\u0441\u0447\u0438\u0442\u0430\u0439\u0442\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u0438\u0437 \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0445\u0441\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432.</li> <li>\u0417\u0430\u043f\u0438\u0448\u0438\u0442\u0435 \u0432 \u0444\u043e\u0440\u043c\u0443 \u043d\u0438\u0436\u0435 \u0442\u0440\u0438 \u0447\u0438\u0441\u043b\u0430 \u0447\u0435\u0440\u0435\u0437 \u043f\u0440\u043e\u0431\u0435\u043b(\u0431\u0435\u0437 \u0437\u0430\u043f\u044f\u0442\u044b\u0445!): \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0435 0, \u0432 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0435 1 \u0438 \u0432 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0435 2.</li> <li>\u0417\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0439\u0442\u0435 \u0441\u0442\u0440\u043e\u0433\u043e \u0432 \u0442\u0430\u043a\u043e\u043c \u043f\u043e\u0440\u044f\u0434\u043a\u0435.</li> </ul> <p>\u0414\u043b\u044f \u043f\u043e\u0434\u0441\u0447\u0451\u0442\u0430 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u0441\u043f\u0438\u0441\u043a\u0435 \u043c\u043e\u0436\u043d\u043e \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0435\u0439 <code>numpy.unique</code> \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u043c <code>return_counts=True</code></p>"},{"location":"portfolio/sfml/ml4_7_%D0%90%D0%B3%D0%BB%D0%BE%D0%BC%D0%B5%D1%80%D0%B0%D1%82%D0%B8%D0%B2%D0%BD%D0%B0%D1%8F_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F.html","title":"ml4 7 \u0410\u0433\u043b\u043e\u043c\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u0430\u044f \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f","text":"In\u00a0[1]: Copied! <pre>from sklearn.preprocessing import StandardScaler\nfrom itertools import cycle, islice\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nimport seaborn as sns; sns.set(style='whitegrid')\n\nnp.random.seed(0)\n\n# \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435\nn_samples = 1500\n\n# \u0412\u043f\u0438\u0441\u0430\u043d\u043d\u044b\u0435 \u043a\u0440\u0443\u0433\u0438\nnoisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n                                      noise=.05)\n# \u041e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430, \u0442\u0430\u043a \u043a\u0430\u043a \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u043d\u0430\u043c \u043d\u0435 \u043d\u0443\u0436\u043d\u044b \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0441\u044b \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432\nX, y = noisy_circles\nnoisy_circles = X\n\n# \u041f\u043e\u043b\u0443\u043a\u0440\u0443\u0433\u0438\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\nX, y = noisy_moons\nnoisy_moons = X\n\n# \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u0432 \u0444\u043e\u0440\u043c\u0435 \u043a\u0440\u0443\u0433\u0430\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\nX, y = blobs\nblobs = X\n\n# \u041e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043d\u043e\u0439 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b\nno_structure = np.random.rand(n_samples, 2)\n\n# \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u043b\u0435\u043d\u0442\u043e\u0432\u0438\u0434\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u044b    \nrandom_state = 170\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = X_aniso\n\n# \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u0432 \u0444\u043e\u0440\u043c\u0435 \u043a\u0440\u0443\u0433\u043e\u0432 \u0441 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u043e\u0439 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0435\u0439\nvaried = datasets.make_blobs(n_samples=n_samples,\n                             cluster_std=[1.0, 2.5, 0.5],\n                             random_state=random_state)\nX, y = varied\nvaried = X\n</pre> from sklearn.preprocessing import StandardScaler from itertools import cycle, islice  import numpy as np import seaborn as sns import matplotlib.pyplot as plt from sklearn import datasets import seaborn as sns; sns.set(style='whitegrid')  np.random.seed(0)  # \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 n_samples = 1500  # \u0412\u043f\u0438\u0441\u0430\u043d\u043d\u044b\u0435 \u043a\u0440\u0443\u0433\u0438 noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,                                       noise=.05) # \u041e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430, \u0442\u0430\u043a \u043a\u0430\u043a \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u043d\u0430\u043c \u043d\u0435 \u043d\u0443\u0436\u043d\u044b \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0441\u044b \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 X, y = noisy_circles noisy_circles = X  # \u041f\u043e\u043b\u0443\u043a\u0440\u0443\u0433\u0438 noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05) X, y = noisy_moons noisy_moons = X  # \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u0432 \u0444\u043e\u0440\u043c\u0435 \u043a\u0440\u0443\u0433\u0430 blobs = datasets.make_blobs(n_samples=n_samples, random_state=8) X, y = blobs blobs = X  # \u041e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043d\u043e\u0439 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b no_structure = np.random.rand(n_samples, 2)  # \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u043b\u0435\u043d\u0442\u043e\u0432\u0438\u0434\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u044b     random_state = 170 X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state) transformation = [[0.6, -0.6], [-0.4, 0.8]] X_aniso = np.dot(X, transformation) aniso = X_aniso  # \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u0432 \u0444\u043e\u0440\u043c\u0435 \u043a\u0440\u0443\u0433\u043e\u0432 \u0441 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u043e\u0439 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0435\u0439 varied = datasets.make_blobs(n_samples=n_samples,                              cluster_std=[1.0, 2.5, 0.5],                              random_state=random_state) X, y = varied varied = X In\u00a0[2]: Copied! <pre>from sklearn.cluster import AgglomerativeClustering\n\n# affinity:\n# \u201ceuclidean\u201d, \u201cl1\u201d, \u201cl2\u201d, \u201cmanhattan\u201d, \u201ccosine\u201d, or \u201cprecomputed\u201d\n\n# linkage:\n# \u201cward\u201d, \u201ccomplete\u201d, \u201caverage\u201d, \u201csingle\u201d\n\nac = AgglomerativeClustering(n_clusters=2,\n                             affinity='euclidean',\n                             linkage='ward', \n                            )\n</pre> from sklearn.cluster import AgglomerativeClustering  # affinity: # \u201ceuclidean\u201d, \u201cl1\u201d, \u201cl2\u201d, \u201cmanhattan\u201d, \u201ccosine\u201d, or \u201cprecomputed\u201d  # linkage: # \u201cward\u201d, \u201ccomplete\u201d, \u201caverage\u201d, \u201csingle\u201d  ac = AgglomerativeClustering(n_clusters=2,                              affinity='euclidean',                              linkage='ward',                              ) <ul> <li>\u0412 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0435 \u043c\u044b \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432, \u0430 \u0437\u0430\u0442\u0435\u043c \u0441\u0442\u0440\u043e\u0438\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0441\u043c\u0435\u0436\u043d\u043e\u0441\u0442\u0438 <code>connectivity</code> (\u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0435 \u043c\u0435\u0436\u0434\u0443 \u043e\u0431\u044a\u0435\u043a\u0442\u0430\u043c\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430)</li> <li>\u0414\u0430\u043b\u0435\u0435 \u0441\u043e\u0437\u0434\u0430\u0451\u043c \u043e\u0431\u044a\u0435\u043a\u0442 \u0430\u0433\u043b\u043e\u043c\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u043e\u0439 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 (<code>AgglomerativeClustering</code>) \u0438 \u043f\u0435\u0440\u0435\u0434\u0430\u0451\u043c \u0432 \u043d\u0435\u0433\u043e \u043c\u0430\u0442\u0440\u0438\u0446\u0443 (<code>X</code>)</li> <li>\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c (<code>fit</code>) \u0438\u0433\u043d\u043e\u0440\u0438\u0440\u0443\u0435\u043c \u043f\u0440\u0435\u0434\u0443\u043f\u0440\u0435\u0436\u0434\u0435\u043d\u0438\u044f \u0438 \u0441\u0442\u0440\u043e\u0438\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f.</li> </ul> <p>\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 AgglomerativeClustering \u043f\u0440\u043e\u0445\u043e\u0434\u0438\u0442 \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044e \u043c\u043e\u0434\u0435\u043b\u0438 K-means:</p> <pre>ac.fit(X)\n</pre> <p>\u041e\u0431\u0443\u0447\u0438\u0432, \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0443\u0437\u043d\u0430\u0442\u044c, \u0432 \u043a\u0430\u043a\u043e\u0439 \u043a\u043b\u0430\u0441\u0442\u0435\u0440 \u043f\u043e\u043f\u0430\u043b \u043a\u0430\u0436\u0434\u044b\u0439 \u0438\u0437 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432:</p> <pre>ac.labels_\n</pre> In\u00a0[3]: Copied! <pre>import warnings; warnings.filterwarnings('ignore')\nfrom sklearn.neighbors import kneighbors_graph\n\n# \u0441\u043e\u0437\u0434\u0430\u0451\u043c \u043c\u0430\u0441\u0441\u0438\u0432 \u043f\u0430\u0440 \u2013 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u0434\u043b\u044f \u043d\u0435\u0433\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430\ndatasets_params_list = [\n    (blobs, {'n_clusters': 3, 'n_neighbors': 10}),\n    (varied, {'n_clusters': 3, 'n_neighbors': 2}),\n    (aniso, {'n_clusters': 3, 'n_neighbors': 2}),\n    (noisy_circles, {'n_clusters': 2, 'n_neighbors': 10}),\n    (noisy_moons, {'n_clusters': 2, 'n_neighbors': 10}),\n    (no_structure, {'n_clusters': 3, 'n_neighbors': 10})]\n\nfor i, (X, ac_params) in enumerate(datasets_params_list, start=1):\n    \n    # \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435_\n    X = StandardScaler().fit_transform(X)\n    \n    # \u0441\u0442\u0440\u043e\u0438\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0441\u043c\u0435\u0436\u043d\u043e\u0441\u0442\u0438\n    connectivity = kneighbors_graph(X, \n                                    n_neighbors=ac_params['n_neighbors'], \n                                    include_self=False)\n    \n    # \u0434\u0435\u043b\u0430\u0435\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0441\u043c\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u0441\u0438\u043c\u043c\u0435\u0442\u0440\u0438\u0447\u043d\u043e\u0439\n    connectivity = 0.5 * (connectivity + connectivity.T)\n    \n    ac = AgglomerativeClustering(n_clusters=ac_params['n_clusters'], \n                                 linkage='average',\n                                 connectivity=connectivity)\n    ac.fit(X)\n    y_pred = ac.labels_.astype(np.int)\n    \n    plt.subplot(f'23{i}')\n    plt.xticks([]); plt.yticks([])\n    colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n                                         '#f781bf', '#a65628', '#984ea3',\n                                         '#999999', '#e41a1c', '#dede00']),\n                                  int(max(y_pred) + 1))))\n    plt.scatter(X[:, 0], X[:, 1], s=5,color=colors[y_pred])\n</pre> import warnings; warnings.filterwarnings('ignore') from sklearn.neighbors import kneighbors_graph  # \u0441\u043e\u0437\u0434\u0430\u0451\u043c \u043c\u0430\u0441\u0441\u0438\u0432 \u043f\u0430\u0440 \u2013 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u0434\u043b\u044f \u043d\u0435\u0433\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 datasets_params_list = [     (blobs, {'n_clusters': 3, 'n_neighbors': 10}),     (varied, {'n_clusters': 3, 'n_neighbors': 2}),     (aniso, {'n_clusters': 3, 'n_neighbors': 2}),     (noisy_circles, {'n_clusters': 2, 'n_neighbors': 10}),     (noisy_moons, {'n_clusters': 2, 'n_neighbors': 10}),     (no_structure, {'n_clusters': 3, 'n_neighbors': 10})]  for i, (X, ac_params) in enumerate(datasets_params_list, start=1):          # \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435_     X = StandardScaler().fit_transform(X)          # \u0441\u0442\u0440\u043e\u0438\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0441\u043c\u0435\u0436\u043d\u043e\u0441\u0442\u0438     connectivity = kneighbors_graph(X,                                      n_neighbors=ac_params['n_neighbors'],                                      include_self=False)          # \u0434\u0435\u043b\u0430\u0435\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0441\u043c\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u0441\u0438\u043c\u043c\u0435\u0442\u0440\u0438\u0447\u043d\u043e\u0439     connectivity = 0.5 * (connectivity + connectivity.T)          ac = AgglomerativeClustering(n_clusters=ac_params['n_clusters'],                                   linkage='average',                                  connectivity=connectivity)     ac.fit(X)     y_pred = ac.labels_.astype(np.int)          plt.subplot(f'23{i}')     plt.xticks([]); plt.yticks([])     colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',                                          '#f781bf', '#a65628', '#984ea3',                                          '#999999', '#e41a1c', '#dede00']),                                   int(max(y_pred) + 1))))     plt.scatter(X[:, 0], X[:, 1], s=5,color=colors[y_pred]) In\u00a0[4]: Copied! <pre>ac = AgglomerativeClustering(linkage='average')\n</pre> ac = AgglomerativeClustering(linkage='average') In\u00a0[5]: Copied! <pre>model = AgglomerativeClustering(n_clusters=3)\n</pre> model = AgglomerativeClustering(n_clusters=3) In\u00a0[6]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\nn_samples = 1500\ndataset = datasets.make_blobs(n_samples=n_samples, centers=2, center_box=(-7.0, 7.5),\n                              cluster_std=[1.4, 1.7],\n                              random_state=42)\nX_2, _ = datasets.make_blobs(n_samples=n_samples, random_state=170, centers=[[-4, -3]], cluster_std=[1.9])\ntransformation = [[1.2, -0.8], [-0.4, 1.7]]\nX_2 = np.dot(X_2, transformation)\nX, y = np.concatenate((dataset[0], X_2)), np.concatenate((dataset[1], np.array([2] * len(X_2))))\n\n# \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\nplt.rcParams['figure.figsize'] = 5, 5\nplt.scatter(X[:, 0], X[:, 1], s=5,c=y, alpha=0.5)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn import datasets  n_samples = 1500 dataset = datasets.make_blobs(n_samples=n_samples, centers=2, center_box=(-7.0, 7.5),                               cluster_std=[1.4, 1.7],                               random_state=42) X_2, _ = datasets.make_blobs(n_samples=n_samples, random_state=170, centers=[[-4, -3]], cluster_std=[1.9]) transformation = [[1.2, -0.8], [-0.4, 1.7]] X_2 = np.dot(X_2, transformation) X, y = np.concatenate((dataset[0], X_2)), np.concatenate((dataset[1], np.array([2] * len(X_2))))  # \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 plt.rcParams['figure.figsize'] = 5, 5 plt.scatter(X[:, 0], X[:, 1], s=5,c=y, alpha=0.5) plt.show() In\u00a0[7]: Copied! <pre>model.fit(X)\nlabels = model.labels_\nlabels\n</pre> model.fit(X) labels = model.labels_ labels Out[7]: <pre>array([2, 2, 0, ..., 1, 1, 1])</pre> In\u00a0[8]: Copied! <pre># \u043a\u043e\u043b\u0438\u0447\u043a\u0441\u0442\u0432\u043e \u043b\u0438\u0441\u044c\u0442\u0435\u0432\nmodel.n_leaves_\n</pre> # \u043a\u043e\u043b\u0438\u0447\u043a\u0441\u0442\u0432\u043e \u043b\u0438\u0441\u044c\u0442\u0435\u0432 model.n_leaves_ Out[8]: <pre>3000</pre> In\u00a0[9]: Copied! <pre>from collections import Counter\n\nCounter(labels)\n</pre> from collections import Counter  Counter(labels) Out[9]: <pre>Counter({2: 746, 0: 1112, 1: 1142})</pre> In\u00a0[10]: Copied! <pre>plt.rcParams['figure.figsize'] = 5, 5\nplt.scatter(X[:, 0], X[:, 1], s=5,c=labels, alpha=0.5)\nplt.show()\n</pre> plt.rcParams['figure.figsize'] = 5, 5 plt.scatter(X[:, 0], X[:, 1], s=5,c=labels, alpha=0.5) plt.show() In\u00a0[11]: Copied! <pre>from sklearn.neighbors import kneighbors_graph\n\nconnectivity = kneighbors_graph(X, n_neighbors=6, include_self=False)\nconnectivity = 0.5 * (connectivity + connectivity.T)\n\nmodel = AgglomerativeClustering(n_clusters=3,\n                                connectivity=connectivity)\n\nmodel.fit(X)\nlabels = model.labels_\nprint(Counter(labels))\n\nplt.rcParams['figure.figsize'] = 5, 5\nplt.scatter(X[:, 0], X[:, 1], s=5,c=labels, alpha=0.5)\nplt.show()\n</pre> from sklearn.neighbors import kneighbors_graph  connectivity = kneighbors_graph(X, n_neighbors=6, include_self=False) connectivity = 0.5 * (connectivity + connectivity.T)  model = AgglomerativeClustering(n_clusters=3,                                 connectivity=connectivity)  model.fit(X) labels = model.labels_ print(Counter(labels))  plt.rcParams['figure.figsize'] = 5, 5 plt.scatter(X[:, 0], X[:, 1], s=5,c=labels, alpha=0.5) plt.show() <pre>Counter({0: 1486, 2: 768, 1: 746})\n</pre> In\u00a0[13]: Copied! <pre>from scipy.cluster.hierarchy import dendrogram, linkage\n\n# \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0434\u0435\u043d\u0434\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b\n# \u0435\u0449\u0451 \u043e\u0434\u0438\u043d \u0441\u043f\u043e\u0441\u043e\u0431 \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u0442\u044c \u0430\u0433\u043b\u043e\u043c\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u0443\u044e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044e\nZ = linkage(X, \"ward\")\n\n# \u0441\u0442\u0440\u043e\u0438\u043c \u0434\u0435\u043d\u0434\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0443\nfig,ax = plt.subplots(1,1)\ndendrogram(Z, leaf_rotation=90.,ax=ax)\nax.axhline(y=150,xmin=0,xmax=200)\nplt.show()\n</pre> from scipy.cluster.hierarchy import dendrogram, linkage  # \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0434\u0435\u043d\u0434\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b # \u0435\u0449\u0451 \u043e\u0434\u0438\u043d \u0441\u043f\u043e\u0441\u043e\u0431 \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u0442\u044c \u0430\u0433\u043b\u043e\u043c\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u0443\u044e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044e Z = linkage(X, \"ward\")  # \u0441\u0442\u0440\u043e\u0438\u043c \u0434\u0435\u043d\u0434\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0443 fig,ax = plt.subplots(1,1) dendrogram(Z, leaf_rotation=90.,ax=ax) ax.axhline(y=150,xmin=0,xmax=200) plt.show() In\u00a0[14]: Copied! <pre>from scipy.cluster.hierarchy import dendrogram, linkage\n\n# \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0434\u0435\u043d\u0434\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b\n# \u0435\u0449\u0451 \u043e\u0434\u0438\u043d \u0441\u043f\u043e\u0441\u043e\u0431 \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u0442\u044c \u0430\u0433\u043b\u043e\u043c\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u0443\u044e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044e\nZ = linkage(X, \"ward\")\n\n# \u0441\u0442\u0440\u043e\u0438\u043c \u0434\u0435\u043d\u0434\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0443\nfig,ax = plt.subplots(1,1)\ndendrogram(Z, leaf_rotation=90.,ax=ax)\nax.axhline(y=100,xmin=0,xmax=200)\nplt.show()\n</pre> from scipy.cluster.hierarchy import dendrogram, linkage  # \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0434\u0435\u043d\u0434\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b # \u0435\u0449\u0451 \u043e\u0434\u0438\u043d \u0441\u043f\u043e\u0441\u043e\u0431 \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u0442\u044c \u0430\u0433\u043b\u043e\u043c\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u0443\u044e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044e Z = linkage(X, \"ward\")  # \u0441\u0442\u0440\u043e\u0438\u043c \u0434\u0435\u043d\u0434\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0443 fig,ax = plt.subplots(1,1) dendrogram(Z, leaf_rotation=90.,ax=ax) ax.axhline(y=100,xmin=0,xmax=200) plt.show()"},{"location":"portfolio/sfml/ml4_7_%D0%90%D0%B3%D0%BB%D0%BE%D0%BC%D0%B5%D1%80%D0%B0%D1%82%D0%B8%D0%B2%D0%BD%D0%B0%D1%8F_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F.html","title":"\u0410\u0433\u043b\u043e\u043c\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u0430\u044f \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430\u00b6","text":""},{"location":"portfolio/sfml/ml4_7_%D0%90%D0%B3%D0%BB%D0%BE%D0%BC%D0%B5%D1%80%D0%B0%D1%82%D0%B8%D0%B2%D0%BD%D0%B0%D1%8F_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F.html","title":"\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\u00b6","text":"<p>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0443\u0436\u0435 \u0437\u043d\u0430\u043a\u043e\u043c\u044b\u0439 \u043d\u0430\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 <code>make_blobs</code></p>"},{"location":"portfolio/sfml/ml4_7_%D0%90%D0%B3%D0%BB%D0%BE%D0%BC%D0%B5%D1%80%D0%B0%D1%82%D0%B8%D0%B2%D0%BD%D0%B0%D1%8F_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F.html","title":"\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u041c\u043e\u0434\u0435\u043b\u044c\u00b6","text":"<p>\u0413\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438</p> <ul> <li><code>n_clusters</code> \u0447\u0438\u0441\u043b\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043c\u044b \u043e\u0436\u0438\u0434\u0430\u0435\u043c</li> <li><code>affinity</code> \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0438 \u043e\u0446\u0435\u043d\u043a\u0438 \u043a\u043e\u0433\u0434\u0430 \u043d\u0430\u0445\u043e\u0434\u0438\u043c \u0434\u0438\u0441\u0442\u0430\u043d\u0446\u0438\u044e \u043c\u0435\u0436\u0434\u0443 \u0434\u0430\u043d\u043d\u044b\u043c\u0438</li> <li><code>linkage</code> \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0438 \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432</li> </ul>"},{"location":"portfolio/sfml/ml4_7_%D0%90%D0%B3%D0%BB%D0%BE%D0%BC%D0%B5%D1%80%D0%B0%D1%82%D0%B8%D0%B2%D0%BD%D0%B0%D1%8F_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F.html","title":"\u041f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u041c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<ul> <li>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c, \u043a\u0430\u043a \u0430\u0433\u043b\u043e\u043c\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u0431\u0443\u0434\u0435\u0442 \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 \u043d\u0430\u0448\u0438\u043c\u0438 \u043d\u0430\u0431\u043e\u0440\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u0445.</li> <li>\u041c\u044b \u0431\u0443\u0434\u0435\u043c \u0432\u0430\u0440\u044c\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u0447\u0438\u0441\u043b\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432.</li> <li>\u0411\u0443\u0434\u0435\u043c \u0442\u0430\u043a\u0436\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>\u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0441\u043c\u0435\u0436\u043d\u043e\u0441\u0442\u0438</code>, \u0434\u043b\u044f \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u043d\u0443\u0436\u0435\u043d \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 n_neighbors, \u0442\u043e \u0435\u0441\u0442\u044c \u0447\u0438\u0441\u043b\u043e \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0438\u0445 \u0441\u043e\u0441\u0435\u0434\u0435\u0439.</li> </ul>"},{"location":"portfolio/sfml/ml4_7_%D0%90%D0%B3%D0%BB%D0%BE%D0%BC%D0%B5%D1%80%D0%B0%D1%82%D0%B8%D0%B2%D0%BD%D0%B0%D1%8F_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F.html","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/sfml/ml4_7_%D0%90%D0%B3%D0%BB%D0%BE%D0%BC%D0%B5%D1%80%D0%B0%D1%82%D0%B8%D0%B2%D0%BD%D0%B0%D1%8F_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F.html#471","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.7.1\u00b6","text":"<p>\u041f\u0440\u0438 \u043a\u0430\u043a\u043e\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430 <code>linkage</code> \u043f\u0440\u0438 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0438 \u043e\u0431\u044a\u0435\u043a\u0442\u0430 <code>AgglomerativeClustering</code> \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0435\u043c \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u0431\u0443\u0434\u0435\u0442 \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0435 \u043c\u0435\u0436\u0434\u0443 \u043e\u0431\u044a\u0435\u043a\u0442\u0430\u043c\u0438 \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c\u044b\u0445 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432?</p>"},{"location":"portfolio/sfml/ml4_7_%D0%90%D0%B3%D0%BB%D0%BE%D0%BC%D0%B5%D1%80%D0%B0%D1%82%D0%B8%D0%B2%D0%BD%D0%B0%D1%8F_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F.html#472","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.7.2\u00b6","text":"<p>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u044c AgglomerativeClustering \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u043c <code>n_clusters=3</code> \u043d\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430. \u041d\u0430\u043f\u0438\u0448\u0438\u0442\u0435 \u0447\u0438\u0441\u043b\u043e <code>\u043b\u0438\u0441\u0442\u044c\u0435\u0432</code> \u0432 \u0438\u0435\u0440\u0430\u0440\u0445\u0438\u0447\u0435\u0441\u043a\u043e\u043c \u0434\u0435\u0440\u0435\u0432\u0435, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u043c \u043f\u0440\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438:</p>"},{"location":"portfolio/sfml/ml4_7_%D0%90%D0%B3%D0%BB%D0%BE%D0%BC%D0%B5%D1%80%D0%B0%D1%82%D0%B8%D0%B2%D0%BD%D0%B0%D1%8F_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F.html#473","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.7.3\u00b6","text":"<ul> <li>\u041f\u043e\u0434\u0441\u0447\u0438\u0442\u0430\u0439\u0442\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u0438\u0437 \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0445\u0441\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432.</li> <li>\u0417\u0430\u043f\u0438\u0448\u0438\u0442\u0435 \u0432 \u0444\u043e\u0440\u043c\u0443 \u043d\u0438\u0436\u0435 \u0442\u0440\u0438 \u0447\u0438\u0441\u043b\u0430 \u0447\u0435\u0440\u0435\u0437 \u043f\u0440\u043e\u0431\u0435\u043b(\u0431\u0435\u0437 \u0437\u0430\u043f\u044f\u0442\u044b\u0445!): \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0435 0, \u0432 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0435 1 \u0438 \u0432 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0435 2.</li> <li>\u0417\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0439\u0442\u0435 \u0441\u0442\u0440\u043e\u0433\u043e \u0432 \u0442\u0430\u043a\u043e\u043c \u043f\u043e\u0440\u044f\u0434\u043a\u0435.</li> </ul> <p>\u0414\u043b\u044f \u043f\u043e\u0434\u0441\u0447\u0451\u0442\u0430 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u0441\u043f\u0438\u0441\u043a\u0435 \u043c\u043e\u0436\u043d\u043e \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0435\u0439 <code>numpy.unique</code> \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u043c <code>return_counts=True</code></p>"},{"location":"portfolio/sfml/ml4_7_%D0%90%D0%B3%D0%BB%D0%BE%D0%BC%D0%B5%D1%80%D0%B0%D1%82%D0%B8%D0%B2%D0%BD%D0%B0%D1%8F_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F.html#474","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.7.4\u00b6","text":"<p>\u041f\u0440\u0438 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 AgglomerativeClustering \u043c\u043e\u0436\u043d\u043e \u0443\u043a\u0430\u0437\u0430\u0442\u044c <code>\u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0441\u043c\u0435\u0436\u043d\u043e\u0441\u0442\u0438</code>, \u043f\u0435\u0440\u0435\u0434\u0430\u0432 \u0435\u0451 \u0432 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 <code>connectivity</code>.</p> <p>\u0412 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 connectivity \u0431\u0443\u0434\u0435\u0442 \u0445\u0440\u0430\u043d\u0438\u0442\u0441\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u0441\u043c\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043d\u0435\u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u043e\u0433\u043e \u0433\u0440\u0430\u0444\u0430 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430.</p> <p>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u044c AgglomerativeClustering \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u043c n_clusters=3 \u0438 \u0441\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u043e\u0439 \u043c\u0430\u0442\u0440\u0438\u0446\u0435\u0439 \u0441\u043c\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043d\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430. \u041f\u043e\u0434\u0441\u0447\u0438\u0442\u0430\u0439\u0442\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u0438\u0437 \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0445\u0441\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432. \u0412\u044b\u0432\u0435\u0434\u0438\u0442\u0435 \u0442\u0440\u0438 \u0447\u0438\u0441\u043b\u0430 \u0447\u0435\u0440\u0435\u0437 \u043f\u0440\u043e\u0431\u0435\u043b: \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0435 0, \u0432 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0435 1 \u0438 \u0432 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0435 2.</p> <p>\u041f\u043e\u0434\u0441\u043a\u0430\u0437\u043a\u0430: \u041f\u0440\u0438 \u0440\u0435\u0448\u0435\u043d\u0438\u0438 \u0434\u0430\u043d\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u0430\u0446\u0438\u044f \u0434\u043e\u043b\u0436\u043d\u0430 \u0431\u044b\u0442\u044c \u0432\u044b\u043a\u043b\u044e\u0447\u0435\u043d\u0430.</p>"},{"location":"portfolio/sfml/ml4_7_%D0%90%D0%B3%D0%BB%D0%BE%D0%BC%D0%B5%D1%80%D0%B0%D1%82%D0%B8%D0%B2%D0%BD%D0%B0%D1%8F_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F.html#475","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.7.5\u00b6","text":"<ul> <li>\u041f\u043e\u0441\u0442\u0440\u043e\u0439\u0442\u0435 <code>\u0434\u0435\u043d\u0434\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0443</code> \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043f\u0430\u043a\u0435\u0442\u0430 scipy</li> <li>\u041d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435 \u0434\u043e\u043b\u0436\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c\u0441\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0438\u0435\u0440\u0430\u0440\u0445\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0434\u0435\u0440\u0435\u0432\u0430.</li> <li>\u041f\u043e\u0441\u0442\u0440\u043e\u0439\u0442\u0435 \u0434\u0435\u043d\u0434\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0443 \u043d\u0430 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u043c \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445.</li> <li>\u0421\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u0441\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432, \u0435\u0441\u043b\u0438 \u0433\u0440\u0430\u043d\u0438\u0447\u043d\u044b\u043c \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0435\u043c \u0434\u043b\u044f \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u0432\u0437\u044f\u0442\u044c 150.</li> </ul>"},{"location":"portfolio/sfml/ml4_7_%D0%90%D0%B3%D0%BB%D0%BE%D0%BC%D0%B5%D1%80%D0%B0%D1%82%D0%B8%D0%B2%D0%BD%D0%B0%D1%8F_%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F.html#476","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.7.6\u00b6","text":"<ul> <li>\u041f\u043e\u0441\u0442\u0440\u043e\u0439\u0442\u0435 <code>\u0434\u0435\u043d\u0434\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0443</code> \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043f\u0430\u043a\u0435\u0442\u0430 scipy</li> <li>\u041d\u0430 \u0432\u044b\u0445\u043e\u0434\u0435 \u0434\u043e\u043b\u0436\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c\u0441\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0438\u0435\u0440\u0430\u0440\u0445\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0434\u0435\u0440\u0435\u0432\u0430.</li> <li>\u041f\u043e\u0441\u0442\u0440\u043e\u0439\u0442\u0435 \u0434\u0435\u043d\u0434\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0443 \u043d\u0430 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u043c \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445.</li> <li>\u0421\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u0441\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432, \u0435\u0441\u043b\u0438 \u0433\u0440\u0430\u043d\u0438\u0447\u043d\u044b\u043c \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0435\u043c \u0434\u043b\u044f \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u0432\u0437\u044f\u0442\u044c 100</li> </ul>"},{"location":"portfolio/sfml/ml4_8_DBSCAN.html","title":"ml4 8 DBSCAN","text":"In\u00a0[2]: Copied! <pre>from sklearn.preprocessing import StandardScaler\nfrom itertools import cycle, islice\nimport seaborn as sns; sns.set(style='white')\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nimport seaborn as sns\n\nnp.random.seed(0)\n\n# \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435\nn_samples = 1500\n\n# \u0412\u043f\u0438\u0441\u0430\u043d\u043d\u044b\u0435 \u043a\u0440\u0443\u0433\u0438\nnoisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n                                      noise=.05)\n# \u041e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430, \u0442\u0430\u043a \u043a\u0430\u043a \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u043d\u0430\u043c \u043d\u0435 \u043d\u0443\u0436\u043d\u044b \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0441\u044b \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432\nX, y = noisy_circles\nnoisy_circles = X\n\n# \u041f\u043e\u043b\u0443\u043a\u0440\u0443\u0433\u0438\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\nX, y = noisy_moons\nnoisy_moons = X\n\n# \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u0432 \u0444\u043e\u0440\u043c\u0435 \u043a\u0440\u0443\u0433\u0430\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\nX, y = blobs\nblobs = X\n\n# \u041e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043d\u043e\u0439 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b\nno_structure = np.random.rand(n_samples, 2)\n\n# \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u043b\u0435\u043d\u0442\u043e\u0432\u0438\u0434\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u044b    \nrandom_state = 170\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = X_aniso\n\n# \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u0432 \u0444\u043e\u0440\u043c\u0435 \u043a\u0440\u0443\u0433\u043e\u0432 \u0441 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u043e\u0439 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0435\u0439\nvaried = datasets.make_blobs(n_samples=n_samples,\n                             cluster_std=[1.0, 2.5, 0.5],\n                             random_state=random_state)\nX, y = varied\nvaried = X\n</pre> from sklearn.preprocessing import StandardScaler from itertools import cycle, islice import seaborn as sns; sns.set(style='white')  import numpy as np import seaborn as sns import matplotlib.pyplot as plt from sklearn import datasets import seaborn as sns  np.random.seed(0)  # \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 n_samples = 1500  # \u0412\u043f\u0438\u0441\u0430\u043d\u043d\u044b\u0435 \u043a\u0440\u0443\u0433\u0438 noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,                                       noise=.05) # \u041e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430, \u0442\u0430\u043a \u043a\u0430\u043a \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u043d\u0430\u043c \u043d\u0435 \u043d\u0443\u0436\u043d\u044b \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u043a\u043b\u0430\u0441\u0441\u044b \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 X, y = noisy_circles noisy_circles = X  # \u041f\u043e\u043b\u0443\u043a\u0440\u0443\u0433\u0438 noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05) X, y = noisy_moons noisy_moons = X  # \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u0432 \u0444\u043e\u0440\u043c\u0435 \u043a\u0440\u0443\u0433\u0430 blobs = datasets.make_blobs(n_samples=n_samples, random_state=8) X, y = blobs blobs = X  # \u041e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043d\u043e\u0439 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b no_structure = np.random.rand(n_samples, 2)  # \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u043b\u0435\u043d\u0442\u043e\u0432\u0438\u0434\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u044b     random_state = 170 X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state) transformation = [[0.6, -0.6], [-0.4, 0.8]] X_aniso = np.dot(X, transformation) aniso = X_aniso  # \u041a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u0432 \u0444\u043e\u0440\u043c\u0435 \u043a\u0440\u0443\u0433\u043e\u0432 \u0441 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u043e\u0439 \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u0435\u0439 varied = datasets.make_blobs(n_samples=n_samples,                              cluster_std=[1.0, 2.5, 0.5],                              random_state=random_state) X, y = varied varied = X In\u00a0[3]: Copied! <pre>from sklearn.cluster import DBSCAN\n\n# dbscan = DBSCAN(eps=0.5,\n#                 min_samples=5)\n</pre> from sklearn.cluster import DBSCAN  # dbscan = DBSCAN(eps=0.5, #                 min_samples=5) In\u00a0[4]: Copied! <pre>datasets_params_list = [\n    (blobs, {'eps': 0.3}),\n    (varied, {'eps': 0.18}),\n    (aniso, {'eps': 0.184}),\n    (noisy_circles, {'eps': 0.3}),\n    (noisy_moons, {'eps': 0.3}),\n    (no_structure, {'eps': 0.3})]\n\nplt.rcParams['figure.figsize'] = 6, 4\nfor i, (X, dbscan_params) in enumerate(datasets_params_list, start=1):\n    \n    X = StandardScaler().fit_transform(X)\n    \n    dbscan = DBSCAN(eps=dbscan_params['eps'])\n    dbscan.fit(X)\n    y_pred = dbscan.labels_.astype(np.int)\n\n    plt.subplot(f'23{i}')\n    plt.xticks([]); plt.yticks([])\n    colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n                                         '#f781bf', '#a65628', '#984ea3',\n                                         '#999999', '#e41a1c', '#dede00']),\n                                  int(max(y_pred) + 1))))\n    # \u0447\u0451\u0440\u043d\u044b\u043c \u0446\u0432\u0435\u0442\u043e\u043c \u043e\u0442\u043c\u0435\u0442\u0438\u043c \u0432\u044b\u0431\u0440\u043e\u0441\u044b\n    colors = np.append(colors, [\"#000000\"])\n    plt.scatter(X[:, 0], X[:, 1], s=10,ec='k',alpha=0.25,color=colors[y_pred])\n</pre> datasets_params_list = [     (blobs, {'eps': 0.3}),     (varied, {'eps': 0.18}),     (aniso, {'eps': 0.184}),     (noisy_circles, {'eps': 0.3}),     (noisy_moons, {'eps': 0.3}),     (no_structure, {'eps': 0.3})]  plt.rcParams['figure.figsize'] = 6, 4 for i, (X, dbscan_params) in enumerate(datasets_params_list, start=1):          X = StandardScaler().fit_transform(X)          dbscan = DBSCAN(eps=dbscan_params['eps'])     dbscan.fit(X)     y_pred = dbscan.labels_.astype(np.int)      plt.subplot(f'23{i}')     plt.xticks([]); plt.yticks([])     colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',                                          '#f781bf', '#a65628', '#984ea3',                                          '#999999', '#e41a1c', '#dede00']),                                   int(max(y_pred) + 1))))     # \u0447\u0451\u0440\u043d\u044b\u043c \u0446\u0432\u0435\u0442\u043e\u043c \u043e\u0442\u043c\u0435\u0442\u0438\u043c \u0432\u044b\u0431\u0440\u043e\u0441\u044b     colors = np.append(colors, [\"#000000\"])     plt.scatter(X[:, 0], X[:, 1], s=10,ec='k',alpha=0.25,color=colors[y_pred]) <ul> <li>\u0412 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0435 \u043f\u0440\u0438 \u0445\u043e\u0440\u043e\u0448\u043e \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u043c\u044b\u0445 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0430\u0445 <code>DBSCAN</code> \u0441\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043e\u0442\u043b\u0438\u0447\u043d\u043e, \u043e\u043d \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043b \u0441\u043b\u043e\u0436\u043d\u044b\u0435 \u0441\u043b\u0443\u0447\u0430\u0438 \u0438 \u043b\u0435\u043d\u0442\u043e\u0447\u043d\u0443\u044e \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0443, \u043f\u0440\u0430\u0432\u0434\u0430, \u043d\u0430\u0448\u0451\u043b \u043b\u0438\u0448\u043d\u0438\u0435 \u0432\u044b\u0431\u0440\u043e\u0441\u044b</li> <li>\u042d\u0442\u043e \u0435\u0434\u0438\u043d\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442, \u043f\u043e \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044e \u0441 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u043c\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043b, \u0447\u0442\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043d\u043e\u0439 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b \u0432 \u043f\u043e\u0441\u043b\u0435\u043d\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u043d\u0435\u0442</li> <li>\u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0448\u0443\u043c\u0430 \u0441\u043c\u0443\u0449\u0430\u0435\u0442 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c: \u0432 \u043e\u0434\u043d\u043e\u043c \u0438\u0437 \u043f\u0440\u0438\u043c\u0435\u0440\u043e\u0432 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u0441\u043a\u043b\u0435\u0438\u043b\u0438\u0441\u044c \u043c\u0435\u0436\u0434\u0443 \u0441\u043e\u0431\u043e\u0439</li> </ul> In\u00a0[21]: Copied! <pre># \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\ndef plot_scatter():\n    plt.rcParams['figure.figsize'] = 3, 3\n    colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n                                         '#f781bf', '#a65628', '#984ea3',\n                                         '#999999', '#e41a1c', '#dede00']),\n                                  int(max(y_pred) + 1))))\n    plt.scatter(X[:, 0], X[:, 1], s=10,ec='k',alpha=0.25,color=colors[y_pred])\n</pre> # \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 def plot_scatter():     plt.rcParams['figure.figsize'] = 3, 3     colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',                                          '#f781bf', '#a65628', '#984ea3',                                          '#999999', '#e41a1c', '#dede00']),                                   int(max(y_pred) + 1))))     plt.scatter(X[:, 0], X[:, 1], s=10,ec='k',alpha=0.25,color=colors[y_pred]) In\u00a0[5]: Copied! <pre>dbscan = DBSCAN(min_samples=3)\n</pre> dbscan = DBSCAN(min_samples=3) In\u00a0[13]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nimport matplotlib as mpl\n\nn_samples = 1500\ndataset = datasets.make_blobs(n_samples=n_samples, centers=2, center_box=(-7.0, 7.5),\n                              cluster_std=[1.4, 1.7],\n                              random_state=42)\nX_2, _ = datasets.make_blobs(n_samples=n_samples, random_state=170, centers=[[-4, -3]], cluster_std=[1.9])\ntransformation = [[1.2, -0.8], [-0.4, 1.7]]\nX_2 = np.dot(X_2, transformation)\nX, y = np.concatenate((dataset[0], X_2)), np.concatenate((dataset[1], np.array([2] * len(X_2))))\n\ndbscan = DBSCAN(eps=0.9,min_samples=35)\ndbscan.fit(X)\ny_pred = dbscan.labels_.astype(np.int)\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn import datasets import matplotlib as mpl  n_samples = 1500 dataset = datasets.make_blobs(n_samples=n_samples, centers=2, center_box=(-7.0, 7.5),                               cluster_std=[1.4, 1.7],                               random_state=42) X_2, _ = datasets.make_blobs(n_samples=n_samples, random_state=170, centers=[[-4, -3]], cluster_std=[1.9]) transformation = [[1.2, -0.8], [-0.4, 1.7]] X_2 = np.dot(X_2, transformation) X, y = np.concatenate((dataset[0], X_2)), np.concatenate((dataset[1], np.array([2] * len(X_2))))  dbscan = DBSCAN(eps=0.9,min_samples=35) dbscan.fit(X) y_pred = dbscan.labels_.astype(np.int) In\u00a0[22]: Copied! <pre>plot_scatter()\n</pre> plot_scatter() In\u00a0[23]: Copied! <pre>from collections import Counter\n\nCounter(y_pred)\n</pre> from collections import Counter  Counter(y_pred) Out[23]: <pre>Counter({0: 603, 1: 681, -1: 583, 2: 1133})</pre> In\u00a0[16]: Copied! <pre>Counter(y_pred)[-1]\n</pre> Counter(y_pred)[-1] Out[16]: <pre>368</pre> In\u00a0[24]: Copied! <pre>dbscan = DBSCAN(eps=0.8,min_samples=35)\ndbscan.fit(X)\ny_pred = dbscan.labels_.astype(np.int)\nCounter(y_pred)\n</pre> dbscan = DBSCAN(eps=0.8,min_samples=35) dbscan.fit(X) y_pred = dbscan.labels_.astype(np.int) Counter(y_pred) Out[24]: <pre>Counter({0: 603, 1: 681, -1: 583, 2: 1133})</pre> In\u00a0[25]: Copied! <pre>Counter(y_pred)[-1]\n</pre> Counter(y_pred)[-1] Out[25]: <pre>583</pre> In\u00a0[26]: Copied! <pre>plot_scatter()\n</pre> plot_scatter()"},{"location":"portfolio/sfml/ml4_8_DBSCAN.html#dbscan","title":"DBSCAN \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430\u00b6","text":""},{"location":"portfolio/sfml/ml4_8_DBSCAN.html","title":"\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\u00b6","text":"<p>\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0443\u0436\u0435 \u0437\u043d\u0430\u043a\u043e\u043c\u044b\u0439 \u043d\u0430\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 <code>make_blobs</code></p>"},{"location":"portfolio/sfml/ml4_8_DBSCAN.html","title":"\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u041c\u043e\u0434\u0435\u043b\u044c\u00b6","text":"<p>\u0413\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438</p> <ul> <li><code>eps</code> \u0440\u0430\u0437\u043c\u0435\u0440 \u043e\u043a\u0440\u0435\u0441\u0442\u043d\u043e\u0441\u0442\u0435\u0439</li> <li><code>min_samples</code> \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u043e\u0447\u0435\u043a \u0432 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0435</li> </ul>"},{"location":"portfolio/sfml/ml4_8_DBSCAN.html","title":"\u041f\u043e\u0440\u043e\u0431\u043d\u043e\u0441\u0442\u0438 \u043c\u0435\u0442\u043e\u0434\u0430:\u00b6","text":"<ul> <li><p>\u0418\u0437 \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0442\u043e\u0447\u043a\u0443 \u0438 \u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0435\u0451 \u043e\u043a\u0440\u0435\u0441\u0442\u043d\u043e\u0441\u0442\u0438</p> </li> <li><p>\u0415\u0441\u043b\u0438 \u0442\u0430\u043c \u043d\u0435 \u043c\u0435\u043d\u0435\u0435 X \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u043e\u0447\u0435\u043a <code>min_samples</code>, \u0442\u043e \u043c\u044b \u043f\u043e\u043c\u0435\u0447\u0430\u0435\u043c \u0441\u043a\u043e\u043f\u043b\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u043c \u0438 \u0434\u0430\u043b\u0435\u0435 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u0442\u043e\u0447\u043a\u0438 \u0438\u0437 \u043e\u043a\u0440\u0435\u0441\u0442\u043d\u043e\u0441\u0442\u0438 \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0435\u043c \u0442\u0430\u043a\u043e\u0435 \u0436\u0435 \u043e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u0435</p> </li> <li><p>\u041f\u043e\u0432\u0442\u043e\u0440\u044f\u0435\u043c \u0434\u043e \u043c\u043e\u043c\u0435\u043d\u0442\u0430 \u043e\u043a\u043e\u043d\u0447\u0430\u043d\u0438\u044f \u0442\u043e\u0447\u0435\u043a \u0432 \u043d\u0430\u0431\u043e\u0440\u0435</p> </li> <li><p>\u0412 \u043e\u0442\u043b\u0438\u0447\u0438\u0435 \u043e\u0442 <code>K-means</code> \u0438 <code>AgglomerativeClustering</code>, \u043a\u043b\u0430\u0441\u0441 <code>DBSCAN</code> \u043d\u0435 \u0438\u043c\u0435\u0435\u0442 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430 <code>n_clusters</code></p> </li> <li><p>\u041f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 DBSCAN \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442 \u0447\u0438\u0441\u043b\u043e \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432 \u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0435</p> </li> </ul> <p>\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 <code>DBSCAN</code> \u043f\u0440\u043e\u0445\u043e\u0434\u0438\u0442 \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044e \u043c\u043e\u0434\u0435\u043b\u0438 K-means:</p> <pre>dbscan.fit(X)\n</pre> <p>\u041e\u0431\u0443\u0447\u0438\u0432, \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0443\u0437\u043d\u0430\u0442\u044c, \u0432 \u043a\u0430\u043a\u043e\u0439 \u043a\u043b\u0430\u0441\u0442\u0435\u0440 \u043f\u043e\u043f\u0430\u043b \u043a\u0430\u0436\u0434\u044b\u0439 \u0438\u0437 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432:</p> <pre>dbscan.labels_\n</pre>"},{"location":"portfolio/sfml/ml4_8_DBSCAN.html","title":"\u041f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u041c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<p>\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c 6 \u0440\u0430\u0437\u043d\u044b\u0445 \u0442\u0438\u043f\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445, \u0438 \u043a\u0430\u043a <code>DBSCAN</code> \u0441\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0441 \u043d\u0438\u043c\u0438</p>"},{"location":"portfolio/sfml/ml4_8_DBSCAN.html","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/sfml/ml4_8_DBSCAN.html#481","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.8.1\u00b6","text":"<p>\u041a\u0430\u043a\u043e\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 <code>DBSCAN</code> \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442 \u0447\u0438\u0441\u043b\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0432 \u043e\u043a\u0440\u0435\u0441\u0442\u043d\u043e\u0441\u0442\u0438, \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442, \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043b\u0438 \u043e\u0431\u044a\u0435\u043a\u0442 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u043c \u0438\u043b\u0438 \u043d\u0435\u0442?</p>"},{"location":"portfolio/sfml/ml4_8_DBSCAN.html#482","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.8.2\u00b6","text":"<ul> <li>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u044c <code>DBSCAN</code> \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 <code>eps=0.9</code> \u0438 <code>min_samples=35</code> \u043d\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430</li> <li>\u0412\u044b\u0432\u0435\u0434\u0438\u0442\u0435 \u0447\u0438\u0441\u043b\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0445\u0441\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u043e\u0432</li> <li>\u041e\u0431\u0440\u0430\u0442\u0438\u0442\u0435 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435, \u0447\u0442\u043e <code>DBSCAN</code> \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442 \u0432\u044b\u0431\u0440\u043e\u0441\u044b, \u043e\u0442\u043d\u043e\u0441\u044f \u0438\u0445 \u043a <code>\u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0443 -1</code></li> <li>\u041a\u043b\u0430\u0441\u0442\u0435\u0440 \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432 \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u0442\u044c \u0432 \u043e\u0442\u0432\u0435\u0442\u0435 \u043d\u0435 \u043d\u0443\u0436\u043d\u043e</li> </ul>"},{"location":"portfolio/sfml/ml4_8_DBSCAN.html#483","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.8.3\u00b6","text":"<p>\u0421\u043a\u043e\u043b\u044c\u043a\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0431\u044b\u043b\u043e \u043e\u0442\u043c\u0435\u0447\u0435\u043d\u043e \u043a\u0430\u043a \u0432\u044b\u0431\u0440\u043e\u0441\u044b \u0432 \u043f\u0440\u043e\u0448\u043b\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0435?</p>"},{"location":"portfolio/sfml/ml4_8_DBSCAN.html#484","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 4.8.4\u00b6","text":"<ul> <li>\u041f\u0440\u043e\u0432\u0435\u0440\u044c\u0442\u0435, \u043a\u0430\u043a \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0432\u043b\u0438\u044f\u0435\u0442 \u043d\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0440\u0430\u0431\u043e\u0442\u044b <code>DBSCAN</code></li> <li>\u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u044c <code>DBSCAN</code> \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 <code>eps=0.8</code> \u0438 <code>min_samples=35</code> \u043d\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430</li> <li>\u041d\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0439\u0442\u0435 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043a \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c. \u0412\u044b\u0432\u0435\u0434\u0438\u0442\u0435 \u0447\u0438\u0441\u043b\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u044b\u043b\u0438 \u043e\u0442\u043c\u0435\u0447\u0435\u043d\u044b \u043a\u0430\u043a \u0432\u044b\u0431\u0440\u043e\u0441\u044b.</li> </ul>"},{"location":"portfolio/sfml/ml5_3%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html","title":"ml5 3\u0420\u0435\u0448 \u0414\u0435\u0440","text":"In\u00a0[8]: Copied! <pre>from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\n\nRANDOM_SEED = 139\n\ntrain_data, train_labels = make_blobs(n_samples=200, \n                                      centers=[(0,1),(-3,-3),(4,2)], \n                                      n_features=2, \n                                      random_state=RANDOM_SEED,\n                                      cluster_std=(1.2,1.5,1,))\n\n# \u0434\u043b\u044f \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438\ndef get_grid(data):\n    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n    return np.meshgrid(np.arange(x_min, x_max, 0.01), \n                       np.arange(y_min, y_max, 0.01))\n\ntrain_data.shape, train_labels.shape\n</pre> from sklearn.tree import DecisionTreeClassifier from sklearn.datasets import make_blobs import matplotlib.pyplot as plt import numpy as np %matplotlib inline  RANDOM_SEED = 139  train_data, train_labels = make_blobs(n_samples=200,                                        centers=[(0,1),(-3,-3),(4,2)],                                        n_features=2,                                        random_state=RANDOM_SEED,                                       cluster_std=(1.2,1.5,1,))  # \u0434\u043b\u044f \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 def get_grid(data):     x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1     y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1     return np.meshgrid(np.arange(x_min, x_max, 0.01),                         np.arange(y_min, y_max, 0.01))  train_data.shape, train_labels.shape Out[8]: <pre>((200, 2), (200,))</pre> In\u00a0[6]: Copied! <pre># \u043c\u043e\u0434\u0435\u043b\u044c \nclf_tree = DecisionTreeClassifier(criterion='entropy', \n                                  max_depth=3, \n                                  random_state=RANDOM_SEED)\n</pre> # \u043c\u043e\u0434\u0435\u043b\u044c  clf_tree = DecisionTreeClassifier(criterion='entropy',                                    max_depth=3,                                    random_state=RANDOM_SEED) In\u00a0[9]: Copied! <pre># training the tree\nclf_tree.fit(train_data, train_labels)\n\n# some code to depict separating surface\nxx, yy = get_grid(train_data)\n\n# np.c_ - add along 2nd axis (columns)\n# np.r_ - add along 1st axis (rows)\n\ngrid_data = np.c_[xx.flatten(),yy.flatten()]\nprint(grid_data.shape)\n\n# \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \npredicted = clf_tree.predict(grid_data).reshape(xx.shape)\nprint(predicted.shape)\n</pre> # training the tree clf_tree.fit(train_data, train_labels)  # some code to depict separating surface xx, yy = get_grid(train_data)  # np.c_ - add along 2nd axis (columns) # np.r_ - add along 1st axis (rows)  grid_data = np.c_[xx.flatten(),yy.flatten()] print(grid_data.shape)  # \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c  predicted = clf_tree.predict(grid_data).reshape(xx.shape) print(predicted.shape) <pre>(2156197, 2)\n(1427, 1511)\n</pre> In\u00a0[12]: Copied! <pre>plt.pcolormesh(xx, yy, predicted, cmap='viridis')\nplt.scatter(train_data[:, 0], train_data[:, 1], \n            c=train_labels, s=25, \n            cmap='viridis', \n            edgecolors='black', linewidth=1.5);\n</pre> plt.pcolormesh(xx, yy, predicted, cmap='viridis') plt.scatter(train_data[:, 0], train_data[:, 1],              c=train_labels, s=25,              cmap='viridis',              edgecolors='black', linewidth=1.5); <p>\u043f\u043b\u043e\u0441\u043a\u043e\u0441\u0442\u044c \u0434\u0435\u043b\u044f\u0442 4 \u043f\u0440\u044f\u043c\u044b\u0435, \u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u0435\u0441\u0442\u044c 4 \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043d\u0430\u0434\u0432\u043e\u0435, \u0434\u043b\u044f \u0442\u0430\u043a\u043e\u0433\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0439 \u0434\u0435\u0440\u0435\u0432\u043e \u0434\u043e\u043b\u0436\u043d\u043e \u0431\u044b\u0442\u044c \u043a\u0430\u043a \u043c\u0438\u043d\u0438\u043c\u0443\u043c \u0433\u043b\u0443\u0431\u0438\u043d\u044b 3</p> In\u00a0[13]: Copied! <pre>from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix as cm\n%matplotlib inline\n\nRANDOM_SEED = 139\n\n# Make training data\ntrain_data, train_labels = make_blobs(n_samples=500, \n                                      centers=[(0,1),(-3,-3),(4,2)], \n                                      n_features=2, \n                                      random_state=RANDOM_SEED,\n                                      cluster_std=(1.2,1.5,1,))\n\n# Let\u2019s write an auxiliary function that will return grid for further visualization.\ndef get_grid(data):\n    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n    return np.meshgrid(np.arange(x_min, x_max, 0.01), \n                       np.arange(y_min, y_max, 0.01))\n\n# Decision Tree Model\nmodel = DecisionTreeClassifier(criterion='entropy', \n                                  max_depth=None, \n                                  random_state=RANDOM_SEED)\n\n# training the tree\nmodel.fit(train_data, train_labels)\ny_pred = model.predict(train_data)\nprint(f'model depth: {model.tree_.max_depth}')\n\n# some codee to depict separating surface\nxx, yy = get_grid(train_data)\n\n# predict on grid\npredicted = clf_tree.predict(np.c_[xx.flatten(), \n                                   yy.flatten()])\npredicted_grid = predicted.reshape(xx.shape)\n\n# max depth results in zero error on training set\nprint('\\nconfusion matrix:')\nprint(cm(y_pred,train_labels))\n\n# plot prediction on grid\nplt.pcolormesh(xx, yy, predicted_grid)\n\n# plot true results\nplt.scatter(train_data[:, 0],\n            train_data[:, 1], \n            c=train_labels, \n            s=20, \n            edgecolors='black', \n            linewidth=1)\n</pre> from sklearn.tree import DecisionTreeClassifier from sklearn.datasets import make_blobs import matplotlib.pyplot as plt import numpy as np from sklearn.metrics import confusion_matrix as cm %matplotlib inline  RANDOM_SEED = 139  # Make training data train_data, train_labels = make_blobs(n_samples=500,                                        centers=[(0,1),(-3,-3),(4,2)],                                        n_features=2,                                        random_state=RANDOM_SEED,                                       cluster_std=(1.2,1.5,1,))  # Let\u2019s write an auxiliary function that will return grid for further visualization. def get_grid(data):     x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1     y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1     return np.meshgrid(np.arange(x_min, x_max, 0.01),                         np.arange(y_min, y_max, 0.01))  # Decision Tree Model model = DecisionTreeClassifier(criterion='entropy',                                    max_depth=None,                                    random_state=RANDOM_SEED)  # training the tree model.fit(train_data, train_labels) y_pred = model.predict(train_data) print(f'model depth: {model.tree_.max_depth}')  # some codee to depict separating surface xx, yy = get_grid(train_data)  # predict on grid predicted = clf_tree.predict(np.c_[xx.flatten(),                                     yy.flatten()]) predicted_grid = predicted.reshape(xx.shape)  # max depth results in zero error on training set print('\\nconfusion matrix:') print(cm(y_pred,train_labels))  # plot prediction on grid plt.pcolormesh(xx, yy, predicted_grid)  # plot true results plt.scatter(train_data[:, 0],             train_data[:, 1],              c=train_labels,              s=20,              edgecolors='black',              linewidth=1) <pre>model depth: 10\n\nconfusion matrix:\n[[167   0   0]\n [  0 167   0]\n [  0   0 166]]\n</pre> Out[13]: <pre>&lt;matplotlib.collections.PathCollection at 0x145ba4580&gt;</pre> In\u00a0[14]: Copied! <pre>from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns; sns.set(style='whitegrid')\n\nRANDOM_SEED = 139\n\nn_train = 150        \nn_test = 1000       \nnoise = 0.1\n\ndef f(x):\n    x = x.ravel()\n    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 5) ** 2)\n\ndef generate(n_samples, noise):\n    X = np.random.rand(n_samples) * 10 - 5\n    X = np.sort(X).ravel()\n    y = np.exp(-X ** 2) + 1.5 * np.exp(-(X - 5) ** 2) + \\\n    np.random.normal(0.0, noise, n_samples)\n    X = X.reshape((n_samples, 1))\n    return X, y\n\nX_train, y_train = generate(n_samples=n_train, noise=noise)\nX_test, y_test = generate(n_samples=n_test, noise=noise)\nprint(X_train.shape,X_test.shape)\n</pre> from sklearn.tree import DecisionTreeClassifier from sklearn.datasets import make_blobs import matplotlib.pyplot as plt import numpy as np import seaborn as sns; sns.set(style='whitegrid')  RANDOM_SEED = 139  n_train = 150         n_test = 1000        noise = 0.1  def f(x):     x = x.ravel()     return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 5) ** 2)  def generate(n_samples, noise):     X = np.random.rand(n_samples) * 10 - 5     X = np.sort(X).ravel()     y = np.exp(-X ** 2) + 1.5 * np.exp(-(X - 5) ** 2) + \\     np.random.normal(0.0, noise, n_samples)     X = X.reshape((n_samples, 1))     return X, y  X_train, y_train = generate(n_samples=n_train, noise=noise) X_test, y_test = generate(n_samples=n_test, noise=noise) print(X_train.shape,X_test.shape) <pre>(150, 1) (1000, 1)\n</pre> In\u00a0[17]: Copied! <pre>plt.rcParams['figure.figsize'] = 5, 3\nplt.figure()\nplt.scatter(X_train, y_train, c=\"b\", s=15,edgecolors='k')\nplt.xlim([-5, 5])\nplt.title(\"Training Data\",size=10)\nplt.show()\n</pre>  plt.rcParams['figure.figsize'] = 5, 3 plt.figure() plt.scatter(X_train, y_train, c=\"b\", s=15,edgecolors='k') plt.xlim([-5, 5]) plt.title(\"Training Data\",size=10) plt.show() In\u00a0[18]: Copied! <pre>from sklearn.tree import DecisionTreeRegressor as DTR\n\nmodel = DTR(max_depth=6,\n            random_state=RANDOM_SEED)\nmodel.fit(X_train, y_train)\nym_train = model.predict(X_train)\nym_test = model.predict(X_test)\n\nplt.figure()\nplt.plot(X_test, f(X_test), \"b\")\nplt.scatter(X_train, y_train, c=\"b\", s=15,edgecolors='k',alpha=0.1)\nplt.plot(X_test, ym_test, \"g\", lw=2)\nplt.xlim([-5, 5])\nplt.title(\"Decision tree regressor, MSE = %.2f\" % np.mean((y_train - ym_train) ** 2),size=10)\nplt.show()\n</pre> from sklearn.tree import DecisionTreeRegressor as DTR  model = DTR(max_depth=6,             random_state=RANDOM_SEED) model.fit(X_train, y_train) ym_train = model.predict(X_train) ym_test = model.predict(X_test)  plt.figure() plt.plot(X_test, f(X_test), \"b\") plt.scatter(X_train, y_train, c=\"b\", s=15,edgecolors='k',alpha=0.1) plt.plot(X_test, ym_test, \"g\", lw=2) plt.xlim([-5, 5]) plt.title(\"Decision tree regressor, MSE = %.2f\" % np.mean((y_train - ym_train) ** 2),size=10) plt.show() In\u00a0[8]: Copied! <pre># \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\nplt.figure()\nplt.plot(X_test, f(X_test), \"b\")\nplt.plot(X_test, ym_test, \"g\", lw=2)\nplt.scatter(X_test, y_test, c=\"b\", s=15,edgecolors='k',alpha=0.1)\nplt.xlim([-5, 5])\nplt.title(\"Decision tree regressor, MSE = %.2f\" % np.mean((y_test - ym_test) ** 2),size=10)\nplt.show()\n</pre> # \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 plt.figure() plt.plot(X_test, f(X_test), \"b\") plt.plot(X_test, ym_test, \"g\", lw=2) plt.scatter(X_test, y_test, c=\"b\", s=15,edgecolors='k',alpha=0.1) plt.xlim([-5, 5]) plt.title(\"Decision tree regressor, MSE = %.2f\" % np.mean((y_test - ym_test) ** 2),size=10) plt.show() <p>\u0422\u0430\u043a\u0436\u0435 \u0441\u0442\u043e\u0438\u0442 \u0432\u0432\u0435\u0441\u0442\u0438 \u0442\u0430\u043a\u043e\u0435 \u043f\u043e\u043d\u044f\u0442\u0438\u0435, \u043a\u0430\u043a <code>\u043f\u0440\u0438\u0440\u043e\u0441\u0442 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438</code> (information gain):</p> <p>$IG\\left( X_{m},j,t\\right) = Q\\left( X_{m},j,t\\right) - \\dfrac {\\left| X_{l}\\right| }{\\left| X_{m}\\right| }S\\left( X_{l}\\right) - \\dfrac {\\left| X_{r}\\right| }{\\left| X_{m}\\right| }S\\left( X_{r}\\right)$</p> <p>\u042d\u0442\u043e \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0438\u0437\u043c\u0435\u043d\u0438\u043b\u0430\u0441\u044c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438 \u0434\u0430\u043d\u043d\u043e\u043c \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0438, \u0442.\u0435. \u043f\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044e t \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 j.</p> <p>\u042d\u043d\u0442\u0440\u043e\u043f\u0438\u0439\u043d\u044b\u0439 \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0438 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438</p> <p>$S(X) = -\\sum ^{k}_{k=1}p_{k}\\log _{2}p_{k}$</p> <p>\u042d\u043d\u0442\u0440\u043e\u043f\u0438\u044f \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0441\u0442\u0435\u043f\u0435\u043d\u0438 \u0445\u0430\u043e\u0441\u0430 \u0432 \u0441\u0438\u0441\u0442\u0435\u043c\u0435. \u0427\u0435\u043c \u0432\u044b\u0448\u0435 \u044d\u043d\u0442\u0440\u043e\u043f\u0438\u044f, \u0442\u0435\u043c \u043c\u0435\u043d\u0435\u0435 \u0443\u043f\u043e\u0440\u044f\u0434\u043e\u0447\u0435\u043d\u0430 \u0441\u0438\u0441\u0442\u0435\u043c\u0430 \u0438 \u043d\u0430\u043e\u0431\u043e\u0440\u043e\u0442.</p> <p>\u0433\u0434\u0435 $p_{k}$ \u2014 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0432 k-\u043e\u043c \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0438. \u0412 \u043d\u0430\u0448\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0435  K \u2014 \u044d\u0442\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0441\u043e\u0432, $p_{k}$ \u2014 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044f \u043e\u0431\u044a\u0435\u043a\u0442\u0430 X \u0438\u0437  \u043a -\u0442\u043e\u043c\u0443 \u043a\u043b\u0430\u0441\u0441\u0443.</p> <p>eg. \u041f\u0435\u0440\u0432\u043e\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435 $S_{0}$ : 9 \u0441\u0438\u043d\u0438\u0445 \u0448\u0430\u0440\u0438\u043a\u043e\u0432 \u0438 11 \u0436\u0451\u043b\u0442\u044b\u0445. \u0415\u0441\u043b\u0438 \u043c\u044b \u043d\u0430\u0443\u0434\u0430\u0447\u0443 \u0432\u044b\u0442\u0430\u0449\u0438\u043c \u0448\u0430\u0440\u0438\u043a, \u0442\u043e \u043e\u043d \u0441 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c\u044e 9/20 \u0431\u0443\u0434\u0435\u0442 \u0441\u0438\u043d\u0438\u043c \u0438 \u0441 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c\u044e 11/20 \u2014 \u0436\u0451\u043b\u0442\u044b\u043c.</p> <p>\u041a\u0440\u043e\u043c\u0435 <code>\u043a\u0440\u0438\u0442\u0435\u0440\u0438\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438</code>, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c, \u043a\u0430\u043a\u0438\u0435 \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0432\u0432\u043e\u0434\u0438\u0442\u044c \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u044f \u043d\u0430 \u0434\u0435\u0440\u0435\u0432\u043e:</p> <ul> <li>\u041e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0435 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0433\u043b\u0443\u0431\u0438\u043d\u044b \u0434\u0435\u0440\u0435\u0432\u0430</li> <li>\u041e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0435 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u043b\u0438\u0441\u0442\u044c\u0435\u0432 \u0432 \u0434\u0435\u0440\u0435\u0432\u0435</li> <li>\u041e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0435 \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0447\u0438\u0441\u043b\u0430 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0432 \u043e\u0434\u043d\u043e\u043c \u043b\u0438\u0441\u0442\u0435</li> </ul>"},{"location":"portfolio/sfml/ml5_3%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html","title":"\u0420\u0435\u0448\u0430\u044e\u0449\u0438\u0435 \u0434\u0435\u0440\u0435\u0432\u044c\u044f\u00b6","text":""},{"location":"portfolio/sfml/ml5_3%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html","title":"\u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435\u00b6","text":"<ul> <li>\u042d\u0442\u043e \u0441\u0432\u044f\u0437\u043d\u044b\u0439 \u0430\u0446\u0438\u043a\u043b\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0433\u0440\u0430\u0444. \u0412 \u043d\u0451\u043c \u043c\u043e\u0436\u043d\u043e \u0432\u044b\u0434\u0435\u043b\u0438\u0442\u044c 3 \u0442\u0438\u043f\u0430 \u0432\u0435\u0440\u0448\u0438\u043d</li> <li><code>\u041a\u043e\u0440\u043d\u0435\u0432\u0430\u044f \u0432\u0435\u0440\u0448\u0438\u043d\u0430</code> (root node) \u2014 \u043e\u0442\u043a\u0443\u0434\u0430 \u0432\u0441\u0451 \u043d\u0430\u0447\u0438\u043d\u0430\u0435\u0442\u0441\u044f.</li> <li><code>\u0412\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0438\u0435 \u0432\u0435\u0440\u0448\u0438\u043d\u044b</code> (intermediate nodes).</li> <li><code>\u041b\u0438\u0441\u0442\u044c\u044f</code> (leaves) \u2014 \u0441\u0430\u043c\u044b\u0435 \u0433\u043b\u0443\u0431\u043e\u043a\u0438\u0435 \u0432\u0435\u0440\u0448\u0438\u043d\u044b \u0434\u0435\u0440\u0435\u0432\u0430, \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442\u0441\u044f \"\u043e\u0442\u0432\u0435\u0442\".</li> </ul> <p>\u0412\u043e \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0435\u0439 \u0438\u043b\u0438 \u043a\u043e\u0440\u043d\u0435\u0432\u043e\u0439 \u0432\u0435\u0440\u0448\u0438\u043d\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442\u0441\u044f \u043d\u0430 \u043d\u0435\u043a\u0438\u0439 \u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0439, \u043f\u043e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u043c\u044b \u0434\u0432\u0438\u0436\u0435\u043c\u0441\u044f \u0432\u0441\u0451 \u0433\u043b\u0443\u0431\u0436\u0435 \u043f\u043e \u0434\u0435\u0440\u0435\u0432\u0443.</p>"},{"location":"portfolio/sfml/ml5_3%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html","title":"\u0412 \u0447\u0435\u043c \u043e\u0442\u043b\u0438\u0447\u0438\u0435 \u043e\u0442 \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439\u00b6","text":"<ul> <li><code>\u041b\u0438\u043d\u0435\u0439\u043d\u044b\u0435</code> \u043c\u043e\u0434\u0435\u043b\u0438 \u0431\u044b\u0441\u0442\u0440\u043e \u0443\u0447\u0430\u0442\u0441\u044f (\u043c\u0430\u0442\u0440\u0438\u0447\u043d\u044b\u0435 \u0438\u043b\u0438 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0441\u043f\u0443\u0441\u043a)</li> <li><code>\u041b\u0438\u043d\u0435\u0439\u043d\u044b\u0435</code> \u043c\u043e\u0434\u0435\u043b\u0438 \u043c\u043e\u0433\u0443 \u0432\u043e\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0440\u043e\u0441\u0442\u044b\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 (\u0438\u0437-\u0437\u0430 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u043d\u043e\u0433\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432)</li> <li><code>\u041b\u0438\u043d\u0435\u0439\u043d\u044b\u0435</code> \u043c\u043e\u0434\u0435\u043b\u0438 \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u0432\u043e\u0441\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u043d\u0438\u044f \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0435\u0439 \u0437\u0430 \u0441\u0447\u0435\u0442 \u043f\u0435\u0440\u0435\u0445\u043e\u0434\u0430 \u043a \u0441\u043f\u0440\u044f\u043c\u043b\u044f\u044e\u0449\u0435\u043c\u0443 \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0443</li> <li><code>\u041b\u0438\u043d\u0435\u0439\u043d\u044b\u0435</code> \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0435 \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0430\u044e\u0442 \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0430 \u043f\u0440\u0438\u043d\u044f\u0442\u0438\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u0443 \u043b\u044e\u0434\u0435\u0439 (\u0447\u0435\u043b\u043e\u0432\u0435\u043a \u0431\u0443\u0434\u0435\u0442 \u0437\u0430\u0434\u0430\u0432\u0430\u0442\u044c \u0440\u044f\u0434 \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u0432)</li> </ul>"},{"location":"portfolio/sfml/ml5_3%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html","title":"\u041a\u0430\u043a \u0441\u0442\u0440\u043e\u0438\u0442\u0441\u044f \u0434\u0435\u0440\u0435\u0432\u044c\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u0439\u00b6","text":"<p>\u041f\u0440\u0438\u043c\u0435\u0440 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432:</p> <pre>function decision_tree(X, y):\n    if stopping_criterion(X, y) == True:\n        S = create_leaf_with_prediction(y)\n    else:\n        S = create_node()\n        (X_1, y_1) .. (X_L, y_L) = best_split(X, y)\n        for i in 1..L:\n            C = decision_tree(X_i, y_i)\n            connect_nodes(S, C)\n    return S\n</pre> <p>\u0423 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u0438\u0441\u0445\u043e\u0434\u043d\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 <code>X0</code> \u0438 \u043e\u0442\u0432\u0435\u0442\u044b <code>y0</code></p> <ul> <li><p>\u041f\u0440\u0438 \u043a\u0430\u0436\u0434\u043e\u043c \u043d\u043e\u0432\u043e\u043c \u0432\u044b\u0437\u043e\u0432\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438, \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c <code>\u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0438 \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0438</code> (stopping_criterion) \u0434\u043b\u044f \u0442\u0435\u043a\u0443\u0449\u0435\u0439 \u043f\u043e\u0434\u0432\u044b\u043f\u043e\u0440\u043a\u0438 (Xx,yx)</p> <ul> <li><p>\u0415\u0441\u043b\u0438 \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u044e\u0442\u0441\u044f, \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043d\u043e\u0432\u044b\u0439 <code>\u043b\u0438\u0441\u0442</code> \u0434\u0435\u0440\u0435\u0432\u0430 \u0441 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435\u043c (create_leaf_with_prediction), \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u0445 \u043e\u0442\u0432\u0435\u0442\u043e\u0432 y, \u0432\u0441\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u043f\u043e\u043f\u0430\u0432 \u0432 \u044d\u0442\u043e\u0442 \u043b\u0438\u0441\u0442 \u0431\u0443\u0434\u0443\u0442 \u0438\u043c\u0435\u0442\u044c \u044d\u0442\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435</p> </li> <li><p>\u0412 \u0438\u043d\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435, \u0441\u0442\u043e\u0438\u0442\u0441\u044f \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0430\u044f \u0432\u0435\u0440\u0448\u0438\u043d\u0430 \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0431\u0443\u0434\u0435\u0442 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043e \u043f\u0440\u0430\u0432\u0438\u043b\u043e \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u044f (\u043e\u0431\u044a\u0435\u043a\u0442\u044b \u043f\u043e\u043f\u0430\u0434\u0443\u0442 \u0432 \u043e\u0434\u043d\u0443 \u0438\u0437 \u0432\u0435\u0442\u0432\u0435\u0439). \u041d\u0430\u0438\u043b\u0443\u0447\u0448\u0435\u0435 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 (best_split) \u043c\u043e\u0436\u0435\u0442 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0442\u0441\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0440 \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u0438 (impurity measures)</p> </li> </ul> </li> <li><p>\u041f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0438 (Xx,yx), (Xk,yk) \u043f\u043e\u0434\u0430\u044e\u0442\u0441\u044f \u043d\u0430 \u0432\u0445\u043e\u0434 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0438\u043c \u0432\u044b\u0437\u043e\u0432\u0430\u043d \u044d\u0442\u043e\u0439 \u0440\u0435\u043a\u0443\u0440\u0441\u0438\u0432\u043d\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438</p> </li> </ul>"},{"location":"portfolio/sfml/ml5_3%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html","title":"\u0411\u043e\u0440\u044c\u0431\u0430 \u0441 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\u043c (\u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044f)\u00b6","text":"<p>\u041f\u0440\u0438\u0432\u0435\u0434\u0451\u043c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0445 \u0441\u043f\u043e\u0441\u043e\u0431\u043e\u0432 \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0431\u043e\u043b\u0435\u0435 \u043e\u0431\u043e\u0431\u0449\u0430\u044e\u0449\u0438\u043c, \u0442.\u0435. \u043c\u0435\u043d\u0435\u0435 \u0437\u0430\u0442\u043e\u0447\u0435\u043d\u043d\u044b\u043c \u043f\u043e\u0434 \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443:</p> <ul> <li>\u0417\u0430\u0434\u0430\u0442\u044c \u043f\u043e\u0440\u043e\u0433 \u043f\u043e \u043c\u0435\u0440\u0435 \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0451\u043d\u043d\u043e\u0441\u0442\u0438</li> <li>\u0417\u0430\u0434\u0430\u0442\u044c \u043f\u043e\u0440\u043e\u0433 \u043f\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0443 \u0443\u0437\u043b\u0430</li> <li>\u0417\u0430\u0434\u0430\u0442\u044c \u043f\u043e\u0440\u043e\u0433 \u043d\u0430 \u0433\u043b\u0443\u0431\u0438\u043d\u0443</li> <li>\u0417\u0430\u0434\u0430\u0442\u044c \u043f\u043e\u0440\u043e\u0433 \u043d\u0430 \u0440\u0430\u0437\u043c\u0435\u0440 \u043f\u043e\u0442\u043e\u043c\u043a\u043e\u0432</li> </ul>"},{"location":"portfolio/sfml/ml5_3%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/sfml/ml5_3%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html#522","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 5.2.2\u00b6","text":"<p>\u041f\u043e \u0433\u0440\u0430\u0444\u0438\u043a\u0443 \u0432\u044b\u0448\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u0435, \u043a\u0430\u043a\u043e\u0432\u0430 <code>\u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0433\u043b\u0443\u0431\u0438\u043d\u0430</code> \u044d\u0442\u043e\u0433\u043e \u0440\u0435\u0448\u0430\u044e\u0449\u0435\u0433\u043e \u0434\u0435\u0440\u0435\u0432\u0430? (\u042d\u0442\u043e \u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0435 \u0434\u0435\u0440\u0435\u0432\u043e, \u0442.\u0435. \u0432\u0441\u0435 \u0435\u0433\u043e \u0432\u0435\u0442\u0432\u0438 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u043e\u0439 \u0433\u043b\u0443\u0431\u0438\u043d\u044b)</p>"},{"location":"portfolio/sfml/ml5_3%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html","title":"\u041f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\u00b6","text":"<ul> <li>\u0420\u0435\u0448\u0430\u044e\u0449\u0435\u0435 \u0434\u0435\u0440\u0435\u0432\u043e \u0432\u043f\u043e\u043b\u043d\u0435 \u043c\u043e\u0436\u0435\u0442 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0438\u0442\u044c\u0441\u044f: \u0435\u0433\u043e \u043c\u043e\u0436\u043d\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043d\u0430\u0441\u0442\u043e\u043b\u044c\u043a\u043e \u0433\u043b\u0443\u0431\u043e\u043a\u0438\u043c, \u0447\u0442\u043e \u043a\u0430\u0436\u0434\u044b\u0439 \u043b\u0438\u0441\u0442 \u0440\u0435\u0448\u0430\u044e\u0449\u0435\u0433\u043e \u0434\u0435\u0440\u0435\u0432\u0430 \u0431\u0443\u0434\u0435\u0442 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c \u0440\u043e\u0432\u043d\u043e \u043e\u0434\u043d\u043e\u043c\u0443 \u043e\u0431\u044a\u0435\u043a\u0442\u0443 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 <code>max_depth=None</code></li> <li>\u0412 \u044d\u0442\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435, \u0435\u0441\u043b\u0438 \u0437\u0430\u043f\u0438\u0441\u0430\u0442\u044c \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u043b\u0438\u0441\u0442\u0435 \u043e\u0442\u0432\u0435\u0442 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430, \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u043d\u0443\u043b\u0435\u0432\u0430\u044f \u043e\u0448\u0438\u0431\u043a\u0430</li> <li>\u0414\u0435\u0440\u0435\u0432\u043e \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u044f\u0432\u043d\u043e \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u043c</li> </ul>"},{"location":"portfolio/sfml/ml5_3%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html","title":"\u0420\u0435\u0448\u0430\u044e\u0448\u0438\u0435 \u0434\u0435\u0440\u0435\u0432\u044c\u044f \u0434\u043b\u044f \u0420\u0435\u0433\u0440\u0435\u0441\u0438\u0438\u00b6","text":"<p>\u041c\u044b \u043c\u043e\u0436\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u0435\u043f\u0435\u0432\u044c\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u0438 \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u0447\u0438 \u0440\u0435\u0433\u0440\u0435\u0441\u0438\u0438</p>"},{"location":"portfolio/sfml/ml5_3%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html","title":"\u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0434\u0435\u0440\u0435\u0432\u0430\u00b6","text":"<p>\u0412 \u0441\u043b\u0443\u0447\u0430\u0435 CART-\u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432</p> <ul> <li>\u0414\u0435\u0440\u0435\u0432\u043e \u0441\u0442\u0440\u043e\u0438\u0442\u0441\u044f \u0436\u0430\u0434\u043d\u044b\u043c \u0441\u043f\u043e\u0441\u043e\u0431\u043e\u043c; \u043d\u0430 \u043a\u0430\u0436\u0434\u043e\u043c \u044d\u0442\u0430\u043f\u0435 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u043e\u0447\u0435\u0440\u0435\u0434\u043d\u044b\u0445 \u0434\u0432\u0443\u0445 \u043f\u043e\u0442\u043e\u043c\u043a\u043e\u0432 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0435\u0435 \u0441 \u0442\u043e\u0447\u043a\u0438 \u0437\u0440\u0435\u043d\u0438\u044f \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u044f \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0432\u044b\u0431\u043e\u0440\u043a\u0438.</li> <li>\u0412 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0435, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043c\u044b \u043e\u043f\u0438\u0441\u0430\u043b\u0438 \u0440\u0430\u043d\u0435\u0435, \u0438\u043c\u0435\u043b\u0438 \u043c\u0435\u0441\u0442\u043e \u0432\u044b\u0437\u043e\u0432\u044b \u0444\u0443\u043d\u043a\u0446\u0438\u0438 best_split(X,y), \u0441\u0435\u0439\u0447\u0430\u0441 \u043c\u044b \u0440\u0430\u0437\u0431\u0435\u0440\u0451\u043c, \u0447\u0442\u043e \u0435\u0441\u0442\u044c \u043b\u0443\u0447\u0448\u0435\u0435 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435.</li> </ul> <p>\u0424\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e \u044d\u0442\u043e \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c:</p> <ul> <li>\u041f\u0443\u0441\u0442\u044c \u0432 \u0432\u0435\u0440\u0448\u0438\u043d\u0443 m \u043f\u043e\u043f\u0430\u043b\u043e \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u043e $X_{m}$ \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0438\u0437 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438.</li> <li>\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0432 \u0443\u0441\u043b\u043e\u0432\u0438\u0438 $\\left[ x^{j}\\leq t\\right]$ \u0431\u0443\u0434\u0443\u0442 \u0432\u044b\u0431\u0440\u0430\u043d\u044b \u0442\u0430\u043a, \u0447\u0442\u043e\u0431\u044b \u043c\u0438\u043d\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0439 \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0439 \u043e\u0448\u0438\u0431\u043a\u0438 , \u0437\u0430\u0432\u0438\u0441\u044f\u0449\u0438\u0439 \u043e\u0442 \u044d\u0442\u0438\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432: $Q\\left( x_{m},j,t\\right) =\\min _{j,t}$</li> <li>\u0433\u0434\u0435 j \u2014 \u043d\u043e\u043c\u0435\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430, t \u2014 \u043f\u043e\u0440\u043e\u0433\u043e\u0432\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430. \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b j \u0438 t \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0434\u0431\u0438\u0440\u0430\u0442\u044c \u043f\u0435\u0440\u0435\u0431\u043e\u0440\u043e\u043c.</li> <li>\u0414\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e, \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043a\u043e\u043d\u0435\u0447\u043d\u043e\u0435 \u0447\u0438\u0441\u043b\u043e, \u0430 \u0438\u0437 \u0432\u0441\u0435\u0445\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043f\u043e\u0440\u043e\u0433\u0430  \u043c\u043e\u0436\u043d\u043e \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435, \u043f\u0440\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043f\u043e\u043b\u0443\u0447\u0430\u044e\u0442\u0441\u044f \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f. \u041c\u043e\u0436\u043d\u043e \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u044c, \u0447\u0442\u043e \u0442\u0430\u043a\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430  \u0441\u0442\u043e\u043b\u044c\u043a\u043e, \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430  \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435.</li> </ul> <p>\u041f\u043e\u0441\u043b\u0435 \u0442\u043e\u0433\u043e, \u043a\u0430\u043a \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0431\u044b\u043b\u0438 \u0432\u044b\u0431\u0440\u0430\u043d\u044b, \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u043e $X_{m}$ \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0438\u0437 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u043d\u0430 \u0434\u0432\u0430 \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u0430</p> <ul> <li>$X_{l}=\\left\\{ x\\in X_{m} | x^{j}\\leq t\\right\\}$</li> <li>$X_{r}=\\left\\{ x\\in X_{m} | x^{j}\\gt t\\right\\}$</li> </ul> <p>\u043a\u0430\u0436\u0434\u043e\u0435 \u0438\u0437 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0441\u0432\u043e\u0435\u0439 \u0434\u043e\u0447\u0435\u0440\u043d\u0435\u0439 \u0432\u0435\u0440\u0448\u0438\u043d\u0435.</p> <ul> <li>\u041f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u043d\u0443\u044e \u043f\u0440\u043e\u0446\u0435\u0434\u0443\u0440\u0443 \u043c\u043e\u0436\u043d\u043e \u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0438\u0442\u044c \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u0438\u0437 \u0434\u043e\u0447\u0435\u0440\u043d\u0438\u0445 \u0432\u0435\u0440\u0448\u0438\u043d: \u0432 \u044d\u0442\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u0434\u0435\u0440\u0435\u0432\u043e \u0431\u0443\u0434\u0435\u0442 \u0432\u0441\u0435 \u0431\u043e\u043b\u044c\u0448\u0435 \u0438 \u0431\u043e\u043b\u044c\u0448\u0435 \u0443\u0433\u043b\u0443\u0431\u043b\u044f\u0442\u044c\u0441\u044f.</li> <li>\u0422\u0430\u043a\u043e\u0439 \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u0440\u0430\u043d\u043e \u0438\u043b\u0438 \u043f\u043e\u0437\u0434\u043d\u043e \u0434\u043e\u043b\u0436\u0435\u043d \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u044c\u0441\u044f, \u0438 \u043e\u0447\u0435\u0440\u0435\u0434\u043d\u0430\u044f \u0434\u043e\u0447\u0435\u0440\u043d\u044f\u044f \u0432\u0435\u0440\u0448\u0438\u043d\u0430 \u0431\u0443\u0434\u0435\u0442 \u043e\u0431\u044a\u044f\u0432\u043b\u0435\u043d\u0430 \u043b\u0438\u0441\u0442\u043a\u043e\u043c, \u0430 \u043d\u0435 \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0430 \u043f\u043e\u043f\u043e\u043b\u0430\u043c.</li> <li>\u042d\u0442\u043e\u0442 \u043c\u043e\u043c\u0435\u043d\u0442 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442\u0441\u044f \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0435\u043c \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0438. \u041e\u0434\u043d\u0438\u043c \u0438\u0437 \u0442\u0435\u0445, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u043e\u043f\u0438\u0441\u0430\u043b\u0438 \u0432 \u043f\u0443\u043d\u043a\u0442\u0435 \u043e \u0431\u043e\u0440\u044c\u0431\u0435 \u0441 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\u043c.</li> </ul> <p>\u041f\u043e\u0441\u043b\u0435 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u043b\u0438\u0441\u0442\u043e\u0432\u043e\u0433\u043e \u0443\u0437\u043b\u0430 \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u0437\u0430\u0434\u0430\u0442\u044c \u043f\u0440\u043e\u0433\u043d\u043e\u0437, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u043f\u0440\u0438\u0441\u0432\u043e\u0435\u043d \u0432\u0441\u0435\u043c \u043e\u0431\u044a\u0435\u043a\u0442\u0430\u043c, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0430 \u044d\u0442\u0430\u043f\u0435 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u043e\u043f\u0430\u0434\u0443\u0442 \u0432 \u044d\u0442\u043e\u0442 \u043b\u0438\u0441\u0442. \u0412 \u0441\u043b\u0443\u0447\u0430\u0435 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u043e \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0442\u044c \u0442\u043e\u0442 \u043a\u043b\u0430\u0441\u0441, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u0435\u043d \u0441\u0440\u0435\u0434\u0438 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0432 $X_{m}$</p>"},{"location":"portfolio/sfml/ml5_3%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html","title":"\u041a\u0440\u0438\u0442\u0435\u0440\u0438\u0438 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438\u00b6","text":"<p>\u041d\u0430\u0447\u043d\u0435\u043c \u0441 \u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430</p> <ul> <li>\u041d\u0430\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c <code>\u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0439 \u043e\u0448\u0438\u0431\u043a\u0438</code> $Q\\left( X_{m},j,t\\right) -&gt; \\min _{j,t}$</li> </ul> <p>\u041a\u0440\u0438\u0442\u0435\u0440\u0438\u0439 \u043e\u0448\u0438\u0431\u043a\u0438 \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0438\u0437 \u0434\u0432\u0443\u0445 \u0441\u043b\u0430\u0433\u0430\u0435\u043c\u044b\u0445, \u043a\u0430\u0436\u0434\u044b\u0439 \u0438\u0437 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0434\u043e\u0447\u0435\u0440\u043d\u0438\u043c \u0443\u0437\u043b\u0430\u043c:</p> <p>$Q\\left( X_{m},j,t\\right) = \\dfrac {\\left| X_{l}\\right| }{\\left| X_{m}\\right| }S\\left( X_{l}\\right) + \\dfrac {\\left| X_{r}\\right| }{\\left| X_{m}\\right| }S\\left( X_{r}\\right)$</p> <p>$X_{m}$ \u043e\u0431\u0449\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430, \u043f\u043e\u043f\u0430\u0432\u0448\u0430\u044f \u0432 \u0432\u0435\u0440\u0448\u0438\u043d\u0443, $\\left| X_{m}\\right|$ \u2014 \u0440\u0430\u0437\u043c\u0435\u0440 \u044d\u0442\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438;</p> <ul> <li>j \u043d\u043e\u043c\u0435\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u043e\u0433\u043e \u0434\u043b\u044f \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f;</li> <li>t \u043f\u043e\u0440\u043e\u0433\u043e\u0432\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430;</li> </ul> <p>$X_{l}$ \u0438 $X_{r}$ \u2014 \u0432\u044b\u0431\u043e\u0440\u043a\u0438, \u043f\u043e\u043f\u0430\u0432\u0448\u0438\u0435 \u0432 \u043b\u0435\u0432\u0443\u044e \u0438 \u043f\u0440\u0430\u0432\u0443\u044e \u0432\u0435\u0440\u0448\u0438\u043d\u044b \u043f\u0440\u0438 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0438 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 $X_{m}$ \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 j \u0438 t, $\\left| X_{l}\\right|$ \u0438 $\\left| X_{r}\\right|$ \u2014 \u0440\u0430\u0437\u043c\u0435\u0440\u044b \u044d\u0442\u0438\u0445 \u0432\u044b\u0431\u043e\u0440\u043e\u043a;</p> <p>$S(X)$ \u044d\u0442\u043e \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f <code>\u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0435\u043c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438</code>, \u0435\u0451 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0443\u043c\u0435\u043d\u044c\u0448\u0430\u0435\u0442\u0441\u044f \u0441 \u0443\u043c\u0435\u043d\u044c\u0448\u0435\u043d\u0438\u0435\u043c \u0440\u0430\u0437\u0431\u0440\u043e\u0441\u0430 \u043e\u0442\u0432\u0435\u0442\u043e\u0432 \u043d\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 $X$ (\u043e\u0431 \u044d\u0442\u043e\u043c \u043c\u044b \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u043f\u043e\u0433\u043e\u0432\u043e\u0440\u0438\u043c \u043d\u0438\u0436\u0435).</p>"},{"location":"portfolio/sfml/ml5_4%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html","title":"ml5 4\u0420\u0435\u0448 \u0414\u0435\u0440","text":"In\u00a0[1]: Copied! <pre>from matplotlib.colors import ListedColormap\nfrom sklearn import model_selection, datasets, metrics, tree \nfrom sklearn.datasets import make_classification as make\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ncolors = ListedColormap(['red', 'blue', 'yellow'])\nlight_colors = ListedColormap(['lightcoral', 'lightblue', 'lightyellow'])\n</pre> from matplotlib.colors import ListedColormap from sklearn import model_selection, datasets, metrics, tree  from sklearn.datasets import make_classification as make  import numpy as np import matplotlib.pyplot as plt  colors = ListedColormap(['red', 'blue', 'yellow']) light_colors = ListedColormap(['lightcoral', 'lightblue', 'lightyellow']) In\u00a0[\u00a0]: Copied! <pre># \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 (X,y)\nX,y = make(n_features = 2,\n           n_informative = 2, \n           n_classes = 3,\n           n_redundant=0,\n           n_clusters_per_class=1, \n           random_state=3)\n</pre> # \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 (X,y) X,y = make(n_features = 2,            n_informative = 2,             n_classes = 3,            n_redundant=0,            n_clusters_per_class=1,             random_state=3) In\u00a0[3]: Copied! <pre>X_train, X_test, y_train, y_test = model_selection.train_test_split(X,y,\n                                                                    test_size = 0.3,\n                                                                    random_state = 1)\n</pre> X_train, X_test, y_train, y_test = model_selection.train_test_split(X,y,                                                                     test_size = 0.3,                                                                     random_state = 1) In\u00a0[4]: Copied! <pre># \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043e\u0431\u044a\u0435\u043a\u0442 \u043a\u043b\u0430\u0441\u0441\u0430 DecisionTreeClassifier\n# \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u0443 \u044d\u0442\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u043c\u0435\u0442\u043e\u0434 fit \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\nclf = tree.DecisionTreeClassifier(random_state=1)\nclf.fit(X_train, y_train)\n</pre> # \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043e\u0431\u044a\u0435\u043a\u0442 \u043a\u043b\u0430\u0441\u0441\u0430 DecisionTreeClassifier # \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u0443 \u044d\u0442\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u043c\u0435\u0442\u043e\u0434 fit \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 clf = tree.DecisionTreeClassifier(random_state=1) clf.fit(X_train, y_train) Out[4]: <pre>DecisionTreeClassifier(random_state=1)</pre> In\u00a0[5]: Copied! <pre># \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\n# \u041e\u0446\u0435\u043d\u0438\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0443 accuracy_score\npredictions = clf.predict(X_test)\nmetrics.accuracy_score(y_test, predictions)\n</pre> # \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 # \u041e\u0446\u0435\u043d\u0438\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0443 accuracy_score predictions = clf.predict(X_test) metrics.accuracy_score(y_test, predictions) Out[5]: <pre>0.7666666666666667</pre> In\u00a0[6]: Copied! <pre># \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u043a\u0438\u0435 \u043b\u0435\u0439\u0431\u044b\u043b\u0438 \u043f\u0440\u0438\u0441\u0432\u043e\u0438\u043b \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\npredictions\n</pre> # \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u043a\u0438\u0435 \u043b\u0435\u0439\u0431\u044b\u043b\u0438 \u043f\u0440\u0438\u0441\u0432\u043e\u0438\u043b \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 predictions Out[6]: <pre>array([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 2, 2, 2, 2, 2, 1, 0, 1, 0, 2, 2, 0,\n       2, 0, 0, 0, 2, 1, 2, 0])</pre> In\u00a0[7]: Copied! <pre>def get_meshgrid(data, step=.05, border=.5,):\n    x_min, x_max = data[:, 0].min() - border, data[:, 0].max() + border\n    y_min, y_max = data[:, 1].min() - border, data[:, 1].max() + border\n    return np.meshgrid(np.arange(x_min, x_max, step),\n                       np.arange(y_min, y_max, step))\n\ndef plot_decision_surface(estimator, train_data, \n                          train_labels, test_data, test_labels, \n                          colors = colors, light_colors = light_colors):\n    #fit model\n    estimator.fit(train_data, train_labels)\n    \n    #set figure size\n    plt.figure(figsize = (8, 3))\n    \n    #plot decision surface on the train data \n    plt.subplot(1,2,1)\n    xx, yy = get_meshgrid(train_data)\n    pred = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n    mesh_predictions = np.array(pred).reshape(xx.shape)\n    plt.pcolormesh(xx, yy, mesh_predictions, cmap = light_colors)\n    plt.scatter(train_data[:, 0], train_data[:, 1],\n                c = train_labels, \n                s = 40, cmap = colors,\n                edgecolors='k')\n    accs = metrics.accuracy_score(train_labels,estimator.predict(train_data))\n    plt.title(f\"Train data, accuracy={accs:.2f}\",size=10)\n    \n    #plot decision surface on the test data\n    plt.subplot(1,2,2)\n    plt.pcolormesh(xx, yy, mesh_predictions, cmap = light_colors)\n    plt.scatter(test_data[:, 0], test_data[:, 1], \n                c = test_labels, \n                s = 40, cmap = colors,\n                ec = 'k')\n    accs_test = metrics.accuracy_score(test_labels, estimator.predict(test_data))\n    plt.title(f\"Test data, accuracy= {accs_test:.2f}\",size=10)\n</pre> def get_meshgrid(data, step=.05, border=.5,):     x_min, x_max = data[:, 0].min() - border, data[:, 0].max() + border     y_min, y_max = data[:, 1].min() - border, data[:, 1].max() + border     return np.meshgrid(np.arange(x_min, x_max, step),                        np.arange(y_min, y_max, step))  def plot_decision_surface(estimator, train_data,                            train_labels, test_data, test_labels,                            colors = colors, light_colors = light_colors):     #fit model     estimator.fit(train_data, train_labels)          #set figure size     plt.figure(figsize = (8, 3))          #plot decision surface on the train data      plt.subplot(1,2,1)     xx, yy = get_meshgrid(train_data)     pred = estimator.predict(np.c_[xx.ravel(), yy.ravel()])     mesh_predictions = np.array(pred).reshape(xx.shape)     plt.pcolormesh(xx, yy, mesh_predictions, cmap = light_colors)     plt.scatter(train_data[:, 0], train_data[:, 1],                 c = train_labels,                  s = 40, cmap = colors,                 edgecolors='k')     accs = metrics.accuracy_score(train_labels,estimator.predict(train_data))     plt.title(f\"Train data, accuracy={accs:.2f}\",size=10)          #plot decision surface on the test data     plt.subplot(1,2,2)     plt.pcolormesh(xx, yy, mesh_predictions, cmap = light_colors)     plt.scatter(test_data[:, 0], test_data[:, 1],                  c = test_labels,                  s = 40, cmap = colors,                 ec = 'k')     accs_test = metrics.accuracy_score(test_labels, estimator.predict(test_data))     plt.title(f\"Test data, accuracy= {accs_test:.2f}\",size=10) In\u00a0[8]: Copied! <pre>estimator = tree.DecisionTreeClassifier(random_state = 1, max_depth = 1)\nplot_decision_surface(estimator, X_train, y_train, X_test, y_test)\n</pre> estimator = tree.DecisionTreeClassifier(random_state = 1, max_depth = 1) plot_decision_surface(estimator, X_train, y_train, X_test, y_test) In\u00a0[9]: Copied! <pre>estimator = tree.DecisionTreeClassifier(random_state = 1,max_depth = 2)\nplot_decision_surface(estimator,X_train, y_train, X_test, y_test)\n</pre> estimator = tree.DecisionTreeClassifier(random_state = 1,max_depth = 2) plot_decision_surface(estimator,X_train, y_train, X_test, y_test) In\u00a0[10]: Copied! <pre>estimator = tree.DecisionTreeClassifier(random_state = 1,max_depth = 3)\nplot_decision_surface(estimator,X_train, y_train, X_test, y_test)\n</pre> estimator = tree.DecisionTreeClassifier(random_state = 1,max_depth = 3) plot_decision_surface(estimator,X_train, y_train, X_test, y_test) In\u00a0[11]: Copied! <pre>estimator = tree.DecisionTreeClassifier(random_state = 1)\nplot_decision_surface(estimator,X_train, y_train, X_test, y_test)\n</pre> estimator = tree.DecisionTreeClassifier(random_state = 1) plot_decision_surface(estimator,X_train, y_train, X_test, y_test) In\u00a0[12]: Copied! <pre># \u0421\u043f\u043e\u043c\u043e\u0448\u044c\u044e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0438, \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u043f\u0440\u0435\u0434\u043e\u0432\u0440\u0430\u0442\u0438\u0442\u044c \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\nestimator = tree.DecisionTreeClassifier(random_state = 1,\n                                        min_samples_leaf = 2)\nplot_decision_surface(estimator, X_train, y_train, X_test, y_test)\n</pre> # \u0421\u043f\u043e\u043c\u043e\u0448\u044c\u044e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0438, \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u043f\u0440\u0435\u0434\u043e\u0432\u0440\u0430\u0442\u0438\u0442\u044c \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 estimator = tree.DecisionTreeClassifier(random_state = 1,                                         min_samples_leaf = 2) plot_decision_surface(estimator, X_train, y_train, X_test, y_test) <p>\u041c\u043e\u0436\u043d\u043e \u043b\u0438 \u0440\u0435\u0448\u0430\u0442\u044c \u0437\u0430\u0434\u0430\u0447\u0443 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0440\u0435\u0448\u0430\u044e\u0449\u0438\u0445 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432?</p> <ul> <li>\u0414\u0430, \u043c\u043e\u0436\u043d\u043e \u2014 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043b\u0438\u0448\u044c \u0432\u044b\u0431\u0440\u0430\u0442\u044c \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438, \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u044e\u0449\u0438\u0439 \u0440\u0430\u0437\u0431\u0440\u043e\u0441 \u0432\u0435\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0445 \u043e\u0442\u0432\u0435\u0442\u043e\u0432.</li> </ul>"},{"location":"portfolio/sfml/ml5_4%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html","title":"\u0420\u0435\u0448\u0430\u044e\u0449\u0438\u0435 \u0434\u0435\u0440\u0435\u0432\u044c\u044f \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430\u00b6","text":""},{"location":"portfolio/sfml/ml5_4%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html","title":"\u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u041c\u043e\u0434\u0443\u043b\u0438\u00b6","text":""},{"location":"portfolio/sfml/ml5_4%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html","title":"\u0414\u0430\u0442\u0430\u0441\u0435\u0442\u00b6","text":"<p>\u041d\u0430 \u043f\u0440\u043e\u0441\u0442\u043e\u043c \u043f\u0440\u0438\u043c\u0435\u0440\u0435, \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438</p>"},{"location":"portfolio/sfml/ml5_4%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html","title":"\u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0412\u044b\u0431\u043e\u0440\u043a\u0443\u00b6","text":""},{"location":"portfolio/sfml/ml5_4%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html","title":"\u041c\u043e\u0434\u0435\u043b\u044c \u0414\u0435\u0440\u0435\u0432\u044c\u0435 \u0420\u0435\u0448\u0435\u043d\u0438\u0439\u00b6","text":"<p>\u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0438 \u043e\u0446\u0435\u043d\u0438\u043c \u0435\u0433\u043e \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435</p>"},{"location":"portfolio/sfml/ml5_4%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html","title":"\u0412\u043b\u0438\u044f\u043d\u0438\u044f \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432\u00b6","text":"<p>\u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e \u0434\u0430\u043d\u043d\u044b\u0445, <code>get_meshgrid</code> \u0438 <code>plot_decision_surface</code> \u043f\u043e\u043c\u0430\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438</p>"},{"location":"portfolio/sfml/ml5_4%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html#max_depth-1","title":"MAX_DEPTH = 1\u00b6","text":"<ul> <li>\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u0434\u0435\u0440\u0435\u0432\u043e \u0441 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0433\u043b\u0443\u0431\u0438\u043d\u044b <code>max_depth = 1</code></li> <li>\u0420\u0430\u0437\u0434\u0435\u043b\u044f\u044e\u0448\u0430\u044f \u043f\u0440\u0435\u043c\u0430\u044f \u0443 \u043d\u0430\u0441 \u043e\u0434\u043d\u0430</li> </ul>"},{"location":"portfolio/sfml/ml5_4%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html#max_depth-2","title":"MAX_DEPTH = 2\u00b6","text":"<p>\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u0434\u0435\u0440\u0435\u0432\u043e \u0441 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0433\u043b\u0443\u0431\u0438\u043d\u044b <code>max_depth = 2</code></p> <ul> <li>\u0420\u0430\u0437\u0434\u0435\u043b\u044f\u044e\u0448\u0438\u0445 \u043f\u0440\u044f\u043c\u044b\u0445 \u0443 \u043d\u0430\u0441 \u0434\u0432\u0435</li> </ul>"},{"location":"portfolio/sfml/ml5_4%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html#max_depth-3","title":"MAX_DEPTH = 3\u00b6","text":"<p>\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u0434\u0435\u0440\u0435\u0432\u043e \u0441 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0433\u043b\u0443\u0431\u0438\u043d\u044b <code>max_depth = 3</code></p> <ul> <li>\u0420\u0430\u0437\u0434\u0435\u043b\u044f\u044e\u0448\u0438\u0445 \u043f\u0440\u044f\u043c\u044b\u0445 \u0443 \u043d\u0430\u0441 \u0442\u0440\u0438</li> </ul>"},{"location":"portfolio/sfml/ml5_4%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html#max_depth-none","title":"MAX_DEPTH = None\u00b6","text":"<ul> <li>\u041d\u0435 \u0431\u0443\u0434\u0435\u043c \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0438\u0432\u0430\u0442\u044c \u0434\u0435\u0440\u0435\u0432\u043e \u043f\u043e \u0433\u043b\u0443\u0431\u0438\u043d\u0435 (\u0431\u0430\u0437\u043e\u0432\u044b\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b)</li> <li>\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u0434\u0435\u0440\u0435\u0432\u043e \u0441 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0433\u043b\u0443\u0431\u0438\u043d\u044b <code>max_depth =  'none'</code></li> <li>\u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c <code>\u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0435</code> \u0434\u0435\u0440\u0435\u0432\u043e \u0440\u0435\u0448\u0435\u043d\u0438\u0438</li> </ul>"},{"location":"portfolio/sfml/ml5_4%D0%A0%D0%B5%D1%88_%D0%94%D0%B5%D1%80.html","title":"\u041a\u043e\u043d\u0442\u0440\u043e\u043b\u0438\u0440\u0443\u0435\u043c \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\u00b6","text":"<p>\u041e\u0434\u0438\u043d \u0438\u0437 \u0441\u043f\u043e\u0441\u043e\u0431\u043e\u0432 \u0431\u043e\u0440\u043e\u0442\u0441\u044f \u0441 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\u043c, \u0447\u0435\u0440\u0435\u0437 <code>min_samples_leaf</code> (\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u0435\u043c\u043f\u043b\u043e\u0432 \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u043b\u0438\u0441\u0442\u0435)</p>"},{"location":"portfolio/sfml/ml5_%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8.html","title":"ml5 \u0410\u043d\u0441\u0430\u043c\u0431\u043b\u0438","text":"<p>\u0411\u044d\u0433\u0433\u0438\u043d\u0433 (\u043e\u0442 \u0430\u043d\u0433\u043b. \"<code>Bootstrap Aggregating</code>\") \u2014 \u044d\u0442\u043e \u043c\u0435\u0442\u043e\u0434 \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u0435\u0432\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043f\u043e\u0432\u044b\u0448\u0435\u043d\u0438\u044f \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u0438 \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f. \u041e\u043d \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u0435\u043d \u0434\u043b\u044f \u0443\u043c\u0435\u043d\u044c\u0448\u0435\u043d\u0438\u044f \u0432\u0430\u0440\u0438\u0430\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438 \u0438 \u043f\u0440\u0435\u0434\u043e\u0442\u0432\u0440\u0430\u0449\u0435\u043d\u0438\u044f \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f.</p> <p>\u041e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u0448\u0430\u0433\u0438 \u0431\u044d\u0433\u0433\u0438\u043d\u0433\u0430:</p> <ol> <li><p>\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043e\u043a: \u0418\u0437 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441\u043e\u0437\u0434\u0430\u044e\u0442\u0441\u044f \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043e\u043a \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u043c\u0435\u0442\u043e\u0434\u0430 \u0431\u0443\u0442\u0441\u0442\u0440\u044d\u043f \u2014 \u0442\u043e \u0435\u0441\u0442\u044c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c \u0432\u044b\u0431\u0438\u0440\u0430\u044e\u0442\u0441\u044f \u043e\u0431\u0440\u0430\u0437\u0446\u044b \u0441 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0435\u043d\u0438\u0435\u043c. \u042d\u0442\u043e \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442, \u0447\u0442\u043e \u043e\u0434\u0438\u043d \u0438 \u0442\u043e\u0442 \u0436\u0435 \u043e\u0431\u044a\u0435\u043a\u0442 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0432\u044b\u0431\u0440\u0430\u043d \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0437, \u0430 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u043d\u0435 \u0432\u044b\u0431\u0440\u0430\u043d\u044b \u0432\u043e\u0432\u0441\u0435.</p> </li> <li><p>\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0435\u0439: \u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u0438\u0437 \u0441\u043e\u0437\u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043e\u043a \u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c. \u042d\u0442\u043e \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u043a\u0430\u043a \u043e\u0434\u043d\u043e\u0442\u0438\u043f\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 \u0440\u0435\u0448\u0435\u043d\u0438\u0439), \u0442\u0430\u043a \u0438 \u0440\u0430\u0437\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438.</p> </li> <li><p>\u0410\u0433\u0440\u0435\u0433\u0430\u0446\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432: \u041f\u043e\u0441\u043b\u0435 \u0442\u043e\u0433\u043e \u043a\u0430\u043a \u0432\u0441\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u044b, \u0438\u0445 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u044e\u0442\u0441\u044f. \u0414\u043b\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u043e\u0431\u044b\u0447\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439, \u0430 \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u2014 \u0433\u043e\u043b\u043e\u0441\u043e\u0432\u0430\u043d\u0438\u0435 (\u0442\u043e \u0435\u0441\u0442\u044c \u043a\u043b\u0430\u0441\u0441, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u043e\u043b\u0443\u0447\u0438\u043b \u043d\u0430\u0438\u0431\u043e\u043b\u044c\u0448\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0433\u043e\u043b\u043e\u0441\u043e\u0432, \u0441\u0447\u0438\u0442\u0430\u0435\u0442\u0441\u044f \u0438\u0442\u043e\u0433\u043e\u0432\u044b\u043c).</p> </li> </ol> <p>\u0411\u044d\u0433\u0433\u0438\u043d\u0433 \u043f\u043e\u043c\u043e\u0433\u0430\u0435\u0442 \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u043e\u0431\u043e\u0431\u0449\u0430\u044e\u0449\u0443\u044e \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438, \u0442\u0430\u043a \u043a\u0430\u043a \u0441\u043d\u0438\u0436\u0430\u0435\u0442 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0437\u0430 \u0441\u0447\u0435\u0442 \u0443\u0441\u0440\u0435\u0434\u043d\u0435\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439. \u041e\u0434\u0438\u043d \u0438\u0437 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0449\u0438\u0445 \u0431\u044d\u0433\u0433\u0438\u043d\u0433, \u2014 \u044d\u0442\u043e Random Forest (\u0421\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441), \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0441\u0442\u0440\u043e\u0438\u0442 \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u043e \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 \u0440\u0435\u0448\u0435\u043d\u0438\u0439 \u0438 \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u0442 \u0438\u0445 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f.</p> <p><code>Random Subspaces</code> (RSS)</p> <p>\u0414\u043b\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u043d\u0430\u0431\u043e\u0440\u0430 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0442\u0430\u043a\u0436\u0435 \u043c\u0435\u0442\u043e\u0434 \u0432\u044b\u0431\u043e\u0440\u0430 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0445 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043e\u043a \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 Random Subspaces. \u041c\u0435\u0442\u043e\u0434 \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u0435\u0442 \u0443\u0441\u0442\u043e\u0439\u0447\u0438\u0432\u043e\u0441\u0442\u044c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 \u043a \u043d\u0430\u0431\u043e\u0440\u0443 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432.</p> In\u00a0[3]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets\n\ndf = pd.read_csv('HR-dataset.csv')\n\nnp.random.seed(42)\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\ntarget = 'left'\nfeatures = df.columns.drop(target)\nfeatures = features.drop('empid')  # \u0423\u0434\u0430\u043b\u0438\u043c \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u043a\u0430\u043a \u043d\u0435\u0440\u0435\u043f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0442\u0438\u0432\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\nprint(features)\n\nX, y = df[features].copy(), df[target]\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt  from sklearn.model_selection import train_test_split from sklearn.ensemble import BaggingClassifier, RandomForestClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.linear_model import LogisticRegression from sklearn import datasets  df = pd.read_csv('HR-dataset.csv')  np.random.seed(42) %matplotlib inline %config InlineBackend.figure_format = 'retina'  target = 'left' features = df.columns.drop(target) features = features.drop('empid')  # \u0423\u0434\u0430\u043b\u0438\u043c \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u043a\u0430\u043a \u043d\u0435\u0440\u0435\u043f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0442\u0438\u0432\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a print(features)  X, y = df[features].copy(), df[target] <pre>Index(['satisfaction_level', 'last_evaluation', 'number_project',\n       'average_montly_hours', 'time_spend_company', 'Work_accident',\n       'promotion_last_5years', 'dept', 'salary'],\n      dtype='object')\n</pre> In\u00a0[4]: Copied! <pre># \u0447\u0430\u0441\u0442\u043e\u0442\u043d\u043e\u0435 \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\ndept_val_counts = X['dept'].value_counts()\nX['dept'] = X['dept'].map(dept_val_counts)\n\n# \u043e\u0440\u0434\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\nsalary_ordinals = {'low': 1, 'medium': 2, 'high': 3}\nX['salary'] = X['salary'].map(salary_ordinals)\n# X = X.copy()\n\n# \u044c\u0430\u0448\u0442\u0430\u0431\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX = pd.DataFrame(data=scaler.fit_transform(X), \n                 columns=X.columns)\n</pre> # \u0447\u0430\u0441\u0442\u043e\u0442\u043d\u043e\u0435 \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 dept_val_counts = X['dept'].value_counts() X['dept'] = X['dept'].map(dept_val_counts)  # \u043e\u0440\u0434\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 salary_ordinals = {'low': 1, 'medium': 2, 'high': 3} X['salary'] = X['salary'].map(salary_ordinals) # X = X.copy()  # \u044c\u0430\u0448\u0442\u0430\u0431\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 from sklearn.preprocessing import StandardScaler  scaler = StandardScaler() X = pd.DataFrame(data=scaler.fit_transform(X),                   columns=X.columns) In\u00a0[5]: Copied! <pre>from sklearn.model_selection import cross_val_score\n\ndef estimate_accuracy(clf, X, y, cv=5,metric='f1'):\n    cv_mean = cross_val_score(clf, X, y, cv=5, scoring=metric).mean()\n    return round(cv_mean,3)\n</pre> from sklearn.model_selection import cross_val_score  def estimate_accuracy(clf, X, y, cv=5,metric='f1'):     cv_mean = cross_val_score(clf, X, y, cv=5, scoring=metric).mean()     return round(cv_mean,3) <ul> <li>\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0435\u0440\u0435\u0432\u0430 \u0440\u0435\u0448\u0435\u043d\u0438\u0438 \u0441 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0433\u043b\u0443\u0431\u0438\u043d\u043e\u0439 30 (<code>max_depth=30</code>)</li> <li>\u041f\u0440\u043e\u0432\u0435\u0434\u0451\u043c <code>\u0431\u044d\u0433\u0433\u0438\u043d\u0433</code> : \u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043e\u0431\u0435\u0440\u043d\u0443\u0442\u044c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u0432 <code>BaggingClassifier</code></li> </ul> In\u00a0[6]: Copied! <pre># \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0435\u0440\u0435\u0432\u0430.\ntree = DecisionTreeClassifier(max_depth=30)\nprint(\"Decision tree:\", estimate_accuracy(tree, X, y))\n\n# \u041f\u0440\u043e\u0432\u0435\u0434\u0451\u043c \u0431\u044d\u0433\u0433\u0438\u043d\u0433: \u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043e\u0431\u0435\u0440\u043d\u0443\u0442\u044c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u0432 BaggingClassifier.\nbagging_trees = BaggingClassifier(tree)\nprint(\"Decision tree bagging:\", estimate_accuracy(bagging_trees, X, y))\n\n#  \u042d\u0442\u043e \u044f\u0432\u043d\u043e \u0443\u043b\u0443\u0447\u0448\u0430\u0435\u0442 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u0431\u0435\u0433\u0433\u0438\u043d\u0433\u0430 \u043d\u043e \u043c\u043e\u0434\u0435\u043b\u044c \u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0435\u0440\u0435\u0432\u0430\n</pre> # \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0435\u0440\u0435\u0432\u0430. tree = DecisionTreeClassifier(max_depth=30) print(\"Decision tree:\", estimate_accuracy(tree, X, y))  # \u041f\u0440\u043e\u0432\u0435\u0434\u0451\u043c \u0431\u044d\u0433\u0433\u0438\u043d\u0433: \u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043e\u0431\u0435\u0440\u043d\u0443\u0442\u044c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u0432 BaggingClassifier. bagging_trees = BaggingClassifier(tree) print(\"Decision tree bagging:\", estimate_accuracy(bagging_trees, X, y))  #  \u042d\u0442\u043e \u044f\u0432\u043d\u043e \u0443\u043b\u0443\u0447\u0448\u0430\u0435\u0442 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u0431\u0435\u0433\u0433\u0438\u043d\u0433\u0430 \u043d\u043e \u043c\u043e\u0434\u0435\u043b\u044c \u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0435\u0440\u0435\u0432\u0430 <pre>Decision tree: 0.945\nDecision tree bagging: 0.975\n</pre> <p><code>\u041a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u044f</code> \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0445 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u0441\u0435\u0431\u044f \u043b\u0443\u0447\u0448\u0435, \u0447\u0435\u043c \u043e\u0434\u043d\u043e \u0434\u0435\u0440\u0435\u0432\u043e</p> In\u00a0[7]: Copied! <pre>mfeats = int(np.sqrt(len(features)))\nprint(f'Number of features: {mfeats}')\n\nrandom_tree = DecisionTreeClassifier(max_features=mfeats,\n                                     max_depth=30)\nprint(\"Random tree:\", estimate_accuracy(random_tree, X, y))\n\nbagging_random_trees = BaggingClassifier(random_tree)\nprint(\"Random tree bagging:\", estimate_accuracy(bagging_random_trees, X, y))\n</pre> mfeats = int(np.sqrt(len(features))) print(f'Number of features: {mfeats}')  random_tree = DecisionTreeClassifier(max_features=mfeats,                                      max_depth=30) print(\"Random tree:\", estimate_accuracy(random_tree, X, y))  bagging_random_trees = BaggingClassifier(random_tree) print(\"Random tree bagging:\", estimate_accuracy(bagging_random_trees, X, y)) <pre>Number of features: 3\nRandom tree: 0.954\nRandom tree bagging: 0.979\n</pre> In\u00a0[8]: Copied! <pre># \u0421\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u0430\u044f \u044d\u0432\u0440\u0438\u0441\u0442\u0438\u043a\u0430: \n# - \u0432 \u0437\u0430\u0434\u0430\u0447\u0435 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0431\u0440\u0430\u0442\u044c **\u043a\u0432\u0430\u0434\u0440\u0430\u0442\u043d\u044b\u0439 \u043a\u043e\u0440\u0435\u043d\u044c \u0447\u0438\u0441\u043b\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432**, \u0437\u0430\u0434\u0430\u0447\u0435 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u2014 **\u0442\u0440\u0435\u0442\u044c \u0447\u0438\u0441\u043b\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432**\n\nmfeats = int(np.sqrt(len(features)))\nrandom_forest = RandomForestClassifier(n_estimators=100,\n                                       n_jobs=-1,\n                                       max_features=mfeats,\n                                       max_depth=30)\nprint(\"Random Forest:\", estimate_accuracy(random_forest, X, y))\n</pre> # \u0421\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u0430\u044f \u044d\u0432\u0440\u0438\u0441\u0442\u0438\u043a\u0430:  # - \u0432 \u0437\u0430\u0434\u0430\u0447\u0435 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0431\u0440\u0430\u0442\u044c **\u043a\u0432\u0430\u0434\u0440\u0430\u0442\u043d\u044b\u0439 \u043a\u043e\u0440\u0435\u043d\u044c \u0447\u0438\u0441\u043b\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432**, \u0437\u0430\u0434\u0430\u0447\u0435 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u2014 **\u0442\u0440\u0435\u0442\u044c \u0447\u0438\u0441\u043b\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432**  mfeats = int(np.sqrt(len(features))) random_forest = RandomForestClassifier(n_estimators=100,                                        n_jobs=-1,                                        max_features=mfeats,                                        max_depth=30) print(\"Random Forest:\", estimate_accuracy(random_forest, X, y)) <pre>Random Forest: 0.983\n</pre> In\u00a0[9]: Copied! <pre>mfeats = int(np.sqrt(len(features)))\nrandom_forest = RandomForestClassifier(n_estimators=100,\n                                       max_features=mfeats,\n                                       max_depth=30,\n                                       oob_score=True,\n                                       n_jobs=-1)\nrandom_forest.fit(X, y)\n\n# \u0442\u0435\u043a\u0442\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u043d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\nround(random_forest.oob_score_,3)\n</pre> mfeats = int(np.sqrt(len(features))) random_forest = RandomForestClassifier(n_estimators=100,                                        max_features=mfeats,                                        max_depth=30,                                        oob_score=True,                                        n_jobs=-1) random_forest.fit(X, y)  # \u0442\u0435\u043a\u0442\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u043d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b round(random_forest.oob_score_,3) Out[9]: <pre>0.993</pre> In\u00a0[10]: Copied! <pre>from sklearn.linear_model import LogisticRegression\nimport warnings; warnings.filterwarnings('ignore')\n\n# \u0431\u0430\u0437\u043e\u0432\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\nlr = LogisticRegression(solver='saga', \n                        max_iter=200)\nlr.fit(X, y)\nprint(\"LR:\", estimate_accuracy(lr, X, y))\n\nfrom sklearn.ensemble import BaggingClassifier\n\n# \u0431\u044d\u0433\u0433\u0438\u043d\u0433 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\nrandom_logreg = BaggingClassifier(lr,\n                                  n_estimators=10,\n                                  n_jobs=-1,\n                                  random_state=42)\nprint(\"Bagging for LR:\", estimate_accuracy(random_logreg, X, y))\n</pre> from sklearn.linear_model import LogisticRegression import warnings; warnings.filterwarnings('ignore')  # \u0431\u0430\u0437\u043e\u0432\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c lr = LogisticRegression(solver='saga',                          max_iter=200) lr.fit(X, y) print(\"LR:\", estimate_accuracy(lr, X, y))  from sklearn.ensemble import BaggingClassifier  # \u0431\u044d\u0433\u0433\u0438\u043d\u0433 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 random_logreg = BaggingClassifier(lr,                                   n_estimators=10,                                   n_jobs=-1,                                   random_state=42) print(\"Bagging for LR:\", estimate_accuracy(random_logreg, X, y)) <pre>LR: 0.442\nBagging for LR: 0.437\n</pre> <p>\u041f\u043e\u0447\u0435\u043c\u0443 \u0442\u0430\u043a?</p> <ul> <li>\u0412 \u0435\u0451 \u0441\u043b\u0443\u0447\u0430\u0435 \u043e\u043d \u043d\u0435 \u0442\u0430\u043a \u0441\u0438\u043b\u044c\u043d\u043e \u043f\u043e\u0432\u044b\u0448\u0430\u0435\u0442 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e, \u043f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 <code>\u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0435</code> \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0435 \u0442\u0430\u043a \u0441\u0438\u043b\u044c\u043d\u043e \u0437\u0430\u0432\u0438\u0441\u044f\u0442 \u043e\u0442 \u0441\u043e\u0441\u0442\u0430\u0432\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438</li> <li>\u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0443\u0431\u0440\u0430\u0442\u044c \u0447\u0430\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e <code>max_features</code></li> </ul> In\u00a0[11]: Copied! <pre>random_logreg = BaggingClassifier(\n    lr,\n    n_estimators=10,\n    n_jobs=-1,\n    max_features=0.5,  # \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0447\u0430\u0441\u0442\u044c \u0444\u0438\u0447\n    random_state=42\n)\nprint(\"Bagging for LR:\", estimate_accuracy(random_logreg, X, y))\n</pre> random_logreg = BaggingClassifier(     lr,     n_estimators=10,     n_jobs=-1,     max_features=0.5,  # \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0447\u0430\u0441\u0442\u044c \u0444\u0438\u0447     random_state=42 ) print(\"Bagging for LR:\", estimate_accuracy(random_logreg, X, y)) <pre>Bagging for LR: 0.22\n</pre> <p>\u0412 \u0441\u043b\u0443\u0447\u0430\u0435 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438:</p> <ul> <li>\u041f\u043e\u0432\u044b\u0448\u0435\u043d\u0438\u0435 <code>\u0440\u0430\u0437\u043d\u043e\u043e\u0431\u0440\u0430\u0437\u043d\u043e\u0441\u0442\u0438</code> \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043d\u0435 \u0434\u0430\u0435\u0442 \u0442\u0430\u043a\u043e\u0433\u043e \u043f\u0440\u0438\u0440\u043e\u0441\u0442\u0430, \u043a\u0430\u043a \u0441 \u0434\u0435\u0440\u0435\u0432\u044c\u044f\u043c\u0438, \u043f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u043c\u043e\u0434\u0435\u043b\u0438 \u0441\u0438\u043b\u044c\u043d\u043e \u0442\u0435\u0440\u044f\u044e\u0442 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435.</li> <li>\u0421\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441 \u043d\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u043d\u0430\u0448\u0435\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 \u0441\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043b\u0443\u0447\u0448\u0435.</li> </ul> In\u00a0[12]: Copied! <pre>import pandas as pd\nfrom sklearn.datasets import load_digits\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\ndigits = load_digits()\nX = digits.data\ny = digits.target\n\nscaler = StandardScaler()\nX_view = pd.DataFrame(data=scaler.fit_transform(X), \n                       columns=digits.feature_names)\nfeatures = digits.feature_names\nX_view.head()\n</pre> import pandas as pd from sklearn.datasets import load_digits from sklearn.preprocessing import StandardScaler from sklearn.model_selection import cross_val_score  digits = load_digits() X = digits.data y = digits.target  scaler = StandardScaler() X_view = pd.DataFrame(data=scaler.fit_transform(X),                         columns=digits.feature_names) features = digits.feature_names X_view.head() Out[12]: pixel_0_0 pixel_0_1 pixel_0_2 pixel_0_3 pixel_0_4 pixel_0_5 pixel_0_6 pixel_0_7 pixel_1_0 pixel_1_1 ... pixel_6_6 pixel_6_7 pixel_7_0 pixel_7_1 pixel_7_2 pixel_7_3 pixel_7_4 pixel_7_5 pixel_7_6 pixel_7_7 0 0.0 -0.335016 -0.043081 0.274072 -0.664478 -0.844129 -0.409724 -0.125023 -0.059078 -0.624009 ... -0.757436 -0.209785 -0.023596 -0.299081 0.086719 0.208293 -0.366771 -1.146647 -0.505670 -0.196008 1 0.0 -0.335016 -1.094937 0.038648 0.268751 -0.138020 -0.409724 -0.125023 -0.059078 -0.624009 ... -0.757436 -0.209785 -0.023596 -0.299081 -1.089383 -0.249010 0.849632 0.548561 -0.505670 -0.196008 2 0.0 -0.335016 -1.094937 -1.844742 0.735366 1.097673 -0.409724 -0.125023 -0.059078 -0.624009 ... 0.259230 -0.209785 -0.023596 -0.299081 -1.089383 -2.078218 -0.164037 1.565686 1.695137 -0.196008 3 0.0 -0.335016 0.377661 0.744919 0.268751 -0.844129 -0.409724 -0.125023 -0.059078 1.879691 ... 1.072563 -0.209785 -0.023596 -0.299081 0.282736 0.208293 0.241430 0.379040 -0.505670 -0.196008 4 0.0 -0.335016 -1.094937 -2.551014 -0.197863 -1.020657 -0.409724 -0.125023 -0.059078 -0.624009 ... -0.757436 -0.209785 -0.023596 -0.299081 -1.089383 -2.306869 0.849632 -0.468564 -0.505670 -0.196008 <p>5 rows \u00d7 64 columns</p> In\u00a0[13]: Copied! <pre>from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\nrandom_tree = DecisionTreeClassifier()\nprint(\"Random tree:\", estimate_accuracy(random_tree, X, y,metric='accuracy'))\n</pre> from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import BaggingClassifier  random_tree = DecisionTreeClassifier() print(\"Random tree:\", estimate_accuracy(random_tree, X, y,metric='accuracy')) <pre>Random tree: 0.782\n</pre> In\u00a0[14]: Copied! <pre># \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0431\u0430\u0437\u043e\u0432\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 100\nbagging_random_trees = BaggingClassifier(random_tree,\n                                         n_estimators=100)\nprint(\"Random Tree Bag:\", estimate_accuracy(bagging_random_trees,\n                                                X, y,metric='accuracy'))\n</pre> # \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0431\u0430\u0437\u043e\u0432\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 100 bagging_random_trees = BaggingClassifier(random_tree,                                          n_estimators=100) print(\"Random Tree Bag:\", estimate_accuracy(bagging_random_trees,                                                 X, y,metric='accuracy')) <pre>Random Tree Bag: 0.91\n</pre> In\u00a0[15]: Copied! <pre>import warnings; warnings.filterwarnings('ignore')\nimport numpy as np\n\n# \u0431\u0430\u0437\u043e\u0432\u044b\u0439 \u043a\u043e\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\ndecision_tree = DecisionTreeClassifier()\nprint(\"Decision Tree:\", estimate_accuracy(decision_tree,X,y,metric='accuracy'))\n\n# \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0447\u0430\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\nmfeats = int(np.sqrt(len(features)))\nprint('bagging',mfeats,'features')\n      \n# \u0411\u044d\u0433\u0433\u0438\u043d\u0433 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\nbagging_random_trees = BaggingClassifier(decision_tree,\n                                         n_estimators=100,\n                                         max_features=mfeats,\n                                         random_state=42)\n\nprint(\"Random Tree Bag:\", estimate_accuracy(bagging_random_trees,X,y,metric='accuracy'))\nprint('')\n</pre> import warnings; warnings.filterwarnings('ignore') import numpy as np  # \u0431\u0430\u0437\u043e\u0432\u044b\u0439 \u043a\u043e\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 decision_tree = DecisionTreeClassifier() print(\"Decision Tree:\", estimate_accuracy(decision_tree,X,y,metric='accuracy'))  # \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0447\u0430\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 mfeats = int(np.sqrt(len(features))) print('bagging',mfeats,'features')        # \u0411\u044d\u0433\u0433\u0438\u043d\u0433 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 bagging_random_trees = BaggingClassifier(decision_tree,                                          n_estimators=100,                                          max_features=mfeats,                                          random_state=42)  print(\"Random Tree Bag:\", estimate_accuracy(bagging_random_trees,X,y,metric='accuracy')) print('') <pre>Decision Tree: 0.786\nbagging 8 features\nRandom Tree Bag: 0.919\n\n</pre> In\u00a0[16]: Copied! <pre>'''\nDecision Tree Arguments\ncriterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0\n'''\n\n'''\nBaggingClassifier Arguments\nbase_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0\n'''\n</pre> ''' Decision Tree Arguments criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0 '''  ''' BaggingClassifier Arguments base_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0 ''' Out[16]: <pre>'\\nBaggingClassifier Arguments\\nbase_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0\\n'</pre> In\u00a0[17]: Copied! <pre># mfeats = int(np.log(len(features)))\nmfeats = int(np.sqrt(len(features)))\nprint('bagging',mfeats,'features')    \n\ndecision_tree = DecisionTreeClassifier()\n# decision_tree = DecisionTreeClassifier(max_features=mfeats)\nprint(\"Decision Tree:\", estimate_accuracy(decision_tree,X,y,metric='accuracy'))\n    \nbagging_random_trees = BaggingClassifier(base_estimator=decision_tree,\n                                         n_estimators=100,\n                                         n_jobs=-1,\n                                         random_state=42)\n    \nprint(\"Random Tree Bag:\", estimate_accuracy(bagging_random_trees,X,y,metric='accuracy'))\n</pre> # mfeats = int(np.log(len(features))) mfeats = int(np.sqrt(len(features))) print('bagging',mfeats,'features')      decision_tree = DecisionTreeClassifier() # decision_tree = DecisionTreeClassifier(max_features=mfeats) print(\"Decision Tree:\", estimate_accuracy(decision_tree,X,y,metric='accuracy'))      bagging_random_trees = BaggingClassifier(base_estimator=decision_tree,                                          n_estimators=100,                                          n_jobs=-1,                                          random_state=42)      print(\"Random Tree Bag:\", estimate_accuracy(bagging_random_trees,X,y,metric='accuracy')) <pre>bagging 8 features\nDecision Tree: 0.782\nRandom Tree Bag: 0.914\n</pre> In\u00a0[18]: Copied! <pre>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = RandomForestClassifier()\ngs = GridSearchCV(model,{'n_estimators':[5,10,15,20,25,30,35,40,45,50]})\ngs.fit(X,y)\nresults = gs.cv_results_\nresults.keys()\n</pre> from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import GridSearchCV  model = RandomForestClassifier() gs = GridSearchCV(model,{'n_estimators':[5,10,15,20,25,30,35,40,45,50]}) gs.fit(X,y) results = gs.cv_results_ results.keys() Out[18]: <pre>dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_n_estimators', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score'])</pre> In\u00a0[19]: Copied! <pre>print('\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f ')\ndict(zip([i for i in range(5,55,5)],results['mean_test_score']))\n</pre> print('\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f ') dict(zip([i for i in range(5,55,5)],results['mean_test_score'])) <pre>\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \n</pre> Out[19]: <pre>{5: 0.8698050139275765,\n 10: 0.9048808418446301,\n 15: 0.9237790157845869,\n 20: 0.9243376663571649,\n 25: 0.9304642525533892,\n 30: 0.9332389353141443,\n 35: 0.9326864747756114,\n 40: 0.9310182606004334,\n 45: 0.9338037759207676,\n 50: 0.9349117920148562}</pre> <ul> <li>\u041f\u0440\u0438 \u043e\u0447\u0435\u043d\u044c \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u043e\u043c \u0447\u0438\u0441\u043b\u0435 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 (5, 10, 15) \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0445\u0443\u0436\u0435, \u0447\u0435\u043c \u043f\u0440\u0438 \u0431\u043e\u043b\u044c\u0448\u0435\u043c \u0447\u0438\u0441\u043b\u0435 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432</li> <li>\u0421 \u0440\u043e\u0441\u0442\u043e\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 \u0432 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u043c \u043b\u0435\u0441\u0435, \u0432 \u043a\u0430\u043a\u043e\u0439-\u0442\u043e \u043c\u043e\u043c\u0435\u043d\u0442 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 \u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u0441\u044f \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0434\u043b\u044f \u0432\u044b\u0441\u043e\u043a\u043e\u0433\u043e \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438, \u0430 \u0437\u0430\u0442\u0435\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u043d\u0435 \u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f.</li> <li>\u041f\u0440\u0438 \u0431\u043e\u043b\u044c\u0448\u043e\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (\u0434\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 - 40-50) \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u0441\u044f \u0445\u0443\u0436\u0435, \u0447\u0435\u043c \u043f\u0440\u0438 \u043c\u0430\u043b\u043e\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (10-15). \u042d\u0442\u043e \u0441\u0432\u044f\u0437\u0430\u043d\u043e \u0441 \u0442\u0435\u043c, \u0447\u0442\u043e \u0447\u0435\u043c \u043c\u0435\u043d\u044c\u0448\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u0442\u0441\u044f \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u0443\u0437\u043b\u0435, \u0442\u0435\u043c \u0431\u043e\u043b\u0435\u0435 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u043c\u0438 \u043f\u043e\u043b\u0443\u0447\u0430\u044e\u0442\u0441\u044f \u0434\u0435\u0440\u0435\u0432\u044c\u044f (\u0432\u0435\u0434\u044c \u0434\u0435\u0440\u0435\u0432\u044c\u044f \u0441\u0438\u043b\u044c\u043d\u043e \u043d\u0435\u0443\u0441\u0442\u043e\u0439\u0447\u0438\u0432\u044b \u043a \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044f\u043c \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435), \u0438 \u0442\u0435\u043c \u043b\u0443\u0447\u0448\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0438\u0445 \u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u044f.</li> </ul> In\u00a0[31]: Copied! <pre>model = RandomForestClassifier()\ngs = GridSearchCV(model,{'max_depth':[5,10,15,20,25,30,35,40,None]})\ngs.fit(X,y)\nresults = gs.cv_results_\n\nprint('\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f ')\nparam_vals = [i for i in range(5,45,5)]\nparam_vals.append(None)\ndict(zip(param_vals,results['mean_test_score'])) \n</pre> model = RandomForestClassifier() gs = GridSearchCV(model,{'max_depth':[5,10,15,20,25,30,35,40,None]}) gs.fit(X,y) results = gs.cv_results_  print('\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f ') param_vals = [i for i in range(5,45,5)] param_vals.append(None) dict(zip(param_vals,results['mean_test_score']))  <pre>\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \n</pre> Out[31]: <pre>{5: 0.9009594552770039,\n 10: 0.9321324667285671,\n 15: 0.9343562364593005,\n 20: 0.937137109254101,\n 25: 0.9421417517796348,\n 30: 0.937137109254101,\n 35: 0.9382497678737233,\n 40: 0.9421417517796348,\n None: 0.9382513153822346}</pre> <ul> <li>\u041f\u0440\u0438 \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0433\u043b\u0443\u0431\u0438\u043d\u0435 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 (5-6) \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0430\u0431\u043e\u0442\u044b \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u043b\u0435\u0441\u0430 \u0437\u0430\u043c\u0435\u0442\u043d\u043e \u0445\u0443\u0436\u0435, \u0447\u0435\u043c \u0431\u0435\u0437 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0439, \u0442.\u043a. \u0434\u0435\u0440\u0435\u0432\u044c\u044f \u043f\u043e\u043b\u0443\u0447\u0430\u044e\u0442\u0441\u044f \u043d\u0435\u0434\u043e\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u043c\u0438.</li> <li>\u0421 \u0440\u043e\u0441\u0442\u043e\u043c \u0433\u043b\u0443\u0431\u0438\u043d\u044b \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u0443\u043b\u0443\u0447\u0448\u0430\u0435\u0442\u0441\u044f, \u0430 \u0437\u0430\u0442\u0435\u043c \u043d\u0435 \u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e, \u0442.\u043a. \u0438\u0437-\u0437\u0430 \u0443\u0441\u0440\u0435\u0434\u043d\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u043e\u0432 \u0438 \u0440\u0430\u0437\u043b\u0438\u0447\u0438\u0439 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 \u0438\u0445 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0432 \u0431\u044d\u0433\u0433\u0438\u043d\u0433\u0435 \u043d\u0435 \u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u043d\u0430 \u0438\u0442\u043e\u0433\u043e\u0432\u043e\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 (\u0432\u0441\u0435 \u0434\u0435\u0440\u0435\u0432\u044c\u044f \u043f\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u044b \u043f\u043e-\u0440\u0430\u0437\u043d\u043e\u043c\u0443, \u0438 \u043f\u0440\u0438 \u0443\u0441\u0440\u0435\u0434\u043d\u0435\u043d\u0438\u0438 \u043e\u043d\u0438 \u043a\u043e\u043c\u043f\u0435\u043d\u0441\u0438\u0440\u0443\u044e\u0442 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0434\u0440\u0443\u0433 \u0434\u0440\u0443\u0433\u0430).</li> </ul>"},{"location":"portfolio/sfml/ml5_%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8.html","title":"\u0410\u043d\u0441\u0430\u043c\u0431\u043b\u0438 \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430\u00b6","text":"<p>Bagging \u2014 \u044d\u0442\u043e \u043f\u0430\u0440\u0430\u043b\u043b\u0435\u043b\u044c\u043d\u044b\u0439 \u0441\u043f\u043e\u0441\u043e\u0431 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u044f. \u041a\u043e\u0440\u043e\u0442\u043a\u043e \u043e \u0441\u043f\u043e\u0441\u043e\u0431\u0435 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f:</p> <ul> <li>\u041e\u0431\u0443\u0447\u0430\u044e\u0449\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 \u0441\u044d\u043c\u043f\u043b\u0438\u0440\u0443\u0435\u0442\u0441\u044f  \u0440\u0430\u0437 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e bootstrap (\u0432\u044b\u0431\u043e\u0440 \u0441 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0435\u043d\u0438\u0435\u043c).</li> <li>\u041d\u0430 \u043a\u0430\u0436\u0434\u043e\u043c \u0441\u044d\u043c\u043f\u043b\u0435 \u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u0430\u044f \u0431\u0430\u0437\u043e\u0432\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c.</li> <li>\u041e\u0442\u0432\u0435\u0442\u044b \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0443\u0441\u0440\u0435\u0434\u043d\u044f\u044e\u0442\u0441\u044f (\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e, \u0441 \u0432\u0435\u0441\u0430\u043c\u0438)</li> </ul>"},{"location":"portfolio/sfml/ml5_%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8.html","title":"\u0417\u0430\u0434\u0430\u0447\u0430 \u0423\u0445\u043e\u0434\u0430 \u0420\u0430\u0431\u043e\u0442\u043d\u0438\u043a\u0430\u00b6","text":"<ul> <li>\u0420\u0430\u0437\u0431\u0435\u0440\u0451\u043c\u0441\u044f \u0441 <code>\u0430\u043d\u0441\u0430\u043c\u0431\u043b\u044f\u043c\u0438</code> \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u0438 \u0441\u043e <code>\u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u043c \u043b\u0435\u0441\u043e\u043c</code></li> <li>\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043e \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a\u0430\u0445 \u043a\u043e\u043c\u043f\u0430\u043d\u0438\u0438, \u0433\u0434\u0435 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f, \u0443\u0448\u0451\u043b \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a \u0438\u043b\u0438 \u043d\u0435\u0442, \u0437\u0430\u0434\u0430\u0447\u0430 \u0431\u0438\u043d\u0430\u0440\u043d\u043e\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438</li> </ul>"},{"location":"portfolio/sfml/ml5_%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8.html","title":"\u041a\u043e\u043d\u0446\u0435\u043f\u0446\u0438\u0438\u00b6","text":"<p>\u041c\u0435\u0442\u043e\u0434 \u0411\u0443\u0442\u0441\u0442\u0440\u044d\u043f (\u0438\u043b\u0438 <code>Bootstrap</code>) \u2014 \u044d\u0442\u043e \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043c\u0435\u0442\u043e\u0434, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0432\u044b\u0431\u043e\u0440\u043e\u0447\u043d\u044b\u0445 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a, \u0442\u0430\u043a\u0438\u0445 \u043a\u0430\u043a \u0441\u0440\u0435\u0434\u043d\u0435\u0435, \u043c\u0435\u0434\u0438\u0430\u043d\u0430, \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0435 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0435 \u0438 \u0434\u0440\u0443\u0433\u0438\u0435, \u043f\u0443\u0442\u0435\u043c \u043f\u043e\u0432\u0442\u043e\u0440\u043d\u043e\u0433\u043e \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0438\u0437 \u0438\u043c\u0435\u044e\u0449\u0438\u0445\u0441\u044f \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0435\u043d\u0438\u0435\u043c. \u0412 \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442\u0435 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u044d\u0442\u043e\u0442 \u043c\u0435\u0442\u043e\u0434 \u0447\u0430\u0441\u0442\u043e \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043d\u0430\u0434\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0438 \u0443\u043c\u0435\u043d\u044c\u0448\u0435\u043d\u0438\u044f \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f.</p> <p>\u0412\u043e\u0442 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u0448\u0430\u0433\u0438 \u043c\u0435\u0442\u043e\u0434\u0430 \u0411\u0443\u0442\u0441\u0442\u0440\u044d\u043f:</p> <ol> <li><p>\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0431\u0443\u0442\u0441\u0442\u0440\u044d\u043f-\u0432\u044b\u0431\u043e\u0440\u043e\u043a: \u0418\u0437 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 (\u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c N) \u043c\u043d\u043e\u0433\u043e\u043a\u0440\u0430\u0442\u043d\u043e (\u043e\u0431\u044b\u0447\u043d\u043e \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u043e\u0442\u0435\u043d \u0438\u043b\u0438 \u0442\u044b\u0441\u044f\u0447 \u0440\u0430\u0437) \u0441\u043e\u0437\u0434\u0430\u044e\u0442\u0441\u044f \u043d\u043e\u0432\u044b\u0435 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0442\u043e\u0433\u043e \u0436\u0435 \u0440\u0430\u0437\u043c\u0435\u0440\u0430 N \u043f\u0443\u0442\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u0432\u044b\u0431\u043e\u0440\u0430 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0441 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0435\u043d\u0438\u0435\u043c. \u042d\u0442\u043e \u0437\u043d\u0430\u0447\u0438\u0442, \u0447\u0442\u043e \u043e\u0434\u0438\u043d \u0438 \u0442\u043e\u0442 \u0436\u0435 \u043e\u0431\u044a\u0435\u043a\u0442 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0432\u044b\u0431\u0440\u0430\u043d \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0437 \u0432 \u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435.</p> </li> <li><p>\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438: \u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u0431\u0443\u0442\u0441\u0442\u0440\u044d\u043f-\u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f.</p> </li> <li><p>\u041e\u0446\u0435\u043d\u043a\u0430 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438: \u041f\u043e\u0441\u043b\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043a\u0430\u0436\u0434\u043e\u0439 \u0431\u0443\u0442\u0441\u0442\u0440\u044d\u043f-\u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0441\u044f \u043e\u0446\u0435\u043d\u043a\u0430 \u0435\u0435 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u043d\u0430 \u043e\u0441\u0442\u0430\u0432\u0448\u0438\u0445\u0441\u044f \u0434\u0430\u043d\u043d\u044b\u0445 (\u0442\u0435\u0445, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u0431\u044b\u043b\u0438 \u0432\u044b\u0431\u0440\u0430\u043d\u044b \u0432 \u0434\u0430\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435). \u042d\u0442\u043e \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u043e \u043e\u0446\u0435\u043d\u043e\u043a \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u0438.</p> </li> <li><p>\u0410\u0433\u0440\u0435\u0433\u0430\u0446\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432: \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u043e\u0446\u0435\u043d\u043a\u0438 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u043c\u043e\u0436\u043d\u043e \u0443\u0441\u0440\u0435\u0434\u043d\u0438\u0442\u044c \u0438\u043b\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0434\u043e\u0432\u0435\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0438\u043d\u0442\u0435\u0440\u0432\u0430\u043b\u043e\u0432, \u0447\u0442\u043e \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043b\u0443\u0447\u0448\u0435 \u043f\u043e\u043d\u044f\u0442\u044c \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0438 \u043d\u0430\u0434\u0435\u0436\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438.</p> </li> </ol> <p>\u041c\u0435\u0442\u043e\u0434 \u0411\u0443\u0442\u0441\u0442\u0440\u044d\u043f \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e \u043f\u043e\u043b\u0435\u0437\u0435\u043d, \u043a\u043e\u0433\u0434\u0430 \u0440\u0430\u0437\u043c\u0435\u0440 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043c\u0430\u043b, \u0442\u0430\u043a \u043a\u0430\u043a \u043e\u043d \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u043e \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438. \u041e\u043d \u0442\u0430\u043a\u0436\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0432 \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u0435\u0432\u044b\u0445 \u043c\u0435\u0442\u043e\u0434\u0430\u0445, \u0442\u0430\u043a\u0438\u0445 \u043a\u0430\u043a Random Forest, \u0433\u0434\u0435 \u043a\u0430\u0436\u0434\u043e\u0435 \u0434\u0435\u0440\u0435\u0432\u043e \u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u043d\u0430 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0431\u0443\u0442\u0441\u0442\u0440\u044d\u043f-\u0432\u044b\u0431\u043e\u0440\u043a\u0430\u0445.</p>"},{"location":"portfolio/sfml/ml5_%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8.html","title":"\u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430\u00b6","text":"<p><code>\u0427\u0430\u0441\u0442\u043e\u0442\u043d\u043e\u0435 \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435</code></p> <ul> <li>\u0417\u0430\u043c\u0435\u043d\u0438\u043c \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u043e\u0442\u0434\u0435\u043b\u0430 <code>dept</code>, \u043a \u043a\u043e\u0442\u043e\u0440\u043e\u043c\u0443 \u043e\u0442\u043d\u043e\u0441\u0438\u043b\u0441\u044f \u0441\u043e\u0442\u0440\u0443\u0434\u043d\u0438\u043a, \u043d\u0430 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043b\u044e\u0434\u0435\u0439 \u0432 \u043e\u0442\u0434\u0435\u043b\u0435</li> </ul> <p><code>\u041e\u0440\u0434\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0433\u0435 \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435</code></p> <ul> <li><p>\u0417\u0430\u0440\u043f\u043b\u0430\u0442\u0443 <code>salary</code> \u2014 \u043d\u0430 \u043e\u0440\u0434\u0438\u043d\u0430\u043b\u044c\u043d\u0443\u044e \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044e, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f <code>salary_ordinals</code></p> </li> <li><p>\u041c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u0443\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0434\u043b\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0433\u043e \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0441 \u043f\u043e\u043c\u043e\u0448\u044c\u044e <code>StandardScaler</code></p> </li> </ul>"},{"location":"portfolio/sfml/ml5_%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8.html","title":"\u041e\u0446\u0435\u043d\u043a\u0430 \u041c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<p>\u041a\u0430\u043a \u0431\u0443\u0434\u0435\u043c \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0442\u044c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0438?</p> <p>\u0412 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u043c \u0431\u0443\u0434\u0435\u043c \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0442\u044c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0438</p> <ul> <li>\u043d\u0430 \u043a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 (cross validation) <code>cross_val_score</code></li> <li>\u043d\u0430 \u043f\u044f\u0442\u0438 \u0444\u043e\u043b\u0434\u0430\u0445 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 f1 score (<code>f1 \u043c\u0435\u0440\u044b</code>).</li> </ul>"},{"location":"portfolio/sfml/ml5_%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8.html#bagging","title":"\u0411\u044d\u0433\u0433\u0438\u043d\u0433 (Bagging)\u00b6","text":"<ul> <li>M\u0435\u0442\u043e\u0434 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f <code>\u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u0438 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432</code>, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u043a\u0430\u0436\u0434\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0441\u0442\u0440\u043e\u0438\u0442\u0441\u044f \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e \u043e\u0442 \u0434\u0440\u0443\u0433\u0438\u0445 <code>\u043d\u0430 \u043f\u043e\u0434\u0432\u044b\u0431\u043e\u0440\u043a\u0430\u0445</code> \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438</li> <li><code>\u0418\u0442\u043e\u0433\u043e\u0432\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c</code> \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442 \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u043f\u043e\u0441\u0440\u0435\u0434\u0441\u0442\u0432\u043e\u043c <code>\u0433\u043e\u043b\u043e\u0441\u043e\u0432\u0430\u043d\u0438\u044f</code> \u0441\u0440\u0435\u0434\u0438 \u0432\u0441\u0435\u0445 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 (\u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442\u0441\u044f \u0441\u0430\u043c\u044b\u0439 \u0447\u0430\u0441\u0442\u044b\u0439 \u043e\u0442\u0432\u0435\u0442)</li> <li>\u041e\u0431\u0435\u0440\u0442\u043e\u0432\u0430\u043d\u0438\u0435 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 (<code>BaggingClassifier(clf)</code>)</li> </ul>"},{"location":"portfolio/sfml/ml5_%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8.html","title":"\u041f\u0440\u0438\u0435\u043c\u0443\u0449\u0435\u0441\u0442\u0432\u043e \u0411\u044d\u0433\u0433\u0438\u043d\u0433\u0430\u00b6","text":"<p>\u0421\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430 \u0434\u0435\u0440\u0435\u0432\u0430 \u0441\u0435\u0440\u044c\u0451\u0437\u043d\u043e \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438</p> <ul> <li>\u042d\u0442\u043e \u0437\u043d\u0430\u0447\u0438\u0442, \u0447\u0442\u043e \u0435\u0441\u043b\u0438 \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u0438\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443, \u0442\u043e \u0434\u0435\u0440\u0435\u0432\u043e \u0441\u0438\u043b\u044c\u043d\u043e \u0438\u0437\u043c\u0435\u043d\u0438\u0442\u0441\u044f</li> <li>K\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u044f \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u0433\u043e\u043b\u043e\u0441\u043e\u0432\u0430\u043d\u0438\u044f \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u043a\u043e\u0433\u0434\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b</li> <li>\u0423\u0432\u0435\u043b\u0438\u0447\u0438\u0442\u044c \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u044b\u0445 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 \u043c\u043e\u0436\u043d\u043e, \u0443\u043a\u0430\u0437\u0430\u0432 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b <code>max_features</code> \u0438 <code>max_depth</code></li> </ul>"},{"location":"portfolio/sfml/ml5_%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8.html","title":"\u0421\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u041b\u0435\u0441\u00b6","text":"<p>\u0418\u043c\u0435\u043d\u043d\u043e \u0442\u0430\u043a \u0432\u043d\u0443\u0442\u0440\u0438 \u0438 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0442\u0430\u043a \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u043c\u044b\u0439 <code>\u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441</code></p> <p>O\u043d \u043e\u0431\u0443\u0447\u0430\u0435\u0442 \u043d\u0430\u0431\u043e\u0440 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 <code>n_esimators</code>, \u043a\u0430\u0436\u0434\u043e\u0435 \u0438\u0437 \u043a\u043e\u0442\u043e\u0440\u044b\u0445:</p> <ul> <li>\u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u043d\u0430 \u043f\u043e\u0434\u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (<code>\u043c\u0435\u0442\u043e\u0434 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0445 \u043f\u043e\u0434\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432</code>)</li> <li>\u0438 \u043d\u0430 \u043f\u043e\u0434\u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 (<code>\u0431\u044d\u0433\u0433\u0438\u043d\u0433</code>)</li> <li><code>\u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441</code> \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u043c \u043f\u043e \u0434\u0432\u0443\u043c \u044d\u0442\u0438\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c</li> <li><code>\u043e\u0442\u0432\u0435\u0442\u044b \u0430\u0433\u0433\u0440\u0435\u0433\u0438\u0440\u0443\u044e\u0442\u0441\u044f</code> \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 <code>\u0433\u043e\u043b\u043e\u0441\u043e\u0432\u0430\u043d\u0438\u044f</code></li> </ul>"},{"location":"portfolio/sfml/ml5_%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8.html#oob","title":"\u041c\u0435\u0442\u0440\u0438\u043a\u0430 OOB\u00b6","text":"<p>\u0415\u0449\u0451 \u043e\u0434\u043d\u043e \u043f\u0440\u0435\u0438\u043c\u0443\u0449\u0435\u0441\u0442\u0432\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0431\u0435\u0433\u0433\u0438\u043d\u0433\u0430 \u0434\u043b\u044f \u0430\u0433\u0433\u0440\u0435\u0433\u0430\u0446\u0438\u0438 \u043c\u043e\u0434\u0435\u043b\u0435\u0439</p> <p>\u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043e\u0446\u0435\u043d\u043a\u0438 \u0440\u0430\u0431\u043e\u0442\u044b \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430 \u0431\u0435\u0437 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u044f <code>\u043a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438</code>\u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 <code>out-of-bag</code> \u043c\u0435\u0442\u0440\u0438\u043a\u0438</p> <p>\u042d\u0442\u043e \u043c\u0435\u0442\u043e\u0434 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u043b\u044c\u043d\u043e\u0439 \u043e\u0446\u0435\u043d\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0431\u0435\u0433\u0433\u0438\u043d\u0433\u0430 \u0414\u043b\u044f \u043f\u043e\u0434\u0441\u0447\u0451\u0442\u0430 \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u0443\u043a\u0430\u0437\u0430\u0442\u044c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 <code>oob_score = True</code></p>"},{"location":"portfolio/sfml/ml5_%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8.html","title":"\u0411\u044d\u0433\u0433\u0438\u043d\u0433 \u041b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0420\u0435\u0433\u0440\u0435\u0441\u0438\u0438\u00b6","text":"<p>\u041c\u0435\u0442\u043e\u0434 <code>\u0431\u044d\u0433\u0433\u0438\u043d\u0433\u0430</code> \u043c\u043e\u0436\u043d\u043e \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0442\u044c \u043a \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u043b\u044c\u043d\u044b\u043c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430\u043c, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043a <code>\u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438</code></p>"},{"location":"portfolio/sfml/ml5_%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8.html","title":"\u0417\u0430\u0434\u0430\u0447\u0430 \u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439\u00b6","text":""},{"location":"portfolio/sfml/ml5_%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8.html","title":"\u0434\u0430\u0442\u0430\u0441\u0435\u0442\u00b6","text":"<ul> <li>\u0417\u0430\u0433\u0440\u0443\u0437\u0438\u0442\u0435 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 digits \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u0438 <code>load_digits</code> \u0438\u0437 sklearn.datasets</li> <li>\u041d\u0430\u043c \u043f\u0440\u0435\u0434\u0441\u0442\u043e\u0438\u0442 \u0440\u0435\u0448\u0430\u0442\u044c \u0437\u0430\u0434\u0430\u0447\u0443 <code>\u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439</code> \u0441 \u0446\u0438\u0444\u0440\u0430\u043c\u0438 \u043f\u043e \u0447\u0438\u0441\u043b\u0435\u043d\u043d\u044b\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c</li> </ul>"},{"location":"portfolio/sfml/ml5_%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8.html","title":"\u043e\u0446\u0435\u043d\u043a\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430\u00b6","text":"<ul> <li>\u0414\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c <code>cross_val_score</code> \u0438\u0437 sklearn.model_selection \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u043c . \u042d\u0442\u0430 \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u0442 k-fold cross validation c  \u0440\u0430\u0432\u043d\u044b\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430 .</li> <li>\u041f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u0442\u0441\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c k=10, \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u043e\u0446\u0435\u043d\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u0438\u043c\u0435\u043b\u0438 \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u0440\u0430\u0437\u0431\u0440\u043e\u0441, \u0438 \u0431\u044b\u043b\u043e \u043f\u0440\u043e\u0449\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u043e\u0442\u0432\u0435\u0442\u044b. \u041d\u0430 \u043f\u0440\u0430\u043a\u0442\u0438\u043a\u0435 \u0436\u0435 \u0447\u0430\u0441\u0442\u043e \u0445\u0432\u0430\u0442\u0430\u0435\u0442 \u0438 k=5. \u0424\u0443\u043d\u043a\u0446\u0438\u044f cross_val_score \u0431\u0443\u0434\u0435\u0442 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0442\u044c numpy.ndarray, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0431\u0443\u0434\u0435\u0442  \u0447\u0438\u0441\u0435\u043b \u2014 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u0438\u0437  \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u043e\u0432 k-fold cross validation.</li> <li>\u0414\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0441\u0440\u0435\u0434\u043d\u0435\u0433\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f (\u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u0438 \u0431\u0443\u0434\u0435\u0442 \u043e\u0446\u0435\u043d\u043a\u043e\u0439 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u0440\u0430\u0431\u043e\u0442\u044b) \u0432\u044b\u0437\u043e\u0432\u0438\u0442\u0435 \u043c\u0435\u0442\u043e\u0434 .mean() \u0443 \u043c\u0430\u0441\u0441\u0438\u0432\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 <code>cross_val_score</code></li> </ul>"},{"location":"portfolio/sfml/ml5_%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8.html#571","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 5.7.1\u00b6","text":"<p>\u0421\u043e\u0437\u0434\u0430\u0439\u0442\u0435 <code>DecisionTreeClassifier</code> \u0441 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u043c\u0438 \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e \u0438 \u0438\u0437\u043c\u0435\u0440\u044c\u0442\u0435 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0435\u0433\u043e \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e cross_val_score</p>"},{"location":"portfolio/sfml/ml5_%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8.html#572","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 5.7.2\u00b6","text":"<ul> <li>\u0422\u0435\u043f\u0435\u0440\u044c \u0434\u0430\u0432\u0430\u0439\u0442\u0435 \u043e\u0431\u0443\u0447\u0438\u043c <code>BaggingClassifier</code> \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 <code>DecisionTreeClassifier</code></li> <li>\u0418\u0437 sklearn.ensemble \u0438\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0439\u0442\u0435 <code>BaggingClassifier</code>, \u0432\u0441\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0437\u0430\u0434\u0430\u0439\u0442\u0435 \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e</li> <li>\u041d\u0443\u0436\u043d\u043e \u0438\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0431\u0430\u0437\u043e\u0432\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439, \u0437\u0430\u0434\u0430\u0432 \u0435\u0433\u043e \u0440\u0430\u0432\u043d\u044b\u043c 100</li> <li>\u041f\u043e\u0434\u0443\u043c\u0430\u0439\u0442\u0435, \u043a\u0430\u043a\u0438\u0435 \u0432\u044b\u0432\u043e\u0434\u044b \u043c\u043e\u0436\u043d\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0438\u0437 \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044f \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043e\u0434\u0438\u043d\u043e\u0447\u043d\u043e\u0433\u043e \u0434\u0435\u0440\u0435\u0432\u0430 \u0438 \u0431\u0435\u0433\u0433\u0438\u043d\u0433\u0430 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432?</li> </ul>"},{"location":"portfolio/sfml/ml5_%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8.html#573","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 5.7.3\u00b6","text":"<p>\u0422\u0435\u043f\u0435\u0440\u044c \u0438\u0437\u0443\u0447\u0438\u0442\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b <code>BaggingClassifier</code> \u0438 \u0432\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u0438\u0445 \u0442\u0430\u043a\u0438\u043c\u0438</p> <ul> <li>\u0427\u0442\u043e\u0431\u044b \u043a\u0430\u0436\u0434\u044b\u0439 \u0431\u0430\u0437\u043e\u0432\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u043e\u0431\u0443\u0447\u0430\u043b\u0441\u044f \u043d\u0435 \u043d\u0430 \u0432\u0441\u0435\u0445 d \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445, \u0430 \u043d\u0430 sqrt(d) \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445</li> </ul> <p>\u041a\u043e\u0440\u0435\u043d\u044c \u0438\u0437 \u0447\u0438\u0441\u043b\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 - \u0447\u0430\u0441\u0442\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0430\u044f \u044d\u0432\u0440\u0438\u0441\u0442\u0438\u043a\u0430 \u0432 \u0437\u0430\u0434\u0430\u0447\u0430\u0445 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438</p> <ul> <li>\u0412 \u0437\u0430\u0434\u0430\u0447\u0430\u0445 <code>\u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438</code> \u0436\u0435 \u0447\u0430\u0441\u0442\u043e \u0431\u0435\u0440\u0443\u0442 \u0447\u0438\u0441\u043b\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0435 \u043d\u0430 \u0442\u0440\u0438, log(d) \u0442\u043e\u0436\u0435 \u0438\u043c\u0435\u0435\u0442 \u043c\u0435\u0441\u0442\u043e \u0431\u044b\u0442\u044c</li> </ul> <p>\u041d\u043e \u0432 \u043e\u0431\u0449\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u043d\u0438\u0447\u0442\u043e \u043d\u0435 \u043c\u0435\u0448\u0430\u0435\u0442 \u0432\u0430\u043c \u0432\u044b\u0431\u0438\u0440\u0430\u0442\u044c \u043b\u044e\u0431\u043e\u0435 \u0434\u0440\u0443\u0433\u043e\u0435 \u0447\u0438\u0441\u043b\u043e \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u0434\u043e\u0431\u0438\u0432\u0430\u044f\u0441\u044c \u043b\u0443\u0447\u0448\u0435\u0433\u043e \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043d\u0430 \u043a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438</p>"},{"location":"portfolio/sfml/ml5_%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8.html#574","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 5.7.4\u00b6","text":"<ul> <li><p>\u0412 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u043c \u043f\u0443\u043d\u043a\u0442\u0435 \u043c\u044b \u0432\u044b\u0431\u0438\u0440\u0430\u043b\u0438 \u043f\u043e\u0434\u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u043e \u043e\u0434\u0438\u043d \u0440\u0430\u0437 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043e\u0447\u0435\u0440\u0435\u0434\u043d\u043e\u0433\u043e \u0434\u0435\u0440\u0435\u0432\u0430</p> </li> <li><p>\u0421\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043d\u0430\u0448\u0438\u043c \u0448\u0430\u0433\u043e\u043c \u0431\u0443\u0434\u0435\u0442 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u0431\u0435\u0433\u0433\u0438\u043d\u0433\u0430 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u044b\u0431\u0438\u0440\u0430\u044e\u0442 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0435 \u043f\u043e\u0434\u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u0432\u0435\u0440\u0448\u0438\u043d\u044b \u0434\u0435\u0440\u0435\u0432\u0430</p> </li> <li><p>\u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043d\u0430\u043c \u043f\u043e\u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u043f\u0435\u0440\u0435\u043d\u0435\u0441\u0442\u0438, \u043e\u0442\u0432\u0435\u0447\u0430\u044e\u0449\u0438\u0439 \u0437\u0430 \u044d\u0442\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u0438\u0437 <code>BaggingClassifier</code> \u0432 <code>DecisionTreeClassifier</code></p> </li> <li><p>\u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0432\u0430\u043c \u0438\u0437 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u043d\u0443\u0436\u043d\u043e \u0432\u044b\u044f\u0441\u043d\u0438\u0442\u044c, \u043a\u0430\u043a\u043e\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 <code>DecisionTreeClassifier</code> \u0437\u0430 \u044d\u0442\u043e \u043e\u0442\u0432\u0435\u0447\u0430\u0435\u0442</p> </li> <li><p>\u041f\u043e-\u043f\u0440\u0435\u0436\u043d\u0435\u043c\u0443 \u0441\u044d\u043c\u043f\u043b\u0438\u0440\u0443\u0435\u043c <code>sqrt(d)</code> \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432</p> </li> </ul>"},{"location":"portfolio/sfml/ml5_%D0%90%D0%BD%D1%81%D0%B0%D0%BC%D0%B1%D0%BB%D0%B8.html#575","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 5.7.5\u00b6","text":"<ul> <li>\u041f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0439 \u0432 \u0437\u0430\u0434\u0430\u043d\u0438\u0438 4 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 - \u0431\u044d\u0433\u0433\u0438\u043d\u0433 \u043d\u0430 \u0440\u0430\u043d\u0434\u043e\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0434\u0435\u0440\u0435\u0432\u044c\u044f\u0445 (\u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043f\u0440\u0438 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0438 \u043a\u0430\u0436\u0434\u043e\u0439 \u0432\u0435\u0440\u0448\u0438\u043d\u044b \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u0442\u0441\u044f \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0435 \u043f\u043e\u0434\u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0438 \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u0438\u0449\u0435\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u043e \u043d\u0438\u043c).</li> <li>\u042d\u0442\u043e \u0432 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0443 Random Forest, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043f\u043e\u0447\u0435\u043c\u0443 \u0431\u044b \u043d\u0435 \u0441\u0440\u0430\u0432\u043d\u0438\u0442\u044c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0430\u0431\u043e\u0442\u044b \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430 \u0441 RandomForestClassifier \u0438\u0437 sklearn.ensemble. -\u0421\u0434\u0435\u043b\u0430\u0439\u0442\u0435 \u044d\u0442\u043e, \u0430 \u0437\u0430\u0442\u0435\u043c \u0438\u0437\u0443\u0447\u0438\u0442\u0435, \u043a\u0430\u043a \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u043d\u0430 \u0434\u0430\u043d\u043d\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432, \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c\u044b\u0445 \u043f\u0440\u0438 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0438 \u043a\u0430\u0436\u0434\u043e\u0439 \u0432\u0435\u0440\u0448\u0438\u043d\u044b \u0434\u0435\u0440\u0435\u0432\u0430, \u0430 \u0442\u0430\u043a\u0436\u0435 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0439 \u043d\u0430 \u0433\u043b\u0443\u0431\u0438\u043d\u0443 \u0434\u0435\u0440\u0435\u0432\u0430.</li> <li>\u0414\u043b\u044f \u043d\u0430\u0433\u043b\u044f\u0434\u043d\u043e\u0441\u0442\u0438 \u043b\u0443\u0447\u0448\u0435 \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0433\u0440\u0430\u0444\u0438\u043a\u0438 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043e\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432, \u043d\u043e \u0434\u043b\u044f \u0441\u0434\u0430\u0447\u0438 \u0437\u0430\u0434\u0430\u043d\u0438\u044f \u044d\u0442\u043e \u0434\u0435\u043b\u0430\u0442\u044c \u043d\u0435 \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e.</li> </ul>"},{"location":"portfolio/sfml/ml6_%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3.html","title":"ml6 \u0411\u0443\u0441\u0442\u0438\u043d\u0433","text":"In\u00a0[1]: Copied! <pre>import warnings; warnings.filterwarnings('ignore')\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm import tqdm \nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set(style='whitegrid')\n\nboston_data = datasets.load_boston()\n\ndata = load_boston()\nfeatures, target = data.data, data.target\n\n# \u0420\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u043c \u043d\u0430\u0448\u0443 \u0432\u044b\u0431\u043e\u0440\u043a\u0443\nX_train, X_test, y_train, y_test = train_test_split(features,target,\n                                                    train_size=0.75,\n                                                    shuffle=False)\n</pre> import warnings; warnings.filterwarnings('ignore') from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error from tqdm import tqdm  from sklearn import datasets import matplotlib.pyplot as plt import seaborn as sns; sns.set(style='whitegrid')  boston_data = datasets.load_boston()  data = load_boston() features, target = data.data, data.target  # \u0420\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u043c \u043d\u0430\u0448\u0443 \u0432\u044b\u0431\u043e\u0440\u043a\u0443 X_train, X_test, y_train, y_test = train_test_split(features,target,                                                     train_size=0.75,                                                     shuffle=False) In\u00a0[2]: Copied! <pre># \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u0442\u044c \u043f\u0440\u043e\u0433\u043d\u043e\u0437 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u043e\u0439 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442 \u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u0438 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 \u043d\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 X `gbm_predict`\nimport tqdm.notebook as tqdm\n\ndef gbm_predict(features,algos,coeffs): \n    \n    lst_sum = []\n    \n    # for each row\n    for feature in features:\n        \n        tlst_sum = []\n        # for all models, predict + add component\n        for algo,coeff in zip(lst_algo,lst_coeff):\n            ym_pred = coeff*algo.predict([feature])[0]\n            tlst_sum.append(ym_pred)\n            \n        value = sum(tlst_sum)\n        lst_sum.append(value)\n            \n    return lst_sum\n\ndef Loss(y, z):\n    return (z - y)\n</pre> # \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u0442\u044c \u043f\u0440\u043e\u0433\u043d\u043e\u0437 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u043e\u0439 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442 \u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u0438 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 \u043d\u0430 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 X `gbm_predict` import tqdm.notebook as tqdm  def gbm_predict(features,algos,coeffs):           lst_sum = []          # for each row     for feature in features:                  tlst_sum = []         # for all models, predict + add component         for algo,coeff in zip(lst_algo,lst_coeff):             ym_pred = coeff*algo.predict([feature])[0]             tlst_sum.append(ym_pred)                      value = sum(tlst_sum)         lst_sum.append(value)                  return lst_sum  def Loss(y, z):     return (z - y) In\u00a0[3]: Copied! <pre>from sklearn import datasets\nfrom tqdm import tqdm\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor as DTR\nfrom sklearn import metrics\n\nboston_data = datasets.load_boston()\nX = boston_data.data\ny = boston_data.target\n\n# \u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u0443\u044e \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    train_size = 0.75,\n                                                    shuffle=True)\n\nprint('training size: ',X_train.shape)\nprint('test size:',X_test.shape)\n</pre> from sklearn import datasets from tqdm import tqdm import numpy as np from sklearn.tree import DecisionTreeRegressor as DTR from sklearn import metrics  boston_data = datasets.load_boston() X = boston_data.data y = boston_data.target  # \u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u0443\u044e \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443 from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y,                                                     train_size = 0.75,                                                     shuffle=True)  print('training size: ',X_train.shape) print('test size:',X_test.shape) <pre>training size:  (379, 13)\ntest size: (127, 13)\n</pre> In\u00a0[4]: Copied! <pre>'''\n\n\u0420\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0411\u0443\u0441\u0442\u0438\u043d\u0433 (\u041a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u044b \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b\u0435)\n\n'''\n\nlst_algo = []; lst_coeff = []\ny_curr = np.array(y_train)   # \u043d\u0430\u0447\u0430\u043b\u044c\u043d\u0430\u044f \u0440\u0430\u0437\u043d\u0438\u0446\u0430 (y_train - 0)\n\n# lst_algo \u0432\u0441\u0435 \u0432\u0440\u0435\u043c\u044f \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f\n\niters = 20\nfor i in tqdm(range(iters)):\n    \n    # Shallow Decision Tree Regressor\n    regressor = DTR(max_depth=5,\n                    random_state=139)\n    regressor.fit(X_train, y_curr)\n    y_pred = gbm_predict(X_train,lst_algo,lst_coeff)\n    y_curr = Loss(y_pred, y_train)  # \u0440\u0430\u0437\u043d\u0438\u0446\u0430 (y_train - y_pred)\n    \n    # \u043a\u0430\u0436\u0434\u0443\u044e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u044e \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \n    lst_algo.append(regressor)\n    lst_coeff.append(0.9)      # 0.9 \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u0430\n</pre> '''  \u0420\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0411\u0443\u0441\u0442\u0438\u043d\u0433 (\u041a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u044b \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b\u0435)  '''  lst_algo = []; lst_coeff = [] y_curr = np.array(y_train)   # \u043d\u0430\u0447\u0430\u043b\u044c\u043d\u0430\u044f \u0440\u0430\u0437\u043d\u0438\u0446\u0430 (y_train - 0)  # lst_algo \u0432\u0441\u0435 \u0432\u0440\u0435\u043c\u044f \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f  iters = 20 for i in tqdm(range(iters)):          # Shallow Decision Tree Regressor     regressor = DTR(max_depth=5,                     random_state=139)     regressor.fit(X_train, y_curr)     y_pred = gbm_predict(X_train,lst_algo,lst_coeff)     y_curr = Loss(y_pred, y_train)  # \u0440\u0430\u0437\u043d\u0438\u0446\u0430 (y_train - y_pred)          # \u043a\u0430\u0436\u0434\u0443\u044e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u044e \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c      lst_algo.append(regressor)     lst_coeff.append(0.9)      # 0.9 \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u0430 <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:01&lt;00:00, 10.39it/s]\n</pre> In\u00a0[5]: Copied! <pre># predict for X_test (all rows)\ny_pred = gbm_predict(X_test,lst_algo,lst_coeff)\nmse = metrics.mean_squared_error(y_test,y_pred)\nrmse = np.sqrt(mse)\nprint(f'rmse: {rmse:.3f} mse: {mse:.3f}')\n</pre> # predict for X_test (all rows) y_pred = gbm_predict(X_test,lst_algo,lst_coeff) mse = metrics.mean_squared_error(y_test,y_pred) rmse = np.sqrt(mse) print(f'rmse: {rmse:.3f} mse: {mse:.3f}') <pre>rmse: 5.146 mse: 26.481\n</pre> In\u00a0[6]: Copied! <pre>'''\n\n\u0420\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0411\u0443\u0441\u0442\u0438\u043d\u0433\n\u041a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u044b \u043c\u0435\u043d\u044f\u044e\u0442\u0441\u044f \u043f\u043e \u0444\u043e\u0440\u043c\u0443\u043b\u0435\n\n'''\n\nlst_algo = []\nlst_coeff = []\ny_curr = np.array(y_train)\nprint('training...')\nfor iter in tqdm(range(50)):\n    \n    regressor = DTR(max_depth=5,random_state=139)\n    regressor.fit(X_train, y_curr)\n    lst_algo.append(regressor)\n    lst_coeff.append(0.9/(1.0+iter))    \n    y_pred = gbm_predict(X_train,lst_algo,lst_coeff)\n    y_curr = Loss(y_pred, y_train)\n  \nprint('prediction...')\ny_pred = gbm_predict(X_test,lst_algo,lst_coeff)\nmse = metrics.mean_squared_error(y_test,y_pred)\nrmse = np.sqrt(mse)\nprint(f'rmse: {rmse:.3f},mse: {mse:.3f}')\n</pre> '''  \u0420\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0411\u0443\u0441\u0442\u0438\u043d\u0433 \u041a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u044b \u043c\u0435\u043d\u044f\u044e\u0442\u0441\u044f \u043f\u043e \u0444\u043e\u0440\u043c\u0443\u043b\u0435  '''  lst_algo = [] lst_coeff = [] y_curr = np.array(y_train) print('training...') for iter in tqdm(range(50)):          regressor = DTR(max_depth=5,random_state=139)     regressor.fit(X_train, y_curr)     lst_algo.append(regressor)     lst_coeff.append(0.9/(1.0+iter))         y_pred = gbm_predict(X_train,lst_algo,lst_coeff)     y_curr = Loss(y_pred, y_train)    print('prediction...') y_pred = gbm_predict(X_test,lst_algo,lst_coeff) mse = metrics.mean_squared_error(y_test,y_pred) rmse = np.sqrt(mse) print(f'rmse: {rmse:.3f},mse: {mse:.3f}') <pre>training...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [00:13&lt;00:00,  3.72it/s]</pre> <pre>prediction...\nrmse: 3.058,mse: 9.354\n</pre> <pre>\n</pre> In\u00a0[7]: Copied! <pre>'''\n\u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0411\u0443\u0441\u0442\u0438\u043d\u0433 \u0438\u0437 sklearn\n'''\n\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\n\n# \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b\n# loss='squared_error', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', \n# min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, \n# init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, \n# validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0\n\nn_estimators = [i for i in range(1,1000,50)]\n\nlst_mse_train = []; lst_mse_test = []\nfor estimators in tqdm(n_estimators):\n    model = GBR(n_estimators=estimators)\n    model.fit(X_train,y_train)\n    ym_train = model.predict(X_train)\n    ym_test = model.predict(X_test)\n    mse_train = metrics.mean_squared_error(y_train,ym_train)\n    mse_test = metrics.mean_squared_error(y_test,ym_test)\n    lst_mse_train.append(mse_train); lst_mse_test.append(mse_test)\n</pre> ''' \u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0411\u0443\u0441\u0442\u0438\u043d\u0433 \u0438\u0437 sklearn '''  from sklearn.ensemble import GradientBoostingRegressor as GBR  # \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b # loss='squared_error', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse',  # min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0,  # init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False,  # validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0  n_estimators = [i for i in range(1,1000,50)]  lst_mse_train = []; lst_mse_test = [] for estimators in tqdm(n_estimators):     model = GBR(n_estimators=estimators)     model.fit(X_train,y_train)     ym_train = model.predict(X_train)     ym_test = model.predict(X_test)     mse_train = metrics.mean_squared_error(y_train,ym_train)     mse_test = metrics.mean_squared_error(y_test,ym_test)     lst_mse_train.append(mse_train); lst_mse_test.append(mse_test) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:05&lt;00:00,  3.49it/s]\n</pre> In\u00a0[8]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(lst_mse_train,label='train')\nax.plot(lst_mse_test,label='valid')\nax.legend()\nax.set_xlabel('\u0413\u043b\u0443\u0431\u0438\u043d\u0430 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432',size=10)\nax.set_ylabel('MSE',size=10)\nplt.show()\n</pre> fig, ax = plt.subplots() ax.plot(lst_mse_train,label='train') ax.plot(lst_mse_test,label='valid') ax.legend() ax.set_xlabel('\u0413\u043b\u0443\u0431\u0438\u043d\u0430 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432',size=10) ax.set_ylabel('MSE',size=10) plt.show() In\u00a0[9]: Copied! <pre>n_depth = [i for i in range(1,50,1)]\n\nlst_mse_train = []; lst_mse_test = []\nfor depth in tqdm(n_depth):\n\n    model = GBR(max_depth=depth)\n    model.fit(X_train,y_train)\n    ym_train = model.predict(X_train)\n    ym_test = model.predict(X_test)\n    mse_train = metrics.mean_squared_error(y_train,ym_train)\n    mse_test = metrics.mean_squared_error(y_test,ym_test)\n    lst_mse_train.append(mse_train)\n    lst_mse_test.append(mse_test)\n\nfig, ax = plt.subplots()\nax.plot(lst_mse_train,label='train')\nax.plot(lst_mse_test,label='valid')\nax.legend()\nax.set_xlabel('\u0413\u043b\u0443\u0431\u0438\u043d\u0430 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432',size=10)\nax.set_ylabel('MSE',size=10)\nplt.show()\n</pre> n_depth = [i for i in range(1,50,1)]  lst_mse_train = []; lst_mse_test = [] for depth in tqdm(n_depth):      model = GBR(max_depth=depth)     model.fit(X_train,y_train)     ym_train = model.predict(X_train)     ym_test = model.predict(X_test)     mse_train = metrics.mean_squared_error(y_train,ym_train)     mse_test = metrics.mean_squared_error(y_test,ym_test)     lst_mse_train.append(mse_train)     lst_mse_test.append(mse_test)  fig, ax = plt.subplots() ax.plot(lst_mse_train,label='train') ax.plot(lst_mse_test,label='valid') ax.legend() ax.set_xlabel('\u0413\u043b\u0443\u0431\u0438\u043d\u0430 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432',size=10) ax.set_ylabel('MSE',size=10) plt.show() <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49/49 [00:08&lt;00:00,  5.76it/s]\n</pre> In\u00a0[10]: Copied! <pre>n_depth = [i for i in range(1,50,1)]\n\nlst_mse_train = []; lst_mse_test = []\nfor depth in tqdm(n_depth):\n\n    model = GBR(max_depth=depth)\n    model.fit(X_train,y_train)\n    ym_train = model.predict(X_train)\n    ym_test = model.predict(X_test)\n    mse_train = metrics.mean_squared_error(y_train,ym_train)\n    mse_test = metrics.mean_squared_error(y_test,ym_test)\n    lst_mse_train.append(mse_train)\n    lst_mse_test.append(mse_test)\n\nfig, ax = plt.subplots()\nax.plot(lst_mse_train,label='train')\nax.plot(lst_mse_test,label='valid')\nax.legend()\nax.set_xlabel('\u0413\u043b\u0443\u0431\u0438\u043d\u0430 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432',size=10)\nax.set_ylabel('MSE',size=10)\nplt.show()\n</pre> n_depth = [i for i in range(1,50,1)]  lst_mse_train = []; lst_mse_test = [] for depth in tqdm(n_depth):      model = GBR(max_depth=depth)     model.fit(X_train,y_train)     ym_train = model.predict(X_train)     ym_test = model.predict(X_test)     mse_train = metrics.mean_squared_error(y_train,ym_train)     mse_test = metrics.mean_squared_error(y_test,ym_test)     lst_mse_train.append(mse_train)     lst_mse_test.append(mse_test)  fig, ax = plt.subplots() ax.plot(lst_mse_train,label='train') ax.plot(lst_mse_test,label='valid') ax.legend() ax.set_xlabel('\u0413\u043b\u0443\u0431\u0438\u043d\u0430 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432',size=10) ax.set_ylabel('MSE',size=10) plt.show() <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49/49 [00:08&lt;00:00,  5.89it/s]\n</pre> In\u00a0[11]: Copied! <pre>from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression() #\u041e\u0431\u043e\u0437\u043d\u0430\u0447\u0430\u0435\u043c, \u0447\u0442\u043e \u043d\u0430\u0448\u0430 \u043c\u043e\u0434\u0435\u043b\u044c - \u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f=\nmodel.fit(X_train,y_train) #\u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\nym_test = model.predict(X_test)\nmse = metrics.mean_squared_error(y_test,ym_test)\nrmse = np.sqrt(mse)\nprint(f'rmse: {rmse:.3f},mse: {mse:.3f}')\n</pre> from sklearn.linear_model import LinearRegression  model = LinearRegression() #\u041e\u0431\u043e\u0437\u043d\u0430\u0447\u0430\u0435\u043c, \u0447\u0442\u043e \u043d\u0430\u0448\u0430 \u043c\u043e\u0434\u0435\u043b\u044c - \u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f= model.fit(X_train,y_train) #\u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 ym_test = model.predict(X_test) mse = metrics.mean_squared_error(y_test,ym_test) rmse = np.sqrt(mse) print(f'rmse: {rmse:.3f},mse: {mse:.3f}') <pre>rmse: 4.822,mse: 23.253\n</pre>"},{"location":"portfolio/sfml/ml6_%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3.html","title":"\u0411\u0443\u0441\u0442\u0438\u043d\u0433 \u041f\u0440\u0430\u043a\u0442\u0438\u043a\u0430\u00b6","text":""},{"location":"portfolio/sfml/ml6_%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3.html","title":"\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0421\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0438 \u0414\u043e\u043c\u0430\u00b6","text":""},{"location":"portfolio/sfml/ml6_%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3.html","title":"\u0414\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 \u0411\u043e\u0441\u0442\u043e\u043d\u0430\u00b6","text":"<p>\u0412 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0445 \u0437\u0430\u0434\u0430\u043d\u0438\u044f\u0445 \u0431\u0443\u0434\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0434\u0430\u0442\u0430\u0441\u0435\u0442 boston \u0438\u0437 <code>sklearn.datasets</code></p> <p>\u041e\u0441\u0442\u0430\u0432\u044c\u0442\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0435 25% \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0434\u043b\u044f \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044f \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 (\u0442\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430)</p> <ul> <li>\u0420\u0430\u0437\u0434\u0435\u043b\u0438\u0432 X \u0438 y \u043d\u0430 X_train, y_train \u0438 X_test, y_test \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e train_test_split(X, y, train_size = 0.75, shuffle=False)</li> </ul>"},{"location":"portfolio/sfml/ml6_%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3.html","title":"\u0426\u0435\u043b\u044c \u0417\u0430\u0434\u0430\u043d\u0438\u044f\u00b6","text":"<ul> <li>\u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u043e\u0441\u0442\u043e\u0439 \u0432\u0430\u0440\u0438\u0430\u043d\u0442 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u0430</li> <li>\u043d\u0430\u0434 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u043e\u043d\u044b\u043c\u0438 \u0434\u0435\u0440\u0435\u0432\u044c\u044f\u043c\u0438 \u0434\u043b\u044f \u0441\u043b\u0443\u0447\u0430\u044f \u043a\u0432\u0430\u0434\u0440\u0430\u0442\u0438\u0447\u043d\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c</li> </ul>"},{"location":"portfolio/sfml/ml6_%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3.html","title":"\u0411\u0443\u0441\u0442\u0438\u043d\u0433\u00b6","text":"<ul> <li>\u042d\u0442\u043e \u043c\u0435\u0442\u043e\u0434 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u0439 \u0431\u0430\u0437\u043e\u0432\u044b\u0445 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432</li> <li>\u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u043a \u0442\u0435\u043a\u0443\u0449\u0435\u0439 \u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u0438 \u043d\u043e\u0432\u043e\u0433\u043e \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430</li> <li>\u0421 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u043e\u043c (\u043a\u043e\u044d\u0444\u0444\u0438\u0439\u0438\u0435\u043d\u0442 \u0438\u0437 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430)</li> </ul>"},{"location":"portfolio/sfml/ml6_%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3.html","title":"\u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0411\u0443\u0441\u0442\u0438\u043d\u0433\u00b6","text":"<p>\u041e\u0431\u0443\u0447\u0430\u0435\u0442 \u043a\u0430\u0436\u0434\u044b\u0439 \u043d\u043e\u0432\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0442\u0430\u043a \u0447\u0442\u043e\u0431\u044b \u043e\u043d</p> <ul> <li>\u043f\u0440\u0438\u0431\u043b\u0438\u0436\u0430\u043b \u0430\u043d\u0442\u0438\u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u043e\u0448\u0438\u0431\u043a\u0438 \u043f\u043e \u043e\u0442\u0432\u0435\u0442\u0430\u043c \u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u0438 \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435</li> </ul> <p>\u0410\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u043e \u043c\u0438\u043d\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0444\u0443\u043d\u043a\u0446\u0438\u0439 \u043c\u0435\u0442\u043e\u0434\u043e\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430,</p> <ul> <li>\u0432 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u043c \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u0435 \u043c\u044b \u043f\u043e\u0434\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u043c \u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u044e, \u0438\u0437\u043c\u0435\u043d\u044f\u044f \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0432 \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0438 \u0430\u043d\u0442\u0438\u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0430 \u043e\u0448\u0438\u0431\u043a\u0438</li> </ul>"},{"location":"portfolio/sfml/ml6_%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3.html","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/sfml/ml6_%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3.html#641","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 6.4.1\u00b6","text":"<p>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435\u0441\u044c \u0444\u043e\u0440\u043c\u0443\u043b\u043e\u0439 \u0438\u0437 \u043b\u0435\u043a\u0446\u0438\u0439, \u0437\u0430\u0434\u0430\u044e\u0449\u0435\u0439 \u043e\u0442\u0432\u0435\u0442\u044b \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0443\u0436\u043d\u043e \u043e\u0431\u0443\u0447\u0430\u0442\u044c \u043d\u043e\u0432\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c (\u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u044d\u0442\u043e \u043b\u0438\u0448\u044c \u0447\u0443\u0442\u044c \u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u043e \u0440\u0430\u0441\u043f\u0438\u0441\u0430\u043d\u043d\u044b\u0439 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \u043e\u0442 \u043e\u0448\u0438\u0431\u043a\u0438), \u0438 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u0435 \u0447\u0430\u0441\u0442\u043d\u044b\u0439 \u0435\u0435 \u0441\u043b\u0443\u0447\u0430\u0439, \u0435\u0441\u043b\u0438 \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c  - \u043a\u0432\u0430\u0434\u0440\u0430\u0442 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u044f \u043e\u0442\u0432\u0435\u0442\u0430 \u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u0438  \u043e\u0442 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\u0433\u043e \u043e\u0442\u0432\u0435\u0442\u0430  \u043d\u0430 \u0434\u0430\u043d\u043d\u043e\u043c .</p> <p>\u0415\u0441\u043b\u0438 \u0432\u044b \u0434\u0430\u0432\u043d\u043e \u043d\u0435 \u0441\u0447\u0438\u0442\u0430\u043b\u0438 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u043d\u0443\u044e \u0441\u0430\u043c\u043e\u0441\u0442\u043e\u044f\u0442\u0435\u043b\u044c\u043d\u043e, \u0432\u0430\u043c \u043f\u043e\u043c\u043e\u0436\u0435\u0442 \u0442\u0430\u0431\u043b\u0438\u0446\u0430 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u043d\u044b\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u0430\u0440\u043d\u044b\u0445 \u0444\u0443\u043d\u043a\u0446\u0438\u0439 (\u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043d\u0435\u0441\u043b\u043e\u0436\u043d\u043e \u043d\u0430\u0439\u0442\u0438 \u0432 \u0438\u043d\u0442\u0435\u0440\u043d\u0435\u0442\u0435) \u0438 \u043f\u0440\u0430\u0432\u0438\u043b\u043e \u0434\u0438\u0444\u0444\u0435\u0440\u0435\u043d\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0441\u043b\u043e\u0436\u043d\u043e\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438. \u041f\u043e\u0441\u043b\u0435 \u0434\u0438\u0444\u0444\u0435\u0440\u0435\u043d\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043a\u0432\u0430\u0434\u0440\u0430\u0442\u0430 \u0443 \u0432\u0430\u0441 \u0432\u043e\u0437\u043d\u0438\u043a\u043d\u0435\u0442 \u043c\u043d\u043e\u0436\u0438\u0442\u0435\u043b\u044c  \u2014 \u0442.\u043a. \u043d\u0430\u043c \u0432\u0441\u0451 \u0440\u0430\u0432\u043d\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u043e\u0438\u0442 \u0432\u044b\u0431\u0438\u0440\u0430\u0442\u044c \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442, \u0441 \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u0431\u0443\u0434\u0435\u0442 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d \u043d\u043e\u0432\u044b\u0439 \u0431\u0430\u0437\u043e\u0432\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c, \u043f\u0440\u043e\u0438\u0433\u043d\u043e\u0438\u0440\u0443\u0439\u0442\u0435 \u044d\u0442\u043e\u0442 \u043c\u043d\u043e\u0436\u0438\u0442\u0435\u043b\u044c \u043f\u0440\u0438 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u043c \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0438 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430.</p> <p>\u0417\u0430\u043f\u0438\u0448\u0438\u0442\u0435 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0442\u0432\u0435\u0442\u0430 \u0444\u043e\u0440\u043c\u0443\u043b\u0443 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u043d\u043e\u0439 \u0431\u0435\u0437 \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u044b</p> <p>(a(x)-y)*a\u2019(x)</p>"},{"location":"portfolio/sfml/ml6_%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3.html#642","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 6.4.2\u00b6","text":"<ul> <li>\u0417\u0430\u0432\u0435\u0434\u0438\u0442\u0435 \u043c\u0430\u0441\u0441\u0438\u0432 \u0434\u043b\u044f \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 <code>DecisionTreeRegressor</code> ( \u041d\u0430\u0448 \u0431\u0430\u0437\u043e\u0432\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c )</li> <li>\u0418 \u0434\u043b\u044f \u0432\u0435\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0445 \u0447\u0438\u0441\u0435\u043b ( \u0431\u0443\u0434\u0443\u0442 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u044b \u043f\u0435\u0440\u0435\u0434 \u0431\u0430\u0437\u043e\u0432\u044b\u043c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430\u043c\u0438 )</li> </ul> <p>\u0412 \u0446\u0438\u043a\u043b\u0435 \u043e\u0431\u0443\u0447\u0438\u0442\u0435:</p> <ul> <li><p>\u041f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e 20 \u0440\u0435\u0448\u0430\u044e\u0449\u0438\u0445 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 (<code>max_depth=5</code> \u0438<code>random_state=139</code>)</p> </li> <li><p>\u0412 \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u0435 \u0437\u0430\u0447\u0430\u0441\u0442\u0443\u044e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u0441\u043e\u0442\u043d\u0438 \u0438 \u0442\u044b\u0441\u044f\u0447\u0438 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432, \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0438\u043c\u0441\u044f 50</p> </li> <li><p>\u041a\u0430\u0436\u0434\u043e\u0435 \u0434\u0435\u0440\u0435\u0432\u043e \u0434\u043e\u043b\u0436\u043d\u043e \u043e\u0431\u0443\u0447\u0430\u0442\u044c\u0441\u044f \u043d\u0430 \u043e\u0434\u043d\u043e\u043c \u0438 \u0442\u043e\u043c \u0436\u0435 \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432, \u043d\u043e \u043e\u0442\u0432\u0435\u0442\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0443\u0447\u0438\u0442\u0441\u044f \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0434\u0435\u0440\u0435\u0432\u043e, \u0431\u0443\u0434\u0443\u0442 \u043c\u0435\u043d\u044f\u0442\u044c\u0441\u044f \u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0435 \u0441 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u043c \u0432 \u0437\u0430\u0434\u0430\u043d\u0438\u0438 1 \u043f\u0440\u0430\u0432\u0438\u043b\u043e\u043c (\u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 (<code>coeff</code>) \u0440\u0430\u0432\u043d\u044b\u043c 0.9)</p> </li> <li><p>\u041e\u0431\u044b\u0447\u043d\u043e \u043e\u043f\u0440\u0430\u0432\u0434\u0430\u043d\u043e \u0432\u044b\u0431\u0438\u0440\u0430\u0442\u044c \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043c\u0435\u043d\u044c\u0448\u0438\u043c - \u043f\u043e\u0440\u044f\u0434\u043a\u0430 0.05 \u0438\u043b\u0438 0.1, \u043d\u043e \u0442.\u043a. \u0432 \u043d\u0430\u0448\u0435\u043c \u0443\u0447\u0435\u0431\u043d\u043e\u043c \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u043d\u0430 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u0431\u0443\u0434\u0435\u0442 \u0432\u0441\u0435\u0433\u043e 50 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432, \u0432\u043e\u0437\u044c\u043c\u0435\u043c \u0434\u043b\u044f \u043d\u0430\u0447\u0430\u043b\u0430 \u0448\u0430\u0433 \u043f\u043e\u0431\u043e\u043b\u044c\u0448\u0435</p> </li> </ul>"},{"location":"portfolio/sfml/ml6_%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3.html#643","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 6.4.3\u00b6","text":"<p>\u0412\u0430\u0441 \u043c\u043e\u0436\u0435\u0442 \u0442\u0430\u043a\u0436\u0435 \u0431\u0435\u0441\u043f\u043e\u043a\u043e\u0438\u0442\u044c, \u0447\u0442\u043e \u0434\u0432\u0438\u0433\u0430\u044f\u0441\u044c \u0441 \u043f\u043e\u0441\u0442\u043e\u044f\u043d\u043d\u044b\u043c \u0448\u0430\u0433\u043e\u043c, \u0432\u0431\u043b\u0438\u0437\u0438 \u043c\u0438\u043d\u0438\u043c\u0443\u043c\u0430 \u043e\u0448\u0438\u0431\u043a\u0438 \u043e\u0442\u0432\u0435\u0442\u044b \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u043c\u0435\u043d\u044f\u044e\u0442\u0441\u044f \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0440\u0435\u0437\u043a\u043e, \u043f\u0435\u0440\u0435\u0441\u043a\u0430\u043a\u0438\u0432\u0430\u044f \u0447\u0435\u0440\u0435\u0437 \u043c\u0438\u043d\u0438\u043c\u0443\u043c.</p> <p>\u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0439\u0442\u0435 \u0443\u043c\u0435\u043d\u044c\u0448\u0430\u0442\u044c \u0432\u0435\u0441 \u043f\u0435\u0440\u0435\u0434 \u043a\u0430\u0436\u0434\u044b\u043c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u043c \u0441 \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0439 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0435\u0439 \u043f\u043e \u0444\u043e\u0440\u043c\u0443\u043b\u0435 0.9 / (1.0 + iter), \u0433\u0434\u0435 iter - \u043d\u043e\u043c\u0435\u0440 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438 (\u043e\u0442 0 \u0434\u043e 0.49)</p>"},{"location":"portfolio/sfml/ml6_%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3.html#644","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 6.4.4\u00b6","text":"<ul> <li>\u0420\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u0432\u0430\u043c\u0438 \u043c\u0435\u0442\u043e\u0434 - <code>\u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433</code> \u043d\u0430\u0434 \u0434\u0435\u0440\u0435\u0432\u044c\u044f\u043c\u0438 - \u043e\u0447\u0435\u043d\u044c \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u0435\u043d</li> <li>\u041e\u043d \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d \u043a\u0430\u043a \u0432 \u0441\u0430\u043c\u043e\u0439 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0435 sklearn, \u0442\u0430\u043a \u0438 \u0432 \u0441\u0442\u043e\u0440\u043e\u043d\u043d\u0435\u0439 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0435 XGBoost...</li> <li>\u041d\u0430 \u043f\u0440\u0430\u043a\u0442\u0438\u043a\u0435 <code>XGBoost</code> \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0437\u0430\u043c\u0435\u0442\u043d\u043e \u043b\u0443\u0447\u0448\u0435 <code>GradientBoostingRegressor</code></li> </ul> <p>\u0418\u0441\u0441\u043b\u0435\u0434\u0443\u0439\u0442\u0435</p> <ul> <li>\u041f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u043b\u0438 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433 \u0441 \u0440\u043e\u0441\u0442\u043e\u043c <code>\u0447\u0438\u0441\u043b\u0430 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0439</code> (\u0438 \u043f\u043e\u0434\u0443\u043c\u0430\u0439\u0442\u0435, \u043f\u043e\u0447\u0435\u043c\u0443)</li> <li>\u0430 \u0442\u0430\u043a\u0436\u0435 \u0441 \u0440\u043e\u0441\u0442\u043e\u043c <code>\u0433\u043b\u0443\u0431\u0438\u043d\u044b</code> \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432</li> <li>\u041d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u0439 \u0432\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u0435 \u0443\u0442\u0432\u0435\u0440\u0436\u0434\u0435\u043d\u0438\u044f \u0438\u0437 \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u043d\u044b\u0445 \u043d\u0438\u0436\u0435:</li> </ul>"},{"location":"portfolio/sfml/ml6_%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3.html","title":"\u041f\u0440\u043e\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0435 \u0443\u0442\u0432\u0435\u0440\u0436\u0434\u0435\u043d\u0438\u0435\u00b6","text":"<p>\u0421 \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u0435\u043c \u0447\u0438\u0441\u043b\u0430 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432, \u043d\u0430\u0447\u0438\u043d\u0430\u044f \u0441 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u043c\u043e\u043c\u0435\u043d\u0442\u0430, \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433 \u043d\u0430\u0447\u0438\u043d\u0430\u0435\u0442 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0430\u0442\u044c\u0441\u044f.</p>"},{"location":"portfolio/sfml/ml6_%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3.html","title":"\u041f\u0440\u043e\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0435 \u0443\u0442\u0432\u0435\u0440\u0436\u0434\u0435\u043d\u0438\u0435\u00b6","text":"<p>\u0421 \u0440\u043e\u0441\u0442\u043e\u043c \u0433\u043b\u0443\u0431\u0438\u043d\u044b \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432, \u043d\u0430\u0447\u0438\u043d\u0430\u044f \u0441 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u043c\u043e\u043c\u0435\u043d\u0442\u0430, \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0430\u0431\u043e\u0442\u044b \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u0430 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u043d\u0430\u0447\u0438\u043d\u0430\u0435\u0442 \u0443\u0445\u0443\u0434\u0448\u0430\u0442\u044c\u0441\u044f.</p>"},{"location":"portfolio/sfml/ml6_%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3.html","title":"\u041f\u0440\u043e\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0435 \u0443\u0442\u0432\u0435\u0440\u0436\u0434\u0435\u043d\u0438\u0435\u00b6","text":"<ul> <li>\u0421 \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u0435\u043c \u0447\u0438\u0441\u043b\u0430 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432, \u043d\u0430\u0447\u0438\u043d\u0430\u044f \u0441 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u043c\u043e\u043c\u0435\u043d\u0442\u0430, \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433 \u043d\u0430\u0447\u0438\u043d\u0430\u0435\u0442 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0430\u0442\u044c\u0441\u044f.</li> <li>\u0421 \u0440\u043e\u0441\u0442\u043e\u043c \u0433\u043b\u0443\u0431\u0438\u043d\u044b \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432, \u043d\u0430\u0447\u0438\u043d\u0430\u044f \u0441 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u043c\u043e\u043c\u0435\u043d\u0442\u0430, \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0430\u0431\u043e\u0442\u044b \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u0430 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u043d\u0430\u0447\u0438\u043d\u0430\u0435\u0442 \u0443\u0445\u0443\u0434\u0448\u0430\u0442\u044c\u0441\u044f.</li> </ul>"},{"location":"portfolio/sfml/ml6_%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3.html#645","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 6.4.5\u00b6","text":"<ul> <li>\u0421\u0440\u0430\u0432\u043d\u0438\u0442\u0435 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c\u043e\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0441 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e\u043c \u0440\u0430\u0431\u043e\u0442\u044b <code>\u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0438\u0438</code></li> <li>\u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0438\u0442\u0435 <code>LinearRegression</code> (\u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e) \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0438 \u043e\u0446\u0435\u043d\u0438\u0442\u0435 \u0434\u043b\u044f \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u043e\u0432 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0433\u043e \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 <code>RMSE</code></li> <li>\u0412 \u0434\u0430\u043d\u043d\u043e\u043c \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0430\u0431\u043e\u0442\u044b \u043f\u0440\u043e\u0441\u0442\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u043e\u043b\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u043e\u043a\u0430\u0437\u0430\u0442\u044c\u0441\u044f \u0445\u0443\u0436\u0435, \u043d\u043e \u043d\u0435 \u0441\u0442\u043e\u0438\u0442 \u0437\u0430\u0431\u044b\u0432\u0430\u0442\u044c, \u0447\u0442\u043e \u0442\u0430\u043a \u0431\u044b\u0432\u0430\u0435\u0442 \u043d\u0435 \u0432\u0441\u0435\u0433\u0434\u0430</li> </ul>"},{"location":"portfolio/sfml/ml6_%D0%9B%D0%B5%D1%81%D0%B0_%D0%9C%D0%B0%D0%BB%D0%BE.html","title":"ml6 \u041b\u0435\u0441\u0430 \u041c\u0430\u043b\u043e","text":"In\u00a0[10]: Copied! <pre>from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_blobs, make_circles\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\nimport seaborn as sns; sns.set(style='whitegrid')\n\ndef get_labels(data):\n    labels = []\n    for idx, item in enumerate(data):\n        if item[0]**2 + item[1]**2 &lt; 1:\n            labels.append(0)\n        elif item[0] &gt; 2 and item[1] &gt; 2:\n            labels.append(0)\n        else:\n            labels.append(1)\n    return np.array(labels)\n\nN = 500\ntrain_data = 7 * np.random.random_sample((N,2)) - np.array([3,3])\ntrain_labels = get_labels(train_data)\n\n# Let\u2019s write an auxiliary function that will return grid for further visualization.\ndef get_grid(data):\n    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n    return np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n\n# \u043c\u0435\u043b\u043a\u0438\u0439\nshallow_rf = RandomForestClassifier(n_estimators=3, max_depth=3, \n                                    n_jobs=-1, random_state=139)\n\n# \u0413\u043b\u0443\u0431\u043e\u043a\u0438\u0439 \ndeep_rf = RandomForestClassifier(n_estimators=3, max_depth=6, \n                                 n_jobs=-1, random_state=139)\n\n# training the tree\nshallow_rf.fit(train_data, train_labels)\ndeep_rf.fit(train_data, train_labels)\n\n# some code to depict separating surface\nxx, yy = get_grid(train_data)\npredicted_shallow = shallow_rf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\npredicted_deep = deep_rf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\nplt.scatter(train_data[:, 0],train_data[:, 1], c=train_labels, s=20, \n            cmap='coolwarm', edgecolors='black', linewidth=1.5);\n</pre> from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import make_blobs, make_circles import matplotlib.pyplot as plt import numpy as np %matplotlib inline import seaborn as sns; sns.set(style='whitegrid')  def get_labels(data):     labels = []     for idx, item in enumerate(data):         if item[0]**2 + item[1]**2 &lt; 1:             labels.append(0)         elif item[0] &gt; 2 and item[1] &gt; 2:             labels.append(0)         else:             labels.append(1)     return np.array(labels)  N = 500 train_data = 7 * np.random.random_sample((N,2)) - np.array([3,3]) train_labels = get_labels(train_data)  # Let\u2019s write an auxiliary function that will return grid for further visualization. def get_grid(data):     x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1     y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1     return np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))  # \u043c\u0435\u043b\u043a\u0438\u0439 shallow_rf = RandomForestClassifier(n_estimators=3, max_depth=3,                                      n_jobs=-1, random_state=139)  # \u0413\u043b\u0443\u0431\u043e\u043a\u0438\u0439  deep_rf = RandomForestClassifier(n_estimators=3, max_depth=6,                                   n_jobs=-1, random_state=139)  # training the tree shallow_rf.fit(train_data, train_labels) deep_rf.fit(train_data, train_labels)  # some code to depict separating surface xx, yy = get_grid(train_data) predicted_shallow = shallow_rf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape) predicted_deep = deep_rf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)  plt.scatter(train_data[:, 0],train_data[:, 1], c=train_labels, s=20,              cmap='coolwarm', edgecolors='black', linewidth=1.5); In\u00a0[11]: Copied! <pre>fig, ax = plt.subplots(1, 2, figsize=(10,3))\n\nax[0].pcolormesh(xx, yy, predicted_shallow, cmap='coolwarm')\nax[0].scatter(train_data[:, 0], train_data[:, 1], \n              c=train_labels, s=30, cmap='coolwarm', \n              edgecolors='black', linewidth=1.5);\nax[0].set_title('Shallow Random Forest',size=10)\n\nax[1].pcolormesh(xx, yy, predicted_deep, cmap='coolwarm')\nax[1].scatter(train_data[:, 0], train_data[:, 1], \n              c=train_labels, s=30, cmap='coolwarm', \n              edgecolors='black', linewidth=1.5);\nax[1].set_title('Deep Random Forest',size=10)\nplt.show()\n</pre> fig, ax = plt.subplots(1, 2, figsize=(10,3))  ax[0].pcolormesh(xx, yy, predicted_shallow, cmap='coolwarm') ax[0].scatter(train_data[:, 0], train_data[:, 1],                c=train_labels, s=30, cmap='coolwarm',                edgecolors='black', linewidth=1.5); ax[0].set_title('Shallow Random Forest',size=10)  ax[1].pcolormesh(xx, yy, predicted_deep, cmap='coolwarm') ax[1].scatter(train_data[:, 0], train_data[:, 1],                c=train_labels, s=30, cmap='coolwarm',                edgecolors='black', linewidth=1.5); ax[1].set_title('Deep Random Forest',size=10) plt.show()"},{"location":"portfolio/sfml/ml6_%D0%9B%D0%B5%D1%81%D0%B0_%D0%9C%D0%B0%D0%BB%D0%BE.html","title":"\u041a\u043e\u0433\u0434\u0430 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441 \u043d\u0435 \u0441\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f\u00b6","text":"<p>\u041f\u043e\u0434\u0447\u0435\u043a\u043d\u0435\u043c \u0434\u0432\u0435 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u043b\u0435\u0441\u0430</p> <ul> <li>\u0412 \u043f\u0440\u043e\u0448\u043b\u043e\u043c \u043c\u043e\u0434\u0443\u043b\u0435 \u043c\u044b \u0443\u0437\u043d\u0430\u043b\u0438 \u043e\u0431 <code>\u0430\u043d\u0441\u0430\u043c\u0431\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0438</code> \u043c\u043e\u0434\u0435\u043b\u0435\u0439</li> <li>\u041e\u0434\u043d\u0438\u043c \u0438\u0437 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u0435\u043b\u0435\u0439 \u0442\u0430\u043a\u043e\u0433\u043e \u043f\u043e\u0434\u0445\u043e\u0434\u0430 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f <code>\u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441</code></li> <li><code>\u0421\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441</code> \u044d\u0442\u043e \u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u044f \u0433\u043b\u0443\u0431\u043e\u043a\u0438\u0445 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u0442\u0440\u043e\u044f\u0442\u0441\u044f \u043d\u0435\u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e \u0434\u0440\u0443\u0433 \u043e\u0442 \u0434\u0440\u0443\u0433\u0430</li> </ul>"},{"location":"portfolio/sfml/ml6_%D0%9B%D0%B5%D1%81%D0%B0_%D0%9C%D0%B0%D0%BB%D0%BE.html#1","title":"\u041f\u0420\u041e\u0411\u041b\u0415\u041c\u0410 1\u00b6","text":"<ul> <li>\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0433\u043b\u0443\u0431\u043e\u043a\u0438\u0445 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 \u0442\u0440\u0435\u0431\u0443\u0435\u0442 \u043e\u0447\u0435\u043d\u044c \u043c\u043d\u043e\u0433\u043e \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0440\u0435\u0441\u0443\u0440\u0441\u043e\u0432</li> <li>\u041e\u0441\u043e\u0431\u0435\u043d\u043d\u043e \u0432 \u0441\u043b\u0443\u0447\u0430\u0435 \u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0438\u043b\u0438 \u0431\u043e\u043b\u044c\u0448\u043e\u0433\u043e \u0447\u0438\u0441\u043b\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432</li> <li>\u0415\u0441\u043b\u0438 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0438\u0442\u044c \u0433\u043b\u0443\u0431\u0438\u043d\u0443 \u0440\u0435\u0448\u0430\u044e\u0449\u0438\u0445 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 \u0432 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u043c \u043b\u0435\u0441\u0435, \u0442\u043e \u043e\u043d\u0438 \u0443\u0436\u0435 \u043d\u0435 \u0441\u043c\u043e\u0433\u0443\u0442 \u0443\u043b\u0430\u0432\u043b\u0438\u0432\u0430\u0442\u044c \u0441\u043b\u043e\u0436\u043d\u044b\u0435 \u0437\u0430\u043a\u043e\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445</li> <li>\u042d\u0442\u043e \u043f\u0440\u0438\u0432\u0435\u0434\u0451\u0442 \u043a \u0442\u043e\u043c\u0443, \u0447\u0442\u043e \u0441\u0434\u0432\u0438\u0433 (<code>bias</code>) \u0431\u0443\u0434\u0435\u0442 \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0431\u043e\u043b\u044c\u0448\u0438\u043c</li> </ul>"},{"location":"portfolio/sfml/ml6_%D0%9B%D0%B5%D1%81%D0%B0_%D0%9C%D0%B0%D0%BB%D0%BE.html#2","title":"\u041f\u0420\u041e\u0411\u041b\u0415\u041c\u0410 2\u00b6","text":"<ul> <li>\u041f\u0440\u043e\u0446\u0435\u0441\u0441 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043d\u0435\u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u043c</li> <li>ie. \u043a\u0430\u0436\u0434\u043e\u0435 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0435 \u0434\u0435\u0440\u0435\u0432\u043e \u0432 \u043a\u043e\u043c\u043f\u043e\u0437\u0438\u0446\u0438\u0438 \u043d\u0438\u043a\u0430\u043a \u043d\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0445</li> <li>\u0418\u0437-\u0437\u0430 \u044d\u0442\u043e\u0433\u043e \u0434\u043b\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u0441\u043b\u043e\u0436\u043d\u044b\u0445 \u0437\u0430\u0434\u0430\u0447 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043e\u0433\u0440\u043e\u043c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432</li> </ul>"},{"location":"portfolio/sfml/ml8_ts_specifics.html","title":"Ml8 ts specifics","text":"In\u00a0[2]: Copied! <pre>%%capture\n\n!wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n!tar -xzvf ta-lib-0.4.0-src.tar.gz\n%cd ta-lib\n!./configure --prefix=/usr\n!make\n!make install\n!pip install Ta-Lib kaleido yfinance catboost\n</pre> %%capture  !wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz !tar -xzvf ta-lib-0.4.0-src.tar.gz %cd ta-lib !./configure --prefix=/usr !make !make install !pip install Ta-Lib kaleido yfinance catboost In\u00a0[3]: Copied! <pre>import talib\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom matplotlib import figure\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\nimport talib as ta\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix\nimport warnings;warnings.filterwarnings('ignore')\n</pre> import talib import plotly import plotly.express as px import plotly.graph_objects as go from matplotlib import figure import matplotlib.pyplot as plt  import numpy as np import pandas as pd import talib as ta  from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import roc_curve, auc, confusion_matrix import warnings;warnings.filterwarnings('ignore') <pre>/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}\"\n</pre> In\u00a0[4]: Copied! <pre>import yfinance as yf\n\ntickers = ['AAPL','MSFT','AMZN','GOOG',\n           'AMD','NVDA','TSLA','YELP',\n           'NFLX','ADBE','BA','AIG']\n\nstart = '2012-05-05'\nend = '2023-05-05'\ndf_ticker = pd.DataFrame()\nfor ticker in tickers:\n    ticker_yf = yf.Ticker(ticker)\n    if df_ticker.empty:\n        df_ticker = ticker_yf.history(start = start, end = end, interval='1d')\n        df_ticker['ticker']= ticker\n    else:\n        data_temp = ticker_yf.history(start = start, end = end, interval='1d')\n        data_temp['ticker']= ticker\n        df_ticker = df_ticker.append(data_temp)\n\n# df_ticker = pd.read_csv('return_data.csv',index_col=['Datetime'])\ndf_ticker['ticker'] = df_ticker['ticker'].replace({'META':'FB'})\ndf_ticker.index = pd.to_datetime(df_ticker.index)\nprint(df_ticker['ticker'].unique())\n</pre> import yfinance as yf  tickers = ['AAPL','MSFT','AMZN','GOOG',            'AMD','NVDA','TSLA','YELP',            'NFLX','ADBE','BA','AIG']  start = '2012-05-05' end = '2023-05-05' df_ticker = pd.DataFrame() for ticker in tickers:     ticker_yf = yf.Ticker(ticker)     if df_ticker.empty:         df_ticker = ticker_yf.history(start = start, end = end, interval='1d')         df_ticker['ticker']= ticker     else:         data_temp = ticker_yf.history(start = start, end = end, interval='1d')         data_temp['ticker']= ticker         df_ticker = df_ticker.append(data_temp)  # df_ticker = pd.read_csv('return_data.csv',index_col=['Datetime']) df_ticker['ticker'] = df_ticker['ticker'].replace({'META':'FB'}) df_ticker.index = pd.to_datetime(df_ticker.index) print(df_ticker['ticker'].unique()) <pre>['AAPL' 'MSFT' 'AMZN' 'GOOG' 'AMD' 'NVDA' 'TSLA' 'YELP' 'NFLX' 'ADBE' 'BA'\n 'AIG']\n</pre> In\u00a0[5]: Copied! <pre>print(f'Data size: {df_ticker.shape[0]/12}')\nprint(f'Step size: {df_ticker.index[1] - df_ticker.index[0]}')\nprint(f'isna().sum() =\\n{df_ticker.isna().sum()}')\n</pre> print(f'Data size: {df_ticker.shape[0]/12}') print(f'Step size: {df_ticker.index[1] - df_ticker.index[0]}') print(f'isna().sum() =\\n{df_ticker.isna().sum()}') <pre>Data size: 2767.0\nStep size: 1 days 00:00:00\nisna().sum() =\nOpen            0\nHigh            0\nLow             0\nClose           0\nVolume          0\nDividends       0\nStock Splits    0\nticker          0\ndtype: int64\n</pre> In\u00a0[6]: Copied! <pre>df = dict(tuple(df_ticker.groupby(['ticker'])))['MSFT']\ndf.index\n</pre> df = dict(tuple(df_ticker.groupby(['ticker'])))['MSFT'] df.index Out[6]: <pre>DatetimeIndex(['2012-05-07 00:00:00-04:00', '2012-05-08 00:00:00-04:00',\n               '2012-05-09 00:00:00-04:00', '2012-05-10 00:00:00-04:00',\n               '2012-05-11 00:00:00-04:00', '2012-05-14 00:00:00-04:00',\n               '2012-05-15 00:00:00-04:00', '2012-05-16 00:00:00-04:00',\n               '2012-05-17 00:00:00-04:00', '2012-05-18 00:00:00-04:00',\n               ...\n               '2023-04-21 00:00:00-04:00', '2023-04-24 00:00:00-04:00',\n               '2023-04-25 00:00:00-04:00', '2023-04-26 00:00:00-04:00',\n               '2023-04-27 00:00:00-04:00', '2023-04-28 00:00:00-04:00',\n               '2023-05-01 00:00:00-04:00', '2023-05-02 00:00:00-04:00',\n               '2023-05-03 00:00:00-04:00', '2023-05-04 00:00:00-04:00'],\n              dtype='datetime64[ns, America/New_York]', name='Date', length=2767, freq=None)</pre> In\u00a0[7]: Copied! <pre>fig = px.line(df.Close.values,template='plotly_white',\n              height=300,width=800,title='\u0410\u043a\u0446\u0438\u0438 MSFT')\nfig.show('png',dpi=300)\n</pre> fig = px.line(df.Close.values,template='plotly_white',               height=300,width=800,title='\u0410\u043a\u0446\u0438\u0438 MSFT') fig.show('png',dpi=300) In\u00a0[9]: Copied! <pre># print(f'df.shape (before) = {df.shape}')\n\n# \u043a\u0430\u043a \u0430\u0433\u0433\u0440\u0435\u0433\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438\n# conversion = {\n#                 'Open'  : 'first', ## \u0446\u0435\u043d\u0430 \u043e\u0442\u043a\u0440\u044b\u0442\u0438\u044f - \u043f\u0435\u0440\u0432\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\n#                 'High'  : 'max',   ## \u043f\u0438\u043a\u043e\u0432\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 - \u043c\u0430\u043a\u0441\u0438\u043c\u0443\u043c\n#                 'Low'   : 'min',   ## \u043d\u0430\u0438\u043c\u0435\u043d\u044c\u0448\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 - \u043c\u0438\u043d\u0438\u043c\u0443\u043c\n#                 'Close' : 'last',  ## \u0446\u0435\u043d\u0430 \u0437\u0430\u043a\u0440\u044b\u0442\u0438\u044f - \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\n#                 'Volume'   : 'sum',   ## \u043e\u0431\u044a\u0451\u043c - \u0441\u0443\u043c\u043c\u0430\n#              }\n\n# \u043f\u043e\u043d\u0438\u0436\u0430\u0435\u043c \u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u0438\u0435 \u0434\u043e \u0447\u0430\u0441\u043e\u0432\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439\n\n# df = df.resample('1h').apply(conversion) # main funct\n# df = df.reset_index()\n# df = df.dropna()\n\n# print(f'df.shape (after) = {df.shape}')\n\nh1 = df.copy()\n</pre> # print(f'df.shape (before) = {df.shape}')  # \u043a\u0430\u043a \u0430\u0433\u0433\u0440\u0435\u0433\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 # conversion = { #                 'Open'  : 'first', ## \u0446\u0435\u043d\u0430 \u043e\u0442\u043a\u0440\u044b\u0442\u0438\u044f - \u043f\u0435\u0440\u0432\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 #                 'High'  : 'max',   ## \u043f\u0438\u043a\u043e\u0432\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 - \u043c\u0430\u043a\u0441\u0438\u043c\u0443\u043c #                 'Low'   : 'min',   ## \u043d\u0430\u0438\u043c\u0435\u043d\u044c\u0448\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 - \u043c\u0438\u043d\u0438\u043c\u0443\u043c #                 'Close' : 'last',  ## \u0446\u0435\u043d\u0430 \u0437\u0430\u043a\u0440\u044b\u0442\u0438\u044f - \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 #                 'Volume'   : 'sum',   ## \u043e\u0431\u044a\u0451\u043c - \u0441\u0443\u043c\u043c\u0430 #              }  # \u043f\u043e\u043d\u0438\u0436\u0430\u0435\u043c \u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u0438\u0435 \u0434\u043e \u0447\u0430\u0441\u043e\u0432\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439  # df = df.resample('1h').apply(conversion) # main funct # df = df.reset_index() # df = df.dropna()  # print(f'df.shape (after) = {df.shape}')  h1 = df.copy() In\u00a0[10]: Copied! <pre>df = h1.copy()\n\nfeatures = pd.DataFrame()\nfeatures.index = df.index\n\n# \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0444\u0438\u0447\u0438: \u043e\u0431\u044a\u0451\u043c \u0441\u0434\u0435\u043b\u043e\u043a\nfeatures['vol_0']   = df.Volume\nfeatures['vol_1']   = df.Volume.shift(1) ## \u0437\u0430 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0439 \u0434\u0435\u043d\u044c\nfeatures['vol_2']   = df.Volume.shift(2) ## \u0437\u0430 \u043f\u0440\u0435\u0434\u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0439 \u0434\u0435\u043d\u044c\nfeatures['LMA'] = df['Close'].rolling(window=15, min_periods=1, center=False).mean()\n\n# \u0438\u043d\u0434\u0438\u043a\u0430\u0442\u043e\u0440\u044b \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 ta-lib\n\nperiod = 14 # \u043e\u043a\u043d\u043e \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0431\u0443\u0434\u0443\u0442 \u0440\u0430\u0441\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0442\u044c\u0441\u044f \u0438\u043d\u0434\u0438\u043a\u0430\u0442\u043e\u0440\u044b\n\n# RSI:\nfeatures['RSI1'] = ta.RSI(df.Close.values, period)\n\n# ## \u043f\u0440\u043e\u0447\u0438\u0435 \u0438\u043d\u0434\u0438\u043a\u0430\u0442\u043e\u0440\u044b (\u0442\u0440\u0435\u0431\u0443\u044e\u0442 \u0432\u0441\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0441\u0432\u0435\u0447\u0438: open, close, high, low)\nfeatures['willr'] = ta.WILLR(df.High.values, df.Low.values, df.Close.values, period)\nfeatures['BOP']   = ta.BOP(df.Open.values, df.High.values, df.Low.values, df.Close.values)\nfeatures['MOM']   = ta.MOM(df.Close.values, period)\nfeatures['MOM10']   = ta.MOM(df.Close.values,period*10)\nfeatures['mdm']   = ta.MINUS_DM(df.High.values, df.Low.values, period)\nfeatures['pdm']   = ta.PLUS_DM(df.High.values, df.Low.values, period)\nfeatures['arosc'] = ta.AROONOSC(df.High.values, df.Low.values, period)\n\n# ## \u0441\u043e\u0437\u0434\u0430\u0451\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0444\u0438\u0447\nfeaturesList = features.columns.values\n\n# ## \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0442\u0430\u0439\u043c\u0441\u0442\u0435\u043c\u043f\nfeatures['ts'] = h1.index\n</pre> df = h1.copy()  features = pd.DataFrame() features.index = df.index  # \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0444\u0438\u0447\u0438: \u043e\u0431\u044a\u0451\u043c \u0441\u0434\u0435\u043b\u043e\u043a features['vol_0']   = df.Volume features['vol_1']   = df.Volume.shift(1) ## \u0437\u0430 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0439 \u0434\u0435\u043d\u044c features['vol_2']   = df.Volume.shift(2) ## \u0437\u0430 \u043f\u0440\u0435\u0434\u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0439 \u0434\u0435\u043d\u044c features['LMA'] = df['Close'].rolling(window=15, min_periods=1, center=False).mean()  # \u0438\u043d\u0434\u0438\u043a\u0430\u0442\u043e\u0440\u044b \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0438\u0437 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 ta-lib  period = 14 # \u043e\u043a\u043d\u043e \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0431\u0443\u0434\u0443\u0442 \u0440\u0430\u0441\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0442\u044c\u0441\u044f \u0438\u043d\u0434\u0438\u043a\u0430\u0442\u043e\u0440\u044b  # RSI: features['RSI1'] = ta.RSI(df.Close.values, period)  # ## \u043f\u0440\u043e\u0447\u0438\u0435 \u0438\u043d\u0434\u0438\u043a\u0430\u0442\u043e\u0440\u044b (\u0442\u0440\u0435\u0431\u0443\u044e\u0442 \u0432\u0441\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0441\u0432\u0435\u0447\u0438: open, close, high, low) features['willr'] = ta.WILLR(df.High.values, df.Low.values, df.Close.values, period) features['BOP']   = ta.BOP(df.Open.values, df.High.values, df.Low.values, df.Close.values) features['MOM']   = ta.MOM(df.Close.values, period) features['MOM10']   = ta.MOM(df.Close.values,period*10) features['mdm']   = ta.MINUS_DM(df.High.values, df.Low.values, period) features['pdm']   = ta.PLUS_DM(df.High.values, df.Low.values, period) features['arosc'] = ta.AROONOSC(df.High.values, df.Low.values, period)  # ## \u0441\u043e\u0437\u0434\u0430\u0451\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0444\u0438\u0447 featuresList = features.columns.values  # ## \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0442\u0430\u0439\u043c\u0441\u0442\u0435\u043c\u043f features['ts'] = h1.index In\u00a0[11]: Copied! <pre> # \u0440\u0430\u0437\u043d\u0438\u0446\u0430 \u043c\u0435\u0436\u0434\u0443 \u0442\u0435\u043a\u0443\u0449\u0435\u0439 \u0446\u0435\u043d\u043e\u0439 \u0438 \u0446\u0435\u043d\u043e\u0439 \u0447\u0435\u0440\u0435\u0437 1 \u0447\u0430\u0441\nfeatures['deltaPrice'] = df.Close.shift(-1) - df.Close # next day - today\n\n# \u043b\u0435\u0439\u0431\u043b \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438: \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0451\u0442 \u043b\u0438 \u0446\u0435\u043d\u0430 \u0447\u0435\u0440\u0435\u0437 X \u0447\u0430\u0441\u043e\u0432 \u0438\u043b\u0438 \u0443\u043f\u0430\u0434\u0451\u0442?\nfeatures['label'] = np.where(features['deltaPrice'] &gt; 0, 1, 0)\nfeatures = features.dropna()\n\nprint(\"Features:\")\nprint(featuresList)\nprint(f'features.shape = {features.shape}')\n</pre>  # \u0440\u0430\u0437\u043d\u0438\u0446\u0430 \u043c\u0435\u0436\u0434\u0443 \u0442\u0435\u043a\u0443\u0449\u0435\u0439 \u0446\u0435\u043d\u043e\u0439 \u0438 \u0446\u0435\u043d\u043e\u0439 \u0447\u0435\u0440\u0435\u0437 1 \u0447\u0430\u0441 features['deltaPrice'] = df.Close.shift(-1) - df.Close # next day - today  # \u043b\u0435\u0439\u0431\u043b \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438: \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0451\u0442 \u043b\u0438 \u0446\u0435\u043d\u0430 \u0447\u0435\u0440\u0435\u0437 X \u0447\u0430\u0441\u043e\u0432 \u0438\u043b\u0438 \u0443\u043f\u0430\u0434\u0451\u0442? features['label'] = np.where(features['deltaPrice'] &gt; 0, 1, 0) features = features.dropna()  print(\"Features:\") print(featuresList) print(f'features.shape = {features.shape}') <pre>Features:\n['vol_0' 'vol_1' 'vol_2' 'LMA' 'RSI1' 'willr' 'BOP' 'MOM' 'MOM10' 'mdm'\n 'pdm' 'arosc']\nfeatures.shape = (2626, 15)\n</pre> In\u00a0[12]: Copied! <pre>test = pd.DataFrame()\ntest.index = h1.index\n\n## \u0437\u0430\u0434\u0430\u0451\u043c \u043e\u043a\u043d\u043e\nperiod = 7\n\n## \u0438\u043d\u0434\u0438\u043a\u0430\u0442\u043e\u0440\u044b\ntest['willr'] = ta.WILLR(h1.High.values, h1.Low.values, h1.Close.values, period)\ntest['mdi']   = ta.MINUS_DI(h1.High.values, h1.Low.values, h1.Close.values, period)\n\n# \u0446\u0435\u043d\u0430 \u0437\u0430\u043a\u0440\u044b\u0442\u0438\u044f\ntest['Close'] = h1.Close\n\nprint(f'test.shape = {test.shape}')\n# \u0431\u0435\u0440\u0451\u043c \u043a\u0430\u0436\u0434\u0443\u044e \u0434\u0435\u0441\u044f\u0442\u0443\u044e \u0442\u043e\u0447\u043a\u0443 (\u0431\u044b\u0441\u0442\u0440\u0435\u0435 \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0430\u0435\u0442\u0441\u044f)\n# test = test.iloc[::10]\n# test = test.dropna()\n\n# \u0441\u043e\u0437\u0434\u0430\u0451\u043c \u0433\u0440\u0430\u0444\u0438\u043a\u0438 \u0434\u043b\u044f \u0446\u0435\u043d\u044b \u0437\u0430\u043a\u0440\u044b\u0442\u0438\u044f \u0438 \u0438\u043d\u0434\u0438\u043a\u0430\u0442\u043e\u0440\u043e\u0432\ntrace1 = go.Scatter(x=test.index, y=test.Close, name='close')\ntrace2 = go.Scatter(x=test.index, y=test.willr, name='willr')\ntrace3 = go.Scatter(x=test.index, y=test.mdi, name='mdi')\n\n# \u0441\u043e\u0437\u0434\u0430\u0451\u043c \u0444\u0438\u0433\u0443\u0440\u0443, \u0441\u043e\u0441\u0442\u043e\u044f\u0449\u0443\u044e \u0438\u0437 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u043f\u043e\u0434\u0444\u0438\u0433\u0443\u0440\nfrom plotly.subplots import make_subplots\n\nfig = make_subplots(rows=3, cols=1,\n                    specs=[[{}], [{}], [{}]],\n                    shared_xaxes=True,\n                    shared_yaxes=False,\n                    vertical_spacing=0.1\n                    )\n\n## \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0433\u0440\u0430\u0444\u0438\u043a\u0438 \u043d\u0430 \u0444\u0438\u0433\u0443\u0440\u044b\nfig.append_trace(trace1, 3, 1)\nfig.append_trace(trace2, 2, 1)\nfig.append_trace(trace3, 1, 1)\n\nfig['layout'].update(height=600, width=950, title='\u0418\u043d\u0434\u0438\u043a\u0430\u0442\u043e\u0440\u044b')\nfig.update_layout(template='plotly_white')\n\nfig.show('svg',dpi=300)\n</pre> test = pd.DataFrame() test.index = h1.index  ## \u0437\u0430\u0434\u0430\u0451\u043c \u043e\u043a\u043d\u043e period = 7  ## \u0438\u043d\u0434\u0438\u043a\u0430\u0442\u043e\u0440\u044b test['willr'] = ta.WILLR(h1.High.values, h1.Low.values, h1.Close.values, period) test['mdi']   = ta.MINUS_DI(h1.High.values, h1.Low.values, h1.Close.values, period)  # \u0446\u0435\u043d\u0430 \u0437\u0430\u043a\u0440\u044b\u0442\u0438\u044f test['Close'] = h1.Close  print(f'test.shape = {test.shape}') # \u0431\u0435\u0440\u0451\u043c \u043a\u0430\u0436\u0434\u0443\u044e \u0434\u0435\u0441\u044f\u0442\u0443\u044e \u0442\u043e\u0447\u043a\u0443 (\u0431\u044b\u0441\u0442\u0440\u0435\u0435 \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0430\u0435\u0442\u0441\u044f) # test = test.iloc[::10] # test = test.dropna()  # \u0441\u043e\u0437\u0434\u0430\u0451\u043c \u0433\u0440\u0430\u0444\u0438\u043a\u0438 \u0434\u043b\u044f \u0446\u0435\u043d\u044b \u0437\u0430\u043a\u0440\u044b\u0442\u0438\u044f \u0438 \u0438\u043d\u0434\u0438\u043a\u0430\u0442\u043e\u0440\u043e\u0432 trace1 = go.Scatter(x=test.index, y=test.Close, name='close') trace2 = go.Scatter(x=test.index, y=test.willr, name='willr') trace3 = go.Scatter(x=test.index, y=test.mdi, name='mdi')  # \u0441\u043e\u0437\u0434\u0430\u0451\u043c \u0444\u0438\u0433\u0443\u0440\u0443, \u0441\u043e\u0441\u0442\u043e\u044f\u0449\u0443\u044e \u0438\u0437 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u043f\u043e\u0434\u0444\u0438\u0433\u0443\u0440 from plotly.subplots import make_subplots  fig = make_subplots(rows=3, cols=1,                     specs=[[{}], [{}], [{}]],                     shared_xaxes=True,                     shared_yaxes=False,                     vertical_spacing=0.1                     )  ## \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0433\u0440\u0430\u0444\u0438\u043a\u0438 \u043d\u0430 \u0444\u0438\u0433\u0443\u0440\u044b fig.append_trace(trace1, 3, 1) fig.append_trace(trace2, 2, 1) fig.append_trace(trace3, 1, 1)  fig['layout'].update(height=600, width=950, title='\u0418\u043d\u0434\u0438\u043a\u0430\u0442\u043e\u0440\u044b') fig.update_layout(template='plotly_white')  fig.show('svg',dpi=300) <pre>test.shape = (2767, 3)\n</pre> In\u00a0[13]: Copied! <pre>splitCoef = 0.01 # 80 - \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0439 \u0441\u0435\u0442, 20 - \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0439 \u0441\u0435\u0442\n\n## \u0441\u0447\u0438\u0442\u0430\u0435\u043c, \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0446\u0435\u043d \u0432\u044b\u0448\u0435, \u0438 \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043d\u0438\u0436\u0435 \u0442\u0435\u043a\u0443\u0449\u0435\u0439 (\u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u043d \u0434\u0435\u0439\u0442\u0430\u0441\u0435\u0442)\ncountPos = len(features[features.label == 1])\ncountNeg = len(features[features.label == 0])\nprint('total')\nprint(f'Pos: {countPos}   Neg: {countNeg}\\n')\n\n# \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c\n# features_train, features_test = train_test_split(features, test_size=1-splitCoef, random_state=43)\n# \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043f\u043e \u043f\u043e\u0440\u044f\u0434\u043a\u0443\nfeatures_train, features_test = train_test_split(features,test_size=splitCoef,shuffle=False)\n\nprint(f'train start: {features_train.ts.iloc[0]}')\nprint(f'train end:   {features_train.ts.iloc[-1]}')\nprint(f'test start:  {features_test.ts.iloc[0]}')\nprint(f'test end:    {features_test.ts.iloc[-1]}')\n\n# \u043f\u043e\u0434\u0433\u043e\u0442\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c \u0441\u0435\u0442\u044b\nx_train = features_train[featuresList]\nx_test  = features_test[featuresList]\ny_train = features_train.label\ny_test  = features_test.label\n</pre> splitCoef = 0.01 # 80 - \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0439 \u0441\u0435\u0442, 20 - \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0439 \u0441\u0435\u0442  ## \u0441\u0447\u0438\u0442\u0430\u0435\u043c, \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0446\u0435\u043d \u0432\u044b\u0448\u0435, \u0438 \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043d\u0438\u0436\u0435 \u0442\u0435\u043a\u0443\u0449\u0435\u0439 (\u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u043d \u0434\u0435\u0439\u0442\u0430\u0441\u0435\u0442) countPos = len(features[features.label == 1]) countNeg = len(features[features.label == 0]) print('total') print(f'Pos: {countPos}   Neg: {countNeg}\\n')  # \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c # features_train, features_test = train_test_split(features, test_size=1-splitCoef, random_state=43) # \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043f\u043e \u043f\u043e\u0440\u044f\u0434\u043a\u0443 features_train, features_test = train_test_split(features,test_size=splitCoef,shuffle=False)  print(f'train start: {features_train.ts.iloc[0]}') print(f'train end:   {features_train.ts.iloc[-1]}') print(f'test start:  {features_test.ts.iloc[0]}') print(f'test end:    {features_test.ts.iloc[-1]}')  # \u043f\u043e\u0434\u0433\u043e\u0442\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c \u0441\u0435\u0442\u044b x_train = features_train[featuresList] x_test  = features_test[featuresList] y_train = features_train.label y_test  = features_test.label <pre>total\nPos: 1390   Neg: 1236\n\ntrain start: 2012-11-27 00:00:00-05:00\ntrain end:   2023-03-24 00:00:00-04:00\ntest start:  2023-03-27 00:00:00-04:00\ntest end:    2023-05-03 00:00:00-04:00\n</pre> In\u00a0[14]: Copied! <pre># \u0420\u0430\u0441\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u043c\u043e\u0436\u043d\u043e \u0443\u0447\u0435\u0441\u0442\u044c \u0441 compute_class_Weigh\nfrom sklearn.utils.class_weight import compute_class_weight\n\nprint(y_train.value_counts())\n\nclasses = np.unique(y_train)\nprint(classes)\nweights = compute_class_weight(\n                               class_weight='balanced',\n                               classes=classes,\n                               y=y_train)\nclass_weights = dict(zip(classes, weights))\nclass_weights\n</pre> # \u0420\u0430\u0441\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u043c\u043e\u0436\u043d\u043e \u0443\u0447\u0435\u0441\u0442\u044c \u0441 compute_class_Weigh from sklearn.utils.class_weight import compute_class_weight  print(y_train.value_counts())  classes = np.unique(y_train) print(classes) weights = compute_class_weight(                                class_weight='balanced',                                classes=classes,                                y=y_train) class_weights = dict(zip(classes, weights)) class_weights <pre>1    1378\n0    1221\nName: label, dtype: int64\n[0 1]\n</pre> Out[14]: <pre>{0: 1.0642915642915642, 1: 0.943033381712627}</pre> In\u00a0[15]: Copied! <pre>from catboost import CatBoostClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\ncatparams = {\n            #  'l2_leaf_reg':1000,\n            #  'random_state':32,\n            #  'iterations':3000,\n            #  'learning_rate':0.1,\n            #  'class_weights':class_weights,\n            #  'bagging_temperature':0.001,\n             'silent':True}\n\n# fit model\nclf = CatBoostClassifier(**catparams).fit(x_train,y_train)\n# clf = LinearDiscriminantAnalysis()\nclf.fit(x_train,y_train)\n\n# \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b\nres = features_test.copy()\nres['pred'] = clf.predict(x_test)\nres['pred_proba'] = clf.predict_proba(x_test)[:, 1] ## \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043a\u043b\u0430\u0441\u0441\u0443 1\n</pre> from catboost import CatBoostClassifier from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  catparams = {             #  'l2_leaf_reg':1000,             #  'random_state':32,             #  'iterations':3000,             #  'learning_rate':0.1,             #  'class_weights':class_weights,             #  'bagging_temperature':0.001,              'silent':True}  # fit model clf = CatBoostClassifier(**catparams).fit(x_train,y_train) # clf = LinearDiscriminantAnalysis() clf.fit(x_train,y_train)  # \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b res = features_test.copy() res['pred'] = clf.predict(x_test) res['pred_proba'] = clf.predict_proba(x_test)[:, 1] ## \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043a\u043b\u0430\u0441\u0441\u0443 1 In\u00a0[16]: Copied! <pre>import seaborn as sns; sns.set(style='whitegrid')\n\n# \u0432\u044b\u0432\u043e\u0434\u0438\u043c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 (\u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0442\u043e\u0440\u0430)\n\nfi,ax = plt.subplots(1,2,figsize=(10,4))\nax[0].hist(res.pred_proba,bins=30)\nax[0].set_title('\u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439')\n\n# \u043e\u0448\u0438\u0431\u043a\u0438 \u043f\u0435\u0440\u0432\u043e\u0433\u043e/\u0432\u0442\u043e\u0440\u043e\u0433\u043e \u0440\u043e\u0434\u0430 \u0438 \u043f\u043b\u043e\u0449\u0430\u0442\u044c \u043f\u043e\u0434 ROC-\u043a\u0440\u0438\u0432\u043e\u0439\nFPR, TPR, thresholds = roc_curve(res.label, res.pred_proba)\nroc_auc = auc(FPR, TPR)\n\n# \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u043f\u043e\u0440\u043e\u0433 \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438\noptimal_idx = np.argmax(TPR - FPR)\noptimal_threshold = thresholds[optimal_idx]\nprint(\"Threshold value is:\", optimal_threshold)\n\n# \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c\nacc = len(res[res.pred == res.label]) / len(res)\nprint(f\"\\nAUC = {roc_auc:.3f}\\tAccuracy = {acc:.3f}\\n\")\n\n# ROC-\u043a\u0440\u0438\u0432\u0430\u044f\nax[1].set_title('Receiver Operating Characteristic')\nax[1].plot(FPR, TPR, 'b', label=f'AUC = {roc_auc:.2f}')\nax[1].legend(loc='lower right')\nax[1].plot([0, 1], [0, 1], 'r--')\nax[1].set_xlim([0, 1])\nax[1].set_ylim([0, 1])\nax[1].set_ylabel('True Positive Rate')\nax[1].set_xlabel('False Positive Rate')\nplt.tight_layout()\nplt.show()\n</pre> import seaborn as sns; sns.set(style='whitegrid')  # \u0432\u044b\u0432\u043e\u0434\u0438\u043c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 (\u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0442\u043e\u0440\u0430)  fi,ax = plt.subplots(1,2,figsize=(10,4)) ax[0].hist(res.pred_proba,bins=30) ax[0].set_title('\u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439')  # \u043e\u0448\u0438\u0431\u043a\u0438 \u043f\u0435\u0440\u0432\u043e\u0433\u043e/\u0432\u0442\u043e\u0440\u043e\u0433\u043e \u0440\u043e\u0434\u0430 \u0438 \u043f\u043b\u043e\u0449\u0430\u0442\u044c \u043f\u043e\u0434 ROC-\u043a\u0440\u0438\u0432\u043e\u0439 FPR, TPR, thresholds = roc_curve(res.label, res.pred_proba) roc_auc = auc(FPR, TPR)  # \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u043f\u043e\u0440\u043e\u0433 \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 optimal_idx = np.argmax(TPR - FPR) optimal_threshold = thresholds[optimal_idx] print(\"Threshold value is:\", optimal_threshold)  # \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c acc = len(res[res.pred == res.label]) / len(res) print(f\"\\nAUC = {roc_auc:.3f}\\tAccuracy = {acc:.3f}\\n\")  # ROC-\u043a\u0440\u0438\u0432\u0430\u044f ax[1].set_title('Receiver Operating Characteristic') ax[1].plot(FPR, TPR, 'b', label=f'AUC = {roc_auc:.2f}') ax[1].legend(loc='lower right') ax[1].plot([0, 1], [0, 1], 'r--') ax[1].set_xlim([0, 1]) ax[1].set_ylim([0, 1]) ax[1].set_ylabel('True Positive Rate') ax[1].set_xlabel('False Positive Rate') plt.tight_layout() plt.show() <pre>Threshold value is: 0.5106559932577258\n\nAUC = 0.711\tAccuracy = 0.630\n\n</pre> In\u00a0[18]: Copied! <pre># \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043e\u0448\u0438\u0431\u043e\u043a\nCM = confusion_matrix(res.label, res.pred)\nCM_DF = pd.DataFrame(data=CM, columns = ['Pos', 'Neg'])\nprint('\\n\\nConfusion matrix:')\nCM_DF\n</pre> # \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043e\u0448\u0438\u0431\u043e\u043a CM = confusion_matrix(res.label, res.pred) CM_DF = pd.DataFrame(data=CM, columns = ['Pos', 'Neg']) print('\\n\\nConfusion matrix:') CM_DF <pre>\n\nConfusion matrix:\n</pre> Out[18]: Pos Neg 0 10 5 1 5 7 In\u00a0[20]: Copied! <pre>## \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0444\u0438\u0447 (feature importance)\nftmprt = pd.DataFrame()\nftmprt['features'] = featuresList\nftmprt['importances'] = clf.feature_importances_\nftmprt = ftmprt.sort_values('importances', ascending=False).reset_index(drop=True)\nprint(ftmprt.head(10))\n</pre> ## \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0444\u0438\u0447 (feature importance) ftmprt = pd.DataFrame() ftmprt['features'] = featuresList ftmprt['importances'] = clf.feature_importances_ ftmprt = ftmprt.sort_values('importances', ascending=False).reset_index(drop=True) print(ftmprt.head(10)) <pre>  features  importances\n0      BOP    11.475951\n1    vol_2    10.940661\n2    vol_1    10.215244\n3    willr     9.672942\n4    vol_0     9.661190\n5     RSI1     7.749184\n6      LMA     7.626725\n7      mdm     6.833816\n8    arosc     6.684506\n9    MOM10     6.545823\n</pre> In\u00a0[23]: Copied! <pre>features_train.head(1).columns\n</pre> features_train.head(1).columns Out[23]: <pre>Index(['vol_0', 'vol_1', 'vol_2', 'LMA', 'RSI1', 'willr', 'BOP', 'MOM',\n       'MOM10', 'mdm', 'pdm', 'arosc', 'ts', 'deltaPrice', 'label'],\n      dtype='object')</pre> In\u00a0[56]: Copied! <pre>from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# sns.set({\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\nsns.set_style(\"whitegrid\", {\"grid.color\": \".5\", \"grid.linestyle\": \":\"})\n\n## \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u0431\u0443\u0434\u0435\u043c \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0446\u0435\u043d\u044b\ny_train_values = features_train.deltaPrice.values\ny_test_values  = features_test.deltaPrice.values\n\n## \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0434\u043b\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u043e\u0440\u0430\nrfRegressorParams = {'n_estimators': 1000,\n                     'max_depth': 10,\n                     'min_samples_split': 5,\n                     'max_features' : 1.0}\n\n## \u043e\u0431\u0443\u0447\u0430\u0435\u043c\nrgs = RandomForestRegressor(**rfRegressorParams)\n# rgs = HistGradientBoostingRegressor()\nrgs.fit(x_train, y_train_values)\n\n## \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\ny_pred_values = rgs.predict(x_test)\n\nprint('RMSE: ',mean_squared_error(y_test_values,y_pred_values,squared=False))\nprint('y_test_values mean: ',np.mean(y_test_values))\n</pre> from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor from sklearn.metrics import mean_squared_error  # sns.set({\"grid.color\": \".6\", \"grid.linestyle\": \":\"}) sns.set_style(\"whitegrid\", {\"grid.color\": \".5\", \"grid.linestyle\": \":\"})  ## \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u0431\u0443\u0434\u0435\u043c \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0446\u0435\u043d\u044b y_train_values = features_train.deltaPrice.values y_test_values  = features_test.deltaPrice.values  ## \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0434\u043b\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u043e\u0440\u0430 rfRegressorParams = {'n_estimators': 1000,                      'max_depth': 10,                      'min_samples_split': 5,                      'max_features' : 1.0}  ## \u043e\u0431\u0443\u0447\u0430\u0435\u043c rgs = RandomForestRegressor(**rfRegressorParams) # rgs = HistGradientBoostingRegressor() rgs.fit(x_train, y_train_values)  ## \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 y_pred_values = rgs.predict(x_test)  print('RMSE: ',mean_squared_error(y_test_values,y_pred_values,squared=False)) print('y_test_values mean: ',np.mean(y_test_values)) <pre>RMSE:  5.273953349351167\ny_test_values mean:  1.0728398075810186\n</pre> In\u00a0[25]: Copied! <pre>y_pred_values\n</pre> y_pred_values Out[25]: <pre>array([-0.06634456,  0.33899738, -0.21315316, -0.00264486, -0.66301309,\n       -0.22613101, -0.07564346, -0.20487035, -0.42682511, -0.02224715,\n       -0.04698163,  0.28295432,  0.01322574, -0.21992817,  0.28357387,\n        0.14028292,  0.02675805, -0.09454659,  0.10296832,  0.06555934,\n        1.36162034, -0.37465602, -1.09982678, -0.76912443, -0.35951312,\n       -0.10130011,  0.1729671 ])</pre> In\u00a0[50]: Copied! <pre>def compare_test_plot(y,y_pred,index=False):\n\n    # \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\n    output = pd.DataFrame()\n    output1 = pd.DataFrame({'values':y_test_values})\n    output1['type'] = 'real'\n    output1.index = index\n    \n    output2 = pd.DataFrame({'values':y_pred_values})\n    output2['type'] = 'prediction'\n    output2.index = index\n    output = pd.concat([output1,output2])\n\n    fig = px.line(output,color='type',template='plotly_white',height=300)\n    fig.show('svg',dpi=300)\n    \ncompare_test_plot(y_test_values,y_pred_values,features_test.index)\n</pre> def compare_test_plot(y,y_pred,index=False):      # \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442     output = pd.DataFrame()     output1 = pd.DataFrame({'values':y_test_values})     output1['type'] = 'real'     output1.index = index          output2 = pd.DataFrame({'values':y_pred_values})     output2['type'] = 'prediction'     output2.index = index     output = pd.concat([output1,output2])      fig = px.line(output,color='type',template='plotly_white',height=300)     fig.show('svg',dpi=300)      compare_test_plot(y_test_values,y_pred_values,features_test.index)"},{"location":"portfolio/sfml/ml8_ts_specifics.html","title":"\u041e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043d\u0430 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0440\u044f\u0434\u0430\u0445\u00b6","text":""},{"location":"portfolio/sfml/ml8_ts_specifics.html#1-background","title":"1 \u276f BACKGROUND\u00b6","text":""},{"location":"portfolio/sfml/ml8_ts_specifics.html","title":"\u276f\u276f \u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0417\u0430\u0434\u0430\u0447\u0438\u00b6","text":"<p>\u0418\u043c\u0435\u044f \u0434\u0430\u043d\u043d\u044b\u0435 \u0431\u0438\u0440\u0436\u043e\u0432\u044b\u0445 \u0430\u043a\u0446\u0438\u0438, \u0434\u0430\u0432\u0430\u0439\u0442\u0435 \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u0443\u0434\u0443\u0442 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u0431\u0443\u0434\u0443\u0439\u0449\u0435\u0435</p> <p>\u0420\u0430\u0437\u0434\u0435\u043b\u0438\u043c \u0437\u0430\u0434\u0430\u0447\u0443 \u043d\u0430 \u0432\u0434\u0435 \u0447\u0430\u0441\u0442\u0438:</p> <ul> <li>\u0412 \u043f\u0435\u0440\u0432\u043e\u0439 \u0447\u0430\u0441\u0442\u0438 \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0431\u0438\u043d\u0430\u0440\u043d\u044b\u0445 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u043a\u043e\u0442\u043e\u0440\u044b \u0431\u0443\u0434\u0435\u0442 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u043f\u043e\u0432\u044b\u0441\u0438\u0442\u0441\u044f \u043b\u0438 \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u044c \u0447\u0435\u0440\u0435\u0437 X \u0438\u043b\u0438 \u043d\u0435\u0442</li> <li>\u0412 \u0432\u0442\u043e\u0440\u043e\u0439 \u0447\u0430\u0441\u0442\u0438 \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u043e\u0440, \u0438 \u0431\u0443\u0434\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u0434\u0438\u043d\u0430\u043c\u0438\u043a\u0443 \u0431\u0438\u0440\u0436\u043e\u0432\u043e\u0439 \u0446\u0435\u043d\u044b</li> </ul> <p>\u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043d\u0430 \u0431\u0430\u0437\u0435 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u043e\u0431\u0443\u0447\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438</p> <ul> <li>\u0412 \u043e\u0442\u043b\u0438\u0447\u0438\u0438 \u043e\u0442 \u0441\u0438\u0442\u0443\u0430\u0446\u0438\u0438 \u043a\u043e\u0433\u0434\u0430 \u043c\u044b \u0443\u0436\u0435 \u0438\u043c\u0435\u043b\u0438 \u0432\u0441\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0438 \u043f\u0440\u043e\u0441\u0442\u043e \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u0446\u0438\u0440\u0443\u0435\u043c</li> <li>\u0412 \u0434\u0430\u043d\u043d\u043e\u043c \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u043c\u044b \u0445\u043e\u0442\u0438\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u0431\u0443\u0434\u0443\u0439\u0448\u0435\u0435 (\u0440\u044f\u0434 \u0435\u0449\u0435 \u043d\u0435 \u0434\u043e\u0441\u0442\u0440\u043e\u0435\u043d)</li> </ul>"},{"location":"portfolio/sfml/ml8_ts_specifics.html#2","title":"2 \u276f \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0414\u0430\u043d\u043d\u044b\u0445\u00b6","text":"<ul> <li>\u0421\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0435 \u0431\u0438\u0440\u0436\u043e\u0432\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 <code>open</code>, <code>close</code>, <code>high</code>, <code>low</code> &amp; <code>vol</code></li> <li>\u041f\u0435\u0440\u0438\u0443\u0434 \u0434\u0430\u043d\u043d\u044b\u0445 [2012-05-07-2023-05-05] \u0441 \u0438\u043d\u0442\u0435\u0440\u0432\u0430\u043b\u043e\u043c 1 \u0434\u0435\u043d\u044c</li> </ul>"},{"location":"portfolio/sfml/ml8_ts_specifics.html","title":"\u276f\u276f\u276f \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0414\u0430\u043d\u043d\u044b\u0445\u00b6","text":""},{"location":"portfolio/sfml/ml8_ts_specifics.html#3","title":"3 \u276f \u041f\u043e\u043d\u0438\u0436\u0435\u043d\u0438\u0435 \u0420\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438\u00b6","text":"<ul> <li>\u0414\u0430\u043d\u043d\u044b\u0445 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043c\u043d\u043e\u0433\u043e, \u043f\u043e\u043d\u0438\u0437\u0438\u043c \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 sampling \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e resample</li> <li>\u0411\u0435\u0440\u0435\u043c \u0447\u0430\u0441\u043e\u0432\u044b\u0435 \u0438\u043d\u0442\u0435\u0440\u0432\u0430\u043b\u044b, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0431\u0435\u0440\u0435\u043c:<ul> <li>\u043f\u0435\u0440\u0432\u044b\u0439 <code>open</code> \u0432 \u0438\u043d\u0442\u0435\u0440\u0432\u0430\u043b\u0435</li> <li>\u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0445 <code>high</code> \u0438 \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 <code>min</code></li> <li>\u0441\u0443\u043c\u043c\u0430 <code>vol</code> \u0438 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 <code>close</code></li> </ul> </li> </ul>"},{"location":"portfolio/sfml/ml8_ts_specifics.html#4-feature-engineering","title":"4 \u276f Feature Engineering\u00b6","text":""},{"location":"portfolio/sfml/ml8_ts_specifics.html#talib","title":"\u276f\u276f \u0422\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0438\u043d\u0434\u0438\u043a\u0430\u0442\u043e\u0440\u044b \u0441 talib\u00b6","text":"<ul> <li>\u0421 \u043f\u043e\u043c\u043e\u0448\u044c\u044e <code>talib</code> \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0434\u0435\u0433\u043a\u043e \u0441\u043e\u0437\u0434\u0430\u0432\u0430\u0442\u044c \u0442\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0438\u043d\u0434\u0438\u043a\u0430\u0442\u043e\u0440\u044b \u0434\u043b\u044f \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0440\u044f\u0434\u043e\u0432</li> <li>\u0414\u043b\u044f \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0438 \u0432\u043e\u0437\u043c\u0435\u043c \u0441\u0434\u0432\u0438\u0433 shift(-1) \u0447\u0442\u043e \u0441\u0434\u0432\u0438\u0433\u0430\u0435\u0442 series \u0432\u0432\u0435\u0440\u0445, \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0437\u0430\u0432\u0442\u0440\u0430\u0448\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 day + 1</li> </ul>"},{"location":"portfolio/sfml/ml8_ts_specifics.html","title":"\u276f\u276f \u041c\u0435\u0442\u043a\u0438 \u0434\u043b\u044f \u0411\u0438\u043d\u0430\u0440\u043d\u043e\u0439 \u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438\u00b6","text":"<p>\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043b\u0435\u0439\u0431\u043b \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438: \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0451\u0442 \u043b\u0438 \u0446\u0435\u043d\u0430 \u0447\u0435\u0440\u0435\u0437 X \u0447\u0430\u0441\u043e\u0432 \u0438\u043b\u0438 \u0443\u043f\u0430\u0434\u0451\u0442?</p> <ul> <li>1 = \u0446\u0435\u043d\u0430 \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0451\u0442, 0 = \u0446\u0435\u043d\u0430 \u0443\u043f\u0430\u0434\u0451\u0442</li> </ul>"},{"location":"portfolio/sfml/ml8_ts_specifics.html","title":"\u276f\u276f \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0422\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0438\u043d\u0434\u0438\u043a\u0430\u0442\u043e\u0440\u043e\u0432\u00b6","text":""},{"location":"portfolio/sfml/ml8_ts_specifics.html#5","title":"5 \u276f \u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0412\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0420\u044f\u0434\u044b\u00b6","text":""},{"location":"portfolio/sfml/ml8_ts_specifics.html#traintest-split","title":"\u276f\u276f TRAIN/TEST SPLIT\u00b6","text":"<p>\u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u0434\u0432\u0435 \u0440\u043e\u0434\u0433\u0440\u0443\u043f\u043f\u044b, \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0439 <code>tts</code></p>"},{"location":"portfolio/sfml/ml8_ts_specifics.html#6","title":"6 \u276f \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u00b6","text":""},{"location":"portfolio/sfml/ml8_ts_specifics.html","title":"\u276f\u276f \u041a\u043b\u0430\u0441\u0441\u043e\u0432\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0432\u0435\u0441\u043e\u0432\u00b6","text":"<p>\u0414\u043b\u044f <code>catboost</code> \u043c\u043e\u0436\u0435\u043c \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435 \u043f\u043e\u043c\u0435\u043d\u044f\u0442\u044c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432\u044b\u0445 \u0432\u0435\u0441\u043e\u0432, \u044d\u0442\u043e \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e \u0430\u043a\u0442\u0443\u0430\u043b\u044c\u043d\u043e \u0434\u043b\u044f \u0434\u0438\u0441\u0431\u0430\u043b\u0430\u043d\u0441\u043e\u0432 \u043a\u043b\u0430\u0441\u0441\u043e\u0432</p>"},{"location":"portfolio/sfml/ml8_ts_specifics.html","title":"\u276f\u276f \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430\u00b6","text":"<p>\u041e\u0431\u0443\u0447\u0438\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433 <code>CatBoostClassifier</code> \u0441 \u0431\u0430\u0437\u043e\u0432\u044b\u043c\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c\u0438</p>"},{"location":"portfolio/sfml/ml8_ts_specifics.html","title":"\u276f\u276f \u041e\u0446\u0435\u043d\u043a\u0430 \u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430\u00b6","text":"<p>\u041e\u0446\u0435\u043d\u0432\u0438\u0430\u0435\u043c \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u043f\u043e \u0434\u0432\u0443\u043c \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u044f\u043c <code>accuracy</code> \u0438 <code>ROC</code></p> <ul> <li>\u041e\u0446\u0435\u043d\u043a\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 accuracy \u043d\u0430\u043c \u0434\u0430\u0435\u0442 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0434\u043b\u044f threshold 0.5</li> <li><code>ROC</code> \u0434\u0430\u0435\u0442 \u043e\u0446\u0435\u043d\u043a\u0443 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u043b\u044f \u0440\u0430\u0437\u043d\u044b\u0445 \u043f\u043e\u0440\u043e\u0433\u043e\u0432\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0438 threshold</li> <li>\u041e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043f\u043e\u0440\u043e\u0433\u043e\u0432\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0436\u043d\u043e \u043d\u0430\u0439\u0442\u0438 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e max(TPR-FPR)<ul> <li>\u041c\u0430\u043a\u0441\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u0435\u043c <code>TPR</code>, \u043c\u0438\u043d\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u0435\u043c <code>FPR</code></li> </ul> </li> </ul>"},{"location":"portfolio/sfml/ml8_ts_specifics.html","title":"\u276f\u276f \u0412\u0430\u0436\u043d\u044b\u0435 \u041f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\u00b6","text":"<p>\u0421\u0430\u043c\u044b\u0435 \u0432\u0430\u0436\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0438\u0437 \u043c\u043e\u0434\u0435\u043b\u0438</p>"},{"location":"portfolio/sfml/ml8_ts_specifics.html#7","title":"7 \u276f \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u0420\u0435\u0433\u0440\u0435\u0441\u0441\u043e\u0440\u00b6","text":"<p>\u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044f \u0442\u0435\u043c \u0436\u0435 \u043d\u0430\u0431\u043e\u0440\u043e\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043a\u0430\u043a \u0438 \u0432 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438</p>"},{"location":"portfolio/sfml/ml9_2_recsys_%D1%85%D0%B0%D1%80%D0%B0%D0%BA%D1%82%D0%B5%D1%80%D0%B8%D1%81%D1%82%D0%B8%D0%BA%D0%B8.html","title":"Ml9 2 recsys \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a\u0438","text":"<p>Support (\u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430)</p> <ul> <li>\u042d\u0442\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0430, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442, \u043a\u0430\u043a \u0447\u0430\u0441\u0442\u043e \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0451\u043d\u043d\u0430\u044f \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u044f \u0442\u043e\u0432\u0430\u0440\u043e\u0432 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u0442\u0441\u044f \u0432 \u043d\u0430\u0431\u043e\u0440\u0435 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0439.</li> <li>\u041e\u043d\u0430 \u0440\u0430\u0441\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u043a\u0430\u043a \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u0447\u0438\u0441\u043b\u0430 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0439, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0439 \u043d\u0430\u0431\u043e\u0440 \u0442\u043e\u0432\u0430\u0440\u043e\u0432, \u043a \u043e\u0431\u0449\u0435\u043c\u0443 \u0447\u0438\u0441\u043b\u0443 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0439</li> </ul> <p>\u0424\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e, \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u0434\u043b\u044f \u043d\u0430\u0431\u043e\u0440\u0430 \u0442\u043e\u0432\u0430\u0440\u043e\u0432  X  \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442\u0441\u044f \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c:</p> <ul> <li><p>Support(X) = \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0439, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u0445 X / \u041e\u0431\u0449\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438</p> </li> <li><p>\u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0435\u0441\u043b\u0438 \u0443 \u0432\u0430\u0441 \u0435\u0441\u0442\u044c 1000 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0439, \u0438 100 \u0438\u0437 \u043d\u0438\u0445 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 \u0442\u043e\u0432\u0430\u0440\u044b A \u0438 B, \u0442\u043e \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u0434\u043b\u044f \u043d\u0430\u0431\u043e\u0440\u0430 \u0442\u043e\u0432\u0430\u0440\u043e\u0432 {A, B} \u0431\u0443\u0434\u0435\u0442:</p> </li> <li><p>Support(A, B) = 100 / 1000 = 0.1</p> </li> <li><p>\u042d\u0442\u043e \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442, \u0447\u0442\u043e 10% \u0432\u0441\u0435\u0445 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0439 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 \u0442\u043e\u0432\u0430\u0440\u044b A \u0438 B \u0432\u043c\u0435\u0441\u0442\u0435.</p> </li> </ul> <p>\u041f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u043f\u043e\u043c\u043e\u0433\u0430\u0435\u0442 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0447\u0430\u0441\u0442\u043e \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u0442\u0441\u044f \u0434\u0430\u043d\u043d\u044b\u0439 \u043d\u0430\u0431\u043e\u0440 \u0442\u043e\u0432\u0430\u0440\u043e\u0432 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0432\u0430\u0436\u043d\u044b\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u043c \u0434\u043b\u044f \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0438 \u043d\u0435\u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0430\u0432\u0438\u043b.</p> <p>\u0412 \u0430\u0441\u0441\u043e\u0446\u0438\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u043f\u0440\u0430\u0432\u0438\u043b\u0430\u0445 <code>confidence</code> (\u0434\u043e\u0432\u0435\u0440\u0438\u0435) \u2014 \u044d\u0442\u043e \u043c\u0435\u0440\u0430, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0447\u0430\u0441\u0442\u043e \u043f\u0440\u0430\u0432\u0438\u043b\u043e \u0432\u0435\u0440\u043d\u043e, \u043a\u043e\u0433\u0434\u0430 \u0443\u0441\u043b\u043e\u0432\u0438\u0435 \u0441\u043e\u0431\u043b\u044e\u0434\u0435\u043d\u043e. \u041e\u043d\u0430 \u0440\u0430\u0441\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u043a\u0430\u043a \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0439, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u0445 \u0438 \u0443\u0441\u043b\u043e\u0432\u0438\u0435, \u0438 \u0441\u043b\u0435\u0434\u0441\u0442\u0432\u0438\u0435, \u043a \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0443 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0439, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u0445 \u0442\u043e\u043b\u044c\u043a\u043e \u0443\u0441\u043b\u043e\u0432\u0438\u0435.</p> <p>\u0424\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e, \u0435\u0441\u043b\u0438 \u0443 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u043f\u0440\u0430\u0432\u0438\u043b\u043e \u0432\u0438\u0434\u0430  A \u2192 B , \u0442\u043e confidence \u0440\u0430\u0441\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u043f\u043e \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0439 \u0444\u043e\u0440\u043c\u0443\u043b\u0435:</p> <p>Confidence(A \u2192 B) = Support(A \u2229 B) / Suppor(A)}</p> <p>\u0433\u0434\u0435:</p> <ul> <li><p>Support(A \u2229 B)  \u2014 \u044d\u0442\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0439, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u0445 \u043a\u0430\u043a  A , \u0442\u0430\u043a \u0438  B .</p> </li> <li><p>Support(A)  \u2014 \u044d\u0442\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0439, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u0445 \u0442\u043e\u043b\u044c\u043a\u043e  A .</p> </li> </ul> <p>\u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, confidence \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u0442\u043e\u0433\u043e, \u0447\u0442\u043e  B  \u043f\u0440\u043e\u0438\u0437\u043e\u0439\u0434\u0435\u0442, \u0435\u0441\u043b\u0438  A  \u0443\u0436\u0435 \u043f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u043e. \u0417\u043d\u0430\u0447\u0435\u043d\u0438\u0435 confidence \u0432\u0430\u0440\u044c\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u043e\u0442 0 \u0434\u043e 1 (\u0438\u043b\u0438 \u043e\u0442 0% \u0434\u043e 100%), \u0433\u0434\u0435 \u0431\u043e\u043b\u0435\u0435 \u0432\u044b\u0441\u043e\u043a\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0442 \u043d\u0430 \u0431\u043e\u043b\u0435\u0435 \u0441\u0438\u043b\u044c\u043d\u0443\u044e \u0441\u0432\u044f\u0437\u044c \u043c\u0435\u0436\u0434\u0443  A  \u0438  B .</p> <p>\u0410\u043b\u0433\u043e\u0440\u0438\u0442\u043c <code>Apriori</code> \u2014 \u044d\u0442\u043e \u043e\u0434\u0438\u043d \u0438\u0437 \u0441\u0430\u043c\u044b\u0445 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u0434\u043b\u044f \u0432\u044b\u044f\u0432\u043b\u0435\u043d\u0438\u044f \u0430\u0441\u0441\u043e\u0446\u0438\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u043f\u0440\u0430\u0432\u0438\u043b \u0432 \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u043d\u0430\u0431\u043e\u0440\u0430\u0445 \u0434\u0430\u043d\u043d\u044b\u0445. \u041e\u043d \u0431\u044b\u043b \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d \u0432 1994 \u0433\u043e\u0434\u0443 \u0438 \u0448\u0438\u0440\u043e\u043a\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0432 \u0437\u0430\u0434\u0430\u0447\u0430\u0445 \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0434\u0430\u043d\u043d\u044b\u0445, \u0442\u0430\u043a\u0438\u0445 \u043a\u0430\u043a \u0430\u043d\u0430\u043b\u0438\u0437 \u043f\u043e\u043a\u0443\u043f\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u043e\u0433\u043e \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u044f, \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0438 \u0434\u0440\u0443\u0433\u0438\u0435.</p> <p>\u258e\u041e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u0438\u0434\u0435\u0438 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 Apriori:</p> <ol> <li><p>\u0410\u0441\u0441\u043e\u0446\u0438\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u043f\u0440\u0430\u0432\u0438\u043b\u0430: \u0410\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u043d\u0430\u0446\u0435\u043b\u0435\u043d \u043d\u0430 \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043f\u0440\u0430\u0432\u0438\u043b \u0432\u0438\u0434\u0430 \"\u0435\u0441\u043b\u0438 A, \u0442\u043e B\", \u0433\u0434\u0435 A \u0438 B \u2014 \u044d\u0442\u043e \u043d\u0430\u0431\u043e\u0440\u044b \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0442\u043e\u0432\u0430\u0440\u044b). \u041f\u0440\u0430\u0432\u0438\u043b\u0430 \u043f\u043e\u043c\u043e\u0433\u0430\u044e\u0442 \u043f\u043e\u043d\u044f\u0442\u044c, \u043a\u0430\u043a\u0438\u0435 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u044b \u0447\u0430\u0441\u0442\u043e \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0442\u0441\u044f \u0432\u043c\u0435\u0441\u0442\u0435.</p> </li> <li><p>\u041f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 (Support): \u042d\u0442\u043e \u043c\u0435\u0440\u0430, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442, \u043a\u0430\u043a \u0447\u0430\u0441\u0442\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442 \u0438\u043b\u0438 \u043d\u0430\u0431\u043e\u0440 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u0442\u0441\u044f \u0432 \u0434\u0430\u043d\u043d\u044b\u0445. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0435\u0441\u043b\u0438 \u0432 100 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u044f\u0445 \u0442\u043e\u0432\u0430\u0440 A \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u0442\u0441\u044f \u0432 20, \u0442\u043e \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 A \u0441\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 20%.</p> </li> <li><p>\u0414\u043e\u0441\u0442\u043e\u0432\u0435\u0440\u043d\u043e\u0441\u0442\u044c (Confidence): \u042d\u0442\u043e \u043c\u0435\u0440\u0430, \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0449\u0430\u044f, \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0447\u0430\u0441\u0442\u043e \u043f\u0440\u0430\u0432\u0438\u043b\u043e \u0432\u0435\u0440\u043d\u043e. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0435\u0441\u043b\u0438 \u043f\u0440\u0430\u0432\u0438\u043b\u043e \"\u0435\u0441\u043b\u0438 A, \u0442\u043e B\" \u0438\u043c\u0435\u0435\u0442 \u0434\u043e\u0441\u0442\u043e\u0432\u0435\u0440\u043d\u043e\u0441\u0442\u044c 80%, \u044d\u0442\u043e \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442, \u0447\u0442\u043e \u0432 80% \u0441\u043b\u0443\u0447\u0430\u0435\u0432, \u043a\u043e\u0433\u0434\u0430 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442 A, \u0442\u0430\u043a\u0436\u0435 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442 B.</p> </li> </ol> <p>\u258e\u041f\u0440\u0438\u043d\u0446\u0438\u043f \u0440\u0430\u0431\u043e\u0442\u044b \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430:</p> <ol> <li><p>\u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u0447\u0430\u0441\u0442\u044b\u0445 \u043d\u0430\u0431\u043e\u0440\u043e\u0432: \u0410\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u043d\u0430\u0447\u0438\u043d\u0430\u0435\u0442 \u0441 \u043f\u043e\u0438\u0441\u043a\u0430 \u0432\u0441\u0435\u0445 \u043e\u0434\u0438\u043d\u043e\u0447\u043d\u044b\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 (1-\u043c\u0435\u0440\u043d\u044b\u0445 \u043d\u0430\u0431\u043e\u0440\u043e\u0432) \u0438 \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u0438\u0445 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0443. \u0417\u0430\u0442\u0435\u043c \u043e\u043d \u043e\u0442\u0431\u0438\u0440\u0430\u0435\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u0437\u0430\u0434\u0430\u043d\u043d\u043e\u043c\u0443 \u043f\u043e\u0440\u043e\u0433\u0443 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0438 (\u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430).</p> </li> <li><p>\u0418\u0442\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u043e\u0435 \u0440\u0430\u0441\u0448\u0438\u0440\u0435\u043d\u0438\u0435: \u041f\u043e\u0441\u043b\u0435 \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u0432\u0441\u0435\u0445 \u0447\u0430\u0441\u0442\u044b\u0445 1-\u043c\u0435\u0440\u043d\u044b\u0445 \u043d\u0430\u0431\u043e\u0440\u043e\u0432 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u0438\u0445 \u0434\u043b\u044f \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432 2-\u043c\u0435\u0440\u043d\u044b\u0445 \u043d\u0430\u0431\u043e\u0440\u043e\u0432. \u041f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u044d\u0442\u0438\u0445 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432 \u0442\u0430\u043a\u0436\u0435 \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442\u0441\u044f, \u0438 \u043e\u0442\u0431\u0438\u0440\u0430\u044e\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0435.</p> </li> <li><p>\u041f\u043e\u0432\u0442\u043e\u0440\u0435\u043d\u0438\u0435: \u042d\u0442\u043e\u0442 \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u043f\u043e\u0432\u0442\u043e\u0440\u044f\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043d\u0430\u0431\u043e\u0440\u043e\u0432 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 3, 4 \u0438 \u0442\u0430\u043a \u0434\u0430\u043b\u0435\u0435 \u0434\u043e \u0442\u0435\u0445 \u043f\u043e\u0440, \u043f\u043e\u043a\u0430 \u043d\u0435 \u043e\u0441\u0442\u0430\u043d\u0435\u0442\u0441\u044f \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0435.</p> </li> <li><p>\u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u043f\u0440\u0430\u0432\u0438\u043b: \u041f\u043e\u0441\u043b\u0435 \u0442\u043e\u0433\u043e \u043a\u0430\u043a \u0432\u0441\u0435 \u0447\u0430\u0441\u0442\u044b\u0435 \u043d\u0430\u0431\u043e\u0440\u044b \u043d\u0430\u0439\u0434\u0435\u043d\u044b, \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u0442 \u0430\u0441\u0441\u043e\u0446\u0438\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u043f\u0440\u0430\u0432\u0438\u043b\u0430 \u0438\u0437 \u044d\u0442\u0438\u0445 \u043d\u0430\u0431\u043e\u0440\u043e\u0432 \u0438 \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u0438\u0445 \u0434\u043e\u0441\u0442\u043e\u0432\u0435\u0440\u043d\u043e\u0441\u0442\u044c. \u041f\u0440\u0430\u0432\u0438\u043b\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u0437\u0430\u0434\u0430\u043d\u043d\u043e\u043c\u0443 \u043f\u043e\u0440\u043e\u0433\u0443 \u0434\u043e\u0441\u0442\u043e\u0432\u0435\u0440\u043d\u043e\u0441\u0442\u0438, \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f.</p> </li> </ol> <p>\u258e\u041f\u0440\u0435\u0438\u043c\u0443\u0449\u0435\u0441\u0442\u0432\u0430 \u0438 \u043d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043a\u0438:</p> <p>\u041f\u0440\u0435\u0438\u043c\u0443\u0449\u0435\u0441\u0442\u0432\u0430:</p> <ul> <li>\u041f\u0440\u043e\u0441\u0442\u043e\u0442\u0430 \u0438 \u043f\u043e\u043d\u044f\u0442\u043d\u043e\u0441\u0442\u044c.</li> <li>\u042d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u0435\u043d \u0434\u043b\u044f \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u0438 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 \u043d\u0430\u0431\u043e\u0440\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445.</li> </ul> <p>\u041d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043a\u0438:</p> <ul> <li>\u0412\u044b\u0441\u043e\u043a\u0430\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438 \u0440\u0430\u0431\u043e\u0442\u0435 \u0441 \u0431\u043e\u043b\u044c\u0448\u0438\u043c\u0438 \u043d\u0430\u0431\u043e\u0440\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0437-\u0437\u0430 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u043c\u043d\u043e\u0433\u043e\u043a\u0440\u0430\u0442\u043d\u043e\u0433\u043e \u043f\u0440\u043e\u0445\u043e\u0434\u0430 \u043f\u043e \u0434\u0430\u043d\u043d\u044b\u043c.</li> <li>\u041d\u0435\u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u0435\u043d \u043f\u0440\u0438 \u043d\u0430\u043b\u0438\u0447\u0438\u0438 \u0431\u043e\u043b\u044c\u0448\u043e\u0433\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0440\u0435\u0434\u043a\u0438\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432.</li> <li>\u041c\u043e\u0436\u0435\u043c \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u0441\u0442\u0430\u0440\u044b\u0435 \u0442\u043e\u0432\u0430\u0440\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u043b\u0438\u0441\u0442\u044c \u0432 \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u043d\u044b\u0445 \u043f\u043e\u043a\u0443\u043f\u043a\u0430\u0445</li> </ul> <p>\u0410\u043b\u0433\u043e\u0440\u0438\u0442\u043c Apriori \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043e\u0441\u043d\u043e\u0432\u043e\u0439 \u0434\u043b\u044f \u0431\u043e\u043b\u0435\u0435 \u0441\u043b\u043e\u0436\u043d\u044b\u0445 \u0438 \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u044b\u0445 \u043c\u0435\u0442\u043e\u0434\u043e\u0432, \u0442\u0430\u043a\u0438\u0445 \u043a\u0430\u043a FP-Growth, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0440\u0435\u0448\u0430\u0435\u0442 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u0437 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0439 Apriori.</p>"},{"location":"portfolio/sfml/ml9_2_recsys_%D1%85%D0%B0%D1%80%D0%B0%D0%BA%D1%82%D0%B5%D1%80%D0%B8%D1%81%D1%82%D0%B8%D0%BA%D0%B8.html","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435\u00b6","text":""},{"location":"portfolio/sfml/ml9_2_recsys_%D1%85%D0%B0%D1%80%D0%B0%D0%BA%D1%82%D0%B5%D1%80%D0%B8%D1%81%D1%82%D0%B8%D0%BA%D0%B8.html","title":"\u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435\u00b6","text":"<p>\u0417\u0430\u0434\u0430\u0447\u0430 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0437\u0430\u043a\u043b\u044e\u0447\u0430\u0435\u0442\u0441\u044f \u0432 \u0442\u043e\u043c, \u0447\u0442\u043e\u0431\u044b \u0438\u043d\u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u043e \u0442\u043e\u0432\u0430\u0440\u0430\u0445, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u0435\u043c\u0443 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u044b \u0432 \u0442\u0435\u043a\u0443\u0449\u0438\u0439 \u043c\u043e\u043c\u0435\u043d\u0442.</p> <p>\u041a\u043e\u043c\u0443 \u043d\u0443\u0436\u043d\u044b \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0441\u0438\u0441\u0442\u0435\u043c\u044b?</p> <ul> <li><code>\u041a\u043b\u0438\u0435\u043d\u0442\u0443</code> \u2014 \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043e \u0442\u043e\u0432\u0430\u0440\u0430\u0445.</li> <li><code>\u0421\u0435\u0440\u0432\u0438\u0441\u0443</code> \u2014 \u043f\u043e\u043c\u043e\u0433\u0430\u0435\u0442 \u0437\u0430\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0442\u044c \u043d\u0430 \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0438 \u0443\u0441\u043b\u0443\u0433.</li> </ul> <p>\u041f\u0440\u0435\u0434\u043c\u0435\u0442 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u043c\u043e\u0436\u0435\u0442 \u0432\u0430\u0440\u044c\u0438\u0440\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0438 \u0432\u043a\u043b\u044e\u0447\u0430\u0435\u0442:</p> <ul> <li>\u0422\u043e\u0432\u0430\u0440\u044b (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, Amazon, Ozon)</li> <li>\u0421\u0442\u0430\u0442\u044c\u0438 (Arxiv.org)</li> <li>\u041d\u043e\u0432\u043e\u0441\u0442\u0438 (\u042f\u043d\u0434\u0435\u043a\u0441.\u0414\u0437\u0435\u043d)</li> <li>\u0418\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f (500px)</li> <li>\u0412\u0438\u0434\u0435\u043e (YouTube, Netflix)</li> <li>\u041b\u044e\u0434\u0435\u0439 (LinkedIn)</li> <li>\u041c\u0443\u0437\u044b\u043a\u0443 (Last.fm, Pandora) \u0438 \u043f\u043b\u0435\u0439\u043b\u0438\u0441\u0442\u044b.</li> </ul> <p>\u0426\u0435\u043b\u044c \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u043e\u0439, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440:</p> <ul> <li>\u041f\u043e\u043a\u0443\u043f\u043a\u0430</li> <li>\u0418\u043d\u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435</li> <li>\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435</li> <li>\u0417\u0430\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u043a\u043e\u043d\u0442\u0430\u043a\u0442\u043e\u0432.</li> </ul> <p>\u041a\u043e\u043d\u0442\u0435\u043a\u0441\u0442 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0439 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f \u0432 \u0434\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442, \u0442\u0430\u043a\u0438\u0445 \u043a\u0430\u043a \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440 \u0442\u043e\u0432\u0430\u0440\u043e\u0432, \u043f\u0440\u043e\u0441\u043b\u0443\u0448\u0438\u0432\u0430\u043d\u0438\u0435 \u043c\u0443\u0437\u044b\u043a\u0438 \u0438\u043b\u0438 \u043e\u0431\u0449\u0435\u043d\u0438\u0435.</p> <p>\u0418\u0441\u0442\u043e\u0447\u043d\u0438\u043a \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u043c\u043e\u0436\u0435\u0442 \u0432\u043a\u043b\u044e\u0447\u0430\u0442\u044c:</p> <ul> <li>\u0410\u0443\u0434\u0438\u0442\u043e\u0440\u0438\u044e (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0441\u0440\u0435\u0434\u043d\u0438\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u043d\u0430 TripAdvisor)</li> <li>\u0421\u0445\u043e\u0436\u0438\u0445 \u043f\u043e \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u0430\u043c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439</li> <li>\u042d\u043a\u0441\u043f\u0435\u0440\u0442\u043d\u043e\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u0441\u0442\u0432\u043e (\u0434\u043b\u044f \u0441\u043b\u043e\u0436\u043d\u044b\u0445 \u0442\u043e\u0432\u0430\u0440\u043e\u0432, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0432\u0438\u043d\u0430).</li> </ul> <p>\u0421\u0442\u0435\u043f\u0435\u043d\u044c \u043f\u0435\u0440\u0441\u043e\u043d\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0440\u0430\u0437\u043d\u043e\u0439:</p> <ul> <li>\u041d\u0435\u043f\u0435\u0440\u0441\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u2014 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b\u0435 \u0434\u043b\u044f \u0432\u0441\u0435\u0445.</li> <li>\u0411\u043e\u043b\u0435\u0435 \u043f\u0440\u043e\u0434\u0432\u0438\u043d\u0443\u0442\u044b\u0435 \u2014 \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u044b \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0442\u0435\u043a\u0443\u0449\u0435\u0439 \u0441\u0435\u0441\u0441\u0438\u0438.</li> <li>\u041f\u0435\u0440\u0441\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u2014 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442 \u0432\u0441\u044e \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u0443\u044e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043e \u043a\u043b\u0438\u0435\u043d\u0442\u0435.</li> </ul> <p>\u0424\u043e\u0440\u043c\u0430\u0442 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0440\u0430\u0437\u043d\u043e\u043e\u0431\u0440\u0430\u0437\u043d\u044b\u043c: \u0432\u0441\u043f\u043b\u044b\u0432\u0430\u044e\u0449\u0435\u0435 \u043e\u043a\u043d\u043e, \u043e\u0442\u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u0441\u043f\u0438\u0441\u043e\u043a \u0438\u043b\u0438 \u043b\u0435\u043d\u0442\u0430 \u0432\u043d\u0438\u0437\u0443 \u044d\u043a\u0440\u0430\u043d\u0430.</p> <p>\u0410\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u044b \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u0441\u0438\u0441\u0442\u0435\u043c \u0440\u0430\u0437\u043d\u043e\u043e\u0431\u0440\u0430\u0437\u043d\u044b, \u043d\u043e \u0432\u0441\u0435 \u043e\u043d\u0438 \u0441\u0432\u043e\u0434\u044f\u0442\u0441\u044f \u043a \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u043c \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u043c \u043f\u043e\u0434\u0445\u043e\u0434\u0430\u043c.</p>"},{"location":"portfolio/sfml/ml9_2_recsys_%D1%85%D0%B0%D1%80%D0%B0%D0%BA%D1%82%D0%B5%D1%80%D0%B8%D1%81%D1%82%D0%B8%D0%BA%D0%B8.html","title":"\u0410\u0441\u0441\u043e\u0446\u0438\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u043f\u0440\u0430\u0432\u0438\u043b\u00b6","text":"<p>\u0410\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0430\u0441\u0441\u043e\u0446\u0438\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u043f\u0440\u0430\u0432\u0438\u043b \u043e\u0441\u043d\u043e\u0432\u0430\u043d \u043d\u0430 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u043c \u0430\u043d\u0430\u043b\u0438\u0437\u0435 \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u043d\u043e\u0439 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u043c\u043e\u0441\u0442\u0438 \u0442\u043e\u0432\u0430\u0440\u043e\u0432 \u0432 \u043f\u043e\u043a\u0443\u043f\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0445 \u043a\u043e\u0440\u0437\u0438\u043d\u0430\u0445. \u0410\u0441\u0441\u043e\u0446\u0438\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u043f\u0440\u0430\u0432\u0438\u043b\u0430 (ARL) \u043c\u043e\u0436\u043d\u043e \u0444\u043e\u0440\u043c\u0443\u043b\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043a\u0430\u043a \u00ab\u0415\u0441\u043b\u0438 \u043a\u0443\u043f\u0438\u043b\u0438 x, \u0442\u043e \u0442\u0430\u043a\u0436\u0435 \u043a\u0443\u043f\u0438\u043b\u0438 y\u00bb.</p> <p>\u0410\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u043d\u0430\u0431\u043e\u0440 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0439, \u043a\u0430\u0436\u0434\u0430\u044f \u0438\u0437 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0439 \u043d\u0430\u0431\u043e\u0440 \u0442\u043e\u0432\u0430\u0440\u043e\u0432 (itemset). \u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 ARL \u0432\u044b\u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043f\u0440\u0430\u0432\u0438\u043b\u0430 \u0441\u043e\u0432\u043f\u0430\u0434\u0435\u043d\u0438\u044f \u0442\u043e\u0432\u0430\u0440\u043e\u0432 \u0432 \u043e\u0434\u043d\u043e\u0439 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0437\u0430\u0442\u0435\u043c \u0441\u043e\u0440\u0442\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u043f\u043e \u0438\u0445 \u0437\u043d\u0430\u0447\u0438\u043c\u043e\u0441\u0442\u0438.</p> <p>\u0414\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u0441\u0438\u043b\u044b \u0430\u0441\u0441\u043e\u0446\u0438\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u043f\u0440\u0430\u0432\u0438\u043b \u0432\u0432\u043e\u0434\u044f\u0442\u0441\u044f \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043c\u0435\u0442\u0440\u0438\u043a: Support, Confidence \u0438 Lift.</p>"},{"location":"blog/archive/2025/07.html","title":"July, 2025","text":""},{"location":"blog/archive/2025/02.html","title":"February, 2025","text":""},{"location":"blog/archive/2024/10.html","title":"October, 2024","text":""},{"location":"blog/archive/2024/04.html","title":"April, 2024","text":""},{"location":"blog/archive/2024/03.html","title":"March, 2024","text":""},{"location":"blog/archive/2023/12.html","title":"December, 2023","text":""},{"location":"blog/archive/2023/11.html","title":"November, 2023","text":""},{"location":"blog/archive/2023/10.html","title":"October, 2023","text":""},{"location":"blog/archive/2023/09.html","title":"September, 2023","text":""},{"location":"blog/archive/2023/08.html","title":"August, 2023","text":""},{"location":"blog/category/pyspark.html","title":"pyspark","text":""},{"location":"blog/category/sql.html","title":"sql","text":""},{"location":"blog/category/recsys.html","title":"Recsys","text":""},{"location":"blog/category/uplift-modeling.html","title":"uplift modeling","text":""},{"location":"blog/category/code-model.html","title":"code model","text":""},{"location":"blog/category/internship.html","title":"internship","text":""},{"location":"blog/category/science.html","title":"science","text":""},{"location":"blog/category/eda.html","title":"eda","text":""},{"location":"blog/category/nlp.html","title":"nlp","text":""},{"location":"blog/page/2/index.html","title":"Index","text":""},{"location":"blog/page/3/index.html","title":"Index","text":""},{"location":"blog/category/pyspark/page/2.html","title":"pyspark","text":""}]}